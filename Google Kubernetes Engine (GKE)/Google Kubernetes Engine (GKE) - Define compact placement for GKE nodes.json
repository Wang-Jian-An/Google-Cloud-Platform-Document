{"title": "Google Kubernetes Engine (GKE) - Define compact placement for GKE nodes", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/compact-placement", "abstract": "# Google Kubernetes Engine (GKE) - Define compact placement for GKE nodes\nYou can control whether your Google Kubernetes Engine (GKE) nodes are physically located relative to each other within a [zone](/compute/docs/regions-zones#zones_and_clusters) by using a [compact placement policy](/compute/docs/instances/placement-policies-overview) .\n", "content": "## Overview\nWhen you create node pools and workloads in a GKE cluster, you can define a , which specifies that these nodes or workloads should be placed in closer physical proximity to each other within a [zone](/compute/docs/regions-zones#zones_and_clusters) . Having nodes closer to each other can reduce network latency between nodes, which can be especially useful for tightly-coupled batch workloads.\n## Use compact placement with GKE Autopilot\n### Limitations\n- GKE provisions workloads within a compact placement in the same zone.\n- Compact placement is available on`Balanced`and A100 GPU. To learn more, see [machine types](/compute/docs/machine-types) .\n- Compact placement is available for Pods grouped on up to 150 nodes.\n- [Live migration](/compute/docs/instances/live-migration) for nodes is not supported.\n### Enable a compact placement policy\nTo enable compact placement for GKE Autopilot, add a `nodeSelector` to the Pod specification with the following keys:\n- `cloud.google.com/gke-placement-group` is the identifier you assign for the group of Pods that should run together, in the same compact placement group.\n- One of the following keys to define the type of resource:- `cloud.google.com/compute-class: \"Balanced\"`\n- `cloud.google.com/gke-accelerator: \"nvidia-tesla-a100\"`The following example is an excerpt of a Pod specification that enables compact placement. The placement group identifier is `placement-group-1` and the compute class is `Balanced` :\n```\n\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-placement-group: \"placement-group-1\"\u00a0 \u00a0 cloud.google.com/compute-class: \"Balanced\"\n```\nEach placement group is limited to 150 nodes. We recommend you limit a placement group only to the workloads that benefit from the grouping, and distribute your workloads into separate placement groups where possible.\n## Use compact placement with GKE Standard\n### Limitations\nCompact placement in GKE Standard node pools has the following limitations:\n- Supported only in new node pools. You cannot enable or disable compact placement on existing node pools.\n- Available only for [node pools operating in a single zone](/kubernetes-engine/docs/how-to/creating-a-regional-cluster#create-regional-single-zone-nodepool) .\n- Available only on [A2, C2, G2, C2D, C3D, N2, N2D, and C3 machine types](/compute/docs/machine-types) .\n- Supports up to 150 [Compute Engine VM instances](/compute/docs/instances) in each policy. Any node pool that exceeds this limit at any time will be rejected during creation.\n- [Live migration](/compute/docs/instances/live-migration) for nodes is not supported.\n- Supplying a custom resource policy using the`placement-policy`flag is not supported with blue-green upgrades.\n### Create a compact placement policy\nTo create compact placement policies, in the Google Cloud CLI, you specify the `placement-type=COMPACT` option during node pool or cluster creation. With this setting, GKE attempts to place nodes within a node pool in closer physical proximity to each other.\nTo use an existing [resource policy](/compute/docs/reference/rest/v1/resourcePolicies) in your cluster, specify the location of your custom policy for the `placement-policy` flag during node pool or cluster creation. This enables the flexibility of using reserved placements, multiple node pools with the same placement policy, and other advanced placement options. However, it also requires more manual operations than specifying the --placement-type=COMPACT flag. For example, you need to create, delete, and maintain your custom resource policies. Make sure that [the maximum number of VM instances](#limitations-standard) is respected across all node pools using the resource policy. If this limit is reached while some of your node pools haven't reach their maximum size, adding any more nodes will fail.\nIf you don't specify the `placement-type` and `placement-policy` flags, then by default there are no requirements on node placement.\n### Create a compact placement policy in a new cluster\nWhen you create a new cluster, you can specify a compact placement policy that will be applied to the default node pool. Any subsequent node pools that you create for the cluster, you will need to specify whether to apply compact placement.\nTo create a new cluster where the default node pool has a compact placement policy applied, use the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --placement-type COMPACT \\\u00a0 \u00a0 --max-surge-upgrade 0 \\\u00a0 \u00a0 --max-unavailable-upgrade MAX_UNAVAILABLE\n```\nReplace the following:\n- ``: The name of your new cluster.\n- ``: The type of machine to use for nodes, which must be a [C2 machine type](/compute/docs/compute-optimized-machines#c2_vms) (for example,`c2-standard-4`).\n- `--placement-type COMPACT`: Applies compact placement for the nodes in the default node pool.\n- `MAX_UNAVAILABLE`: Maximum number of nodes that can be unavailable at the same time during a node pool upgrade. For compact placement we recommend [fast no surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#optimizing-surge) to optimize the likelihood of finding colocated nodes during upgrades.\n### Create a compact placement policy on an existing cluster\nOn an existing cluster, you can create a node pool that has a compact placement policy applied.\nTo create a node pool that has a compact placement policy applied, use the following command:\n```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --placement-type COMPACT \\\u00a0 \u00a0 --max-surge-upgrade 0 \\\u00a0 \u00a0 --max-unavailable-upgrade MAX_UNAVAILABLE\n```\nReplace the following:\n- ``: The name of your new node pool.\n- ``: The type of machine to use for nodes, which must be a [C2 machine type](/compute/docs/compute-optimized-machines#c2_vms) (for example,`c2-standard-4`).\n- ``: The name of your existing cluster.\n- `--placement-type COMPACT`: Indicates to apply compact placement for the nodes in the new node pool.\n- `MAX_UNAVAILABLE`: Maximum number of nodes that can be unavailable at the same time during a node pool upgrade. For compact placement we recommend [fast no surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#optimizing-surge) to optimize the likelihood of finding colocated nodes during upgrades.\n### Create node pools using a shared custom placement policy\nYou can manually create a [resource policy](/sdk/gcloud/reference/compute/resource-policies/create/group-placement) and use it in multiple node pools.\n- Create the resource policy in the cluster Google Cloud region:```\ngcloud compute resource-policies create group-placement POLICY_NAME \\\u00a0 \u00a0 --region REGION \\\u00a0 \u00a0 --collocation collocated\n```Replace the following:- ``: The name of your resource policy.\n- ``: The region of your cluster.\n- Create a node pool using the custom resource policy:```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --placement-policy POLICY_NAME \\\u00a0 \u00a0 --max-surge-upgrade 0 \\\u00a0 \u00a0 --max-unavailable-upgrade MAX_UNAVAILABLE\n```Replace the following:- ``: The name of your new node pool.\n- ``: The type of machine to use for nodes, which must be a [C2 machine type](/compute/docs/compute-optimized-machines#c2_vms) (for example,`c2-standard-4`).\n- ``: The name of your existing cluster.\n- `MAX_UNAVAILABLE`: Maximum number of nodes that can be unavailable at the same time during a node pool upgrade. For compact placement we recommend [fast no surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#optimizing-surge) to optimize the likelihood of finding colocated nodes during upgrades.\n### Use a Compute Engine reservation with a compact placement policy\n[Reservations](/compute/docs/instances/reservations-overview) help you guarantee that hardware is available in a specified zone, reducing the risk of node pool creation failure caused by insufficient hardware.\n- Create a reservation that specifies a compact placement policy:```\ngcloud compute reservations create RESERVATION_NAME \\\u00a0 \u00a0 --vm-count MACHINE_COUNT \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --resource-policies policy=POLICY_NAME \\\u00a0 \u00a0 --zone ZONE \\\u00a0 \u00a0 --require-specific-reservation\n```Replace the following:- ``: The name of your reservation.\n- ``: The number of reserved nodes.\n- ``: The type of machine to use for nodes, which must be a [C2 machine type](/compute/docs/compute-optimized-machines#c2_vms) . For example, to use a predefined C2 machine type with 4\u00a0vCPUs, specify`c2-standard-4`.\n- ``: The name of your resource policy.\n- ``: The zone where to create your reservation.\n- Create a node pool by specifying both the compact placement policy and the reservation you created in the previous step:```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --placement-policy POLICY_NAME \\\u00a0 \u00a0 --reservation-affinity specific \\\u00a0 \u00a0 --reservation RESERVATION_NAME \\\u00a0 \u00a0 --max-surge-upgrade 0 \\\u00a0 \u00a0 --max-unavailable-upgrade MAX_UNAVAILABLE\n```\nReplace the following:\n- ``: The name of your new node pool.\n- ``: The type of machine to use for nodes, which must be a [C2 machine type](/compute/docs/compute-optimized-machines#c2_vms) (for example,`c2-standard-4`).\n- ``: The name of your existing cluster.\n### Create a workload on nodes that use compact placement\nTo run workloads on dedicated nodes that use compact placement, you can use several Kubernetes mechanisms, such as [assigning pods to nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/) and [preventing scheduling unwanted pods on a group of nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to achieve this.\nIn the following example, we add a [taint](/kubernetes-engine/docs/how-to/node-taints) to the dedicated nodes and add a corresponding toleration and affinity to the Pods.\n- Add a taint to nodes in the node pool that has a compact placement policy:```\nkubectl taint nodes -l cloud.google.com/gke-nodepool=NODEPOOL_NAME dedicated-pool=NODEPOOL_NAME:NoSchedule\n```\n- In the workload definition, specify the necessary toleration and a node affinity. Here's an example with a single Pod:```\napiVersion: v1kind: Podmetadata:\u00a0 ...spec:\u00a0 ...\u00a0 tolerations:\u00a0 - key: dedicated-pool\u00a0 \u00a0 operator: \"Equal\"\u00a0 \u00a0 value: \"NODEPOOL_NAME\"\u00a0 \u00a0 effect: \"NoSchedule\"\u00a0 affinity:\u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: dedicated-pool\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - NODEPOOL_NAME\n```\nIn some locations, it might not be possible to create a large node pool using a compact placement policy. To limit the size of such node pools to what's necessary, you should consider creating a node pool per workload requiring compact placement.\n### Use compact placement for node auto-provisioning\nStarting in GKE version 1.25, node auto-provisioning supports compact placement policy. With node auto-provisioning, GKE automatically provisions node pools based on cluster resource demand. For more information, see [Using node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\nTo enable compact placement for node auto-provisioning, add a `nodeSelector` to the Pod specification with the following keys:\n- `cloud.google.com/gke-placement-group` is the identifier you assign for the group of Pods that should run together, in the same compact placement group.\n- `cloud.google.com/machine-family` is the name of the machine family name. Use one of [the machine families that support compact placement](#limitations-standard) . We recommend you use C2 or C2D machine families for workloads with compute and networking performance requirements.\nThe following example is a Pod specification that enables compact placement:\n```\napiVersion: v1kind: Podmetadata:\u00a0 ...spec:\u00a0 ...\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-placement-group: PLACEMENT_GROUP_IDENTIFIER\u00a0 \u00a0 cloud.google.com/machine-family: MACHINE_FAMILY\n```\nYou can omit the `cloud.google.com/machine-family` key if the Pod configuration already defines a machine type supported with compact placement. For example, if the Pod specification includes `nvidia.com/gpu` and the cluster is configured to use A100 GPUs, you don't need to include the `cloud.google.com/machine-family` key.\nThe following example is a Pod specification that defines `nvidia.com/gpu` request and the cluster is configured to use A100 GPUs. This Pod `spec` doesn't include the `cloud.google.com/machine-family` key:\n```\n\u00a0 apiVersion: v1\u00a0 kind: Pod\u00a0 metadata:\u00a0 \u00a0 ...\u00a0 spec:\u00a0 \u00a0 ...\u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 cloud.google.com/gke-placement-group: PLACEMENT_GROUP_IDENTIFIER\u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: \"nvidia-tesla-a100\"\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\n```\nTo learn more, see [how to configure Pods to consume GPUs](/kubernetes-engine/docs/how-to/gpus#pods_gpus) .\nBecause GKE finds the best placement for smaller deployments, we recommend you instruct GKE to avoid running different type of Pods in the same placement group. Add a toleration key with the `cloud.google.com/gke-placement-group` key and the compact placement identifier you defined.\nThe following example is a Pod specification that defines a Pod toleration with compact placement:\n```\napiVersion: v1kind: Podmetadata:\u00a0 ...spec:\u00a0 ...\u00a0 tolerations:\u00a0 - key: cloud.google.com/gke-placement-group\u00a0 \u00a0 operator: \"Equal\"\u00a0 \u00a0 value: PLACEMENT_GROUP_IDENTIFIER\u00a0 \u00a0 effect: \"NoSchedule\"\n```\nFor more information about node auto-provisioning with Pod toleration, see [Workload separation](/kubernetes-engine/docs/how-to/workload-separation)\n## What's next\n- Learn about [Defining instance placement policies](/compute/docs/instances/define-instance-placement#support_and_restrictions) in Compute Engine.", "guide": "Google Kubernetes Engine (GKE)"}