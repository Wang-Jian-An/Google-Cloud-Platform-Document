{"title": "Google Kubernetes Engine (GKE) - Place GKE Pods in specific zones", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gke-zonal-topology", "abstract": "# Google Kubernetes Engine (GKE) - Place GKE Pods in specific zones\nThis page shows you how to tell Google Kubernetes Engine (GKE) to run your Pods on nodes in specific Google Cloud zones using . This type of placement is useful in situations such as the following:\n- Pods must access data that's stored in a zonal Compute Engine persistent disk.\n- Pods must run alongside other zonal resources such as Cloud SQL instances.\nYou can also use zonal placement with topology-aware traffic routing to reduce latency between clients and workloads. For details about topology-aware traffic routing, see [Topology aware routing](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/) .\nUsing zonal topology to control Pod placement is an advanced Kubernetes mechanism that you should only use if your situation requires that Pods run in specific zones. In most production environments, we recommend that you use regional resources, which is the GKE default, when possible.\n", "content": "## Zonal placement methods\nZonal topology is built into Kubernetes with the `topology.kubernetes.io/zone:` `` node label. To tell GKE to place a Pod in a specific zone, use one of the following methods:\n- [nodeAffinity](#nodeaffinity-placement) : Specify a nodeAffinity rule in your Pod specification for one or more Google Cloud zones. This method is more flexible than a nodeSelector because it lets you place Pods in multiple zones.\n- [nodeSelector](#nodeselector-placement) : Specify a nodeSelector in your Pod specification for a single Google Cloud zone.\n### Considerations\nZonal Pod placement using zonal topology has the following considerations:\n- The cluster must be in the same Google Cloud region as the requested zones.\n- In Standard clusters, you must use node auto-provisioning or create node pools with nodes in the requested zones. Autopilot clusters automatically manage this process for you.\n- Standard clusters must be regional clusters.## Pricing\nZonal topology is a Kubernetes scheduling capability and is offered at no extra cost in GKE.\nFor pricing details, see [GKE pricing](/kubernetes-engine/pricing) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have an existing GKE cluster in the same Google Cloud region as the zones in which you want to place your Pods. To create a new cluster, see [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) .## Place Pods in multiple zones using nodeAffinity\nKubernetes nodeAffinity provides a flexible scheduling control mechanism that supports multiple label selectors and logical operators. Use nodeAffinity if you want to let Pods run in one of a set of zones (for example, in either `us-central1-a` or `us-central1-f` ).\n- Save the following manifest as `multi-zone-affinity.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: nginx-deploymentspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx-multi-zone\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx-multi-zone\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx:latest\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: topology.kubernetes.io/zone\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - us-central1-a\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - us-central1-f\n```This manifest creates a Deployment with three replicas and places the Pods in `us-central1-a` or `us-central1-f` based on node availability.Ensure that your cluster is in the `us-central1` region. If your cluster is in a different region, change the zones in the values field of the manifest to valid zones in your cluster region.\n- Create the Deployment:```\nkubectl create -f multi-zone-affinity.yaml\n```GKE creates the Pods in nodes in one of the specified zones. Multiple Pods might run on the same node. You can optionally use [Pod anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) to tell GKE to place each Pod on a separate node.## Place Pods in a single zone using a nodeSelector\nTo place Pods in a single zone, use a nodeSelector in the Pod specification. A nodeSelector is equivalent to a `requiredDuringSchedulingIgnoredDuringExecution` nodeAffinity rule that has a single zone specified.\n**Caution:** Pods that you place in a single zone by using zonal topology might not be covered by the [Autopilot service level agreement (SLA)](/kubernetes-engine/sla) , which covers Autopilot Pods in . The [Compute Engine SLA](/compute/sla) continues to apply.\n- Save the following manifest as `single-zone-selector.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: nginx-singlezonespec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx-singlezone\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx-singlezone\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 topology.kubernetes.io/zone: \"us-central1-a\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx:latest\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\n```This manifest tells GKE to place all replicas in the Deployment in the `us-central1-a` zone.\n- Create the Deployment:```\nkubectl create -f single-zone-selector.yaml\n```## Verify Pod placement\nTo verify Pod placement, list the Pods and check the node labels. Multiple Pods might run in a single node, so you might not see Pods spread across multiple zones if you used nodeAffinity.\n- List your Pods:```\nkubectl get pods -o wide\n```The output is a list of running Pods and the corresponding GKE node.\n- Describe the nodes:```\nkubectl describe node NODE_NAME | grep \"topology.kubernetes.io/zone\"\n```Replace `` with the name of the node.The output is similar to the following:```\ntopology.kubernetes.io/zone: us-central1-a\n```\nIf you want GKE to spread your Pods evenly across multiple zones for improved failover across multiple failure domains, use [topologySpreadConstraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/) .\n## What's next\n- [Separate GKE workloads from each other](/kubernetes-engine/docs/how-to/workload-separation) \n- [Keep network traffic in the same topology as the node](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/) \n- [Spread Pods across multiple failure domains](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/)", "guide": "Google Kubernetes Engine (GKE)"}