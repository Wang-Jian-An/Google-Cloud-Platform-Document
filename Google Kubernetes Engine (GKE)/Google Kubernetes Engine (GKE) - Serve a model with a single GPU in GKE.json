{"title": "Google Kubernetes Engine (GKE) - Serve a model with a single GPU in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/online-ml-inference", "abstract": "# Google Kubernetes Engine (GKE) - Serve a model with a single GPU in GKE\nThis tutorial shows you how to serve a simple model with GPUs in Google Kubernetes Engine (GKE) mode. This tutorial creates a GKE cluster that use a single [L4 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/l4/) and prepares the GKE infrastructure to do online inference. This tutorial uses two of the most used frameworks for online serving:- [NVIDIA Triton Inference Server](https://developer.nvidia.com/triton-inference-server) \n- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) \n", "content": "## ObjectivesThis tutorial is intended for infrastructure engineers, MLOps engineers, DevOps engineers or cluster administrators that want to host a pre-trained machine learning (ML) model in a GKE cluster.\nThis tutorial covers the following steps:- Create a GKE Autopilot or Standard cluster.\n- Configure a Cloud Storage bucket, where the pre-trained model lives.\n- Deploy the online inference framework you select.\n- Make a test request to the deployed service.\n **Note:** This tutorial is a foundation for online serving. Additional elements like Load Balancers should be considered depending on the use case.## Costs\nThis tutorial uses the following billable components of Google Cloud:\n- [GKE](/kubernetes-engine/pricing) \n- Cloud Storage\n- L4 GPU accelerators\n- Egress traffic\nUse the [Pricing Calculator](/products/calculator) to generate a cost estimate based on your projected usage.\nWhen you finish this tutorial, you can avoid continued billing by deleting the resources you created. For more information, see [Clean up](#clean-up) .## Before you begin\n### Set up your project### Set defaults for the Google Cloud CLI\n- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Download the source code for this sample app:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samplescd kubernetes-engine-samples/ai-ml/gke-online-serving-single-gpu\n```\n- Set the default environment variables:```\ngcloud config set project PROJECT_IDgcloud config set compute/region COMPUTE_REGION\n```Replace the following values:- : your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : the [Compute Engine region](/compute/docs/regions-zones#available) that supports the accelerator type you want to use, for example,`us-central1`for L4 GPU.\n- In Cloud Shell, create the following environment variables:```\nexport PROJECT_ID=$(gcloud config get project)export REGION=$(gcloud config get compute/region)export K8S_SA_NAME=gpu-k8s-saexport GSBUCKET=$PROJECT_ID-gke-bucketexport MODEL_NAME=mnistexport CLUSTER_NAME=online-serving-cluster\n```\n## Create a GKE clusterYou can serve models on a single GPU in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience. With GKE Autopilot the resources scale automatically based on the model requests.\nTo choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\nRun the following command to create a GKE Autopilot cluster:\n```\n\u00a0 gcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 \u00a0 \u00a0 --region=${REGION} \\\u00a0 \u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 \u00a0 --release-channel=rapid\n```\n **Note:** This step can take up to five minutes to complete.\nGKE creates an Autopilot cluster with CPU and GPU nodes as requested by the deployed workloads.- Run the following command to create a GKE Standard cluster:```\n\u00a0 gcloud container clusters create ${CLUSTER_NAME} \\\u00a0 \u00a0 --project=${PROJECT_ID} \u00a0\\\u00a0 \u00a0 --region=${REGION} \u00a0\\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --addons GcsFuseCsiDriver \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --num-nodes=1\n```The cluster creation might take several minutes.\n- Run the following command to create the node pool:```\n\u00a0 gcloud container node-pools create gpupool \\\u00a0 \u00a0 --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 --location=${REGION} \\\u00a0 \u00a0 --node-locations=${REGION}-a \\\u00a0 \u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 \u00a0 --machine-type=g2-standard-8 \\\u00a0 \u00a0 --num-nodes=1\n```GKE creates a single node pool containing one L4 GPU for each node.\n## Create a Cloud Storage bucketCreate a Cloud Storage bucket to store the pre-trained model that will be served.\nIn Cloud Shell, run the following:\n```\ngcloud storage buckets create gs://$GSBUCKET\n```## Configure your cluster to access the bucket using workload identity federation for GKETo let your cluster access the Cloud Storage bucket, you do the following:- Create a Google Cloud service account.\n- Create a Kubernetes ServiceAccount in your cluster.\n- Bind the Kubernetes ServiceAccount to the Google Cloud service account.\n### Create a Google Cloud service account\n- In the Google Cloud console, go to the **Create service account** page: [Go to Create service account](https://console.cloud.google.com/iam-admin/serviceaccounts/create) \n- In the **Service account ID** field, enter `gke-ai-sa` .\n- Click **Create and continue** .\n- In the **Role** list, select the **Cloud Storage > Storage Insights Collector Service** role.\n- Click add **Add another role** .\n- In the **Select a role** list, select the **Cloud Storage > Storage Object Admin** role.\n- Click **Continue** , and then click **Done** .\n### Create a Kubernetes ServiceAccount in your clusterIn Cloud Shell, do the following:- Create a Kubernetes namespace:```\nkubectl create namespace gke-ai-namespace\n```\n- Create a Kubernetes ServiceAccount in the namespace:```\nkubectl create serviceaccount gpu-k8s-sa --namespace=gke-ai-namespace\n```\n### Bind the Kubernetes ServiceAccount to the Google Cloud service accountIn Cloud Shell, run the following commands:- Add an IAM binding to the Google Cloud service account:```\ngcloud iam service-accounts add-iam-policy-binding gke-ai-sa@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[gke-ai-namespace/gpu-k8s-sa]\"\n```The `--member` flag provides the full identity of the Kubernetes ServiceAccount in Google Cloud.\n- Annotate the Kubernetes ServiceAccount:```\nkubectl annotate serviceaccount gpu-k8s-sa \\\u00a0 \u00a0 --namespace gke-ai-namespace \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=gke-ai-sa@PROJECT_ID.iam.gserviceaccount.com\n```\n## Deploy the online inference serverEach online inference framework expects to find the pre-trained ML model in a specific format. The following section shows how to deploy the inference server depending on the framework you want to use:\n- In Cloud Shell, copy the pre-trained ML model into the Cloud Storage bucket:```\ngsutil cp -r src/triton-model-repository gs://$GSBUCKET\n```\n- Deploy the framework on Kubernetes:```\nenvsubst < src/gke-config/deployment-triton.yaml | kubectl --namespace=gke-ai-namespace apply -f ```\n- Validate that GKE deployed the framework:```\nkubectl get deployments --namespace=gke-ai-namespace\n```When the framework is ready, the output is similar to the following:```\nNAME     READY UP-TO-DATE AVAILABLE AGE\ntriton-deployment 1/1  1   1   5m29s\n```\n- Deploy the Services to access the deployment```\nkubectl apply --namespace=gke-ai-namespace -f src/gke-config/service-triton.yaml\n```\n- Check the external IP is assigned:```\nkubectl get services --namespace=gke-ai-namespace\n```The output is similar to the following:```\nNAME   TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)          AGE\nkubernetes  ClusterIP  34.118.224.1  <none>   443/TCP          60m\ntriton-server LoadBalancer 34.118.227.176 35.239.54.228 8000:30866/TCP,8001:31035/TCP,8002:30516/TCP 5m14s\n```Take note of the IP address for the `triton-server` in the **EXTERNAL-IP** column.\n- Check that the service and the deployment are working correctly:```\ncurl -v EXTERNAL_IP:8000/v2/health/ready\n```The output is similar to the following:```\n...\n< HTTP/1.1 200 OK\n< Content-Length: 0\n< Content-Type: text/plain\n...\n```\n- In Cloud Shell, copy the pre-trained ML model into the Cloud Storage bucket:```\ngsutil cp -r src/tfserve-model-repository gs://$GSBUCKET\n```\n- Deploy the framework on Kubernetes:```\nenvsubst < src/gke-config/deployment-tfserve.yaml | kubectl --namespace=gke-ai-namespace apply -f ```\n- Validate that GKE deployed the framework:```\nkubectl get deployments --namespace=gke-ai-namespace\n```When the framework is ready, the output is similar to the following:```\nNAME     READY UP-TO-DATE AVAILABLE AGE\ntfserve-deployment 1/1  1   1   5m29s\n```\n- Deploy the Services to access the deployment```\nkubectl apply --namespace=gke-ai-namespace -f src/gke-config/service-tfserve.yaml\n```\n- Check the external IP is assigned:```\nkubectl get services --namespace=gke-ai-namespace\n```The output is similar to the following:```\nNAME   TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)          AGE\nkubernetes  ClusterIP  34.118.224.1  <none>   443/TCP          60m\ntfserve-server LoadBalancer 34.118.227.176 35.239.54.228 8500:30003/TCP,8000:32194/TCP     5m14s\n```Take note of the IP address for the `tfserve-server` in the **EXTERNAL-IP** column.\n- Check that the service and the deployment are working correctly:```\ncurl -v EXTERNAL_IP:8000/v1/models/mnist\n```Replace the `` with your external IP address.The output is similar to the following:```\n...\n< HTTP/1.1 200 OK\n< Content-Type: application/json\n< Date: Thu, 12 Oct 2023 19:01:19 GMT\n< Content-Length: 154\n<\n{\n \"model_version_status\": [  {\n  \"version\": \"1\",\n  \"state\": \"AVAILABLE\",\n  \"status\": {\n   \"error_code\": \"OK\",\n   \"error_message\": \"\"\n  }\n  }\n ]\n}\n```\n## Serve the model\n- Create a Python virtual environment in Cloud Shell.```\npython -m venv ./mnist_clientsource ./mnist_client/bin/activate\n```\n- Install the required Python packages.```\npip install -r src/client/triton-requirements.txt\n```\n- Test Triton inference server by loading an image:```\ncd src/clientpython triton_mnist_client.py -i EXTERNAL_IP -m mnist -p ./images/TEST_IMAGE.png\n```Replace the following:- ``: Your external IP address.\n- ``: The name of the file that corresponds to the image you want to test. You can use the images stored in`src/client/images`.\nDepending on which image you use, the output is similar to the following:```\nCalling Triton HTTP Service  ->  Prediction result: 7\n```\n **Note:** You can write clients that run inference against the Triton Inference server. To learn more, see [NVIDIA examples and libraries](https://github.com/triton-inference-server/client) .- Create a Python virtual environment in Cloud Shell.```\npython -m venv ./mnist_clientsource ./mnist_client/bin/activate\n```\n- Install the required Python packages.```\npip install -r src/client/tfserve-requirements.txt\n```\n- Test TensorFlow Serving with a few images.```\ncd src/clientpython tfserve_mnist_client.py -i EXTERNAL_IP -m mnist -p ./images/TEST_IMAGE.png\n```\nReplace the following:- ``: Your external IP address.\n- ``: A value from`0`to`9`. You can use the images stored in`src/client/images`.\nDepending on which image you use, you will get an output similar to this:\n```\n Calling TensorFlow Serve HTTP Service ->  Prediction result: 5\n```\n **Note:** You can write clients that run inference against the TensorFlow server. To learn more, see [TensorFlow API](https://www.tensorflow.org/tfx/serving/api_rest) .\n **Success:** You've successfully deployed an online serving framework for a pre-trained ML model.## Clean upTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, do one of the following:- **Keep the GKE cluster:** Delete the Kubernetes resources in the cluster and the Google Cloud resources\n- **Keep the Google Cloud project:** Delete the GKE cluster and the Google Cloud resources\n- **Delete the project** \n### Delete the Kubernetes resources in the cluster and the Google Cloud resources\n- Delete the Kubernetes namespace and the workloads that you deployed:```\nkubectl -n gke-ai-namespace delete -f src/gke-config/service-triton.yamlkubectl -n gke-ai-namespace delete -f src/gke-config/deployment-triton.yamlkubectl delete namespace gke-ai-namespace\n```\n```\nkubectl -n gke-ai-namespace delete -f src/gke-config/service-tfserve.yamlkubectl -n gke-ai-namespace delete -f src/gke-config/deployment-tfserve.yamlkubectl delete namespace gke-ai-namespace\n```\n- Delete the Cloud Storage bucket:- Go to the **Buckets** page: [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Select the checkbox for `` `-gke-bucket` .\n- Click delete **Delete** .\n- To confirm deletion, type `DELETE` and click **Delete** .\n- Delete the Google Cloud service account:- Go to the **Service accounts** page: [Go to Service accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Select the checkbox for `gke-gpu-sa@` `` `.iam.gserviceaccount.com` .\n- Click delete **Delete** .\n- To confirm deletion, click **Delete** .\n### Delete the GKE cluster and the Google Cloud resources\n- Delete the GKE cluster:- Go to the **Clusters** page: [Go to Clusters](https://console.cloud.google.com/kubernetes/list/overview) \n- Select the checkbox for `online-serving-cluster` .\n- Click delete **Delete** .\n- To confirm deletion, type `online-serving-cluster` and click **Delete** .\n- Delete the Cloud Storage bucket:- Go to the **Buckets** page: [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Select the checkbox for `` `-gke-bucket` .\n- Click delete **Delete** .\n- To confirm deletion, type `DELETE` and click **Delete** .\n- Delete the Google Cloud service account:- Go to the **Service accounts** page: [Go to Service accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Select the checkbox for `gke-gpu-sa@` `` `.iam.gserviceaccount.com` .\n- Click delete **Delete** .\n- To confirm deletion, click **Delete** .\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- [Serve an LLM with multiple GPUs in GKE](/kubernetes-engine/docs/tutorials/serve-multiple-gpu) \n- [Serve an LLM using multi-host TPUs on GKE with Saxml](/kubernetes-engine/docs/tutorials/tpu-multihost-saxml)", "guide": "Google Kubernetes Engine (GKE)"}