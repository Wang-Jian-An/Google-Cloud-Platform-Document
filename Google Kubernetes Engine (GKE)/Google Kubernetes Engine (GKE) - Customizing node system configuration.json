{"title": "Google Kubernetes Engine (GKE) - Customizing node system configuration", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/node-system-config", "abstract": "# Google Kubernetes Engine (GKE) - Customizing node system configuration\nThis document shows you how to customize your Google Kubernetes Engine (GKE) node configuration using a configuration file called a .\n", "content": "## Overview\nYou can customize your node configuration by using various methods. For example, you can specify parameters such as the machine type and minimum CPU platform when you create a node pool.\nA is a configuration file that provides a way to adjust a limited set of system settings. You can use a node system configuration to specify custom settings for the Kubernetes node agent ( [kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) ) and low-level Linux kernel configurations ( [sysctl](https://en.wikipedia.org/wiki/Sysctl) ) in your node pools.\n**Note:** Node system configurations are not supported in Windows Server nodes.\n**Caution:** Changing the `kubelet` or `sysctl` settings can lead to unintended behavior that might negatively affect the health of your workloads and nodes. Ensure that you implement extensive testing of any `kubelet` or `sysctl` changes before deploying them on production workloads.\nYou can also make additional customizations using the following methods:\n- Replace system components, such as in [Customizing Cloud Logging logs for GKE with Fluentd](/architecture/customizing-stackdriver-logs-fluentd) \n- Use DaemonSets to customize nodes, such as in [Automatically bootstrapping GKE nodes with DaemonSets](/solutions/automatically-bootstrapping-gke-nodes-with-daemonsets) .## Using a node system configuration\nTo use a node system configuration:\n- [Create a configuration file](#create) . This file contains your`kubelet`and`sysctl`configurations.\n- [Add the configuration](#add) when you create a cluster, or when you create or update a node pool.\n### Creating a configuration file\nWrite your node system configuration file in YAML. The following example shows you how to add configurations for the `kubelet` and `sysctl` options:\n```\nkubeletConfig:\u00a0 cpuManagerPolicy: staticlinuxConfig:\u00a0sysctl:\u00a0 \u00a0net.core.somaxconn: '2048'\u00a0 \u00a0net.ipv4.tcp_rmem: '4096 87380 6291456'\n```\nIn this example:\n- `cpuManagerPolicy: static`configures the`kubelet`to use the [static CPU management policy](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy) .\n- `net.core.somaxconn: '2048'`limits the`socket listen()`backlog to 2,048 bytes.\n- `net.ipv4.tcp_rmem: '4096 87380 6291456'`sets the minimum, default, and maximum value of the TCP socket receive buffer to 4,096 bytes, 87,380 bytes, and 6,291,456 bytes respectively.\nIf you want to add configurations solely for the `kubelet` or `sysctl` , only include that section in your configuration file. For example, to add a `kubelet` configuration, create the following file:\n```\nkubeletConfig:\u00a0 cpuManagerPolicy: static\n```\nFor a complete list of the fields that you can add to your configuration file, see the [Kubelet configuration options](#kubelet-options) and [Sysctl configuration options](#sysctl-options) sections.\n### Adding the configuration to a node pool\nAfter you have created the configuration file, add the [--system-config-from-file](/sdk/gcloud/reference/alpha/container/node-pools/create#--system-config-from-file) flag by using the Google Cloud CLI. You can add this flag when you create a cluster, or when you create or update a node pool. You cannot add a node system configuration with the Google Cloud console.\nTo add a node system configuration, run the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --system-config-from-file=SYSTEM_CONFIG_PATH\n```\nReplace the following:- ``: the name for your cluster\n- ``: the path to the file that contains your`kubelet`and`sysctl`configurations\nAfter you have applied a node system configuration, the default node pool of the cluster uses the settings that you defined.\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 \u00a0--cluster CLUSTER_NAME \\\u00a0 \u00a0 \u00a0--system-config-from-file=SYSTEM_CONFIG_PATH\n```\nReplace the following:- ``: the name for your node pool\n- ``: the name of the cluster that you want to add a node pool to\n- ``: the path to the file that contains your`kubelet`and`sysctl`configurations\n **Note:** Updating the system configuration of an existing node pool requires the recreation of the nodes, which is a disruptive operation.\n```\ngcloud container node-pools update POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --system-config-from-file=SYSTEM_CONFIG_PATH\n```\nReplace the following:- ``: the name of the node pool that you want to update\n- ``: the name of the cluster that you want to update\n- ``: the path to the file that contains your`kubelet`and`sysctl`configurations## Editing a node system configuration\nTo edit a node system configuration, you can create a new node pool with the configuration that you want, or update the node system configuration of an existing node pool.\n### Editing by creating a node pool\nTo edit a node system configuration by creating a node pool:\n- [Create a configuration file](#create) with the configuration that you want.\n- [Add the configuration](#add) to a new node pool.\n- Migrate your workloads to the new node pool.\n- [Delete the old node pool](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool) .\n### Editing by updating an existing node pool\n**Note:** When you update an existing node pool, all the nodes in the node pool are recreated one by one, which is a disruptive operation.\nTo edit a node system configuration by updating an existing node pool, update the node system configuration with the values that you want. Updating a node system configuration overrides the node pool's system configuration with the new configuration. If you omit any parameters during an update, they are set to their respective defaults.\nIf you want to reset the node system configuration back to the defaults, update your configuration file with empty values for the `kubelet` and `sysctl` . For example:\n```\nkubeletConfig: {}linuxConfig:\u00a0 sysctl: {}\n```\n## Deleting a node system configuration\nTo remove a node system configuration:\n- [Create a node pool](/kubernetes-engine/docs/how-to/node-pools#add) .\n- Migrate your workloads to the new node pool.\n- [Delete the node pool](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool) that has the old node system configuration.## Kubelet configuration options\nThe following table shows you the `kubelet` options that you can modify.\n| Kubelet config settings | Restrictions         | Default setting | Description                                                                                 |\n|:--------------------------|:-----------------------------------------------|:------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| cpuManagerPolicy   | Value must be none or static     | none    | This setting controls the kubelet's CPU Manager Policy. The default value is none which is the default CPU affinity scheme, providing no affinity beyond what the OS scheduler does automatically. Setting this value to static allows Pods in the Guaranteed QoS class with integer CPU requests to be assigned exclusive use of CPUs. |\n| cpuCFSQuota    | Value must be true or false     | true    | This setting enforces the Pod's CPU limit. Setting this value to false means that the CPU limits for Pods are ignored. Ignoring CPU limits might be desirable in certain scenarios where Pods are sensitive to CPU limits. The risk of disabling cpuCFSQuota is that a rogue Pod can consume more CPU resources than intended.   |\n| cpuCFSQuotaPeriod   | Value must be a duration of time    | \"100ms\"   | This setting sets the CPU CFS quota period value, cpu.cfs_period_us, which specifies the period of how often a cgroup's access to CPU resources should be reallocated. This option lets you tune the CPU throttling behavior.                            |\n| podPidsLimit    | Value must be must be between 1024 and 4194304 | none    | This setting sets the maximum number of process IDs (PIDs) that each Pod can use.                                                                |\n## Sysctl configuration options\nTo tune the performance of your system, you can modify the following Kernel attributes:\n- [net.core.busy_poll](https://docs.kernel.org/admin-guide/sysctl/net.html#busy-poll) \n- [net.core.busy_read](https://docs.kernel.org/admin-guide/sysctl/net.html#busy-read) \n- [net.core.netdev_max_backlog](https://docs.kernel.org/admin-guide/sysctl/net.html#netdev-max-backlog) \n- [net.core.rmem_max](https://docs.kernel.org/admin-guide/sysctl/net.html#rmem-max) \n- [net.core.wmem_default](https://docs.kernel.org/admin-guide/sysctl/net.html#wmem-default) \n- [net.core.wmem_max](https://docs.kernel.org/admin-guide/sysctl/net.html#wmem-max) \n- [net.core.optmem_max](https://docs.kernel.org/admin-guide/sysctl/net.html#optmem-max) \n- [net.core.somaxconn](https://docs.kernel.org/networking/ip-sysctl.html#tcp-variables) \n- [net.ipv4.tcp_rmem](https://docs.kernel.org/networking/ip-sysctl.html#tcp-variables) \n- [net.ipv4.tcp_wmem](https://docs.kernel.org/networking/ip-sysctl.html#tcp-variables) \n- [net.ipv4.tcp_tw_reuse](https://docs.kernel.org/networking/ip-sysctl.html#tcp-variables) \n- [net.ipv6.conf.all.disable_ipv6](https://docs.kernel.org/networking/ip-sysctl.html#proc-sys-net-ipv6-variables) \n- [net.ipv6.conf.default.disable_ipv6](https://docs.kernel.org/networking/ip-sysctl.html#proc-sys-net-ipv6-variables) \n- [vm.max_map_count](https://docs.kernel.org/admin-guide/sysctl/vm.html#max-map-count) \nDifferent [Linux namespaces](https://en.wikipedia.org/wiki/Linux_namespaces) might have unique values for a given `sysctl` , while others are global for the entire node. Updating `sysctl` options by using a node system configuration ensures that the `sysctl` is applied globally on the node and in each namespace, resulting in each Pod having identical `sysctl` values in each Linux namespace.\n## Linux cgroup mode configuration options\nThe kubelet and the container runtime use Linux kernel [cgroups](https://www.wikipedia.org/wiki/Cgroups) for resource management, such as limiting how much CPU or memory each container in a Pod can access. There are two versions of the cgroup subsystem in the kernel: `cgroupv1` and `cgroupv2` . Kubernetes support for `cgroupv2` was introduced as alpha in Kubernetes version 1.18, beta in 1.22, and GA in 1.25. For more details, refer to the [Kubernetescgroups v2documentation](https://kubernetes.io/docs/concepts/architecture/cgroups/) .\nNode system configuration lets you customize the cgroup configuration of your node pools. You can use `cgroupv1` or `cgroupv2` . GKE uses `cgroupv2` for new node pools running version 1.26 and later, and `cgroupv1` for versions earlier than 1.26.\nYou can use node system configuration to change the setting for a node pool to use `cgroupv1` or `cgroupv2` explicitly. Just upgrading an existing node pool to 1.26 doesn't change the setting to `cgroupv2` , as existing node pools created running a version earlier than 1.26\u2014without a customized cgroup configuration\u2014continue to use `cgroupv1` unless you explicitly specify otherwise.\nFor example, to configure your node pool to use `cgroupv2` , use a node system configuration file such as:\n```\nlinuxConfig:\u00a0 cgroupMode: 'CGROUP_MODE_V2'\n```\nThe supported `cgroupMode` options are:\n- `CGROUP_MODE_V1`: Use`cgroupv1`on the node pool.\n- `CGROUP_MODE_V2`: Use`cgroupv2`on the node pool.\n- `CGROUP_MODE_UNSPECIFIED`: Use the default GKE cgroup configuration.\nTo use `cgroupv2` , the following requirements and limitations apply:\n- For a node pool running a version earlier than 1.26, you must use gcloud CLI version [408.0.0](/sdk/docs/release-notes#40800_2022-11-01) or newer. Alternatively, use [gcloud beta](/sdk/gcloud/reference/beta) with version [395.0.0](/sdk/docs/release-notes#39500_2022-07-26) or newer.\n- Your cluster and node pools must run GKE version 1.24.2-gke.300 or later.\n- You must use the Container-Optimized OS with containerd [node image](/kubernetes-engine/docs/concepts/node-images#cos) .\n- If any of your workloads depend on reading the cgroup filesystem (`/sys/fs/cgroup/...`), ensure they are compatible with the`cgroupv2`API.- Ensure any monitoring or third-party tools are compatible with`cgroupv2`.\n- If you use JDK (Java workload), we recommend that you use versions which [fully support cgroupv2](https://bugs.openjdk.org/browse/JDK-8230305) , including JDK`8u372`, JDK 11.0.16 or later, or JDK 15 or later.\n### Verify cgroup configuration\nWhen you add a node system configuration, GKE must recreate the nodes to implement the changes. After you've [added the configuration to a nodepool](#add) and the nodes have been recreated, you can verify the new configuration.\nTo verify the cgroup configuration for nodes in this node pool, pick a node and connect to it using the following instructions:\n- [Create an interactiveshell](https://kubernetes.io/docs/tasks/debug/debug-cluster/kubectl-node-debug/#debugging-a-node-using-kubectl-debug-node) with any node in the node pool. Replace`mynode`in the command with the name of any node in the node pool.\n- [Identify the cgroup version on Linuxnodes](https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version) .## What's next\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn how to create node pools](/kubernetes-engine/docs/how-to/node-pools) .", "guide": "Google Kubernetes Engine (GKE)"}