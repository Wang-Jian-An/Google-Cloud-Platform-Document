{"title": "Google Kubernetes Engine (GKE) - Use preemptible VMs to run fault-tolerant workloads", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms", "abstract": "# Google Kubernetes Engine (GKE) - Use preemptible VMs to run fault-tolerant workloads\nThis page shows you how to use preemptible VMs in Google Kubernetes Engine (GKE).\n**Note:** Spot VMs are the latest version of preemptible VMs. We recommend that you use either [Spot Pods in Autopilot clusters](/kubernetes-engine/docs/how-to/autopilot-spot-pods) or [Spot VMs in Standard clusters](/kubernetes-engine/docs/how-to/spot-vms) instead of preemptible VMs. GKE continues to support preemptible VMs, which now use the same pricing model as Spot VMs.\n", "content": "## Overview\nPreemptible VMs are Compute Engine [VM instances](/compute/docs/instances/preemptible) that are priced lower than standard VMs and provide no guarantee of availability. Preemptible VMs offer similar functionality to [Spot VMs](/compute/docs/instances/spot) , but only last up to 24 hours after creation.\nIn some cases, a preemptible VM might last longer than 24 hours. This can occur when the new Compute Engine instance comes up too fast and Kubernetes doesn't recognize that a different Compute Engine VM was created. The underlying Compute Engine instance will have a maximum duration of 24 hours and follow the [expected preemptible VM behavior](/compute/docs/instances/preemptible) .\n**Note:** Preemptible VMs share the same pricing model as Spot VMs. For more information, see [Compute Engine Pricing](/compute/all-pricing#compute-engine-pricing) .\n### Comparison to Spot VMs\nPreemptible VMs share many similarities with Spot VMs, including the following:\n- Terminated when Compute Engine requires the resources to run standard VMs.\n- Useful for running stateless, batch, or fault-tolerant workloads.\n- Lower pricing than standard VMs.\n- On clusters running GKE version 1.20 and later, [graceful node shutdown](#graceful-shutdown) is enabled by default.\n- Works with the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) and [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\nIn contrast to Spot VMs, which have no maximum expiration time, preemptible VMs only last for up to 24 hours after creation.\nYou can enable preemptible VMs on new clusters and node pools, use `nodeSelector` or node affinity to control scheduling, and use taints and tolerations to avoid issues with system workloads when nodes are preempted.\n### Termination and graceful shutdown of preemptible VMs\nWhen Compute Engine needs to reclaim the resources used by preemptible VMs, a [preemption notice](/compute/docs/instances/preemptible#preemption) is sent to GKE. Preemptible VMs terminate 30 seconds after receiving a termination notice.\nOn clusters running GKE version 1.20 and later, the [kubelet graceful node shutdown feature](https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown) is enabled by default. The kubelet notices the termination notice and gracefully terminates Pods that are running on the node. If the Pods are part of a Deployment, the controller creates and schedules new Pods to replace the terminated Pods.\nOn a best-effort basis, the kubelet grants the following graceful termination period, based on the GKE version of the node pool:\n- **Later than 1.22.8-gke.200** : 15 seconds for non-system Pods, after which system Pods (with the`system-cluster-critical`or`system-node-critical`priority classes) have 15 seconds to gracefully terminate.\n- **1.22.8-gke.200 and earlier** : 25 seconds for non-system Pods, after which system Pods (with the`system-cluster-critical`or`system-node-critical`priority classes) have 5 seconds to gracefully terminate.\n**Note:** Adjusting the values of `terminationGracePeriodSeconds` , `shutdownGracePeriodCriticalPods` , or `shutdownGracePeriod` in your Pod spec to more than the granted time (15 or 25 seconds) has no effect on the graceful termination of nodes that use preemptible VMs or Spot VMs.\nDuring graceful node termination, the kubelet updates the status of the Pods, assigning a `Failed` phase and a `Terminated` reason to the terminated Pods.\n**Note:** For clusters running version 1.27.2-gke.1800 or later, GKE automatically deletes Pods that were evicted during node termination. Use the following instructions to manually delete terminated Pods for earlier GKE versions.\nWhen the number of terminated Pods reaches a threshold of 1000 for clusters with fewer than 100 nodes or 5000 for clusters with 100 nodes or more, [garbage collection](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection) cleans up the Pods.\nYou can also delete terminated Pods manually using the following commands:\n```\n\u00a0 kubectl get pods --all-namespaces | grep -i NodeShutdown | awk '{print $1, $2}' | xargs -n2 kubectl delete pod -n\u00a0 kubectl get pods --all-namespaces | grep -i Terminated | awk '{print $1, $2}' | xargs -n2 kubectl delete pod -n\n```\n**Warning:** In Kubernetes versions 1.22 and earlier, manually deleting shutdown Pods resets the counter that Jobs might use to determine the [backoffLimit](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers) .\n### Modifications to Kubernetes behavior\nUsing preemptible VMs on GKE modifies some guarantees and constraints that Kubernetes provides, such as the following:\n- GKE shuts down preemptible VMs without a grace period for Pods, 30 seconds after receiving a preemption notice from Compute Engine.\n- Reclamation of preemptible VMs is involuntary and is not covered by the guarantees of [PodDisruptionBudgets](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) . You might experience greater unavailability than your configured `PodDisruptionBudget` .\n### Limitations\n- The [kubelet graceful node shutdown feature](https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown) is only enabled on clusters running GKE version 1.20 and later. For GKE versions prior to 1.20, you can use the [Kubernetes on GCP Node Termination Event Handler](https://github.com/GoogleCloudPlatform/k8s-node-termination-handler) to gracefully terminate your Pods when preemptible VMs are terminated.\n- Preemptible VMs do not support Windows Server node pools.## Create a cluster or node pool with preemptible VMs\nYou can use the Google Cloud CLI to create a cluster or node pool with preemptible VMs.\nTo create a cluster with preemptible VMs, run the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --preemptible\n```\nReplace `` with the name of your new cluster.\nTo create a node pool with preemptible VMs, run the following command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --preemptible\n```\nReplace `` with the name of your new node pool.\n**Note:** If you use the Google Cloud console to duplicate an existing cluster that uses preemptible VMs, you must enable Spot VMs for each node pool and configure your workloads to use Spot VMs instead of preemptible VMs. For instructions, refer to [Use Spot VMs to run fault-tolerant workloads](/kubernetes-engine/docs/how-to/spot-vms) .\n## Use nodeSelector to schedule Pods on preemptible VMs\nGKE adds the `cloud.google.com/gke-preemptible=true` and `cloud.google.com/gke-provisioning=preemptible` (for nodes running GKE version 1.25.5-gke.2500 or later) labels to nodes that use preemptible VMs. You can use a `nodeSelector` in your deployments to tell GKE to schedule Pods onto preemptible VMs.\nFor example, the following Deployment filters for preemptible VMs using the `cloud.google.com/gke-preemptible` label:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: hello-appspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: hello-app\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: hello-app\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 200m\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-preemptible: \"true\"\n```\n## Use node taints for preemptible VMs\nYou can taint nodes that use preemptible VMs so that GKE can only place Pods with the corresponding toleration on those nodes.\nTo add a node taint to a node pool that uses preemptible VMs, use the `--node-taints` flag when creating the node pool, similar to the following command:\n```\ngcloud container node-pools create POOL2_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --node-taints=cloud.google.com/gke-preemptible=\"true\":NoSchedule\n```\nNow, only Pods that tolerate the node taint are scheduled to the node.\nTo add the relevant toleration to your Pods, modify your deployments and add the following to your Pod specification:\n```\ntolerations:- key: cloud.google.com/gke-preemptible\u00a0 operator: Equal\u00a0 value: \"true\"\u00a0 effect: NoSchedule\n```\n## Node taints for GPU preemptible VMs\nPreemptible VMs support using [GPUs](/kubernetes-engine/docs/how-to/gpus) . You should create at least one other node pool in your cluster that doesn't use preemptible VMs before adding a GPU node pool that uses preemptible VMs. Having a standard node pool ensures that GKE can safely place system components like DNS.\nIf you create a new cluster with GPU node pools that use preemptible VMs, or if you add a new GPU node pool that uses preemptible VMs to a cluster that does not already have a standard node pool, GKE does not automatically add the `nvidia.com/gpu=present:NoSchedule` taint to the nodes. GKE might schedule system Pods onto the preemptible VMs, which can lead to disruptions. This behavior also increases your resource consumption, because GPU nodes are more expensive than non-GPU nodes.\n## What's next\n- [Learn how to run a GKE application on Spot VMs with on-demand nodes as fallback](/blog/topics/developers-practitioners/running-gke-application-spot-nodes-demand-nodes-fallback) .\n- [Learn more about Spot VMs in GKE](/kubernetes-engine/docs/concepts/spot-vms) .\n- [Learn about taints and tolerations](/kubernetes-engine/docs/how-to/node-taints) .", "guide": "Google Kubernetes Engine (GKE)"}