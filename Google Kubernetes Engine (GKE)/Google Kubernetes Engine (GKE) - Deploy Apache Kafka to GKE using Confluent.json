{"title": "Google Kubernetes Engine (GKE) - Deploy Apache Kafka to GKE using Confluent", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/confluent", "abstract": "# Google Kubernetes Engine (GKE) - Deploy Apache Kafka to GKE using Confluent\nThe guide shows you how to use the [Confluent for Kubernetes (CFK)](https://docs.confluent.io/operator/current/overview.html) operator to deploy Apache [Kafka](https://kafka.apache.org/) clusters on Google Kubernetes Engine (GKE).\nKafka is an open source, distributed publish-subscribe messaging system for handling high-volume, high-throughput, and real-time streaming data. You can use Kafka to build streaming data pipelines that move data reliably across different systems and applications for processing and analysis.\nThis guide is intended for platform administrators, cloud architects, and operations professionals interested in deploying Kafka clusters on GKE.\nYou can also use the CFK operator to deploy other components of the [Confluent Platform](https://docs.confluent.io/platform/current/overview.html) , such as the web-based Confluent Control center, Schema Registry, or KsqlDB. However, this guide focuses only on Kafka deployments.\n **Note:** The CFK operator and most of the Confluent platform components are distributed under a [\u0421onfluent Enterprise License](https://docs.confluent.io/platform/current/installation/license.html) , and license restrictions require users to have an active [subscription](https://www.confluent.io/subscription/) after 30 days trial to keep using the operator. The Developer License also is available for an indefinite duration, however it's limited to a single broker configuration per cluster.", "content": "## Objectives\n- Plan and deploy GKE infrastructure for Apache Kafka\n- Deploy and configure the CFK operator\n- Configure Apache Kafka using the CFK operator to ensure availability, security, observability, and performance\n### BenefitsCFK offers the following benefits:- Automated rolling updates for configuration changes.\n- Automated rolling upgrades with no impact to Kafka availability.\n- If a failure occurs, CFK restores a Kafka Pod with the same Kafka broker ID, configuration, and persistent storage volumes.\n- Automated rack awareness to spread replicas of a partition across different racks (or zones), improving availability of Kafka brokers and limiting the risk of data loss.\n- Support for aggregated metrics export to Prometheus.\n## Deployment architectureEach data partition in a Kafka cluster has one broker and can have one or more brokers. The leader broker handles all reads and writes to the partition. Each follower broker passively replicates the leader broker.\nIn a typical Kafka setup, you also use an open source service called [ZooKeeper](https://zookeeper.apache.org/) to coordinate your Kafka clusters. This service helps by electing a leader among the brokers and triggering failover in case of failures.\nYou can also deploy Kafka configuration without Zookeeper by activating [KRaft mode](https://docs.confluent.io/operator/current/co-deploy-cfk.html#deploy-co-with-kraft) , but this method is not considered production-ready due to lack of support for [KafkaTopic resources](https://docs.confluent.io/operator/current/co-manage-topics.html) , and credential authentication.\n### Availability and disaster recoveryThis tutorial uses separate [node pools](/kubernetes-engine/docs/concepts/node-pools) and [zones](/compute/docs/regions-zones) for Kafka and ZooKeeper clusters to ensure high availability and prepare for disaster recovery.\nHighly available Kubernetes clusters in Google Cloud rely on regional clusters spanning multiple nodes and availability zones. This configuration improves fault tolerance, scalability, and geographic redundancy. This configuration also lets you perform rolling updates and maintenance while providing SLAs for uptime and availability. For more information, see [Regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) .\n### Deployment diagramThe following diagram shows a Kafka cluster running on multiple nodes and zones in a GKE cluster:In the diagram, the Kafka StatefulSet is deployed across three nodes in three different zones. You can control this configuration by setting the required Pod [affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/) and [topology spread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/) rules on the `Kafka` custom resource specification.\nIf one zone fails, using the recommended configuration, GKE reschedules Pods on new nodes and replicates data from the remaining replicas, for both Kafka and Zookeeper.\nThe following diagram shows a ZooKeeper `StatefulSet` deployed across three nodes in three different zones:## Costs\nIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Managed Service for Prometheus](/stackdriver/pricing#mgd-prometheus-pricing-summary) \n- [Compute Engine](/compute/disks-image-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `role/storage.objectViewer, role/logging.logWriter, roles/container.clusterAdmin, role/container.serviceAgent, roles/iam.serviceAccountAdmin, roles/serviceusage.serviceUsageAdmin, roles/iam.serviceAccountAdmin` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.## Prepare the environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell is preinstalled with the software you need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) .\nTo set up your environment with Cloud Shell, follow these steps:- Launch a Cloud Shell session from the Google Cloud console, by clicking **Activate Cloud Shell** in the [Google Cloud console](https://console.cloud.google.com/) . This launches a session in the bottom pane of the Google Cloud console.\n- Set environment variables:```\nexport PROJECT_ID=PROJECT_ID\nexport KUBERNETES_CLUSTER_PREFIX=kafka\nexport REGION=us-central1\n```Replace `` : your Google Cloud with your [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory:```\ncd kubernetes-engine-samples/streaming\n```\n## Create your cluster infrastructureIn this section, you run a Terraform script to create a private, highly-available, regional GKE cluster. The following steps allow public access to the control plane. To restrict access, create a [private cluster](/kubernetes-engine/docs/concepts/private-cluster-concept) .\nYou can install the operator using a [Standard or Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode) cluster.\nThe following diagram shows a private regional Standard GKE cluster deployed across three different zones:To deploy this infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/gke-standard initterraform -chdir=kafka/terraform/gke-standard apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes.\n- A router to access the internet through NAT.\n- A private GKE cluster in the`us-central1`region.\n- 2 node pools with autoscaling enabled (1-2 nodes per zone, 1 node per zone minimum)\n- A`ServiceAccount`with logging and monitoring permissions.\n- Backup for GKE for disaster recovery.\n- Google Cloud Managed Service for Prometheus for cluster monitoring.\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 14 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials kafka-cluster --region us-central1\"\n```\nThe following diagram shows a private regional Autopilot GKE cluster:To deploy the infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/gke-autopilot initterraform -chdir=kafka/terraform/gke-autopilot apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- VPC network and private subnet for the Kubernetes nodes.\n- A router to access the internet through NAT.\n- A private GKE cluster in the`us-central1`region.\n- A`ServiceAccount`with logging and monitoring permissions\n- Google Cloud Managed Service for Prometheus for cluster monitoring.\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 12 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials kafka-cluster --region us-central1\"\n```\n### Connect to the clusterConfigure `kubectl` to communicate with the cluster:\n```\ngcloud container clusters get-credentials ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the CFK operator to your clusterIn this section, you deploy the Confluent for Kubernetes (CFK) operator using a Helm chart and then deploy a Kafka cluster.- Add the Confluent Helm Chart repository:```\nhelm repo add confluentinc https://packages.confluent.io/helm\n```\n- Add a namespace for the CFK operator and the Kafka cluster:```\nkubectl create ns kafka\n```\n- Deploy the CFK cluster operator using Helm:```\nhelm install confluent-operator confluentinc/confluent-for-kubernetes -n kafka\n```To enable CFK to manage resources across all namespaces, add the parameter `--set-namespaced=false` to the Helm command.\n- Verify that the Confluent operator has been deployed successfully using Helm:```\nhelm ls -n kafka\n```The output is similar to the following:```\nNAME     NAMESPACE REVISION UPDATED         STATUS  CHART        APP VERSION\nconfluent-operator kafka  1  2023-07-07 10:57:45.409158 +0200 CEST deployed confluent-for-kubernetes-0.771.13 2.6.0\n```\n## Deploy KafkaIn this section, you deploy Kafka in a basic configuration and then try various advanced configuration scenarios to address availability, security, and observability requirements.\n### Basic configurationThe basic configuration for the Kafka instance includes the following components:- Three replicas of Kafka brokers, with a minimum of two available replicas required for cluster consistency.\n- Three replicas of ZooKeeper nodes, forming a cluster.\n- Two Kafka listeners: one without authentication, and one utilizing TLS authentication with a certificate generated by CFK.\n- Javaandset to 4\u00a0GB for Kafka.\n- CPU resource allocation of 1 CPU request and 2 CPU limits, and 5\u00a0GB memory requests and limits for Kafka (4\u00a0GB for the main service and 0.5\u00a0GB for the metrics exporter) and 3\u00a0GB for Zookeeper (2\u00a0GB for the main service and 0.5\u00a0GB for the metrics exporter).\n- 100\u00a0GB of storage allocated to each Pod using the`premium-rwo`storageClass, 100 for Kafka Data and 90/10 for Zookeeper Data/Log.\n- Tolerations, nodeAffinities, and podAntiAffinities configured for each workload, ensuring proper distribution across nodes, utilizing their respective node pools and different zones.\n- Communication inside the cluster secured by self-signed certificates using a Certificate Authority that you provide.\nThis configuration represents the minimal setup required to create a production-ready Kafka cluster. The following sections demonstrate custom configurations to address aspects such as cluster security, Access Control Lists (ACLs), topic management, certificate management and more.\n### Create a basic Kafka cluster\n- Generate a CA pair:```\nopenssl genrsa -out ca-key.pem 2048openssl req -new -key ca-key.pem -x509 \\\u00a0 -days 1000 \\\u00a0 -out ca.pem \\\u00a0 -subj \"/C=US/ST=CA/L=Confluent/O=Confluent/OU=Operator/CN=MyCA\"\n```Confluent for Kubernetes provides auto-generated certificates for Confluent Platform components to use for TLS network encryption. You must generate and provide a Certificate Authority (CA).\n- Create a Kubernetes Secret for the certificate authority:```\nkubectl create secret tls ca-pair-sslcerts --cert=ca.pem --key=ca-key.pem -n kafka\n```The name of the Secret is [predefined](https://github.com/confluentinc/confluent-kubernetes-examples/blob/master/security/production-secure-deploy-auto-gen-certs/README.rst) \n- Create a new Kafka cluster using the basic configuration:```\nkubectl apply -n kafka -f kafka-confluent/manifests/01-basic-cluster/my-cluster.yaml\n```This command creates a Kafka custom resource and Zookeeper custom resource of the CFK operator that include CPU and memory requests and limits, block storage requests, and taints and affinities to distribute the provisioned Pods across Kubernetes nodes.\n- Wait a few minutes while Kubernetes starts the required workloads:```\nkubectl wait pods -l app=my-cluster --for condition=Ready --timeout=300s -n kafka\n```\n- Verify that the Kafka workloads were created:```\nkubectl get pod,svc,statefulset,deploy,pdb -n kafka\n```The output is similar to the following:```\nNAME         READY STATUS RESTARTS AGE\npod/confluent-operator-864c74d4b4-fvpxs 1/1 Running 0  49m\npod/my-cluster-0      1/1 Running 0  17m\npod/my-cluster-1      1/1 Running 0  17m\npod/my-cluster-2      1/1 Running 0  17m\npod/zookeeper-0       1/1 Running 0  18m\npod/zookeeper-1       1/1 Running 0  18m\npod/zookeeper-2       1/1 Running 0  18m\nNAME       TYPE  CLUSTER-IP EXTERNAL-IP PORT(S)              AGE\nservice/confluent-operator ClusterIP 10.52.13.164 <none>  7778/TCP              49m\nservice/my-cluster   ClusterIP None   <none>  9092/TCP,8090/TCP,9071/TCP,7203/TCP,7777/TCP,7778/TCP,9072/TCP 17m\nservice/my-cluster-0-internal ClusterIP 10.52.2.242 <none>  9092/TCP,8090/TCP,9071/TCP,7203/TCP,7777/TCP,7778/TCP,9072/TCP 17m\nservice/my-cluster-1-internal ClusterIP 10.52.7.98 <none>  9092/TCP,8090/TCP,9071/TCP,7203/TCP,7777/TCP,7778/TCP,9072/TCP 17m\nservice/my-cluster-2-internal ClusterIP 10.52.4.226 <none>  9092/TCP,8090/TCP,9071/TCP,7203/TCP,7777/TCP,7778/TCP,9072/TCP 17m\nservice/zookeeper    ClusterIP None   <none>  2181/TCP,7203/TCP,7777/TCP,3888/TCP,2888/TCP,7778/TCP   18m\nservice/zookeeper-0-internal ClusterIP 10.52.8.52 <none>  2181/TCP,7203/TCP,7777/TCP,3888/TCP,2888/TCP,7778/TCP   18m\nservice/zookeeper-1-internal ClusterIP 10.52.12.44 <none>  2181/TCP,7203/TCP,7777/TCP,3888/TCP,2888/TCP,7778/TCP   18m\nservice/zookeeper-2-internal ClusterIP 10.52.12.134 <none>  2181/TCP,7203/TCP,7777/TCP,3888/TCP,2888/TCP,7778/TCP   18m\nNAME      READY AGE\nstatefulset.apps/my-cluster 3/3 17m\nstatefulset.apps/zookeeper 3/3 18m\nNAME        READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/confluent-operator 1/1 1   1   49m\nNAME         MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE\npoddisruptionbudget.policy/my-cluster N/A   1    1     17m\npoddisruptionbudget.policy/zookeeper N/A   1    1     18m\n```\nThe operator creates the following resources:- Two StatefulSets for Kafka and ZooKeeper.\n- Three Pods for Kafka broker replicas.\n- Three Pods for ZooKeeper replicas.\n- Two`PodDisruptionBudget`resources, ensuring a maximum one unavailable replica for cluster consistency.\n- The Service`my-cluster`which serves as the bootstrap server for Kafka clients connecting from within the Kubernetes cluster. All internal Kafka listeners are available in this Service.\n- The Service`zookeeper`which allows Kafka brokers to connect to ZooKeeper nodes as clients.\n## Authentication and user managementThis section shows you how to enable the authentication and authorization to secure Kafka Listeners and share credentials with clients.\nConfluent for Kubernetes supports various authentication methods for Kafka, such as:- [SASL/PLAIN authentication](https://docs.confluent.io/operator/current/co-authenticate.html#co-authenticate-kafka-plain) : Clients use a username and password for authentication. The username and password are stored server-side in a Kubernetes secret.\n- [SASL/PLAIN with LDAP authentication](https://docs.confluent.io/operator/current/co-authenticate.html#co-authenticate-kafka-plain-ldap) : Clients use a username and password for authentication. The credentials are stored in an LDAP server.\n- [mTLS authentication](https://docs.confluent.io/operator/current/co-authenticate.html#co-authenticate-kafka-mtls) : Clients use TLS certificates for authentication.\n### Limitations\n- CFK does not provide Custom Resources for user management. However, you can store credentials in Secrets and refer to Secrets to in listener specs.\n- Although there's no Custom Resource to manage ACLs directly, the official [Confluent for Kubernetes](https://docs.confluent.io/platform/current/kafka/authorization.html) provides guidance on configuring ACLs using the Kafka CLI.\n### Create a userThis section shows you how to deploy a CFK operator that demonstrates user management capabilities, including:- A Kafka cluster with password-based authentication (SASL/PLAIN) enabled on one of the listeners\n- A`KafkaTopic`with 3 replicas\n- User credentials with read and write permissions\n- Create a Secret with user credentials:```\nexport USERNAME=my-userexport PASSWORD=$(openssl rand -base64 12)kubectl create secret generic my-user-credentials -n kafka \\\u00a0 --from-literal=plain-users.json=\"{\\\"$USERNAME\\\":\\\"$PASSWORD\\\"}\"\n```Credentials should be stored in the following format:```\n{\"username1\": \"password1\",\"username2\": \"password2\",...\"usernameN\": \"passwordN\"}\n```\n- Configure Kafka cluster to use a listener with password-based authentication SCRAM-SHA-512 authentication on port 9094:```\nkubectl apply -n kafka -f kafka-confluent/manifests/02-auth/my-cluster.yaml\n```\n- Set up a topic and a client Pod to interact with your Kafka cluster and execute Kafka commands:```\nkubectl apply -n kafka -f kafka-confluent/manifests/02-auth/my-topic.yamlkubectl apply -n kafka -f kafka-confluent/manifests/02-auth/kafkacat.yaml\n```GKE mounts the Secret `my-user-credentials` to the client Pod as a [Volume](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod) .\n- When the client Pod is ready, connect to it and start producing and consuming messages using the provided credentials:```\nkubectl wait pod kafkacat --for=condition=Ready --timeout=300s -n kafkakubectl exec -it kafkacat -n kafka -- /bin/sh\n```\n- Produce a message using the `my-user` credentials and then consume the message to verify its receipt.```\nexport USERNAME=$(cat /my-user/plain-users.json|cut -d'\"' -f 2)export PASSWORD=$(cat /my-user/plain-users.json|cut -d'\"' -f 4)echo \"Message from my-user\" |kcat \\\u00a0 -b my-cluster.kafka.svc.cluster.local:9094 \\\u00a0 -X security.protocol=SASL_SSL \\\u00a0 -X sasl.mechanisms=PLAIN \\\u00a0 -X sasl.username=$USERNAME \\\u00a0 -X sasl.password=$PASSWORD \u00a0\\\u00a0 -t my-topic -Pkcat -b my-cluster.kafka.svc.cluster.local:9094 \\\u00a0 -X security.protocol=SASL_SSL \\\u00a0 -X sasl.mechanisms=PLAIN \\\u00a0 -X sasl.username=$USERNAME \\\u00a0 -X sasl.password=$PASSWORD \u00a0\\\u00a0 -t my-topic -C\n```The output is similar to the following:```\nMessage from my-user\n% Reached end of topic my-topic [1] at offset 1\n% Reached end of topic my-topic [2] at offset 0\n% Reached end of topic my-topic [0] at offset 0\n```Type `CTRL+C` to stop the consumer process. If you get a `Connect refused` error, wait a few minutes and then try again.\n- Exit the Pod shell```\nexit\n```\n## Backups and disaster recoveryUsing the Confluent operator, you can implement efficient backup strategies by following certain patterns.\nYou can use [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) to backup:- Kubernetes resource manifests.\n- Confluent API custom resources and their definitions extracted from the Kubernetes API server of the cluster undergoing backup.\n- Volumes that correspond to PersistentVolumeClaim resources found in the manifests.\nFor more information about how to backup and restore Kafka clusters using Backup for GKE, see [Prepare for disaster recovery](/kubernetes-engine/docs/tutorials/stateful-workloads/kafka#disaster-recovery) .\nYou can also perform a manual backup of your Kafka cluster. You should backup:- The Kafka configuration, which includes all custom resources of the Confluent API such as`KafkaTopics`or`Connect`\n- The data, which is stored in the PersistentVolumes of the Kafka brokers\nStoring Kubernetes resource manifests, including Confluent configurations, in Git repositories can eliminate the need for a separate backup for Kafka configuration as the resources can be reapplied to a new Kubernetes cluster when necessary.\nTo safeguard Kafka data recovery in scenarios where a Kafka server instance, or Kubernetes cluster where Kafka is deployed, is lost, we recommend that you configure the Kubernetes storage class used for provisioning volumes for Kafka brokers with the `reclaimPolicy` option set to `Retain` . We also recommended that you take [snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) of Kafka broker volumes.\nThe following manifest describes a StorageClass that uses the `reclaimPolicy` option `Retain` :\n```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: premium-rwo-retain...reclaimPolicy: RetainvolumeBindingMode: WaitForFirstConsumer\n```\nThe following example shows the StorageClass added to the `spec` of a Kafka cluster custom resource:\n```\n...spec:\u00a0 ...\u00a0 dataVolumeCapacity: 100Gi\u00a0 storageClass:\u00a0 name: premium-rwo-retain\n```\nWith this configuration, PersistentVolumes provisioned using the storage class are not deleted even when the corresponding PersistentVolumeClaim is deleted.\nTo recover the Kafka instance on a new Kubernetes cluster using the existing configuration and broker instance data:- Apply the existing Confluent custom resources (`Kafka`,`KafkaTopic`,`Zookeeper`, etc.) to a new Kubernetes cluster\n- Update the PersistentVolumeClaims with the name of the new Kafka broker instances to the old PersistentVolumes using the`spec.volumeName`property on the PersistentVolumeClaim.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete the individual resourcesIf you used an existing project and you don't want to delete it, delete the individual resources.- Set environment variables:```\nexport PROJECT_ID=PROJECT_IDexport KUBERNETES_CLUSTER_PREFIX=kafkaexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/FOLDER destroy -var project_id=${PROJECT_ID} \u00a0 \\\u00a0 -var region=${REGION} \u00a0\\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,zone)\")\n```\n- Delete the disks:```\nfor i in $disk_list; do\u00a0 disk_name=$(echo $i| cut -d'|' -f1)\u00a0 disk_zone=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0 echo \"Deleting $disk_name\"\u00a0 gcloud compute disks delete $disk_name --zone $disk_zone --quietdone\n```\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}