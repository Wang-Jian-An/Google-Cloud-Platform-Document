{"title": "Google Kubernetes Engine (GKE) - Deploy Apache Kafka to GKE using Strimzi", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/apache-kafka-strimzi", "abstract": "# Google Kubernetes Engine (GKE) - Deploy Apache Kafka to GKE using Strimzi\nThe guide shows you how to use the [Strimzi](http://www.strimzi.io) operator to deploy Apache [Kafka](https://kafka.apache.org/) clusters.\nKafka is an open-source, distributed messaging system designed to handle high-volume, high-throughput, and real-time streaming data. It lets you build streaming data pipelines for reliable data transfer across different systems and applications, to support processing and analysis tasks.\nOperators are software extensions that make use of custom resources to manage applications and their components. To learn more about the motivation for using operators, see [Operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) in the open source Kubernetes documentation. The Strimzi operator offers flexibility in deployment options and lets you use Kubernetes taints and tolerances to run Kafka on dedicated nodes.\nThis guide is intended for platform administrators, cloud architects, and operations professionals interested in deploying Kafka clusters on GKE.\nThis solution is a good starting point if you want to learn how to deploy Kafka clusters using a third-party operator to automate management and reduce errors. If you prefer more granular operational control, see [Deploy a highly-available Kafka clusters on GKE](/kubernetes-engine/docs/tutorials/stateful-workloads/kafka) .", "content": "## Objectives\n- Plan and deploy GKE infrastructure for Apache Kafka\n- Deploy and configure the Strimzi operator\n- Configure Apache Kafka using the Strimzi operator\n### BenefitsStrimzi includes the following benefits:- Strimzi operators provide a simplified and Kubernetes-native approach to managing Kafka clusters. Strimzi utilizes custom resources that represent Kafka topics and users, making cluster management much more straightforward and aligned with Kubernetes best practices.\n- Strimzi prioritizes security by default by generating certificates for listeners and supporting secure authentication methods such as TLS, SCRAM-SHA, and OAuth. Strimzi also handles NetworkPolicies for all Kafka listeners.\n- Strimzi doesn't rely on external dependencies. It includes Kafka and [ZooKeeper](https://zookeeper.apache.org/) clusters with built-in metrics exporters, saving you from dealing with additional tools. You can also fine-tune broker configurations to meet specific requirements.\n## Deployment architectureA Kafka cluster consists of one or more servers, known as , which collaborate to manage incoming data streams and facilitate publish-subscribe messaging for Kafka clients, referred to as .\nEvery data partition within the Kafka cluster is assigned a , which is responsible for managing all read and write operations to that partition. The partition can also have one or more which passively replicate the actions of the leader broker.\nIn a typical setup, ZooKeeper coordinates Kafka clusters by helping choose a leader among the brokers and ensuring a smooth failover in case of any issues.\nYou can also deploy Kafka configuration without Zookeeper by activating KRaft mode, but this method is not considered production-ready by the Strimzi community because it does not include support for KafkaTopic resources, credential authentication, and more.\n### Availability and disaster recoveryThis tutorial uses separate [node pools](/kubernetes-engine/docs/concepts/node-pools) and [zones](/compute/docs/regions-zones) for Kafka and ZooKeeper clusters to ensure high availability and prepare for disaster recovery.\nUsing multiple nodes and zones is crucial for achieving a high-available Kubernetes cluster in Google Cloud for the following reasons:- Fault tolerance: Multiple nodes distribute the workload across the cluster, ensuring that if one node fails, the other nodes can take over the tasks, preventing downtime and service interruptions.\n- Scalability: Using multiple nodes ensures horizontal scaling can add or remove nodes as needed, ensuring optimal resource allocation and accommodating increased traffic or workload demands.\n- High availability: Using multiple zones within a region ensures redundancy and minimizes the risk of a single point of failure. If an entire availability zone experiences an outage, the cluster can continue running in other zones, maintaining service availability.\n- Geographic redundancy: By spanning nodes across regions, the cluster's data and services are geographically distributed, providing resilience against natural disasters, power outages, or other local disruptions that might impact a single zone.\n- Rolling updates and maintenance: Using multiple zones ensure that rolling updates and maintenance can be performed on individual nodes without impacting the overall availability of the cluster. This ensures continuous service while allowing for necessary updates and patches to be applied seamlessly.\n- Service Level Agreements (SLAs): Google Cloud provides SLAs for multi-zone deployments, guaranteeing a minimum level of uptime and availability.\n### Deployment diagramThe following diagram shows a Kafka cluster running on multiple nodes and zones in a GKE cluster:In the diagram, the Kafka `StrimziPodSet` is deployed across three nodes in three different zones. You can control this configuration by setting the required Pod [affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/) and [topology spread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/) rules on the `StrimziPodSet` custom resource specification.\nIf one Zone fails, using the recommended configuration, GKE reschedules Pods on new nodes and replicates data from the remaining replicas, for both Kafka and Zookeeper.\nThe following diagram shows a ZooKeeper `StrimziPodSet` deployed across three nodes in three different zones:\n### The StrimziPodSet custom resourceThis tutorial uses the [StrimziPodSet](https://strimzi.io/docs/operators/in-development/configuring.html#type-StrimziPodSet-reference) custom resource introduced in version 0.29 of Strimzi instead of `StatefulSets` .\nThe `StrimziPodSet` resources offers enhanced scalability for the cluster and lets you pass configuration options, allowing you to make more granular changes to Pods. The `StrimziPodSet` resource is enabled by default in Strimzi versions 0.35 and later.## Costs\nIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Managed Service for Prometheus](/stackdriver/pricing#mgd-prometheus-pricing-summary) \n- [Backup for GKE](/kubernetes-engine/pricing#backup-for-gke) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/storage.objectViewer, roles/logging.logWriter, roles/container.clusterAdmin, roles/container.serviceAgent, roles/iam.serviceAccountAdmin, roles/serviceusage.serviceUsageAdmin, roles/iam.serviceAccountAdmin` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.## Prepare the environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell is preinstalled with the software you need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) and [Terraform](/docs/terraform) .\nTo set up your environment with Cloud Shell, follow these steps:- Launch a Cloud Shell session from the Google Cloud console, by clicking **Activate Cloud Shell** in the [Google Cloud console](https://console.cloud.google.com/) . This launches a session in the bottom pane of the Google Cloud console.\n- Set environment variables:```\nexport PROJECT_ID=PROJECT_ID\nexport KUBERNETES_CLUSTER_PREFIX=kafka\nexport REGION=us-central1\n```Replace `` : your Google Cloud with your [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory:```\ncd kubernetes-engine-samples/streaming/\n```\n## Create your cluster infrastructureIn this section, you run a Terraform script to create a private, highly-available, regional GKE cluster. The following steps allow public access to the control plane. To restrict access, create a [private cluster](/kubernetes-engine/docs/concepts/private-cluster-concept) .\nYou can install the operator using a [Standard or Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode) cluster.\nThe following diagram shows a private regional Standard GKE cluster deployed across three different zones:To deploy this infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/gke-standard initterraform -chdir=kafka/terraform/gke-standard apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes.\n- A router to access the internet through NAT.\n- A private GKE cluster in the`us-central1`region.\n- 2 node pools with autoscaling enabled (1-2 nodes per zone, 1 node per zone minimum)\n- A`ServiceAccount`with logging and monitoring permissions.\n- Backup for GKE for disaster recovery.\n- Google Cloud Managed Service for Prometheus for cluster monitoring.\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 14 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials strimzi-cluster --region us-central1\"\n```\nThe following diagram shows a private regional Autopilot GKE cluster:To deploy the infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/gke-autopilot initterraform -chdir=kafka/terraform/gke-autopilot apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- VPC network and private subnet for the Kubernetes nodes.\n- A router to access the internet through NAT.\n- A private GKE cluster in the`us-central1`region.\n- A`ServiceAccount`with logging and monitoring permissions\n- Google Cloud Managed Service for Prometheus for cluster monitoring.\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 12 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials strimzi-cluster --region us-central1\"\n```\n### Connecting to the clusterConfigure `kubectl` to communicate with the cluster:\n```\ngcloud container clusters get-credentials ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the Strimzi operator to your clusterIn this section, you deploy the Strimzi operator using a Helm chart. There are also [several other ways](https://strimzi.io/docs/operators/in-development/full/deploying.html#con-strimzi-installation-methods_str) to deploy Strimzi.- Add the Strimzi Helm Chart repository:```\nhelm repo add strimzi https://strimzi.io/charts/\n```\n- Add a namespace for the Strimzi Operator and the Kafka cluster:```\nkubectl create ns kafka\n```\n- Deploy the Strimzi cluster operator using Helm:```\nhelm install strimzi-operator strimzi/strimzi-kafka-operator -n kafka\n```To deploy Strimzi Cluster Operator and Kafka clusters to different namespaces, add the parameter `--set watchNamespaces=\"{kafka-namespace,kafka-namespace-2,...}\"` to the helm command.\n- Verify that the Strimzi Cluster Operator has been deployed successfully using Helm:```\nhelm ls -n kafka\n```The output is similar to the following:```\nNAME   NAMESPACE REVISION UPDATED        STATUS CHART      APP VERSION\nstrimzi-operator kafka  1  2023-06-27 11:22:15.850545 +0200 CEST deployed strimzi-kafka-operator-0.35.0 0.35.0\n```\n## Deploy KafkaAfter the operator is deployed to the cluster, you are ready to deploy a Kafka cluster instance.\nIn this section, you deploy Kafka in a basic configuration and then try various advanced configuration scenarios to address availability, security, and observability requirements.\n### Basic configurationThe basic configuration for the Kafka instance includes the following components:- Three replicas of Kafka brokers, with a minimum of two available replicas required for cluster consistency.\n- Three replicas of ZooKeeper nodes, forming a cluster.\n- Two Kafka listeners: one without authentication, and one utilizing TLS authentication with a certificate generated by Strimzi.\n- Javaandset to 4\u00a0GB for Kafka and 2\u00a0GB for ZooKeeper.\n- CPU resource allocation of 1 CPU request and 2 CPU limits both for Kafka and ZooKeeper, along with 5\u00a0GB memory requests and limits for Kafka (4\u00a0GB for the main service and 0.5\u00a0GB for the metrics exporter) and 2.5\u00a0GB for ZooKeeper (2\u00a0GB for the main service and 0.5\u00a0GB for the metrics exporter).\n- Entity-operator with the following requests and limits:- `tlsSidecar`: 100\u00a0m/500\u00a0m CPU and 128\u00a0Mi memory.\n- `topicOperator`: 100\u00a0m/500\u00a0m CPU and 512\u00a0Mi memory.\n- `userOperator`: 500\u00a0m CPU and 2\u00a0Gi memory.\n- 100\u00a0GB of storage allocated to each Pod using the`premium-rwo``storageClass`.\n- Tolerations, nodeAffinities, and podAntiAffinities configured for each workload, ensuring proper distribution across nodes, utilizing their respective node pools and different zones.\n- Communication inside the cluster secured by self-signed certificates: separate Certificate Authorities (CAs) for cluster and clients (mTLS). You can also configure to use a different Certificate Authority.\nThis configuration represents the minimal setup required to create a production-ready Kafka cluster. The following sections demonstrate custom configurations to address aspects such as cluster security, Access Control Lists (ACLs), topic management, certificate management and more.\n### Create a basic Kafka cluster\n- Create a new Kafka cluster using the basic configuration:```\nkubectl apply -n kafka -f kafka-strimzi/manifests/01-basic-cluster/my-cluster.yaml\n```This command creates a Kafka custom resource of the Strimzi operator that includes CPU and memory requests and limits, block storage requests, and a combination of taints and affinities to distribute the provisioned Pods across Kubernetes nodes.\n- Wait a few minutes while Kubernetes starts the required workloads:```\nkubectl wait kafka/my-cluster --for=condition=Ready --timeout=600s -n kafka\n```\n- Verify that the Kafka workloads were created:```\nkubectl get pod,service,deploy,pdb -l=strimzi.io/cluster=my-cluster -n kafka\n```The output is similar to the following:```\nNAME           READY STATUS RESTARTS AGE\npod/my-cluster-entity-operator-848698874f-j5m7f 3/3 Running 0  44m\npod/my-cluster-kafka-0       1/1 Running 0  5m\npod/my-cluster-kafka-1       1/1 Running 0  5m\npod/my-cluster-kafka-2       1/1 Running 0  5m\npod/my-cluster-zookeeper-0      1/1 Running 0  6m\npod/my-cluster-zookeeper-1      1/1 Running 0  6m\npod/my-cluster-zookeeper-2      1/1 Running 0  6m\nNAME        TYPE  CLUSTER-IP EXTERNAL-IP PORT(S)        AGE\nservice/my-cluster-kafka-bootstrap ClusterIP 10.52.8.80 <none>  9091/TCP,9092/TCP,9093/TCP   5m\nservice/my-cluster-kafka-brokers ClusterIP None   <none>  9090/TCP,9091/TCP,9092/TCP,9093/TCP 5m\nservice/my-cluster-zookeeper-client ClusterIP 10.52.11.144 <none>  2181/TCP       6m\nservice/my-cluster-zookeeper-nodes ClusterIP None   <none>  2181/TCP,2888/TCP,3888/TCP   6m\nNAME          READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/my-cluster-entity-operator 1/1 1   1   44m\nNAME           MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE\npoddisruptionbudget.policy/my-cluster-kafka  2    N/A    1     5m\npoddisruptionbudget.policy/my-cluster-zookeeper 2    N/A    1     6m\n```\nThe operator creates the following resources:- Two`StrimziPodSets`for Kafka and ZooKeeper.\n- Three Pods for Kafka broker replicas.\n- Three Pods for ZooKeeper replicas.\n- Two`PodDisruptionBudgets`, ensuring a minimum availability of two replicas for cluster consistency.\n- A Service named`my-cluster-kafka-bootstrap`, which serves as the bootstrap server for Kafka clients connecting from within the Kubernetes cluster. All internal Kafka listeners are available in this Service.\n- A headless Service named`my-cluster-kafka-brokers`that enables DNS resolution of Kafka broker Pod IP addresses directly. This service is used for inter broker communication.\n- A Service named`my-cluster-zookeeper-client`that lets Kafka brokers connect to ZooKeeper nodes as clients.\n- A headless Service named`my-cluster-zookeeper-nodes`that enables DNS resolution of ZooKeeper Pod IP addresses directly. This service is used to connect between ZooKeeper replicas.\n- A Deployment named`my-cluster-entity-operator`that contains the [topic-operator and user-operator](https://strimzi.io/docs/operators/latest/configuring.html#assembly-kafka-entity-operator-str) and facilitates the management of custom resources`KafkaTopics`and`KafkaUsers`.\nYou can also configure two `NetworkPolicies` to facilitate connectivity to Kafka listeners from any Pod and Namespace. These policies would also restrict connections to ZooKeeper to brokers, and enable communication between the cluster Pods and internal Service ports exclusive to cluster communication.## Authentication and user managementThis section shows you how to enable the authentication and authorization to secure Kafka Listeners and share credentials with clients.\nStrimzi provides a Kubernetes-native method for user management using a separate [User Operator](https://strimzi.io/docs/operators/latest/configuring.html#user-operator-str) and its corresponding Kubernetes custom resource, [KafkaUser](https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaUser-reference) , which defines the user configuration. The user configuration includes settings for authentication and authorization, and provisions the corresponding user in Kafka.\nStrimzi can create Kafka listeners and users that support several [authentication mechanisms](https://strimzi.io/docs/operators/latest/configuring.html#type-KafkaUserSpec-reference) such as username and password-based authentication (SCRAM-SHA-512) or TLS. You can also use OAuth 2.0 authentication, which is often considered a better approach compared to using passwords or certificates for authentication because of security and external credential management.\n### Deploy a Kafka clusterThis section shows you how to deploy a Strimzi operator that demonstrates user management capabilities, including:- A Kafka cluster with password-based authentication (SCRAM-SHA-512) enabled on one of the listeners.\n- A`KafkaTopic`with 3 replicas.\n- A`KafkaUser`with an ACL that specifies that user has read and write permissions to the topic.\n- Configure your Kafka cluster to use a listener with password-based SCRAM-SHA-512 authentication on port 9094 and simple authorization:```\nkubectl apply -n kafka -f kafka-strimzi/manifests/03-auth/my-cluster.yaml\n```\n- Create a `Topic` , `User` and a client Pod to execute commands against the Kafka cluster:```\nkubectl apply -n kafka -f kafka-strimzi/manifests/03-auth/topic.yamlkubectl apply -n kafka -f kafka-strimzi/manifests/03-auth/my-user.yaml\n```The `Secret` `my-user` with the user credentials is mounted to the client Pod as a [Volume](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod) .These credentials confirm that the user has permissions to publish messages to the topic using the listener with the password-based authentication (SCRAM-SHA-512) enabled.\n- Create a client pod:```\nkubectl apply -n kafka -f kafka-strimzi/manifests/03-auth/kafkacat.yaml\n```\n- Wait a few minutes for the client Pod becomes `Ready` then connect to it:```\nkubectl wait --for=condition=Ready pod --all -n kafka --timeout=600skubectl exec -it kafkacat -n kafka -- /bin/sh\n```\n- Produce a new message with `my-user` credentials and try to consume it:```\necho \"Message from my-user\" |kcat \\\u00a0 -b my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9094 \\\u00a0 -X security.protocol=SASL_SSL \\\u00a0 -X sasl.mechanisms=SCRAM-SHA-512 \\\u00a0 -X sasl.username=my-user \\\u00a0 -X sasl.password=$(cat /my-user/password) \\\u00a0 -t my-topic -Pkcat -b my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9094 \\\u00a0 -X security.protocol=SASL_SSL \\\u00a0 -X sasl.mechanisms=SCRAM-SHA-512 \\\u00a0 -X sasl.username=my-user \\\u00a0 -X sasl.password=$(cat /my-user/password) \\\u00a0 -t my-topic -C\n```The output is similar to the following:```\nMessage from my-user\n% Reached end of topic my-topic [0] at offset 0\n% Reached end of topic my-topic [2] at offset 1\n% Reached end of topic my-topic [1] at offset 0\n```Type `CTRL+C` to stop the consumer process.\n- Exit the Pod shell```\nexit\n```\n## Backups and disaster recoveryAlthough the Strimzi operator does not offer built-in backup functionality, you can implement efficient backup strategies by following certain patterns.\nYou can use [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) to backup:- Kubernetes resource manifests.\n- Strimzi API custom resources and their definitions extracted from the Kubernetes API server of the cluster undergoing backup.\n- Volumes that correspond to PersistentVolumeClaim resources found in the manifests.\nFor more information about how to backup and restore Kafka clusters using Backup for GKE, see [Prepare for disaster recovery](/kubernetes-engine/docs/tutorials/stateful-workloads/kafka#disaster-recovery) .\nYou can also perform a backup of a Kafka cluster that has been deployed using the Strimzi operator. You should backup:- The Kafka configuration, which includes all custom resources of the Strimzi API such as`KafkaTopics`and`KafkaUsers`.\n- The data, which is stored in the PersistentVolumes of the Kafka brokers.\nStoring Kubernetes resource manifests, including Strimzi configurations, in Git repositories can eliminate the need for a separate backup for Kafka configuration, because the resources can be reapplied to a new Kubernetes cluster when necessary.\nTo safeguard Kafka data recovery in scenarios where a Kafka server instance, or a Kubernetes cluster where Kafka is deployed, is lost, we recommend that you configure the Kubernetes storage class used for provisioning volumes for Kafka brokers with the `reclaimPolicy` option set to `Retain` . We also recommend that you take [snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) of Kafka broker volumes.\nThe following manifest describes a StorageClass that uses the `reclaimPolicy` option `Retain` :\n```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: premium-rwo-retain...reclaimPolicy: RetainvolumeBindingMode: WaitForFirstConsumer\n```\nThe following example shows the StorageClass added to the `spec` of a Kafka cluster custom resource:\n```\n# ...spec:\u00a0 kafka:\u00a0 \u00a0 # ...\u00a0 \u00a0 storage:\u00a0 \u00a0 \u00a0 type: persistent-claim\u00a0 \u00a0 \u00a0 size: 100Gi\u00a0 \u00a0 \u00a0 class: premium-rwo-retain\n```\nWith this configuration, PersistentVolumes provisioned using the storage class are not deleted even when the corresponding PersistentVolumeClaim is deleted.\nTo recover the Kafka instance on a new Kubernetes cluster using the existing configuration and broker instance data:- Apply the existing Strimzi Kafka custom resources (`Kakfa`,`KafkaTopic`,`KafkaUser`, etc.) to a new Kubernetes cluster\n- Update the PersistentVolumeClaims with the name of the new Kafka broker instances to the old PersistentVolumes using the`spec.volumeName`property on the PersistentVolumeClaim.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete the individual resourcesIf you used an existing project and you don't want to delete it, delete the individual resources.- Set environment variables.```\nexport PROJECT_ID=${PROJECT_ID}export KUBERNETES_CLUSTER_PREFIX=kafkaexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=kafka/terraform/FOLDER destroy -var project_id=${PROJECT_ID} \u00a0 \\\u00a0 -var region=${REGION} \u00a0\\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,zone)\")\n```This step is needed because, by default Strimzi uses the `deleteClaim: false` parameter for storage. If you delete the cluster, all disks remain available.\n- Delete the disks:```\nfor i in $disk_list; do\u00a0 disk_name=$(echo $i| cut -d'|' -f1)\u00a0 disk_zone=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0 echo \"Deleting $disk_name\"\u00a0 gcloud compute disks delete $disk_name --zone $disk_zone --quietdone\n```\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}