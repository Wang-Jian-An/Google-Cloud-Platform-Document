{"title": "Google Kubernetes Engine (GKE) - Security overview", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/security-overview", "abstract": "# Google Kubernetes Engine (GKE) - Security overview\n[workloads](/kubernetes-engine/docs/how-to/deploying-workloads-overview)\nIt's best to take a layered approach to protecting your clusters and workloads. You can apply the [principle of least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege) to the level of access provided to your users and your application. In each layer there may be different tradeoffs that must be made that allow the right level of flexibility and security for your organization to securely deploy and maintain their workloads. For example, some security settings may be too constraining for certain types of applications or use cases to function without significant refactoring.\nThis document provides an overview of each layer of your infrastructure, and shows how you can configure its security features to best suit your needs.\n**Note:** GKE Autopilot clusters implement many security configurations for you. For details, refer to [Autopilot security capabilities](/kubernetes-engine/docs/concepts/autopilot-security) .\n", "content": "## Authentication and authorization\nKubernetes supports [two types of authentication](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes) :\n- **User accounts** are accounts that are known to Kubernetes, but are not managed by Kubernetes - for example, you cannot create or delete them using`kubectl`.\n- **Service accounts** are accounts that are created and managed by Kubernetes, but can only be used by Kubernetes-created entities, [such as pods](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/) .\nIn a GKE [cluster](/kubernetes-engine/docs/concepts/cluster-architecture) , Kubernetes user accounts are managed by Google Cloud, and may be one of the following two types:\n- [Google Account](https://support.google.com/accounts/answer/27441) \n- [Google Cloud service account](/compute/docs/access/service-accounts) \nOnce authenticated, you need to authorize these identities to create, read, update or delete Kubernetes resources.\nDespite the similar names, Kubernetes service accounts and Google Cloud service accounts are different entities. Kubernetes service accounts are part of the cluster in which they are defined and are typically used within that cluster. By contrast, Google Cloud service accounts are part of a Google Cloud project, and can easily be granted permissions both within clusters and to Google Cloud project clusters themselves, as well as to any Google Cloud resource using [Identity and Access Management (IAM)](/kubernetes-engine/docs/how-to/iam-integration#managing_iam_roles) . This makes Google Cloud service accounts more powerful than Kubernetes service accounts; in order to follow the security principle of least privilege, you should consider using Google Cloud service accounts only when their capabilities are required.\nTo configure more granular access to Kubernetes resources at the cluster level or within Kubernetes namespaces, you use [Role-Based Access Control (RBAC)](/kubernetes-engine/docs/how-to/role-based-access-control) . RBAC allows you to create detailed policies that define which operations and resources you allow users and service accounts to access. With RBAC, you can control access for Google Accounts, Google Cloud service accounts, and Kubernetes service accounts. To further simplify and streamline your authentication and authorization strategy for GKE, you should ensure that the [legacy Attribute Based Access Control](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters#legacyabac) is disabled so that Kubernetes RBAC and IAM are the sources of truth.\nFor more information:\n- Read the [GKE RBAC documentation](/kubernetes-engine/docs/how-to/role-based-access-control) .\n- Learn about supported authentication methods when connecting to the Kubernetes API server in [Authenticating to the Kubernetes API server](/kubernetes-engine/docs/how-to/api-server-authentication) .## Control plane security\nIn GKE, the [Kubernetes control plane components](https://kubernetes.io/docs/concepts/overview/components/#control-plane-components) are managed and maintained by Google. The control plane components host the software that runs the Kubernetes control plane, including the API server, scheduler, controller manager and the [etcd database](https://github.com/coreos/etcd) where your Kubernetes configuration is persisted.\nBy default, the control plane components use a public IP address. You can protect the Kubernetes API server by using [authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) , and [private clusters](/kubernetes-engine/docs/how-to/private-clusters) , which allow you to assign a private IP address to the control plane and disable access on the public IP address.\nYou can handle cluster authentication in Google Kubernetes Engine by using IAM as the identity provider. For information on authentication, see [Authenticating to the Kubernetes API server](/kubernetes-engine/docs/how-to/api-server-authentication) .\nAnother way to help secure your control plane is to ensure that you are doing [credential rotation](/kubernetes-engine/docs/how-to/credential-rotation) on a regular basis. When credential rotation is initiated, the SSL certificates and cluster certificate authority are rotated. This process is automated by GKE and also ensures that your control plane IP address rotates.\nFor more information:\n- Read more about [control plane security](/kubernetes-engine/docs/concepts/control-plane-security) .\n- Read the [Role-Based Access Control documentation](/kubernetes-engine/docs/how-to/role-based-access-control) .\n- Follow the [Credential Rotation guide](/kubernetes-engine/docs/how-to/credential-rotation) .## Node security\nGKE deploys your workloads on Compute Engine instances running in your Google Cloud project. These instances are attached to your GKE cluster as [nodes](https://kubernetes.io/docs/concepts/architecture/nodes/) . The following sections show you how to leverage the node-level security features available to you in Google Cloud.\n### Container-Optimized OS\nBy default, GKE [nodes](/kubernetes-engine/docs/concepts/cluster-architecture#nodes) use Google's [Container-Optimized OS](/container-optimized-os/docs/concepts/features-and-benefits) as the operating system on which to run Kubernetes and its components. Container-Optimized OS implements several [advanced features](/container-optimized-os/docs/concepts/security) for enhancing the security of GKE clusters, including:\n- Locked-down firewall\n- Read-only filesystem where possible\n- Limited user accounts and disabled root login\nGKE Autopilot nodes always use Container-Optimized OS as the operating system.\n### Node upgrades\nA best practice is to patch your OS on a regular basis. From time to time, security issues in the container runtime, Kubernetes itself, or the node operating system might require you to upgrade your nodes more urgently. When you upgrade your node, the node's software is upgraded to their latest versions.\nGKE clusters support [automatic upgrades](/kubernetes-engine/docs/concepts/node-auto-upgrades) . In Autopilot clusters, automatic upgrades are always enabled. You can also [manually upgrade](/kubernetes-engine/docs/how-to/upgrading-a-container-cluster) the nodes in a Standard cluster.\n### Protecting nodes from untrusted workloads\nFor clusters that run unknown or untrusted workloads, a good practice is to protect the operating system on the node from the untrusted workload running in a Pod.\nFor example, [multi-tenant clusters](/kubernetes-engine/docs/concepts/multitenancy-overview) such as software-as-a-service (SaaS) providers often execute unknown code submitted by their users. Security research is another application where workloads may need stronger isolation than nodes provide by default.\nYou can enable [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) on your cluster to isolate untrusted workloads in sandboxes on the node. GKE Sandbox is built using [gVisor](https://gvisor.dev/) , an open source project.\n### Securing instance metadata\nGKE uses [instance metadata](/compute/docs/storing-retrieving-metadata) from the underlying Compute Engine instances to provide nodes with credentials and configurations that are used to bootstrap nodes and to connect to the control plane. This metadata contains sensitive information that Pods on the node don't need access to, such as the node's service account key.\nYou can lock down sensitive instance metadata paths by using [workload identity federation for GKE](/kubernetes-engine/docs/concepts/workload-identity) . Workload identity federation for GKE enables the [GKE metadata server](/kubernetes-engine/docs/concepts/workload-identity#metadata_server) in your cluster, which filters requests to sensitive fields such as `kube-env` .\nWorkload identity federation for GKE is always enabled in Autopilot clusters. In Standard clusters, Pods have access to instance metadata unless you manually enable workload identity federation for GKE.\n## Network security\nMost workloads running in GKE need to communicate with other services that could be running either inside or outside of the cluster. You can use several different methods to control what traffic is allowed to flow through your clusters and their Pods.\n### Limiting Pod-to-Pod communication\nBy default, all Pods in a cluster can be reached over the network via their Pod IP address. Similarly, by default, egress traffic allows outbound connections to any address accessible in the VPC into which the cluster was deployed.\nCluster administrators and users can lock down the ingress and egress connections created to and from the Pods in a namespace by using [network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) . By default, when there are no network policies defined, all ingress and egress traffic is allowed to flow into and out of all Pods. Network policies allow you to use tags to define the traffic flowing through your Pods.\nOnce a network policy is applied in a namespace, all traffic is dropped to and from Pods that don't match the configured labels. As part of your creation of clusters and/or namespaces, you can apply [the default deny traffic to both ingress and egress](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic) of every Pod to ensure that all new workloads added to the cluster must explicitly authorize the traffic they require.\nFor more information:\n- Read more about [network policies](/kubernetes-engine/docs/how-to/network-policy) \n- Follow the [network policy tutorial](/kubernetes-engine/docs/tutorials/network-policy) \n- Read more about [default policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies) \n### Filtering load balanced traffic\nTo load balance your Kubernetes Pods with a [network load balancer](/compute/docs/load-balancing/network) , you need to create a Service of type `LoadBalancer` that matches your Pod's labels. With the Service created, you will have an external-facing IP that maps to ports on your Kubernetes Pods. Filtering authorized traffic is achieved at the node level by [kube-proxy](https://kubernetes.io/docs/reference/generated/kube-proxy/) , which filters based on IP address.\nTo configure this filtering, you can use the `loadBalancerSourceRanges` configuration of the Service object. With this configuration parameter, you can provide a list of CIDR ranges that you would like to allow for access to the Service. If you do not configure `loadBalancerSourceRanges` , all addresses are allowed to access the Service via its external IP.\nFor cases in which external access to the Service is not required, consider using an [internal load balancer](/compute/docs/load-balancing/internal) . The internal load balancer also respects the `loadBalancerSourceRanges` when it is necessary to filter out traffic from inside of the VPC.\nFor more information, follow the [internal load balancing tutorial](/kubernetes-engine/docs/how-to/internal-load-balancing) .\n## Securing your workloads\nKubernetes allows users to quickly provision, scale, and update container-based workloads. This section describes tactics that administrators and users can employ to limit the effect a running container can have on other containers in the same cluster, the nodes where containers can run, and the Google Cloud services enabled in users' projects.\n### Limiting Pod container process privileges\nLimiting the privileges of containerized processes is important for the overall security of your cluster. GKE Autopilot clusters always restrict specific privileges, as described in [Autopilot security capabilities](/kubernetes-engine/docs/concepts/autopilot-security#built-in-security) .\nGKE also allows you to set security-related options via the [Security Context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) on both Pods and containers. These settings allow you to change security settings of your processes like:\n- User and group to run as\n- Available Linux capabilities\n- Ability to escalate privileges\nTo enforce these restrictions at the cluster level rather than at the Pod or container levels, use the [PodSecurityAdmission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) . Cluster administrators can use PodSecurityAdmission to ensure that all Pods in a cluster or namespace adhere to a pre-defined policy in the [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) . You can also set custom Pod security policies at the cluster level by using [Gatekeeper](/kubernetes-engine/docs/how-to/pod-security-policies-with-gatekeeper) .\n**Note:** You can't use these policies to override the built-in security configurations in GKE Autopilot.\nThe GKE node operating systems, both Container-Optimized OS and Ubuntu, [apply the default Docker AppArmor security policies](/container-optimized-os/docs/how-to/secure-apparmor#using_the_default_docker_apparmor_security_profile) to all containers started by Kubernetes. You can view the profile's template on [GitHub](https://github.com/moby/moby/blob/master/profiles/apparmor/template.go) . Among other things, the profile **denies** the following abilities to containers:\n- Write files directly in`/proc/`\n- Write to files that are not in a process ID directory (`/proc/<number>`)\n- Write to files in`/proc/sys`other than`/proc/sys/kernel/shm*`\n- Mount filesystems\n**Note:** [SELinux](https://selinuxproject.org/page/Main_Page) is not supported on GKE. Use [AppArmor](/container-optimized-os/docs/how-to/secure-apparmor) instead.\n**Note:** The default Docker AppArmor profile is applied by the container runtime at the node level, which means the profile is applied even if `container.apparmor.security.beta.kubernetes.io/container-name` annotations are missing from a Pod. As a result, security scanners that only examine Kubernetes API resources might falsely identify Pods without this annotation as missing an AppArmor profile, even though the AppArmor profile is applied at the node level.\nFor more information:\n- Read the [Pod Security Context documentation](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) .\n- Learn more about existing protections in the [Container-Optimized OS AppArmor documentation](/container-optimized-os/docs/how-to/secure-apparmor) .\n### Giving Pods access to Google Cloud resources\nYour containers and Pods might need access to other resources in Google Cloud. There are three ways to do this.\nThe most secure way to authorize Pods to access Google Cloud resources is with [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) . Workload identity federation for GKE allows a Kubernetes service account to run as an IAM service account. Pods that run as the Kubernetes service account have the permissions of the IAM service account.\nWorkload identity federation for GKE can be used with [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\nIn Standard clusters, your Pods can also authenticate to Google Cloud using the credentials of the [service account](/compute/docs/access/service-accounts) used by the node's Compute Engine virtual machine (VM).\n**Caution:** Any Pod running in the cluster can use the node's service account credentials from the VM metadata. If you use this method, create and configure a custom service account that has the [minimum IAM roles](/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa) that are required by the Pods running in the cluster.\nThis approach is not compatible with [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) because GKE Sandbox blocks access to the Compute Engine metadata server.\n**Caution:** Service account keys are a security risk if not managed correctly. You should [ choose a more secure alternative to service account keys](/docs/authentication#auth-decision-tree) whenever possible. If you must authenticate with a service account key, you are responsible for the security of the private key and for other management operations such as [key rotation](/iam/docs/key-rotation) . For more information, see [best practices for managingservice account keys](/iam/docs/best-practices-for-managing-service-account-keys) .\nYou can grant credentials for Google Cloud resources to applications by using the [service account key](/iam/docs/creating-managing-service-account-keys) . This approach is strongly discouraged because of the difficulty of securely managing account keys.\nIf you choose this method, use custom IAM service accounts for each application so that applications have the minimal necessary permissions. Grant each service account the minimum IAM roles that are needed for its paired application to operate successfully. Keeping the service accounts application-specific makes it easier to revoke access in the case of a compromise without affecting other applications. After you have assigned your service account the correct IAM roles, you can create a JSON service account key, and then mount the key into your Pod using a Kubernetes [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) .\n### Using Binary Authorization\n[Binary Authorization](/binary-authorization/docs/overview) is a service on Google Cloud that provides software supply-chain security for applications that run in the cloud. Binary Authorization works with images that you deploy to GKE from Artifact Registry or another container image registry.\nWith Binary Authorization enforcement, you can ensure that internal processes that safeguard the quality and integrity of your software have successfully completed before an application is deployed to your production environment. For instructions about creating a cluster with Binary Authorization enabled, visit [Creating a cluster](/binary-authorization/docs/creating-cluster) in the [Binary Authorization documentation](/binary-authorization/docs) .\nWith [Binary Authorization continuous validation (CV)](/binary-authorization/docs/overview-cv) , you can ensure that container images associated with Pods are regularly monitored to ensure that they conform to your evolving internal processes.\n## Audit logging\nAudit logging provides a way for administrators to retain, query, process, and alert on events that occur in your GKE environments. Administrators can use the logged information to do forensic analysis, real-time alerting, or for cataloging how a fleet of GKE clusters are being used and by whom.\nBy default, GKE logs Admin Activity logs. You can optionally also log Data Access events, depending on the types of operations you are interested in inspecting.\nFor more information:\n- Follow the [GKE audit logging tutorial](/kubernetes-engine/docs/how-to/audit-logging) .\n- Read more about [Cloud Audit Logs](/logging/docs/audit) .## Built-in security measures\nGKE enforces specific restrictions on what you can do to system objects in your clusters. When you perform an operation like patching a workload, an admission webhook named validates your request against a set of restricted operations and decides whether to allow the request.\n### Autopilot cluster security measures\nAutopilot clusters apply multiple security settings based on our expertise and industry best practices. For details, see [Security measures in Autopilot](/kubernetes-engine/docs/concepts/autopilot-security#security-measures) .\n### Standard cluster security measures\nStandard clusters are more permissive by default than Autopilot clusters. GKE Standard clusters have the following security settings:\n- You can't update the ServiceAccount used by GKE-managed system workloads, such as workloads in the`kube-system`namespace.\n- You can't bind the`cluster-admin`default ClusterRole to the`system:anonymous`,`system:unauthenticated`, or`system:authenticated`groups.", "guide": "Google Kubernetes Engine (GKE)"}