{"title": "Google Kubernetes Engine (GKE) - Gateway traffic management", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management", "abstract": "# Google Kubernetes Engine (GKE) - Gateway traffic management\nThis page explains how Gateway traffic management works.\n", "content": "## Overview\nGoogle Kubernetes Engine (GKE) networking is built upon [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) . With Cloud Load Balancing, a single anycast IP address delivers global traffic management. [Google's traffic management](/load-balancing/docs/tutorials/about-capacity-optimization-with-global-lb#how_https_load_balancing_works) provides global and regional load balancing, autoscaling, and capacity management to provide equalized, stable, and low latency traffic distribution. Using the [GKE Gateway controller](/kubernetes-engine/docs/concepts/gateway-api) , GKE users can utilize Google's global traffic management control in a declarative and Kubernetes-native manner.\nTo try traffic spillover between clusters, see [Deploying capacity-based load balancing](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#capacity-load-balancing) . To try traffic-based autoscaling, see [Autoscaling based on load balancer traffic](/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling#autoscale-traffic) .\n## Traffic management\nLoad balancing, autoscaling, and capacity management are the foundations of a traffic management system. They operate together to equalize and stabilize system load.\n- **Load balancing** distributes traffic across backend Pods according to location, health, and different load balancing algorithms.\n- **Autoscaling** scales workload replicas to create more capacity to absorb more traffic.\n- **Capacity management** monitors the utilization of Services so that traffic can overflow to backends with capacity rather than impacting application availability or performance.\nThese capabilities can be combined in different ways depending on your goals. For example:\n- If you want to take advantage of low-cost [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) , you might want to optimize for evenly distributing traffic across Spot VMs at the cost of latency. Using load balancing and capacity management, GKE would overflow traffic between regions based on capacity so that Spot VMs are fully utilized wherever they are available.\n- If you want to optimize user latency at the cost of over-provisioning, you could deploy GKE clusters in many regions and increase capacity dynamically wherever load increases. Using load balancing and autoscaling GKE would autoscale the number of Pods when traffic spikes so that traffic does not have to overflow over to other regions. Regions would grow in capacity so that they are able to fully handle load as close as possible to users.\nThe following diagram shows load balancing, autoscaling, and capacity management operating together:\nIn the diagram, the workload in the `gke-us` cluster has failed. Load balancing and health checking drains active connections and redirects traffic to the next closest cluster. The workload in `gke-asia` receives more traffic than it has capacity for, so it sheds load to `gke-eu` . The `gke-eu` receives more load than typical because of events in `gke-us` and `gke-asia` and so `gke-eu` autoscales to increase its traffic capacity.\nTo learn more about how Cloud Load Balancing handles traffic management, see [global capacity management](/load-balancing/about-capacity-optimization-with-global-lb#how_https_load_balancing_works) .\n### Traffic management capabilities\nGateway, HTTPRoute, Service, and Policy resources provide the controls to manage traffic in GKE. The [GKE Gateway controller](/kubernetes-engine/docs/concepts/gateway-api) is the control plane that monitors these resources.\nThe following traffic management capabilities are available when deploying Services in GKE:\n- **Service capacity** : the ability to specify the amount of traffic capacity that a Service can receive before Pods are autoscaled or traffic overflows to other available clusters.\n- **Traffic-based autoscaling** : autoscaling Pods within a Service based on HTTP requests received per second.\n- **Multi-cluster load balancing** : the ability to load balance to Services hosted across multiple GKE clusters or multiple regions.\n- **Traffic splitting** : explicit, weight-based traffic distribution across backends. Traffic splitting is supported with single-cluster Gateways in GA.\n### Traffic management support\nThe available traffic management capabilities depend on the GatewayClass that you deploy. For a complete list of feature support, see [GatewayClass capabilities](/kubernetes-engine/docs/how-to/gatewayclass-capabilities) . The following table summarizes GatewayClass support for traffic management:\n| GatewayClass      | Service capacity | Traffic autoscaling | Multi-cluster load balancing | Traffic splitting1 |\n|:------------------------------------|-------------------:|----------------------:|-------------------------------:|---------------------:|\n| gke-l7-global-external-managed  |    nan |     nan |       nan |     nan |\n| gke-l7-regional-external-managed |    nan |     nan |       nan |     nan |\n| gke-l7-rilb       |    nan |     nan |       nan |     nan |\n| gke-l7-gxlb       |    nan |     nan |       nan |     nan |\n| gke-l7-global-external-managed-mc |    nan |     nan |       nan |     nan |\n| gke-l7-regional-external-managed-mc |    nan |     nan |       nan |     nan |\n| gke-l7-rilb-mc      |    nan |     nan |       nan |     nan |\n| gke-l7-gxlb-mc      |    nan |     nan |       nan |     nan |\nTraffic splitting is supported with single-cluster Gateways in GA.\n### Global, regional, and zonal load balancing\nService capacity, location, and health all determine how much traffic the load balancer sends to a given backend. Load balancing decisions are made at the following levels, starting with global for global load balancers and regional for regional load balancers:\n- **Global** : traffic is sent to the closest Google Cloud region to the client that has healthy backends with capacity. As long as the region has capacity, it receives all of its closest traffic. If a region does not have capacity, excess traffic overflows to the next closest region with capacity. To learn more, see [global load balancing](#global) .\n- **Regional** : traffic is sent by the load balancer to a specific region. The traffic is load balanced across zones in proportion to the zone's available serving capacity. To learn more, see [regional load balancing](#regional) .\n- **Zonal** : after traffic is determined for a specific zone, the load balancer distributes traffic evenly across backends within that zone. Existing TCP connections and session persistence settings are preserved, so that future requests go to the same backends, as long as the backend Pod is healthy. To learn more, see [zonal load balancing](#zonal) .To try the following concepts in your own cluster, see [Capacity-based load balancing](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#capacity-load-balancing) .\nUnder normal conditions, traffic is sent to the closest backend to the client. Traffic terminates at the closest Google point of presence (PoP) to the client and then traverses the Google backbone until it reaches the closest backend, as determined by network latency. When the backends in a region do not have remaining capacity, traffic overflows to the next closest cluster with healthy backends that have capacity. If less than 50% of backend Pods within a zone are unhealthy, then traffic gradually fails over to other zones or regions, independent of the configured capacity.\nTraffic overflow only occurs under the following conditions:\n- You are using a [multi-cluster Gateway](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways) .\n- You have the same Service deployed across multiple clusters, served by the multi-cluster Gateway.\n- You have Service capacities configured such that traffic exceeds service capacities in one cluster, but not others.\nThe following diagram demonstrates how global load balancing works with traffic overflow:\nIn the diagram:\n- A multi-cluster Gateway provides global internet load balancing for the`store`Service. The service is deployed across two GKE clusters, one in`us-west1`and another in`europe-west1`. Each cluster is running 2 replicas.\n- Each Service is configured with`max-rate-per-endpoint=\"10\"`, which means that each Service has a total capacity 2 replicas * 10 RPS = 20 RPS in each cluster.\n- Google PoPs in North America receive 6 RPS. All traffic is sent to the nearest healthy backend with capacity, the GKE cluster in`us-west1`.\n- European PoPs receive 30 cumulative RPS. The closest backends are in`europe-west1`, but they only have 20 RPS of capacity. Because the backends in`us-west1`have excess capacity, 10 RPS overflows to`us-west1`so that it receives 16 RPS in total and distributes 8 RPS to each pod.\n**Note:** If you are using single-cluster Gateways, a Service of type `LoadBalancer` , or Ingress resources, then this global load balancing example does not apply since these resources only load balance to a single cluster within a single region. Multi-cluster Gateways are recommended if you have global load balancing requirements.\nTraffic overflow helps prevent exceeding application capacity that can impact performance or availability.\nHowever, you might not want to overflow traffic. Latency-sensitive applications, for example, might not benefit from traffic overflow to a much more distant backend.\nYou can use any of following methods to prevent traffic overflow:\n- Use only single-cluster Gateways which can host Services in only a single cluster.\n- Even if using multi-cluster Gateways, replicas of an application deployed across multiple clusters can be deployed as separate Services. From the perspective of the Gateway, this enables multi-cluster load balancing, but does not aggregate all endpoints of a Service between clusters.\n- Set Service capacities at a high enough level that traffic capacity is never realistically exceeded unless absolutely necessary.Within a region, traffic is distributed across zones according to the available capacities of the backends. This is not using overflow, but rather load balancing in direct proportion to the Service capacities in each zone. Any individual flow or session is always sent to a single, consistent backend Pod and is not split.\nThe following diagram shows how traffic is distributed within a region:\nIn the diagram:\n- A Service is deployed in a regional GKE cluster. The Service has 4 Pods which are deployed unevenly across zones. 3 Pods are in zone A, 1 Pod is in zone B, and 0 Pods are in zone C.\n- The Service is configured with`max-rate-per-endpoint=\"10\"`. Zone A has 30 RPS of total capacity, zone B has 10 RPS of total capacity, and zone C has 0 RPS of total capacity, because it has no Pods.\n- The Gateway receives a total of 16 RPS of traffic from different clients. This traffic is distributed across zones in proportion to the remaining capacity in each zone.\n- Traffic flow from any individual source or client is consistently load balanced to a single backend Pod according to the session persistence settings. The distribution of traffic splits across different source traffic flows so that any individual flows are never split. As a result, a minimum amount of source or client diversity is required to granularly distribute traffic across backends.\nFor example, if the incoming traffic spikes from 16 RPS to 60 RPS, either of the following scenarios would occur:\n- If using single-cluster Gateways, then there are no other clusters or regions for this traffic to overflow to. Traffic continues to be distributed according to the relative zonal capacities, even if incoming traffic exceeds the total capacity. As a result, zone A receives 45 RPS and zone B receives 15 RPS.\n- If using multi-cluster Gateways with Services distributed across multiple clusters, then traffic can overflow to other clusters and other regions as described in [Global load balancing and traffic overflow](#global) . Zone A receives 30 RPS, zone B receives 10 RPS, and 20 RPS overflows to another cluster.\n**Note:** Cloud Load Balancing does not automatically drop traffic when backends are over-capacity. Traffic overflows to any available backends that have capacity. If all backends are over-capacity, then the overflow traffic is distributed across all backends in proportion to their total capacities. This exceeds the total Service capacity.\nOnce traffic has been sent to a zone, it is distributed evenly across all the backends within that zone. HTTP sessions are persistent depending on the session affinity setting. Unless the backend becomes unavailable, existing TCP connections never move to a different backend. This means that long-lived connections continue going to the same backend Pod even if new connections overflow because of limited capacity. The load balancer prioritizes maintaining existing connections over new ones.\n## Service capacity\nWith service capacity, you can define a Requests per Second (RPS) value per Pod in a Service. This value represents the maximum RPS per-Pod on average that a Service can receive. This value is configurable on Services and is used to determine traffic-based autoscaling and capacity-based load balancing.\n### Requirements\nService capacity has the following requirements and limitations:\n- Only supported with the GatewayClass resources and Ingress types defined in [Traffic management support](#traffic_management_support) .\n- Only impacts load balancing if you are using traffic-based autoscaling or multi-cluster Gateways. If you are not using these capabilities, Service capacity has no effect on network traffic.\n### Configure Service capacity\nTo configure Service capacity, create a Service using the annotation `networking.gke.io/max-rate-per-endpoint` . The following manifest describes a Service with a maximum RPS:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/max-rate-per-endpoint: \"RATE_PER_SECOND\"spec:\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080\u00a0 \u00a0 name: http\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 type: ClusterIP\n```\nReplace `` with the maximum HTTP/HTTPS requests per second that a single Pod in this Service should receive.\nThe `max-rate-per-endpoint` value creates a dynamic capacity for a Service based on the number of Pods in the Service. The total Service capacity value is calculated by multiplying the `max-rate-per-endpoint` value with the number of replicas, as described in the following formula:\n```\nTotal Service capacity = max-rate-per-endpoint * number of replicas\n```\nIf an autoscaler scales up the number of Pods within a Service, then the Service's total capacity is computed accordingly. If a Service is scaled down to zero Pods, then it has zero capacity and does not receive any traffic from the load balancer.\n**Note:** Service capacity configuration using the `max-rate-per-endpoint` annotation only applies to traffic ingressing through a [supported Gateway](#traffic_management_support) . It does not influence capacity-based load balancing for ClusterIP load balancing inside the cluster or for Services of type `LoadBalancer` .\n### Service capacity and standalone NEGs\nService capacity can also be configured when using [standalone NEGs](/kubernetes-engine/docs/how-to/standalone-neg) , however it does not use the `max-rate-per-endpoint` annotation. When using standalone NEGs, the `max-rate-per-endpoint` is configured manually when adding the NEG to a Backend Service resource. Using the [gcloud compute backend-services add- backend](/sdk/gcloud/reference/compute/backend-services/add-backend) command, the `--max-rate-per-endpoint` flag can configure capacity for each NEG individually.\nThis can be useful for any of the following workflows:\n- When deploying internal and external load balancers manually using standalone NEGs\n- When deploying [Traffic Director on GKE using standalone NEGs](/traffic-director/docs/set-up-gke-pods-auto#deploying_a_kubernetes_service_for_testing) \nThere is no functional difference when configuring service capacity with standalone NEGs. Both traffic autoscaling and traffic spillover are supported.\n### Determine your Service's capacity\nDetermining the value for `max-rate-per-endpoint` requires an understanding of your applications performance characteristics and your load balancing goals. The following strategies can help you define your application performance characteristics:\n- Observe your application in both test and production environments when configured without Service capacity.\n- Use [Cloud Monitoring](/monitoring/docs) to create a correlation between traffic requests and your performance service level objectives (SLOs).\n- Use [load balancer metrics](/load-balancing/docs/https/https-logging-monitoring#monitoring_metrics) , such as`https`or`request_count`to map RPS levels.\n- Define what your performance SLOs are for your application. They might be one or more of the following, depending on what you consider \"bad\" or \"unstable\" performance. All of the following can be gathered from Cloud Monitoring load balancer metrics:- Response error codes\n- Response or total latency\n- Backend unhealthiness or downtime\n- Observe your application under traffic load in both test and production environments. In test environments, stress your application under increasing request load so you can see how the different performance metrics are impacted as traffic increases. In production environments, observe realistic traffic patterns levels.\n### Default Service capacity\nAll Services attached to GKE resources have a default Service capacity configured even if it isn't explicitly configured using the annotation. To learn more, see [Default service capacity](#default) .\nThe following table describes the default capacities:\n| Load balancing resource type | Default max-rate-per-endpoint |\n|:--------------------------------|:--------------------------------|\n| Ingress (internal and external) | 1 RPS       |\n| Gateway (all GatewayClasses) | 100,000,000 RPS     |\n| MultiClusterIngress    | 100,000,000 RPS     |\n## Traffic-based autoscaling\nTraffic-based autoscaling is a capability of GKE that natively integrates traffic signals from load balancers to autoscale Pods. Traffic-based autoscaling is only supported for single-cluster Gateways.\n[Autoscaling based on load balancer traffic](/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling#autoscale-traffic)\nTraffic-based autoscaling provides the following benefits:\n- Applications which are not strictly CPU or memory bound might have capacity limits which are not reflected in their CPU or memory usage.\n- Traffic, or requests per second (RPS) is an easier metric to understand in some cases because it is more aligned with app usage and business metrics such as page views or daily active users (DAUs).\n- Traffic is a leading indicator that represents instantaneous demand compared with CPU or memory which are lagging indicators.\n- The combination of CPU, memory, and traffic autoscaling metrics provides a holistic way of autoscaling applications that uses multiple dimensions to ensure that capacity is appropriately provisioned.\nThe following diagram demonstrates how traffic-based autoscaling works:\nIn the diagram:\n- The Service owner configures Service capacity and a target utilization for the Deployment.\n- The Gateway receives traffic from clients going to the`store`Service. The Gateway sends utilization telemetry to the GKE Pod Autoscaler. Utilization is equal to the actual traffic received by an individual Pod divided by the Pod's configured capacity.\n- The GKE Pod Autoscaler scales Pods up or down according to the configured target utilization.\n### Autoscaling behavior\nThe following diagram shows how traffic-based autoscaling works on an application receiving 10 RPS through the load balancer:\nIn the diagram, the service owner has configured the capacity of the store Service to 10 RPS, which means that each Pod can receive a maximum of 10 RPS. The HorizontalPodAutoscaler is configured with `averageUtilization` is set to `70` , which means that the target utilization is 70% of 10 RPS per Pod.\nThe autoscaler attempts to scale replicas to achieve the following equation:\n```\nreplicas = ceiling[ current traffic / ( averageUtilization * max-rate-per-endpoint) ]\n```\nIn the diagram, this equation computes to:\n```\nceiling[ 10 rps / (0.7 * 10 rps) ] = ceiling[ 1.4 ] = 2 replicas\n```\n10 RPS of traffic results in 2 replicas. Each replica receives 6 RPS, which is under the target utilization of 7 RPS.\n**Note:** Changes in traffic levels or traffic spikes can cause the autoscaler to approximate the target replicas equation. In steady-state conditions, the amount of replicas approach the target but it might not always be exact, especially with fluctuating traffic.\n## Traffic splitting\nTraffic splitting uses an explicit ratio, called a **weight** , that defines the proportion of HTTP requests that are sent to a Service. HTTPRoute resources let you configure weights on a list of Services. The relative weights between Services define the split of traffic between them. This is useful for splitting traffic during rollouts, canarying changes, or for emergencies.\nThe following diagram describes an example traffic splitting configuration:\nIn the diagram:\n- The Service owner configures two services for a single route, with a rule splitting traffic 90% to`store-v1`and 10% to`store-v2`.\n- The Gateway receives traffic from clients going to the URL of the store application and traffic is split according to the configured rule. 90% of traffic routes to`store-v1`and 10% routes to`store-v2`.\nTraffic splitting is supported between Services in the same cluster and also between Services in different clusters:\n- **Traffic splitting between Services** : used for splitting traffic for application version rollouts. Using the traffic splitting example, you would have two separate Deployments, `store-v1` and `store-v2` , which each have their own Service, `store-v1` and `store-v2` . Weights are configured between the two Services to gradually shift traffic until `store-v2` is fully rolled out. **Note:** The `gke-l7-gxlb` GatewayClass does not support traffic splitting.\n- **Traffic splitting between ServiceImports** : used for shifting traffic to or from specific clusters for maintenance, migration, or emergencies. ServiceImports represent multi-cluster Services and enable traffic splitting between different Services on different clusters. The exercise [Blue-green, multi-cluster routing with Gateway](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#blue-green) demonstrates splitting traffic across clusters. **Note:** The `gke-l7-gxlb-mc` GatewayClass does not support traffic splitting.\n### Weight vs capacity\nWeights and capacities both control how much traffic is sent to different Services. While they have similar effects, they operate differently and have different use cases. They can and should be used together, though for different purposes.\nWeight is an **explicit** control of traffic. It defines the exact proportions of traffic, independent of incoming traffic and backend utilization. In the [traffic splitting example](#splitting) , if `store-v2` was over-capacity, or if all of its replicas failed, 10% of the traffic would still be allocated to `store-v2` , potentially causing traffic to be dropped. That is because weight does not change the proportion of traffic based on utilization or health.\nWeight is best suited for the following use cases:\n- Shifting traffic between different versions of a service for rollouts.\n- Manually onboarding services using explicit traffic splits.\n- Shifting traffic away from a set of backends for emergency or maintenance purposes.Capacity is an **implicit** control of traffic. It defines the proportions of traffic indirectly as they depend on the amount of incoming traffic, backend utilization, and the source location of traffic. Capacity is an inherent property of a Service and is typically updated much less frequently.\nCapacity is best suited for the following use cases:\n- Preventing backend over-utilization during traffic spikes.\n- Controlling the rate of autoscaling with respect to traffic.\nConfiguring Service capacity to overflow traffic may not always be a behavior that you want. Consider the [global load balancing example](#global) . Service capacity protects backends from over-utilization by overflowing traffic, but this might result in extra latency for the requests that have overflowed, since those requests are traveling to a more remote region.\nIf your application is not very sensitive to overutilization then you might want to configure a very high Service capacity so that traffic is unlikely to ever overflow to another region. If your application's availability or latency is sensitive to overutilization, then overflowing traffic to other clusters or regions may be better than absorbing excess traffic on over-utilized backends. To learn more about how to configure Service capacity for your application, see [Determine your Service's capacity](#capacity) .\n## What's next\n- Learn about [Deploying Gateways](/kubernetes-engine/docs/how-to/deploying-gateways) .\n- Learn about [Deploying multi-cluster Gateways](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways) .", "guide": "Google Kubernetes Engine (GKE)"}