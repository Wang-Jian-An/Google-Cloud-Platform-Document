{"title": "Google Kubernetes Engine (GKE) - Migrate to multi-cluster networking with Google Kubernetes Engine (GKE) Autopilot and Standard clusters", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/migrate-gke-multi-cluster", "abstract": "# Google Kubernetes Engine (GKE) - Migrate to multi-cluster networking with Google Kubernetes Engine (GKE) Autopilot and Standard clusters\nLast reviewed 2022-02-17 UTC\nMulti-cluster networking is a valuable tool that enables use cases like regional high availability, globally distributed proximity to users for lower latency, and organizational isolation between teams. [Google Kubernetes Engine (GKE)](/kubernetes-engine) provides built-in capabilities for multi-cluster networking that you can enable and use at scale across a fleet of GKE clusters. This feature also lets you combine, or migrate deployed infrastructure between, GKE Standard and Autopilot to meet the architectural needs of each application.\nThis document demonstrates these features through several deployment topologies. You learn how to take an application deployed in a single GKE cluster and migrate it to a multi-cluster deployment across GKE Standard and Autopilot clusters. You use GKE [multi-cluster Services](/kubernetes-engine/docs/concepts/multi-cluster-services) for east-west traffic and [multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) to enable multi-cluster north-south networking.\nThe document is for cloud architects and operations teams that use, or plan to use, GKE to deploy services across multiple Kubernetes clusters. This document assumes that you're familiar with [Kubernetes](https://kubernetes.io/docs/home/) .", "content": "## GKE Standard and GKE AutopilotGKE provides a managed Kubernetes deployment with a full feature set including a high availability control plane. GKE clusters can be started quickly, and scale to up to 15,000 nodes. With GKE [Autopilot](/kubernetes-engine/docs/concepts/autopilot-overview) clusters, Google manages the infrastructure, including the control plane and the nodes. If you want to configure and manage your nodes instead, GKE provides [Standard mode](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-standard) .\nFor more information on the differences between modes, see [Choose a cluster mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .## Multi-cluster Services and multi-cluster GatewaysKubernetes can run using a single control plane across disparate cloud zones to provide resiliency and higher availability for your services. GKE takes this one step further and provides GKE [multi-cluster Services](/kubernetes-engine/docs/concepts/multi-cluster-services) (MCS) that provides a cross-cluster service discovery and invocation mechanism. Services that use this feature are discoverable and accessible across clusters with a virtual IP, which matches the behavior of a `ClusterIP` Service accessible in a cluster. This approach allows for the following benefits:- Services can be load-balanced across multiple clusters in the same region or different regions (east-west traffic).\n- Cross-region service high availability options are achievable.\n- Stateful and stateless workloads can be deployed and managed across separate clusters.\n- Shared services are available across clusters.\nTo learn more about how to deploy MCS, see [Configuring multi-cluster Services](/kubernetes-engine/docs/how-to/multi-cluster-services) .\nGKE provides an implementation of the Kubernetes Gateway API that uses the [GKE Gateway controller](/kubernetes-engine/docs/concepts/gateway-api#gateway_controller) . Gateway allows GKE to deploy Google Cloud load balancers to provide inbound (north-south) traffic routing for services deployed on GKE. GKE also provides [multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) (MCG) which extend the GKE Gateway controller to provision load balancers that route traffic to services deployed on disparate GKE clusters.\nThe following diagram shows how, when you combine MCS and MCG, you are able to manage the complementary aspects of service deployment and traffic routing from a single control plane:For more information, see [Deploying multi-cluster Gateways](/kubernetes-engine/docs/how-to/multi-cluster-services) .\n **Note:** In a multi-cluster scenario where clusters belong to different custom subnets and are registered to the same fleet, you must create a firewall rule to allow cross-cluster traffic. For more information, see [create firewall rule](/service-mesh/docs/unified-install/gke-install-multi-cluster#create_firewall_rule) .## Overview of migrationGKE multi-cluster networking capabilities benefit workloads of various profiles. For example, you might have stateless components with bursty traffic that you wish to move to Autopilot because of its more efficient cost model.\nOr, you might want to place your application frontends closer to users. This approach provides lower latency and caching that improves application performance and user experience. At the same time, you might have some stateful components that your application relies on that can only reside in one location. This configuration requires north-south, multi-cluster load balancing to send client traffic to the correct cluster in that location. You also need east-west, multi-cluster load balancing to send traffic between clusters to reach the stateful components.\nThis document uses the [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo) cloud microservices demo application to demonstrate a multi-cluster pattern that can be used to enhance the deployment of the single zone demo. You start with a single-zone version of the application. You then add elements of high availability and resilience by using multi-cluster Services and multi-cluster Gateways, and reduce operational toil by taking advantage of Autopilot.\n### Initial single-cluster deploymentIn the following diagram, the Online Boutique application is initially deployed to a single GKE Standard mode cluster named , and is exposed by using a `LoadBalancer` Service:\n### Migrate to multi-cluster ServicesIn the next intermediate step, you create two additional clusters, and the stateless services are deployed in additional regions. You create two GKE Autopilot clusters named and in two separate regions distinct from the single GKE Standard cluster, and register the clusters to the [Google Cloud fleet](/anthos/fleet-management/docs) .\nFleets are a Google Cloud concept for logically organizing clusters and other resources, and let you use and manage multi-cluster capabilities and apply consistent policies across your systems.\nYou export the on the cluster in the namespace to the new fleet clusters by using [ServiceExport](/kubernetes-engine/docs/how-to/multi-cluster-services#registering_a_service_for_export) . You deploy Online Boutique's Service on all three clusters and expose it through a [ClusterIP](/kubernetes-engine/docs/concepts/service#services_of_type_clusterip) service. You then export the service to the fleet using `ServiceExports` . Services such as the middleware layer of the Online Boutique (like , , and ) are also deployed to all three clusters.\nA `Pod` that runs in any cluster in the fleet can access an exported `Service` by sending a request to the `ClusterSet` URI for that service. The request routes to an endpoint that backs the service.\nThe Service is able to consume the middleware services (such as or ) locally in the same cluster. This architecture helps keep incoming requests local to the regions whose frontend responds to the request, and avoids unnecessary inter-region network traffic charges.\nThe following diagram illustrates the two multi-cluster Services. The stateless Service is deployed to three clusters, and the stateful backend is deployed to one cluster. The diagram also shows that in this intermediate step, inbound traffic for the frontend service remains routed to the original GKE Standard cluster in using an external passthrough Network Load Balancer created by the frontend-external `LoadBalancer` Service:\n### Migrate to multi-cluster GatewayIn the final step, you route inbound traffic for the Service from external client requests to services in multiple clusters in the fleet using a multi-cluster Gateway.\nA fourth cluster named is added to the fleet to host and manage the configuration for the [Gateway](/kubernetes-engine/docs/concepts/gateway-api) and [HTTPRoute](https://gateway-api.sigs.k8s.io/api-types/httproute/) resources that are created as part of this configuration. The `HTTPRoute` resource maps the prefix to the frontend [ServiceImport](/kubernetes-engine/docs/how-to/multi-cluster-services#consuming_cross-cluster_services) . Traffic for the Online Boutique's frontend is sent to a healthy endpoint in one of the available regions. This approach adds elements of high availability to the Online Boutique application architecture.\nIn the following diagram, the multi-cluster Gateway deploys a Global Cloud Load Balancer that routes external traffic to the stateless Service deployed on each of the three application clusters in the fleet.In the final state, this opinionated pattern demonstrates loose coupling between the stateful ( and ) and stateless portions of the application ( , , , , , , , , and ). While outside the scope of this document, this approach gives you a future opportunity to add resilience and high availability to the stateful services layer.## Objectives\n- Create and configure GKE Standard and Autopilot clusters.\n- Deploy Online Boutique to a zonal GKE Standard cluster.\n- Export multi-cluster`Services`.\n- Deploy manifests to Standard and Autopilot clusters.\n- Enable and configure multi-cluster Gateways.\n- Test multi-region application behavior.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Compute Engine](/compute/all-pricing) for [GKE worker nodes](/kubernetes-engine/docs/concepts/cluster-architecture#nodes) \n- [Cloud Load Balancing](/vpc/network-pricing#lb) \n- [Multi Cluster Ingress](/kubernetes-engine/pricing#multi-cluster-ingress) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you beginSecurity constraints defined by your organization might prevent you from completing the following steps. For troubleshooting information, see [Develop applications in a constrained Google Cloud environment](/resource-manager/docs/organization-policy/develop-apps-constrained-environment) .\nBefore you begin, ensure you have the following requirements met:- It is recommended that you use a new project for this guide, as the easiest way to clean up is to delete the project once you are finished.\n- This guide assumes you have therole for your Google Cloud project. For production or real-world settings, it is best practice to scope permissions to least privilege. For more information, see [Using IAM securely](/iam/docs/using-iam-securely) and [Manage identity and access](/architecture/framework/security/identity-access) .\n- Familiarize yourself with the [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo) microservices demo application architecture.\n## Prepare the environmentIn this guide, you use [Cloud Shell](/shell/docs/features) to enter commands. Cloud Shell gives you access to the command line in the Google Cloud console and includes Google Cloud SDK and other tools, such as the [Google Cloud CLI](/sdk/gcloud) . Cloud Shell appears as a window at the bottom of the Google Cloud console. It can take several minutes to initialize, but the window appears immediately.- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- In Cloud Shell, define environment variables used in this guide. Replace with your own project ID:```\nexport PROJECT=PROJECT_IDgcloud config set project ${PROJECT}\n```\n- Enable the services required for this document:```\ngcloud services enable \\\u00a0 \u00a0 gkehub.googleapis.com \\\u00a0 \u00a0 multiclusteringress.googleapis.com \\\u00a0 \u00a0 dns.googleapis.com \\\u00a0 \u00a0 trafficdirector.googleapis.com \\\u00a0 \u00a0 cloudresourcemanager.googleapis.com \\\u00a0 \u00a0 multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 container.googleapis.comgcloud container fleet multi-cluster-services enable\n```Multi-cluster Services manages Google Cloud components like Cloud DNS, firewall rules, and Traffic Director, so these APIs must also be enabled. For more information, see [Traffic Director overview](/traffic-director/docs/overview#multi-cluster_kubernetes) .The output is similar to the following example:```\nOperation \"operations/acf.p2-822685001869-ee4ebe78-6dd8-465e-b0fd-3b0e5f964bad\"\nfinished successfully.\nWaiting for Feature Multi-cluster Services to be created...done.\n```\n- Verify that multi-cluster Services shows the state:```\ngcloud container fleet multi-cluster-services describe\n```The output is similar to the following example:```\ncreateTime: '2021-11-30T21:59:25.245190894Z'\nname: projects/PROJECT_ID/locations/global/features/multiclusterservicediscovery\nresourceState:\n state: ACTIVE\nspec: {}\nupdateTime: '2021-11-30T21:59:27.459063070Z'\n```If the value of is not , see the [troubleshooting section](/kubernetes-engine/docs/how-to/multi-cluster-services#troubleshooting) of the MCS documentation.\n## Create and configure GKE clustersTo demonstrate the multi-cluster pattern in this guide, you use three in three separate cloud regions, and one cluster to for Gateway resources. You register all clusters with the fleet associated with your project. A Google Cloud project can only have a single fleet associated with it. This project is known as the fleet host project.- Create Standard and Autopilot GKE clusters:```\ngcloud container clusters create std-west \\\u00a0 \u00a0 --zone us-west1-a \\\u00a0 \u00a0 --num-nodes=6 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --release-channel regular \\\u00a0 \u00a0 --workload-pool=${PROJECT}.svc.id.goog \\\u00a0 \u00a0 --asyncgcloud container clusters create-auto auto-east \\\u00a0 \u00a0 --region us-east1 \\\u00a0 \u00a0 --release-channel regular \\\u00a0 \u00a0 --asyncgcloud container clusters create-auto auto-central \\\u00a0 \u00a0 --region us-central1 \\\u00a0 \u00a0 --release-channel regular \\\u00a0 \u00a0 --asyncgcloud container clusters create config-central \\\u00a0 \u00a0 --region us-central1 \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --release-channel regular \\\u00a0 \u00a0 --workload-pool=${PROJECT}.svc.id.goog \\\u00a0 \u00a0 --async\n```workload identity federation for GKE is enabled by default on GKE Autopilot clusters so you don't have to use the `--workload-pool` flag when you create those clusters like you do with the GKE Standard clusters.\n- Wait for the clusters' to change from to . This process can take up to 10 minutes. You can monitor the progress by using a watch loop while you grab a cup of caffeine or do some light stretching exercises before you proceed with the rest of the document:```\nwatch -n 20 --difference=permanent \"gcloud container clusters list\"\n```The output is similar to the following example:```\nNAME: auto-central\nLOCATION: us-central1\nMASTER_VERSION: 1.21.5-gke.1802\nMASTER_IP: 107.178.213.138\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.21.5-gke.1802\nNUM_NODES: 3\nSTATUS: PROVISIONING\nNAME: config-central\nLOCATION: us-central1\nMASTER_VERSION: 1.21.5-gke.1802\nMASTER_IP:\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.21.5-gke.1802\nNUM_NODES: 9\nSTATUS: PROVISIONING\nNAME: auto-east\nLOCATION: us-east1\nMASTER_VERSION: 1.21.5-gke.1802\nMASTER_IP: 35.229.88.209\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.21.5-gke.1802\nNUM_NODES: 3\nSTATUS: PROVISIONING\nNAME: std-west\nLOCATION: us-west1-a\nMASTER_VERSION: 1.21.5-gke.1802\nMASTER_IP: 35.197.93.113\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.21.5-gke.1802\nNUM_NODES: 6\nSTATUS: PROVISIONING\n```\n- After all the clusters are in the state, press `CTRL-C` to interrupt the command.\n- Add an Identity and Access Management (IAM) policy binding granting the fleet host project MCS service account the role for its own project:```\ngcloud projects add-iam-policy-binding ${PROJECT} \\\u00a0 \u00a0 --member \"serviceAccount:${PROJECT}.svc.id.goog[gke-mcs/gke-mcs-importer]\" \\\u00a0 \u00a0 --role \"roles/compute.networkViewer\"\n```You use workload identity federation for GKE to grant the MCS service read access to your project VPC network configuration. As a result, the fleet host project's MCS service account needs this role.The output is similar to the following example:```\n- members:\n - serviceAccount:PROJECT_ID.svc.id.goog[gke-mcs/gke-mcs-importer]\n role: roles/compute.networkViewer\n[...]\n```\n- Register the GKE Standard and Autopilot clusters to your project's fleet. See [Registering a cluster](/anthos/fleet-management/docs/register/gke) for more details. This step can take up to 5 minutes:```\ngcloud container fleet memberships register std-west \\\u00a0 \u00a0 --gke-cluster us-west1-a/std-west \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=${PROJECT}gcloud container fleet memberships register auto-east \\\u00a0 \u00a0 --gke-cluster us-east1/auto-east \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=${PROJECT}gcloud container fleet memberships register auto-central \\\u00a0 \u00a0 --gke-cluster us-central1/auto-central \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=${PROJECT}gcloud container fleet memberships register config-central \\\u00a0 \u00a0 --gke-cluster us-central1/config-central \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=${PROJECT}\n```For each command, the output is similar to the following example:```\nWaiting for membership to be created...done.\nCreated a new membership [projects/PROJECT_ID/locations/global/memberships/std-west] for the cluster [std-west]\nGenerating the Connect Agent manifest...\nDeploying the Connect Agent on cluster [std-west] in namespace [gke-connect]...\nDeployed the Connect Agent on cluster [std-west] in namespace [gke-connect].\nFinished registering the cluster [std-west] with the Hub.\n```\n- Connect to the clusters and generate kubeconfig entries:```\ngcloud container clusters get-credentials std-west \\\u00a0 \u00a0 --zone us-west1-a --project $PROJECTgcloud container clusters get-credentials auto-east \\\u00a0 \u00a0 --region us-east1 --project $PROJECTgcloud container clusters get-credentials auto-central \\\u00a0 \u00a0 --region us-central1 --project $PROJECTgcloud container clusters get-credentials config-central \\\u00a0 \u00a0 --region us-central1 --project $PROJECT\n```For each command, the output is similar to the following example:```\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for std-west.\n```\n- Rename contexts for clusters so that they are easier to work with in the rest of this document:```\nkubectl config rename-context \\\u00a0 \u00a0 gke_${PROJECT}_us-west1-a_std-west \\\u00a0 \u00a0 std-westkubectl config rename-context \\\u00a0 \u00a0 gke_${PROJECT}_us-east1_auto-east \\\u00a0 \u00a0 auto-eastkubectl config rename-context \\\u00a0 \u00a0 gke_${PROJECT}_us-central1_auto-central \\\u00a0 \u00a0 auto-centralkubectl config rename-context \\\u00a0 \u00a0 gke_${PROJECT}_us-central1_config-central \\\u00a0 \u00a0 config-central\n```In this guide, the contexts are named based on their location. Although you can provide alternate names, the remaining steps in this guide use the names used in this step.\n## Deploy Online Boutique on GKE StandardIn the first step of the demonstration deployment, you deploy the full set of Online Boutique application services to the single GKE Standard cluster in .- Create the namespace on :```\nkubectl create namespace onlineboutique --context std-west\n```The output is similar to the following example:```\nnamespace/onlineboutique created\n```\n- Clone the Online Boutique GitHub repository and set up a variable:```\ncd ~git clone --branch release/v0.4.1 \\\u00a0 \u00a0 https://github.com/GoogleCloudPlatform/microservices-demo.gitcd microservices-demo/release && export WORKDIR=`pwd`\n```\n- Deploy Online Boutique on . This process creates `Deployments` and `Services` for all of Online Boutique's microservices, and includes a [LoadBalancer](/kubernetes-engine/docs/concepts/service#services_of_type_loadbalancer) type Service which externally exposes Online Boutique's frontend service:```\ncd $WORKDIRkubectl apply -f kubernetes-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=std-west\n```\n- Wait for the `LoadBalancer` Service to get an external IP:```\nwatch -n 20 --difference=permanent \\\u00a0 \u00a0 \u00a0\"kubectl get svc frontend-external -n onlineboutique --context=std-west\"\n```Initially, the output is similar to the following example:```\nNAME    TYPE   CLUSTER-IP EXTERNAL-IP PORT(S)  AGE\nfrontend-external LoadBalancer 10.60.5.62 <pending>  80:30359/TCP 43s\n```When the `Service` is ready the column displays the public IP address of the load balancer.\n- After the `Service` is ready, get the external IP address of the load balancer and use curl to verify that the frontend is ready. If this curl command returns an error, wait a few moments before you try again:```\n\u00a0 curl $(kubectl get svc frontend-external \\\u00a0 \u00a0 \u00a0 -n onlineboutique --context=std-west \\\u00a0 \u00a0 \u00a0 -o=jsonpath=\"{.status.loadBalancer.ingress[0].ip}\") | \\\u00a0 \u00a0 \u00a0 \u00a0 grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>std-west<br/>\n<b>Zone: </b>us-west1-a<br/>\n<b>Pod: </b>frontend-b7bddcc97-wdjsk\n```\nYou now have a single-zone version of Online Boutique running in . You can also use a web browser to navigate to the external IP assigned to the `LoadBalancer` Service to access the application and observe its behavior. This initial single deployment is shown in the following diagram:## Export cartservice as a multi-cluster ServiceIn this section, you start to add elements of high availability to the application. You export the backend as a multi-cluster Service to the GKE Autopilot clusters.- Create the namespace on the remaining clusters:```\nkubectl create namespace onlineboutique --context auto-eastkubectl create namespace onlineboutique --context auto-centralkubectl create namespace onlineboutique --context config-central\n```For each command, the output is similar to the following example:```\nnamespace/onlineboutique created\n```\n- Export from the cluster to all other clusters in the `ClusterSet` . The `ServiceExport` object registers the Service, with GKE multi-cluster Services, for export to all clusters in the fleet that have the namespace present. For more details, see [registering a service for export](/kubernetes-engine/docs/how-to/multi-cluster-services#registering_a_service_for_export) .```\ncat <<EOF>> $WORKDIR/cartservice-export.yamlkind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0namespace: onlineboutique\u00a0name: cartserviceEOFkubectl apply -f $WORKDIR/cartservice-export.yaml \\\u00a0 \u00a0 -n onlineboutique --context=std-west\n```\n## Apply application manifests for multi-cluster patternIn this section you apply two curated manifests to deploy the multi-cluster pattern. These manifests contain selected portions of the that you previously applied to the cluster:- The first manifest is used for the`Deployment`,`Service`, and`ServiceExport`.\n- The second manifest is used to deploy the middleware`Services`(,,,,,,, and) to all regions that have arunning. By keeping a request local to a region for as long as possible, you avoid unnecessary inter-region network traffic charges.\nA `Pod` that runs in any cluster in the fleet can access an exported `Service` by sending a request to the `ClusterSet` URI for that service in the format `SERVICE_NAME.NAMESPACE.svc.clusterset.local` . For example, the `Deployments` in all three example clusters are able to consume the , in the `onlineboutique` namespace, by making a request to .\nFor this reason, in each manifest, the hostname for the has been updated to its `ClusterSet` URI. This step is critical. If this service hostname is not updated, the Service would ask kube-dns for `cartservice` instead of `cartservice.onlineboutique.svc.clusterset.local` . This behavior would result in `HTTP Status 500` errors on clusters where a local version of is not available, and cause the pods to be unhealthy.- Set an environment varianble for the GitHub repository that contains the manifests:```\nexport MANIFEST_REPO_PATH=https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/master/gateway/docs/cluster-migration\n```\n- Apply the manifests to deploy the frontend layer to all three workload clusters:```\nkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-frontend-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=std-westkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-frontend-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=auto-eastkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-frontend-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=auto-central\n```\n- Apply the manifests to deploy the middleware layer to all three workload clusters:```\nkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-middleware-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=std-westkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-middleware-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=auto-eastkubectl apply -f ${MANIFEST_REPO_PATH}/onlineboutique-middleware-manifests.yaml \\\u00a0 \u00a0 -n onlineboutique --context=auto-central\n```\nYou now have the `Deployment` , `Service` , and `ServiceExport` active in clusters , , and . You also have locally running Online Boutique middleware services in each cluster. However, external traffic is still routed only to the `Service` that runs in the initial cluster in , as shown in the following diagram:## Enable and configure multi-cluster GatewaysIn this section, you route traffic to and load balance external traffic across frontends in all three clusters. To achieve this configuration, you use multi-cluster Gateways (MCG). This document follows the directions to set up MCG as described in [Enabling multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) .\nIn this guide you use the cluster to host the configuration for Gateway resources.- Confirm that all clusters have successfully registered to the fleet:```\ngcloud container fleet memberships list --project=$PROJECT\n```The following example output shows that all the clusters are successfully registered:```\nNAME: auto-central\nEXTERNAL_ID: 21537493-32ea-4a41-990d-02be2c1b319f\nNAME: config-central\nEXTERNAL_ID: 4369423e-ea7b-482d-a0eb-93b560e67b98\nNAME: std-west\nEXTERNAL_ID: 7fcb048b-c796-476b-9698-001a00f91ab3\nNAME: auto-east\nEXTERNAL_ID: aae2d2ff-b861-4a38-bcaf-612f14810012\n```\n- Install the Gateway API on the cluster:```\nkubectl --context=config-central kustomize \"github.com/kubernetes-sigs/gateway-api/config/crd?ref=v0.5.0\" \\\u00a0 \u00a0 | kubectl apply -f ```This step installs the Gateway API custom resource definitions including the `GatewayClass` , `Gateway` , and `HTTPRoute` resources. The custom resource definitions are maintained by the Kubernetes [Network Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-network) . Once installed, you can use the GKE Gateway controller.\n- Enable Multi Cluster Ingress for your fleet if you haven't done so already. Enabling this feature also enables the multi-cluster Gateway controller.```\ngcloud container fleet ingress enable \\\u00a0 \u00a0 --config-membership=config-central \\\u00a0 \u00a0 --project=$PROJECTgcloud container fleet ingress describe --project=$PROJECT\n```The output is similar to the following example:```\ncreateTime: '2021-12-08T23:10:52.505888854Z'\nname: projects/PROJECT_ID/locations/global/features/multiclusteringress\nresourceState:\n state: ACTIVE\nspec:\n multiclusteringress:\n configMembership: projects/zl-mcs-expf61cbd13/locations/global/memberships/config-central\nstate:\n state:\n code: OK\n description: Ready to use\n updateTime: '2021-12-08T23:11:37.994971649Z'\nupdateTime: '2021-12-08T23:11:38.098244178Z'\n```If the value of is not , see [Troubleshooting and operations for Multi Cluster Ingress](/kubernetes-engine/docs/how-to/troubleshooting-and-ops#config_cluster_migration) .\n- Confirm that `GatewayClasses` are available on the cluster:```\nkubectl get gatewayclasses --context=config-central\n```The output is similar to the following example:```\nNAME         CONTROLLER     AGE\ngke-l7-global-external-managed  networking.gke.io/gateway 18s\ngke-l7-global-external-managed-mc  networking.gke.io/gateway 19s\ngke-l7-regional-external-managed  networking.gke.io/gateway 18s\ngke-l7-regional-external-managed-mc networking.gke.io/gateway 19s\ngke-l7-gxlb       networking.gke.io/gateway 74s\ngke-l7-gxlb-mc      networking.gke.io/gateway 16s\ngke-l7-rilb       networking.gke.io/gateway 74s\ngke-l7-rilb-mc      networking.gke.io/gateway 16s\n```Different `GatewayClass` resources have different capabilities. For more information on when to use which type, see [GatewayClass capabilities](/kubernetes-engine/docs/how-to/gatewayclass-capabilities) .\n- Deploy the `external-http` Gateway resource to `config-central` :```\ncat <<EOF>> $WORKDIR/external-http-gateway.yamlkind: GatewayapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: external-http\u00a0 namespace: onlineboutiquespec:\u00a0 gatewayClassName: gke-l7-global-external-managed-mc\u00a0 listeners:\u00a0 - protocol: HTTP\u00a0 \u00a0 port: 80\u00a0 \u00a0 name: httpEOFkubectl apply -f external-http-gateway.yaml \\\u00a0 \u00a0 -n onlineboutique --context=config-central\n```As indicated by the `gatewayClassName` field, this resource is of `GatewayClass` which manages Layer 7 external Cloud Load Balancing and exposes the multi-cluster application\n- Deploy the `HTTPRoute` named on :```\ncat <<EOF>> $WORKDIR/public-frontend-route.yamlkind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: public-frontend-route\u00a0 namespace: onlineboutiquespec:\u00a0 parentRefs:\u00a0 - name: \"external-http\"\u00a0 hostnames:\u00a0 - \"store.example.com\"\u00a0 rules:\u00a0 - matches:\u00a0 \u00a0 - path:\u00a0 \u00a0 \u00a0 \u00a0 type: PathPrefix\u00a0 \u00a0 \u00a0 \u00a0 value: /\u00a0 \u00a0 backendRefs:\u00a0 \u00a0 - name: frontend\u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 port: 80EOFkubectl apply -f public-frontend-route.yaml \\\u00a0 \u00a0 -n onlineboutique --context=config-central\n```When you deploy the `HTTPRoute` resource it creates an external Layer 7 Cloud Load Balancing resource and exposes the frontend `ServiceImport` backed by the frontend services that run in the , , and clusters. **Note:** Alternatively, you can replace `store.example.com` in this manifest with a hostname from a domain you or your team owns, and point the respective DNS record to the external IP address of the load balancer created by this step. Requests destined for that hostname would then be directed to this load balancer.The following diagram shows how, after the multi-cluster Gateway is deployed, traffic can be routed to any of the frontend multi-cluster Services on any of the three application clusters:\n- Wait for the load balancer to be ready with a provisioned external IP address before proceeding to the next step. It can take up to 10 minutes for the IP address to be assigned. You can monitor progress by using a watch loop. The load balancer has a name in the pattern like :```\nwatch -n 20 --difference=permanent \\\u00a0 \u00a0 \"gcloud compute forwarding-rules list \\\u00a0 \u00a0 \u00a0 \u00a0 | grep -A 5 NAME..*external-http\"\n```The output is similar to the following example:```\nNAME: gkemcg-onlineboutique-external-http-k09mfhk74gop\nREGION:\nIP_ADDRESS: 34.149.29.176\nIP_PROTOCOL: TCP\nTARGET: gkemcg-onlineboutique-external-http-k09mfhk74gop\n```\n- Once the load balancer is ready, run the following command in Cloud Shell to export the external IP address of the load balancer created through the application of and manifests:```\nexport EXTERNAL_LB_IP=$(kubectl --context=config-central \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -n onlineboutique get gateway external-http \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -o=jsonpath='{.status.addresses[0].value}')\n```\n- When you send a request to the load balancer with the appropriate headers, it returns the HTML contents served by the frontend service. For example, since you configured the `HTTPRoute` resource to map the `store.example.com` hostname to the `ServiceImport` , you must supply the `HOST` header when you make the HTTP request. If the following curl example returns an error, wait a few minutes and try again:```\ncurl -H 'HOST: store.example.com' $EXTERNAL_LB_IP | \\\u00a0 \u00a0 grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>auto-central<br/>\n<b>Zone: </b>us-central1-f<br/>\n<b>Pod: </b>frontend-7c7d596ddc-jdh8f\n``` **Note:** If the matching headers, as configured in the `HTTPRoute` , are not supplied when you make the request, the client receives an `HTTP status 404` error. Any of the three frontend pods that run in the serving path can respond to the HTTP request inbound to the global load balancer. The global load balancer can route requests to healthy backends located closest to where the request originated.\n## Test application multi-region routing behaviorOne of the powerful features obtained by using multi-cluster Services and multi-cluster Gateways is that external requests are routed to the geographically closest cluster.\nTo test application multi-region behavior, generate traffic that originates from the various regions where you have clusters deployed. Create three small pods, one in each of the serving clusters ( , , and ), that you can use to send HTTP requests to the load balancer endpoint. The results let you see which frontend `Pod` responds.- Create the client pods:```\nkubectl run --context=std-west \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --image=radial/busyboxplus:curl client-west \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -- sh -c 'while sleep 3600; do :; done'kubectl run --context=auto-east \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --image=radial/busyboxplus:curl client-east \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -- sh -c 'while sleep 3600; do :; done'kubectl run --context=auto-central \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --image=radial/busyboxplus:curl client-central \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -- sh -c 'while sleep 3600; do :; done'\n```\n- After the pods are running, use a curl command to send a request to the load balancer endpoint from the client `Pod` in the cluster and review the response:```\nkubectl exec -it --context=std-west client-west \\\u00a0 \u00a0 -- curl -H 'HOST: store.example.com' $EXTERNAL_LB_IP | \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>std-west<br/>\n<b>Zone: </b>us-west1-a<br/>\n<b>Pod: </b>frontend-7cf48b79cf-trzc4\n```\n- Run the same curl request from the client `Pod` in the cluster, and look at the response:```\nkubectl exec -it --context=auto-east client-east \\\u00a0 \u00a0 -- curl -H 'HOST: store.example.com' $EXTERNAL_LB_IP | \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>auto-east<br/>\n<b>Zone: </b>us-east1-d<br/>\n<b>Pod: </b>frontend-6784b6df98-scdws\n```As this is an Autopilot cluster, the cluster may need to provision additional resources to schedule the `Pod` . If you see output similar to the following example, wait a moment and try again:```\n Error from server (BadRequest): pod client-east does not have a host assigned\n```\n- Run the curl from the client `Pod` in the cluster and check the response:```\nkubectl exec -it --context=auto-central client-central \\\u00a0 \u00a0 -- curl -H 'HOST: store.example.com' $EXTERNAL_LB_IP | \\\u00a0 \u00a0 \u00a0 \u00a0 grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>auto-central<br/><b>Zone: </b>us-central1-b<br/><b>Pod: </b>frontend-6784b6df98-x2fv4\n```These results confirm that traffic routes to the corresponding pods in the locations closest to request origin.\n## Test application multi-region resiliencyIn addition to efficient traffic routing, running your services in multiple regions provides resiliency in the rare, yet still possible, case of infrastructure failure.\nTest the behavior by deleting the `Deployments` in specific clusters and then retry the curl command from the client `Pod` in those regions. Observe that the application is still available, and look at the location of the `Pod` that responds to the request.- Run the curl command from the `Pod` in the cluster, and see that the result comes from the frontend in :```\nkubectl exec -it --context=std-west client-west \\\u00a0 \u00a0 -- curl -H 'HOST: store.example.com' $EXTERNAL_LB_IP \u00a0| \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0grep -e Cluster -e Zone -e Pod\n```The successful output of the curl command is similar to the following example:```\n<b>Cluster: </b>std-west<br/>\n<b>Zone: </b>us-west1-a<br/>\n<b>Pod: </b>frontend-7cf48b79cf-trzc4\n```\n- Delete the `Deployment` in the cluster:```\nkubectl delete deploy frontend \\\u00a0 \u00a0 -n onlineboutique --context=std-west\n```The output is similar to the following example:```\ndeployment.apps \"frontend\" deleted\n```\n- Send another request from the `Pod` in the cluster. You should see a response from one of the remaining `Deployments` located in the or clusters:```\nkubectl exec -it --context=std-west client-west \\\u00a0 \u00a0 -- curl -H 'HOST: store.example.com' $EXTERNAL_LB_IP | \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0grep -e Cluster -e Zone -e Pod\n```Output similar to the following example indicates the location of the healthy `Pod` that responds to this request:```\n<b>Cluster: </b>auto-central<br/>\n<b>Zone: </b>us-central1-b<br/>\n<b>Pod: </b>frontend-6784b6df98-x2fv4\n```or```\n<b>Cluster: </b>auto-east<br/><b>Zone: </b>us-east1-d<br/><b>Pod: </b>frontend-6784b6df98-scdws\n```Run the command several times to see alternating results.\nWith this demonstration deployment, you have added elements of resiliency and geographic distribution to the Online Boutique application using multi-cluster Services and multi-cluster Gateways. Requests are routed to the closest geographical region and even if the frontend or middleware services in a region experience issues, the end user is still able to successfully use the application.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read more about [Google Cloud fleet and multi-cluster management](/anthos/fleet-management/docs) .\n- Learn about [how to register a cluster](/anthos/fleet-management/docs/fleet-creation) , including GKE Enterprise clusters, to a Google Cloud fleet.\n- Dive deeper into the [Kubernetes Gateway API](https://gateway-api.sigs.k8s.io/) and the [GKE Gateway](/kubernetes-engine/docs/concepts/gateway-api) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}