{"title": "Google Kubernetes Engine (GKE) - Deploy a highly-available PostgreSQL database on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/postgresql", "abstract": "# Google Kubernetes Engine (GKE) - Deploy a highly-available PostgreSQL database on GKE\n[PostgreSQL](https://www.postgresql.org/) is an open source object-relational database known for reliability and data integrity. It is [ACID](https://en.wikipedia.org/wiki/ACID) -compliant, and supports foreign keys, joins, views, triggers, and stored procedures.\nThis document is intended for database administrators, cloud architects, and operations professionals interested in deploying a highly-available PostgreSQL topology on Google Kubernetes Engine (GKE).", "content": "## Objectives\nIn this tutorial, you will learn how to:\n- Use Terraform to create a regional GKE cluster.\n- Deploy a highly-available PostgreSQL database.\n- Set up monitoring for the PostgreSQL application.\n- Perform PostgreSQL database and GKE cluster upgrades.\n- Simulate cluster disruption and PostgreSQL replica failover.\n- Perform backup and restore of the PostgreSQL database.\n## ArchitectureThis section describes the architecture of the solution you'll build in this tutorial.\nYou'll provision two GKE clusters in different regions: a and a . For this tutorial, the primary cluster is in the `us-central1` region and the backup cluster is in the `us-west1` region. This architecture lets you provision a highly-available PostgreSQL database and test for disaster recovery, as described later in this tutorial.\nFor the source cluster, you'll use a [Helm](https://helm.sh/) chart ( [bitnami/postgresql-ha](https://artifacthub.io/packages/helm/bitnami/postgresql-ha) ) to set up a high-availability PostgreSQL cluster.## CostsIn this document, you use the following billable components of Google Cloud:- [Artifact Registry](/artifact-registry/pricing) \n- [Backup for GKE](/kubernetes-engine/pricing#backup-for-gke) \n- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n### Set up your project### Set up roles\n- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `role/storage.objectViewer, role/logging.logWriter, role/artifactregistry.Admin, roles/container.clusterAdmin, role/container.serviceAgent, roles/serviceusage.serviceUsageAdmin, roles/iam.serviceAccountAdmin` ```\n$ gcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.### Set up your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [Docker](https://www.docker.com/) , [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) .\nTo use Cloud Shell to set up your environment:- Launch a Cloud Shell session from the Google Cloud console, by clicking **Activate Cloud Shell** in the [Google Cloud console](http://console.cloud.google.com) . This launches a session in the bottom pane of Google Cloud console.\n- Set environment variables.```\nexport PROJECT_ID=PROJECT_IDexport SOURCE_CLUSTER=cluster-db1export REGION=us-central1\n```Replace the following values:- : your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Set the default environment variables.```\ngcloud config set project PROJECT_ID\n```\n- Clone the code repository.```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory.```\ncd kubernetes-engine-samples/databases/gke-stateful-postgres\n```\n## Create your cluster infrastructureIn this section, you'll run a Terraform script to create a custom Virtual Private Cloud (VPC), a Artifact Registry repository to store PostgreSQL images, and two [regional GKE clusters](/kubernetes-engine/docs/concepts/regional-clusters) . One cluster will be deployed in `us-central1` and the second cluster for backup will be deployed in `us-west1` .\n **Note:** Because you'll use [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) and [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) for this tutorial, it might take up to 8 minutes to create and reconcile the cluster.\nTo create the cluster, follow these steps:\nIn Cloud Shell, run the following commands:\n```\nterraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply -var project_id=$PROJECT_ID\n```\nWhen prompted, type `yes` .The Terraform configuration files create the following resources to deploy your infrastructure:- Create a Artifact Registry repository to store the Docker images. [  databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) ```\nresource \"google_artifact_registry_repository\" \"main\" {\u00a0 location \u00a0 \u00a0 \u00a0= \"us\"\u00a0 repository_id = \"main\"\u00a0 format \u00a0 \u00a0 \u00a0 \u00a0= \"DOCKER\"\u00a0 project \u00a0 \u00a0 \u00a0 = var.project_id}\n```\n- Create the VPC network and subnet for the VM's network interface. [  databases/gke-stateful-postgres/terraform/modules/network/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/modules/network/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/modules/network/main.tf) ```\nmodule \"gcp-network\" {\u00a0 source \u00a0= \"terraform-google-modules/network/google\"\u00a0 version = \"< 8.0.0\"\u00a0 project_id \u00a0 = var.project_id\u00a0 network_name = \"vpc-gke-postgresql\"\u00a0 subnets = [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 subnet_name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"snet-gke-postgresql-us-central1\"\u00a0 \u00a0 \u00a0 subnet_ip \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"10.0.0.0/17\"\u00a0 \u00a0 \u00a0 subnet_region \u00a0 \u00a0 \u00a0 \u00a0 = \"us-central1\"\u00a0 \u00a0 \u00a0 subnet_private_access = true\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 subnet_name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"snet-gke-postgresql-us-west1\"\u00a0 \u00a0 \u00a0 subnet_ip \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"10.0.128.0/17\"\u00a0 \u00a0 \u00a0 subnet_region \u00a0 \u00a0 \u00a0 \u00a0 = \"us-west1\"\u00a0 \u00a0 \u00a0 subnet_private_access = true\u00a0 \u00a0 },\u00a0 ]\u00a0 secondary_ranges = {\u00a0 \u00a0 (\"snet-gke-postgresql-us-central1\") = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-pods-db1\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.0.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-svc-db1\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.64.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 (\"snet-gke-postgresql-us-west1\") = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-pods-db2\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.128.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-svc-db2\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.192.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ]\u00a0 }}output \"network_name\" {\u00a0 value = module.gcp-network.network_name}output \"primary_subnet_name\" {\u00a0 value = module.gcp-network.subnets_names[0]}output \"secondary_subnet_name\" {\u00a0 value = module.gcp-network.subnets_names[1]}\n```\n- Create a primary GKE cluster.Terraform creates a private cluster in the `us-central1` region, and enables Backup for GKE for disaster recovery and Managed Service for Prometheus for cluster monitoring.Managed Service for Prometheus is only supported on Autopilot clusters running GKE version 1.25 or later. [  databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) ```\nmodule \"gke-db1-autopilot\" {\u00a0 source \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"../modules/beta-autopilot-private-cluster\"\u00a0 project_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= var.project_id\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"cluster-db1\"\u00a0 kubernetes_version \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"1.25\" # Will be ignored if use \"REGULAR\" release_channel\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"us-central1\"\u00a0 regional \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= true\u00a0 zones \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = [\"us-central1-a\", \"us-central1-b\", \"us-central1-c\"]\u00a0 network \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = module.network.network_name\u00a0 subnetwork \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= module.network.primary_subnet_name\u00a0 ip_range_pods \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"ip-range-pods-db1\"\u00a0 ip_range_services \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"ip-range-svc-db1\"\u00a0 horizontal_pod_autoscaling \u00a0 \u00a0 \u00a0= true\u00a0 release_channel \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"RAPID\" # Default version is 1.22 in REGULAR. GMP on Autopilot requires V1.25 via var.kubernetes_version\u00a0 enable_vertical_pod_autoscaling = true\u00a0 enable_private_endpoint \u00a0 \u00a0 \u00a0 \u00a0 = false\u00a0 enable_private_nodes \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= true\u00a0 master_ipv4_cidr_block \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"172.16.0.0/28\"\u00a0 create_service_account \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= false}\n```\n- Create a backup cluster in the `us-west1` region for disaster recovery. [  databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-autopilot/main.tf) ```\nmodule \"gke-db2-autopilot\" {\u00a0 source \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"../modules/beta-autopilot-private-cluster\"\u00a0 project_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= var.project_id\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"cluster-db2\"\u00a0 kubernetes_version \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"1.25\" # Will be ignored if use \"REGULAR\" release_channel\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"us-west1\"\u00a0 regional \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= true\u00a0 zones \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = [\"us-west1-a\", \"us-west1-b\", \"us-west1-c\"]\u00a0 network \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = module.network.network_name\u00a0 subnetwork \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= module.network.secondary_subnet_name\u00a0 ip_range_pods \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"ip-range-pods-db2\"\u00a0 ip_range_services \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"ip-range-svc-db2\"\u00a0 horizontal_pod_autoscaling \u00a0 \u00a0 \u00a0= true\u00a0 release_channel \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"RAPID\" # Default version is 1.22 in REGULAR. GMP on Autopilot requires V1.25 via var.kubernetes_version\u00a0 enable_vertical_pod_autoscaling = true\u00a0 enable_private_endpoint \u00a0 \u00a0 \u00a0 \u00a0 = false\u00a0 enable_private_nodes \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= true\u00a0 master_ipv4_cidr_block \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"172.16.0.16/28\"\u00a0 create_service_account \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= false}\n```\nIn Cloud Shell, run the following commands:\n```\nterraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply -var project_id=$PROJECT_ID\n```\nWhen prompted, type `yes` .The Terraform configuration files create the following resources to deploy your infrastructure:- Create a Artifact Registry repository to store the Docker images. [  databases/gke-stateful-postgres/terraform/gke-standard/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) ```\nresource \"google_artifact_registry_repository\" \"main\" {\u00a0 location \u00a0 \u00a0 \u00a0= \"us\"\u00a0 repository_id = \"main\"\u00a0 format \u00a0 \u00a0 \u00a0 \u00a0= \"DOCKER\"\u00a0 project \u00a0 \u00a0 \u00a0 = var.project_id}resource \"google_artifact_registry_repository_iam_binding\" \"binding\" {\u00a0 provider \u00a0 = google-beta\u00a0 project \u00a0 \u00a0= google_artifact_registry_repository.main.project\u00a0 location \u00a0 = google_artifact_registry_repository.main.location\u00a0 repository = google_artifact_registry_repository.main.name\u00a0 role \u00a0 \u00a0 \u00a0 = \"roles/artifactregistry.reader\"\u00a0 members = [\u00a0 \u00a0 \"serviceAccount:${module.gke-db1.service_account}\",\u00a0 ]}\n```\n- Create the VPC network and subnet for the VM's network interface. [  databases/gke-stateful-postgres/terraform/modules/network/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/modules/network/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/modules/network/main.tf) ```\nmodule \"gcp-network\" {\u00a0 source \u00a0= \"terraform-google-modules/network/google\"\u00a0 version = \"< 8.0.0\"\u00a0 project_id \u00a0 = var.project_id\u00a0 network_name = \"vpc-gke-postgresql\"\u00a0 subnets = [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 subnet_name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"snet-gke-postgresql-us-central1\"\u00a0 \u00a0 \u00a0 subnet_ip \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"10.0.0.0/17\"\u00a0 \u00a0 \u00a0 subnet_region \u00a0 \u00a0 \u00a0 \u00a0 = \"us-central1\"\u00a0 \u00a0 \u00a0 subnet_private_access = true\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 subnet_name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"snet-gke-postgresql-us-west1\"\u00a0 \u00a0 \u00a0 subnet_ip \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"10.0.128.0/17\"\u00a0 \u00a0 \u00a0 subnet_region \u00a0 \u00a0 \u00a0 \u00a0 = \"us-west1\"\u00a0 \u00a0 \u00a0 subnet_private_access = true\u00a0 \u00a0 },\u00a0 ]\u00a0 secondary_ranges = {\u00a0 \u00a0 (\"snet-gke-postgresql-us-central1\") = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-pods-db1\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.0.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-svc-db1\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.64.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 (\"snet-gke-postgresql-us-west1\") = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-pods-db2\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.128.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 range_name \u00a0 \u00a0= \"ip-range-svc-db2\"\u00a0 \u00a0 \u00a0 \u00a0 ip_cidr_range = \"192.168.192.0/18\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ]\u00a0 }}output \"network_name\" {\u00a0 value = module.gcp-network.network_name}output \"primary_subnet_name\" {\u00a0 value = module.gcp-network.subnets_names[0]}output \"secondary_subnet_name\" {\u00a0 value = module.gcp-network.subnets_names[1]}\n```\n- Create a primary GKE cluster.Terraform creates a private cluster in the `us-central1` region, and enables Backup for GKE for disaster recovery and Managed Service for Prometheus for cluster monitoring. [  databases/gke-stateful-postgres/terraform/gke-standard/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) ```\nmodule \"gke-db1\" {\u00a0 source \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"../modules/beta-private-cluster\"\u00a0 project_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = var.project_id\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"cluster-db1\"\u00a0 regional \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = true\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"us-central1\"\u00a0 network \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= module.network.network_name\u00a0 subnetwork \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = module.network.primary_subnet_name\u00a0 ip_range_pods \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"ip-range-pods-db1\"\u00a0 ip_range_services \u00a0 \u00a0 \u00a0 \u00a0= \"ip-range-svc-db1\"\u00a0 create_service_account \u00a0 = true\u00a0 enable_private_endpoint \u00a0= false\u00a0 enable_private_nodes \u00a0 \u00a0 = true\u00a0 master_ipv4_cidr_block \u00a0 = \"172.16.0.0/28\"\u00a0 network_policy \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = true\u00a0 cluster_autoscaling = {\u00a0 \u00a0 \"autoscaling_profile\": \"OPTIMIZE_UTILIZATION\",\u00a0 \u00a0 \"enabled\" : true,\u00a0 \u00a0 \"gpu_resources\" : [],\u00a0 \u00a0 \"min_cpu_cores\" : 36,\u00a0 \u00a0 \"min_memory_gb\" : 144,\u00a0 \u00a0 \"max_cpu_cores\" : 48,\u00a0 \u00a0 \"max_memory_gb\" : 192,\u00a0 }\u00a0 monitoring_enable_managed_prometheus = true\u00a0 gke_backup_agent_config = true\u00a0 node_pools = [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"pool-sys\"\u00a0 \u00a0 \u00a0 autoscaling \u00a0 \u00a0 = true\u00a0 \u00a0 \u00a0 min_count \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_count \u00a0 \u00a0 \u00a0 = 3\u00a0 \u00a0 \u00a0 max_surge \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_unavailable = 0\u00a0 \u00a0 \u00a0 machine_type \u00a0 \u00a0= \"e2-standard-4\"\u00a0 \u00a0 \u00a0 node_locations \u00a0= \"us-central1-a,us-central1-b,us-central1-c\"\u00a0 \u00a0 \u00a0 auto_repair \u00a0 \u00a0 = true\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"pool-db\"\u00a0 \u00a0 \u00a0 autoscaling \u00a0 \u00a0 = true\u00a0 \u00a0 \u00a0 max_surge \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_unavailable = 0\u00a0 \u00a0 \u00a0 machine_type \u00a0 \u00a0= \"e2-standard-8\"\u00a0 \u00a0 \u00a0 node_locations \u00a0= \"us-central1-a,us-central1-b,us-central1-c\"\u00a0 \u00a0 \u00a0 auto_repair \u00a0 \u00a0 = true\u00a0 \u00a0 },\u00a0 ]\u00a0 node_pools_labels = {\u00a0 \u00a0 all = {}\u00a0 \u00a0 pool-db = {\u00a0 \u00a0 \u00a0 \"app.stateful/component\" = \"postgresql\"\u00a0 \u00a0 }\u00a0 \u00a0 pool-sys = {\u00a0 \u00a0 \u00a0 \"app.stateful/component\" = \"postgresql-pgpool\"\u00a0 \u00a0 }\u00a0 }\u00a0 node_pools_taints = {\u00a0 \u00a0 all = []\u00a0 \u00a0 pool-db = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 key \u00a0 \u00a0= \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 value \u00a0= \"postgresql\"\u00a0 \u00a0 \u00a0 \u00a0 effect = \"NO_SCHEDULE\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 pool-sys = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 key \u00a0 \u00a0= \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 value \u00a0= \"postgresql-pgpool\"\u00a0 \u00a0 \u00a0 \u00a0 effect = \"NO_SCHEDULE\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 }\u00a0 gce_pd_csi_driver = true}\n```\n- Create a backup cluster in the `us-west1` region for disaster recovery. [  databases/gke-stateful-postgres/terraform/gke-standard/main.tf ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/terraform/gke-standard/main.tf) ```\nmodule \"gke-db2\" {\u00a0 source \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"../modules/beta-private-cluster\"\u00a0 project_id \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = var.project_id\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"cluster-db2\"\u00a0 regional \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = true\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"us-west1\"\u00a0 network \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= module.network.network_name\u00a0 subnetwork \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = module.network.secondary_subnet_name\u00a0 ip_range_pods \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"ip-range-pods-db2\"\u00a0 ip_range_services \u00a0 \u00a0 \u00a0 \u00a0= \"ip-range-svc-db2\"\u00a0 create_service_account \u00a0 = false\u00a0 service_account \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= module.gke-db1.service_account\u00a0 enable_private_endpoint \u00a0= false\u00a0 enable_private_nodes \u00a0 \u00a0 = true\u00a0 master_ipv4_cidr_block \u00a0 = \"172.16.0.16/28\"\u00a0 network_policy \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = true\u00a0 cluster_autoscaling = {\u00a0 \u00a0 \"autoscaling_profile\": \"OPTIMIZE_UTILIZATION\",\u00a0 \u00a0 \"enabled\" : true,\u00a0 \u00a0 \"gpu_resources\" : [],\u00a0 \u00a0 \"min_cpu_cores\" : 10,\u00a0 \u00a0 \"min_memory_gb\" : 144,\u00a0 \u00a0 \"max_cpu_cores\" : 48,\u00a0 \u00a0 \"max_memory_gb\" : 192,\u00a0 }\u00a0 monitoring_enable_managed_prometheus = true\u00a0 gke_backup_agent_config = true\u00a0 node_pools = [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"pool-sys\"\u00a0 \u00a0 \u00a0 autoscaling \u00a0 \u00a0 = true\u00a0 \u00a0 \u00a0 min_count \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_count \u00a0 \u00a0 \u00a0 = 3\u00a0 \u00a0 \u00a0 max_surge \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_unavailable = 0\u00a0 \u00a0 \u00a0 machine_type \u00a0 \u00a0= \"e2-standard-4\"\u00a0 \u00a0 \u00a0 node_locations \u00a0= \"us-west1-a,us-west1-b,us-west1-c\"\u00a0 \u00a0 \u00a0 auto_repair \u00a0 \u00a0 = true\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"pool-db\"\u00a0 \u00a0 \u00a0 autoscaling \u00a0 \u00a0 = true\u00a0 \u00a0 \u00a0 max_surge \u00a0 \u00a0 \u00a0 = 1\u00a0 \u00a0 \u00a0 max_unavailable = 0\u00a0 \u00a0 \u00a0 machine_type \u00a0 \u00a0= \"e2-standard-8\"\u00a0 \u00a0 \u00a0 node_locations \u00a0= \"us-west1-a,us-west1-b,us-west1-c\"\u00a0 \u00a0 \u00a0 auto_repair \u00a0 \u00a0 = true\u00a0 \u00a0 },\u00a0 ]\u00a0 node_pools_labels = {\u00a0 \u00a0 all = {}\u00a0 \u00a0 pool-db = {\u00a0 \u00a0 \u00a0 \"app.stateful/component\" = \"postgresql\"\u00a0 \u00a0 }\u00a0 \u00a0 pool-sys = {\u00a0 \u00a0 \u00a0 \"app.stateful/component\" = \"postgresql-pgpool\"\u00a0 \u00a0 }\u00a0 }\u00a0 node_pools_taints = {\u00a0 \u00a0 all = []\u00a0 \u00a0 pool-db = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 key \u00a0 \u00a0= \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 value \u00a0= \"postgresql\"\u00a0 \u00a0 \u00a0 \u00a0 effect = \"NO_SCHEDULE\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 pool-sys = [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 key \u00a0 \u00a0= \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 value \u00a0= \"postgresql-pgpool\"\u00a0 \u00a0 \u00a0 \u00a0 effect = \"NO_SCHEDULE\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 }\u00a0 gce_pd_csi_driver = true}\n``` **Tip:** To debug issues when running Terraform, you can capture debug output by setting the Terraform log level environment variable `TF_LOG` . For example: `export TF_LOG=\"DEBUG\"` . Valid log levels are (in order of decreasing verbosity): `TRACE` , `DEBUG` , `INFO` , `WARN` , or `ERROR` .## Deploy PostgreSQL on your clusterIn this section, you'll deploy a PostgreSQL database instance to run on GKE by using a Helm chart.\n### Install PostgreSQLTo install PostgreSQL on your cluster, follow these steps.- Configure Docker access.```\ngcloud auth configure-docker us-docker.pkg.dev\n```\n- Populate Artifact Registry with the required PostgreSQL Docker images.```\n./scripts/gcr.sh bitnami/postgresql-repmgr 15.1.0-debian-11-r0./scripts/gcr.sh bitnami/postgres-exporter 0.11.1-debian-11-r27./scripts/gcr.sh bitnami/pgpool 4.3.3-debian-11-r28\n```The script pushes the following Bitnami images to the Artifact Registry for Helm to install:- [postgresql-repmgr](https://hub.docker.com/r/bitnami/postgresql-repmgr) : This PostgreSQL cluster solution includes the [PostgreSQL replication manager (repmgr)](https://repmgr.org/) , an open-source tool for managing replication and failover on PostgreSQL clusters.\n- [postgres-exporter](https://hub.docker.com/r/bitnami/postgres-exporter/) : PostgreSQL Exporter gathers PostgreSQL metrics for Prometheus consumption.\n- [pgpool](https://hub.docker.com/r/bitnami/pgpool) : Pgpool-II is the PostgreSQL proxy. It provides connection pooling and load balancing.\n- Verify that the correct images are stored in the repo.```\ngcloud artifacts docker images list us-docker.pkg.dev/$PROJECT_ID/main \\\u00a0 \u00a0 --format=\"flattened(package)\"\n```The output is similar to the following:```\n--image: us-docker.pkg.dev/[PROJECT_ID]/main/bitnami/pgpool\n--image: us-docker.pkg.dev/[PROJECT_ID]/main/bitnami/postgres-exporter\n--image: us-docker.pkg.dev/h[PROJECT_ID]/main/bitnami/postgresql-repmgr\n```\n- Configure `kubectl` command line access to the primary cluster.```\ngcloud container clusters get-credentials $SOURCE_CLUSTER \\--region=$REGION --project=$PROJECT_ID\n```\n- Create a namespace.```\nexport NAMESPACE=postgresqlkubectl create namespace $NAMESPACE\n```\n- If you are deploying to an Autopilot cluster, configure node provisioning across three zones. You can skip this step if you are deploying to a Standard cluster.By default, Autopilot provisions resources in only two zones. The deployment defined in `prepareforha.yaml` ensures that Autopilot provisions nodes across three zones in your cluster, by setting these values:- `replicas:3`\n- `podAntiAffinity`with`requiredDuringSchedulingIgnoredDuringExecution`and`topologyKey: \"topology.kubernetes.io/zone\"`\n```\nkubectl -n $NAMESPACE apply -f scripts/prepareforha.yaml\n``` [  databases/gke-stateful-postgres/scripts/prepareforha.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/scripts/prepareforha.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-postgres/scripts/prepareforha.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: prepare-three-zone-ha\u00a0 labels:\u00a0 \u00a0 app: prepare-three-zone-ha\u00a0 \u00a0 app.kubernetes.io/name: postgresql-haspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: prepare-three-zone-ha\u00a0 \u00a0 \u00a0 app.kubernetes.io/name: postgresql-ha\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: prepare-three-zone-ha\u00a0 \u00a0 \u00a0 \u00a0 app.kubernetes.io/name: postgresql-ha\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 podAntiAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: app\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - prepare-three-zone-ha\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 topologyKey: \"topology.kubernetes.io/zone\"\u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 preferredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - preference:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: cloud.google.com/compute-class\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"Scale-Out\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 weight: 1\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 app.stateful/component: postgresql\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - effect: NoSchedule\u00a0 \u00a0 \u00a0 \u00a0 key: app.stateful/component\u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 value: postgresql\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: prepare-three-zone-ha\u00a0 \u00a0 \u00a0 \u00a0 image: busybox:latest\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"/bin/sh\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"while true; do sleep 3600; done\"\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"0.5Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"0.5Gi\"\n```\n- Update the Helm dependency.```\ncd helm/postgresql-bootstraphelm dependency update\n```\n- Inspect and verify the charts that Helm will install.```\nhelm -n postgresql template postgresql . \\\u00a0 --set global.imageRegistry=\"us-docker.pkg.dev/$PROJECT_ID/main\"\n```\n- Install the Helm chart.```\nhelm -n postgresql upgrade --install postgresql . \\\u00a0 \u00a0 --set global.imageRegistry=\"us-docker.pkg.dev/$PROJECT_ID/main\"\n```The output is similar to the following:```\nNAMESPACE: postgresql\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```\n- Verify that the PostgreSQL replicas are running.```\nkubectl get all -n $NAMESPACE\n```The output is similar to the following:```\nNAME               READY STATUS RESTARTS AGE\npod/postgresql-postgresql-bootstrap-pgpool-75664444cb-dkl24 1/1  Running 0   8m39s\npod/postgresql-postgresql-ha-pgpool-6d86bf9b58-ff2bg   1/1  Running 0   8m39s\npod/postgresql-postgresql-ha-postgresql-0      2/2  Running 0   8m39s\npod/postgresql-postgresql-ha-postgresql-1      2/2  Running 0   8m39s\npod/postgresql-postgresql-ha-postgresql-2      2/2  Running 0   8m38s\nNAME             TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S) AGE\nservice/postgresql-postgresql-ha-pgpool    ClusterIP 192.168.99.236 <none>  5432/TCP 8m39s\nservice/postgresql-postgresql-ha-postgresql   ClusterIP 192.168.90.20  <none>  5432/TCP 8m39s\nservice/postgresql-postgresql-ha-postgresql-headless ClusterIP None    <none>  5432/TCP 8m39s\nservice/postgresql-postgresql-ha-postgresql-metrics ClusterIP 192.168.127.198 <none>  9187/TCP 8m39s\nNAME              READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/postgresql-postgresql-bootstrap-pgpool 1/1  1   1   8m39s\ndeployment.apps/postgresql-postgresql-ha-pgpool   1/1  1   1   8m39s\nNAME                DESIRED CURRENT READY AGE\nreplicaset.apps/postgresql-postgresql-bootstrap-pgpool-75664444cb 1   1   1  8m39s\nreplicaset.apps/postgresql-postgresql-ha-pgpool-6d86bf9b58   1   1   1  8m39s\nNAME             READY AGE\nstatefulset.apps/postgresql-postgresql-ha-postgresql 3/3  8m39s\n```\n### Create a test datasetIn this section, you'll create a database and a table with sample values. The database serves as a test dataset for the failover process you'll test later in this tutorial.- Connect to your PostgreSQL instance.```\ncd ../.././scripts/launch-client.sh\n```The output is similar to the following:```\nLaunching Pod pg-client in the namespace postgresql ...\npod/pg-client created\nwaiting for the Pod to be ready\nCopying script files to the target Pod pg-client ...\nPod: pg-client is healthy\n```\n- Start a shell session.```\nkubectl exec -it pg-client -n postgresql -- /bin/bash\n```\n- Create a database and a table, and then insert some test rows.```\npsql -h $HOST_PGPOOL -U postgres -a -q -f /tmp/scripts/generate-db.sql\n```\n- Verify the number of rows for each table.```\npsql -h $HOST_PGPOOL -U postgres -a -q -f /tmp/scripts/count-rows.sql\n```The output is similar to the following:```\nselect COUNT(*) from tb01;\n count\n------- 300000\n(1 row)\nselect COUNT(*) from tb02;\n count\n------- 300000\n(1 row)\n``` **Tip:** You could also use `pgbench` to create dummy data, but to more easily differentiate query request traffic, we recommend using the provided script to create a database and table for querying during read/write tests.\n- Generate test data.```\nexport DB=postgrespgbench -i -h $HOST_PGPOOL -U postgres $DB -s 50\n```The output is similar to the following:```\ndropping old tables...\ncreating tables...\ngenerating data (client-side)...\n5000000 of 5000000 tuples (100%) done (elapsed 29.85 s, remaining 0.00 s)\nvacuuming...\ncreating primary keys...\ndone in 36.86 s (drop tables 0.00 s, create tables 0.01 s, client-side generate 31.10 s, vacuum 1.88 s, primary keys 3.86 s).\n```\n- Exit the postgres client Pod.```\nexit\n```\n## Monitor PostgreSQLIn this section, you'll view metrics and set up alerts for your PostgreSQL instance. You'll use [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) to perform monitoring and alerting.\n### View metricsYour PostgreSQL deployment includes a `postgresql-exporter` sidecar container. This container exposes a `/metrics` endpoint. Google Cloud Managed Service for Prometheus is configured to monitor the PostgreSQL Pods on this endpoint. You can view these metrics through Google Cloud console [dashboards](https://console.cloud.google.com/monitoring/dashboards) .\nThe Google Cloud console provides a few ways to create and save dashboard configuration:- **Creation and Export** : You can create dashboards directly in Google Cloud console, then export and store them in a code repository. To do this, in the dashboard toolbar, open theand download the dashboard JSON file.\n- **Storage and Import** : You can import a dashboard from a JSON file by clickingand uploading the dashboard's JSON content using themenu).\nTo visualize data from your PostgreSQL application and GKE cluster, follow these steps:- Create the following dashboards.```\ncd monitoringgcloud monitoring dashboards create \\\u00a0 \u00a0 \u00a0 \u00a0 --config-from-file=dashboard/postgresql-overview.json \\\u00a0 \u00a0 \u00a0 \u00a0 --project=$PROJECT_IDgcloud monitoring dashboards create \\\u00a0 \u00a0 \u00a0 \u00a0 --config-from-file dashboard/gke-postgresql.json \\\u00a0 \u00a0 \u00a0 \u00a0 --project $PROJECT_ID\n```\n- In the Google Cloud console, navigate to the Cloud Monitoring Dashboard. [Go to the Cloud Monitoring Dashboard](https://console.cloud.google.com/monitoring/dashboards) \n- Select **Custom** from the dashboard list. The following dashboards appear:- **PostgreSQL Overview** : Displays metrics from the PostgreSQL application, including database uptime, database size, and transaction latency.\n- **GKE PostgreSQL Cluster** : Displays metrics from the GKE cluster that PostgreSQL is running on, including CPU usage, memory usage, and volume utilization.\n- Click on each link to examine the dashboards generated.\n### Set up alertsAlerting gives you timely awareness of problems in your applications so you can resolve the problems quickly. You can create an [alerting policy](/monitoring/alerts) to specify the circumstances under which you want to be alerted and how you want to be notified. You can also [create notification channels](/monitoring/support/notification-options#creating_channels) that let you select where alerts are sent.\nIn this section, you'll use Terraform to configure the following example alerts:- `db_max_transaction`: Monitors the max lag of transactions in seconds; an alert will be triggered if the value is greater than 10.\n- `db_node_up`: Monitors the status of database Pods; 0 means a Pod is down and triggers an alert.\nTo set up alerts, follow these steps:- Configure alerts with Terraform.```\nEMAIL=YOUR_EMAILcd alerting/terraformterraform initterraform plan -var project_id=$PROJECT_ID -var email_address=$EMAILterraform apply -var project_id=$PROJECT_ID -var email_address=$EMAIL\n```Replace the following values:- : your email address.\nThe output is similar to the following :```\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\n```\n- Connect to the client Pod.```\ncd ../../../kubectl exec -it --namespace postgresql pg-client -- /bin/bash\n```\n- Generate a load test to test the `db_max_transaction` alert.```\npgbench -i -h $HOST_PGPOOL -U postgres -s 200 postgres\n```The output is similar to the following:```\ndropping old tables...\ncreating tables...\ngenerating data (client-side)...\n20000000 of 20000000 tuples (100%) done (elapsed 163.22 s, remaining 0.00 s)\nvacuuming...\ncreating primary keys...\ndone in 191.30 s (drop tables 0.14 s, create tables 0.01 s, client-side generate 165.62 s, vacuum 4.52 s, primary keys 21.00 s).\n```The alert triggers and sends an email to with a subject line that starts with \"[ALERT] Max Lag of transaction\".\n- In the Google Cloud console, navigate to the Alert Policy page. [Go to Alert Policy](https://console.cloud.google.com/monitoring/alerting/policies) \n- Select `db_max_transaction` from the listed policies. From the chart, you should see a spike from the load test which exceeds the threshold hold of 10 for the Prometheus metric `pg_stat_activity_max_tx_duration/gauge` .\n- Exit the postgres client Pod.```\nexit\n```\n## Manage PostgreSQL and GKE upgradesVersion updates for both PostgreSQL and Kubernetes are released on a regular schedule. Follow operational best practices to update your software environment regularly. By default, GKE manages cluster and node pool upgrades for you.\n **Note:** Autopilot clusters are [automatically upgraded](/kubernetes-engine/docs/concepts/cluster-upgrades-autopilot) , based on the release channel you selected.\n### Upgrade PostgreSQLThis section shows how you can perform a version upgrade for PostgreSQL. For this tutorial, you'll use a [rolling update strategy](/kubernetes-engine/docs/how-to/updating-apps) for upgrading your Pods, so that at no point all of the Pods are down.\n **Tip:** If you are upgrading using a Helm chart in production systems, consider other best practices not covered in this tutorial, such as performing data backups, using a canary deployment to test upgrades on a small subset of nodes, and monitoring your cluster during the upgrade process.\nTo perform a version upgrade, follow these steps:- Push an updated version of the `postgresql-repmgr` image to Artifact Registry. Define the new version (for example, `postgresql-repmgr 15.1.0-debian-11-r1` ).```\nNEW_IMAGE=us-docker.pkg.dev/$PROJECT_ID/main/bitnami/postgresql-repmgr:15.1.0-debian-11-r1./scripts/gcr.sh bitnami/postgresql-repmgr 15.1.0-debian-11-r1\n```\n- Trigger a rolling update using `kubectl` .```\nkubectl set image statefulset -n postgresql postgresql-postgresql-ha-postgresql postgresql=$NEW_IMAGEkubectl rollout restart statefulsets -n postgresql postgresql-postgresql-ha-postgresqlkubectl rollout status statefulset -n postgresql postgresql-postgresql-ha-postgresql\n```You will see the StatefulSet complete a [rolling update](https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#rolling-update) , starting with the highest ordinal replica to the lowest.The output is similar to the following:```\nWaiting for 1 pods to be ready...\nwaiting for statefulset rolling update to complete 1 pods at revision postgresql-postgresql-ha-postgresql-5c566ccf49...\nWaiting for 1 pods to be ready...\nWaiting for 1 pods to be ready...\nwaiting for statefulset rolling update to complete 2 pods at revision postgresql-postgresql-ha-postgresql-5c566ccf49...\nWaiting for 1 pods to be ready...\nWaiting for 1 pods to be ready...\nstatefulset rolling update complete 3 pods at revision postgresql-postgresql-ha-postgresql-5c566ccf49...\n```\n### Plan for GKE upgrades on Standard clustersThis section is applicable if you are running Standard clusters. You can take proactive steps and set configurations to mitigate risk and facilitate a smoother cluster upgrade when you are running stateful services, including:- Follow [GKE best practices for upgrading clusters](/kubernetes-engine/docs/best-practices/upgrading-clusters) . Choose an appropriate [upgrade strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) to ensure the upgrades happen during the period of the maintenance window:- Choose [surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) if cost optimization is important and if your workloads can tolerate a graceful shutdown in less than 60 minutes.\n- Choose [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) if your workloads are less tolerant of disruptions, and a temporary cost increase due to higher resource usage is acceptable.\nTo learn more, see [Upgrade a cluster running a stateful workload](/kubernetes-engine/docs/tutorials/upgrading-stateful-workload#configure_a_node_pool_upgrade_strategy) .\n- Use the [Recommender](/recommender/docs/overview) service to check for deprecation insights and recommendations to avoid service interruptions.\n- Use [maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) to ensure upgrades happen when you intend them. Before the maintenance window, ensure your database backups are successful.\n- Before allowing traffic to the upgraded nodes, use [readinessand liveness probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) to ensure they are ready for traffic.\n- Create Probes that assess whether replication is in sync before accepting traffic. This can be done through custom scripts, depending on the complexity and scale of your database.\n### Verify database availability during Standard cluster upgradesThis section is applicable if you are running Standard clusters. To verify PostgreSQL availability during upgrades, the general process is to generate traffic against the PostgreSQL database during the upgrade process. Then, use [pgbench](https://www.postgresql.org/docs/current/pgbench.html) to check that the database can handle a baseline level of traffic during an upgrade, compared to when the database is fully available.- Connect to your PostgreSQL instance.```\n./scripts/launch-client.sh\n```The output is similar to the following:```\nLaunching Pod pg-client in the namespace postgresql ...\npod/pg-client created\nwaiting for the Pod to be ready\nCopying script files to the target Pod pg-client ...\nPod: pg-client is healthy\n```\n- In Cloud Shell, shell into the client Pod.```\nkubectl exec -it -n postgresql pg-client -- /bin/bash\n```\n- Initialize pgbench .```\npgbench -i -h $HOST_PGPOOL -U postgres postgres\n```\n- Use the following command to get baseline results for confirming that your PostgreSQL application stays highly-available during the time window for an upgrade. To get a baseline result, test with multi-connections via multi jobs (threads) for 30 seconds.```\npgbench -h $HOST_PGPOOL -U postgres postgres -c10 -j4 -T 30 -R 200\n```The output looks similar to the following:```\npgbench (14.5)\nstarting vacuum...end.\ntransaction type: <builtin: TPC-B (sort of)>\nscaling factor: 1\nquery mode: simple\nnumber of clients: 10\nnumber of threads: 4\nduration: 30 s\nnumber of transactions actually processed: 5980\nlatency average = 7.613 ms\nlatency stddev = 2.898 ms\nrate limit schedule lag: avg 0.256 (max 36.613) ms\ninitial connection time = 397.804 ms\ntps = 201.955497 (without initial connection time)\n```\n- To ensure availability during upgrades, you can generate some load against your database, and ensure that the PostgreSQL application provides a consistent response rate during the upgrade. To perform this test, generate some traffic against the database, using the `pgbench` command. The following command will run `pgbench` for one hour, targeting 200 TPS (transactions per second), and listing the request rate every 2 seconds.```\npgbench -h $HOST_PGPOOL -U postgres postgres --client=10 --jobs=4 --rate=200 --time=3600 --progress=2 --select-only\n```Where:- `--client`: Number of clients simulated, that is, number of concurrent database sessions.\n- `--jobs`: Number of worker threads within pgbench. Using more than one thread can be helpful on multi-CPU machines. Clients are distributed as evenly as possible among available threads. The default is 1.\n- `--rate`: The rate is given in transactions per second\n- `--progress`: Show progress report every sec seconds.\nThe output is similar to the following:```\npgbench (14.5)starting vacuum...end.progress: 5.0 s, 354.8 tps, lat 25.222 ms stddev 15.038progress: 10.0 s, 393.8 tps, lat 25.396 ms stddev 16.459progress: 15.0 s, 412.8 tps, lat 24.216 ms stddev 14.548progress: 20.0 s, 405.0 tps, lat 24.656 ms stddev 14.066\n```\n- In the Google Cloud console, navigate back to the **PostgreSQL Overview** dashboard in Cloud Monitoring. Notice the spike on the **Connection per DB** and **Connection per Pod** graphs.\n- Exit the client Pod.```\nexit\n```\n- Delete the client Pod.```\nkubectl delete pod -n postgresql pg-client\n```\n## Simulate a PostgreSQL service disruptionIn this section, you'll simulate a service disruption in one of the PostgreSQL replicas by stopping the replication manager service. This will prevent the Pod from serving traffic to its peer replicas and its liveness probes to fail.- Open a new Cloud Shell session and configure `kubectl` command line access to the primary cluster.```\ngcloud container clusters get-credentials $SOURCE_CLUSTER \\--region=$REGION --project=$PROJECT_ID\n```\n- View the PostgreSQL events emitted in Kubernetes.```\nkubectl get events -n postgresql --field-selector=involvedObject.name=postgresql-postgresql-ha-postgresql-0 --watch\n```\n- In the earlier Cloud Shell session, simulate a service failure by stopping PostgreSQL `repmgr` .- Attach your session to the database container.```\nkubectl exec -it -n $NAMESPACE postgresql-postgresql-ha-postgresql-0 -c postgresql -- /bin/bash\n```\n- Stop the service using `repmgr` , and remove the checkpoint and the `dry-run` argument.```\nexport ENTRY='/opt/bitnami/scripts/postgresql-repmgr/entrypoint.sh'export RCONF='/opt/bitnami/repmgr/conf/repmgr.conf'$ENTRY repmgr -f $RCONF node service --action=stop --checkpoint\n```\nThe liveness probe configured for the PostgreSQL container will start to fail within five seconds. This repeats every ten seconds, until the failure threshold of six failures is reached. Once the `failureThreshold` value is reached, the container is restarted. You can configure these parameters to decrease the liveness probe tolerance to tune the SLO requirements of your deployment.\nFrom the event stream, you will see the Pod's liveness and readiness probes fail, and a message that the container needs to be restarted. The output is similar to the following:\n```\n0s   Normal Killing    pod/postgresql-postgresql-ha-postgresql-0 Container postgresql failed liveness probe, will be restarted\n0s   Warning Unhealthy    pod/postgresql-postgresql-ha-postgresql-0 Readiness probe failed: psql: error: connection to server at \"127.0.0.1\", port 5432 failed: Connection refused...\n0s   Normal Pulled     pod/postgresql-postgresql-ha-postgresql-0 Container image \"us-docker.pkg.dev/psch-gke-dev/main/bitnami/postgresql-repmgr:14.5.0-debian-11-r10\" already present on machine\n0s   Normal Created    pod/postgresql-postgresql-ha-postgresql-0 Created container postgresql\n0s   Normal Started    pod/postgresql-postgresql-ha-postgresql-0 Started container postgresql\n```## Prepare for disaster recoveryTo ensure that your production workloads remain available in the event of a service-interrupting event, you should prepare a disaster recovery (DR) plan. To learn more about DR planning, see the [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\nDisaster recovery for Kubernetes can be implemented in two phases:- involves creating a point-in-time snapshot of your state or data before a service-interrupting event occurs.\n- involves restoring your state or data from a backup copy after the occurrence of a disaster.\nTo backup and restore your workloads on GKE clusters, you can use [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) . You can enable this service on [new and existing clusters](/kubernetes-engine/docs/add-on/backup-for-gke/how-to/install) . This deploys a [Backup for GKE agent](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke#agent_overview) that runs in your clusters; the agent is responsible for capturing configuration and volume backup data and orchestrating recovery.\n **Tip:** If you are using Autopilot clusters, check that your infrastructure works with these [Backup for GKE restrictions](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/about-autopilot) .\nBackups and restores can be scoped to an entire cluster, a namespace, or an application (defined by selectors such as `matchLabels` ).\n### Example PostgreSQL backup and restore scenarioThe example in this section shows how you can perform a backup and restore operation at the application scope, using the `ProtectedApplication` Custom Resource.\nThe following diagram shows the component resources in the ProtectedApplication, namely a StatefulSet representing the `postgresql-ha` application and a deployment of `pgpool` , which use the same label ( `app.kubernetes.io/name: postgresql-ha` ).To prepare to backup and restore your PostgreSQL workload, follow these steps:- Set up the environment variables. In this example you'll use a ProtectedApplication to restore the PostgreSQL workload and its volumes from the source GKE cluster ( `us-central1` ), then restore to another GKE cluster in a different region ( `us-west1` ).```\nexport SOURCE_CLUSTER=cluster-db1export TARGET_CLUSTER=cluster-db2export REGION=us-central1export DR_REGION=us-west1export NAME_PREFIX=g-db-protected-appexport BACKUP_PLAN_NAME=$NAME_PREFIX-bkp-plan-01export BACKUP_NAME=bkp-$BACKUP_PLAN_NAMEexport RESTORE_PLAN_NAME=$NAME_PREFIX-rest-plan-01export RESTORE_NAME=rest-$RESTORE_PLAN_NAME\n```\n- Verify that Backup for GKE is enabled on your clusters. It should already be enabled as part of the Terraform setup you performed earlier.```\ngcloud container clusters describe $SOURCE_CLUSTER \\\u00a0 \u00a0 --project=$PROJECT_ID \u00a0\\\u00a0 \u00a0 --region=$REGION \\\u00a0 \u00a0 --format='value(addonsConfig.gkeBackupAgentConfig)'\n```If Backup for GKE is enabled, the output of the command shows `enabled=True` .\nBackup for GKE allows you to create a [backup plan](/kubernetes-engine/docs/add-on/backup-for-gke/how-to/backup-plan) as a cron job. A backup plan contains a backup configuration including the source cluster, the selection of which workloads to back up, and the region in which backup artifacts produced under this plan are stored.\nTo perform a backup and restore, follow these steps:- Verify the status of ProtectedApplication on `cluster-db1` .```\nkubectl get ProtectedApplication -A\n```The output looks similar to the following:```\nNAMESPACE NAME   READY TO BACKUP\npostgresql postgresql-ha true\n```\n- Create a backup plan for the ProtectedApplication.```\nexport NAMESPACE=postgresqlexport PROTECTED_APP=$(kubectl get ProtectedApplication -n $NAMESPACE | grep -v 'NAME' | awk '{ print $1 }')\n``````\ngcloud beta container backup-restore backup-plans create $BACKUP_PLAN_NAME \\--project=$PROJECT_ID \\--location=$DR_REGION \\--cluster=projects/$PROJECT_ID/locations/$REGION/clusters/$SOURCE_CLUSTER \\--selected-applications=$NAMESPACE/$PROTECTED_APP \\--include-secrets \\--include-volume-data \\--cron-schedule=\"0 3 * * *\" \\--backup-retain-days=7 \\--backup-delete-lock-days=0\n```\n- Manually create a backup.```\ngcloud beta container backup-restore backups create $BACKUP_NAME \\--project=$PROJECT_ID \\--location=$DR_REGION \\--backup-plan=$BACKUP_PLAN_NAME \\--wait-for-completion\n```\n- Set up a restore plan.```\ngcloud beta container backup-restore restore-plans create $RESTORE_PLAN_NAME \\\u00a0 --project=$PROJECT_ID \\\u00a0 --location=$DR_REGION \\\u00a0 --backup-plan=projects/$PROJECT_ID/locations/$DR_REGION/backupPlans/$BACKUP_PLAN_NAME \\\u00a0 --cluster=projects/$PROJECT_ID/locations/$DR_REGION/clusters/$TARGET_CLUSTER \\\u00a0 --cluster-resource-conflict-policy=use-existing-version \\\u00a0 --namespaced-resource-restore-mode=delete-and-restore \\\u00a0 --volume-data-restore-policy=restore-volume-data-from-backup \\\u00a0 --selected-applications=$NAMESPACE/$PROTECTED_APP \\\u00a0 --cluster-resource-scope-selected-group-kinds=\"storage.k8s.io/StorageClass\",\"scheduling.k8s.io/PriorityClass\"\n```\n- Restore from the backup.```\ngcloud beta container backup-restore restores create $RESTORE_NAME \\\u00a0 --project=$PROJECT_ID \\\u00a0 --location=$DR_REGION \\\u00a0 --restore-plan=$RESTORE_PLAN_NAME \\\u00a0 --backup=projects/$PROJECT_ID/locations/$DR_REGION/backupPlans/$BACKUP_PLAN_NAME/backups/$BACKUP_NAME \\\u00a0 --wait-for-completion\n```\nTo verify that the restored cluster has all the expected Pods, PersistentVolume, and StorageClass resources, follow these steps:- Configure `kubectl` command line access to the backup cluster `cluster-db2` .```\ngcloud container clusters get-credentials $TARGET_CLUSTER --region $DR_REGION --project $PROJECT_ID\n```\n- Verify that the StatefulSet is ready with 3/3 Pods.```\nkubectl get all -n $NAMESPACE\n```The output is similar to the following:```\nNAME             READY STATUS RESTARTS  AGE\npod/postgresql-postgresql-ha-pgpool-778798b5bd-k2q4b 1/1  Running 0    4m49s\npod/postgresql-postgresql-ha-postgresql-0    2/2  Running 2 (4m13s ago) 4m49s\npod/postgresql-postgresql-ha-postgresql-1    2/2  Running 0    4m49s\npod/postgresql-postgresql-ha-postgresql-2    2/2  Running 0    4m49s\nNAME             TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S) AGE\nservice/postgresql-postgresql-ha-pgpool    ClusterIP 192.168.241.46 <none>  5432/TCP 4m49s\nservice/postgresql-postgresql-ha-postgresql   ClusterIP 192.168.220.20 <none>  5432/TCP 4m49s\nservice/postgresql-postgresql-ha-postgresql-headless ClusterIP None    <none>  5432/TCP 4m49s\nservice/postgresql-postgresql-ha-postgresql-metrics ClusterIP 192.168.226.235 <none>  9187/TCP 4m49s\nNAME            READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/postgresql-postgresql-ha-pgpool 1/1  1   1   4m49s\nNAME               DESIRED CURRENT READY AGE\nreplicaset.apps/postgresql-postgresql-ha-pgpool-778798b5bd 1   1   1  4m49s\nNAME             READY AGE\nstatefulset.apps/postgresql-postgresql-ha-postgresql 3/3  4m49s\n```\n- Verify all Pods in the `postgres` namespace are running.```\nkubectl get pods -n $NAMESPACE\n```The output is similar to the following:```\npostgresql-postgresql-ha-pgpool-569d7b8dfc-2f9zx 1/1  Running 0   7m56s\npostgresql-postgresql-ha-postgresql-0    2/2  Running 0   7m56s\npostgresql-postgresql-ha-postgresql-1    2/2  Running 0   7m56s\npostgresql-postgresql-ha-postgresql-2    2/2  Running 0   7m56s\n```\n- Verify the PersistentVolumes and StorageClass. During the restore process, Backup for GKE creates a Proxy Class in the target workload to replace the StorageClass provisioned in the source workload ( `gce-pd-gkebackup-dn` in the example output).```\nkubectl get pvc -n $NAMESPACE\n```The output is similar to the following:```\nNAME           STATUS VOLUME     CAPACITY ACCESS MODES STORAGECLASS   AGE\ndata-postgresql-postgresql-ha-postgresql-0 Bound pvc-be91c361e9303f96 8Gi  RWO   gce-pd-gkebackup-dn 10m\ndata-postgresql-postgresql-ha-postgresql-1 Bound pvc-6523044f8ce927d3 8Gi  RWO   gce-pd-gkebackup-dn 10m\ndata-postgresql-postgresql-ha-postgresql-2 Bound pvc-c9e71a99ccb99a4c 8Gi  RWO   gce-pd-gkebackup-dn 10m\n```\nTo validate that the expected data is restored, follow these steps:- Connect to your PostgreSQL instance.```\n./scripts/launch-client.shkubectl exec -it pg-client -n postgresql -- /bin/bash\n```\n- Verify the number of rows for each table.```\npsql -h $HOST_PGPOOL -U postgres -a -q -f /tmp/scripts/count-rows.sqlselect COUNT(*) from tb01;\n```You should see a similar result to the data you wrote earlier in the [Create a test dataset](/kubernetes-engine/docs/tutorials/stateful-workloads/postgresql#test-dataset) . The output is similar to the following:```\n300000(1 row)\n```\n- Exit the client Pod.```\nexit\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to avoid billing is to delete the project you created for the tutorial.\n **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\nDelete a Google Cloud project:\n```\ngcloud projects delete PROJECT_ID\n```## What's next\n- Learn the [best practices for deploying databases](/kubernetes-engine/docs/concepts/database-options) on GKE.\n- Explore [Persistent Volumes](/kubernetes-engine/docs/concepts/persistent-volumes) in more detail.\n- See an [example](https://www.pgpool.net/docs/42/en/html/example-cluster.html#EXAMPLE-CLUSTER-PGPOOL-CONFIG) of how to use Pgpool-II for streaming replication with high-availability PostgreSQL clusters.", "guide": "Google Kubernetes Engine (GKE)"}