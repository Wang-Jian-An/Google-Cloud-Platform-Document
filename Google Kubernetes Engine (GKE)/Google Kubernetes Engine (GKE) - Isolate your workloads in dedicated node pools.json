{"title": "Google Kubernetes Engine (GKE) - Isolate your workloads in dedicated node pools", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/isolate-workloads-dedicated-nodes", "abstract": "# Google Kubernetes Engine (GKE) - Isolate your workloads in dedicated node pools\nThis page shows you how to reduce the risk of privilege escalation attacks in your cluster by telling Google Kubernetes Engine (GKE) to schedule your workloads on a separate, dedicated node pool away from privileged GKE-managed workloads. This page applies to Standard clusters without node auto-provisioning. To separate workloads in Autopilot clusters and in Standard clusters with node auto-provisioning enabled, refer to [Configure workload separation in GKE](/kubernetes-engine/docs/how-to/workload-separation) .\n**Note:** In almost all cases, we recommend using [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) , which performs node isolation for you in addition to providing other hardening benefits for your workloads. If you **can't** use GKE Sandbox and still want a layer of isolation for your workloads, follow the instructions in this guide.\n", "content": "## Overview\nGKE clusters use privileged GKE-managed workloads to enable specific cluster functionality and features, such as [metrics gathering](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#understand_how_metrics_server_works_and_how_to_monitor_it) . These workloads are given special permissions to run correctly in the cluster.\nWorkloads that you deploy to your nodes might have the potential to be compromised by a malicious entity. Running these workloads alongside privileged GKE-managed workloads means that an attacker who breaks out of a compromised container can use the credentials of the privileged workload on the node to escalate privileges in your cluster.\n### Preventing container breakouts\nYour primary defense should be your applications. GKE has multiple features that you can use to harden your clusters and Pods. In most cases, we **strongly recommend** using [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) to isolate your workloads. GKE Sandbox is based on the [gVisor open source project](https://github.com/google/gvisor) , and implements the Linux kernel API in the userspace. Each Pod runs on a dedicated kernel that sandboxes applications to prevent access to privileged system calls in the host kernel. Workloads running in GKE Sandbox are automatically scheduled on separate nodes, isolated from other workloads.\nYou should also follow the recommendations in [Harden your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster) .\n### Avoiding privilege escalation attacks\nIf you can't use GKE Sandbox, and you want an extra layer of isolation in addition to other hardening measures, you can use [node taints](/kubernetes-engine/docs/how-to/node-taints) and [node affinity](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/) to schedule your workloads on a dedicated node pool. A node taint tells GKE to avoid scheduling workloads without a corresponding toleration (such as GKE-managed workloads) on those nodes. The node affinity on your own workloads tells GKE to schedule your Pods on the dedicated nodes.\n**Caution:** Node isolation is an advanced defence-in-depth mechanism that you should **only** use alongside other isolation features such as minimally-privileged containers and service accounts. Node isolation might not cover all escalation paths, and should never be used as a primary security boundary. We don't recommend this approach unless you can't use GKE Sandbox.\n### Limitations of node isolation\n- Attackers can still initiate Denial-of-Service (DoS) attacks from the compromised node.\n- Compromised nodes can still read many resources, including all Pods and namespaces in the cluster.\n- Compromised nodes can access Secrets and credentials used by every Pod running on that node.\n- Using a separate node pool to isolate your workloads can impact your cost efficiency, autoscaling, and resource utilization.\n- Compromised nodes can still bypass egress network policies.\n- Some GKE-managed workloads must run on every node in your cluster, and are configured to tolerate all taints.\n- If you deploy DaemonSets that have elevated permissions and can tolerate any taint, those Pods may be a pathway for privilege escalation from a compromised node.## How node isolation works\nTo implement node isolation for your workloads, you must do the following:\n- Taint and label a node pool for your workloads.\n- Update your workloads with the corresponding toleration and node affinity rule.\nThis guide assumes that you start with one node pool in your cluster. Using node affinity in addition to node taints isn't mandatory, but we recommend it because you benefit from greater control over scheduling.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Choose a specific name for the node taint and the node label that you want to use for the dedicated node pools. For this example, we use`workloadType=untrusted`.## Taint and label a node pool for your workloads\nCreate a new node pool for your workloads and apply a node taint and a node label. When you apply a taint or a label at the node pool level, any new nodes, such as those created by autoscaling, will automatically get the specified taints and labels.\nYou can also add node taints and node labels to existing node pools. If you use the `NoExecute` effect, GKE evicts any Pods running on those nodes that don't have a toleration for the new taint.\nTo add a taint and a label to a new node pool, run the following command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --node-taints TAINT_KEY=TAINT_VALUE:TAINT_EFFECT \\\u00a0 \u00a0 --node-labels LABEL_KEY=LABEL_VALUE\n```\nReplace the following:\n- ``: the name of the new node pool for your workloads.\n- ``: the name of your GKE cluster.\n- `` `=` ``: a key-value pair associated with a scheduling``. For example,`workloadType=untrusted`.\n- ``: one of the following [effect values](/kubernetes-engine/docs/how-to/node-taints#effects) :`NoSchedule`,`PreferNoSchedule`, or`NoExecute`.`NoExecute`provides a better eviction guarantee than`NoSchedule`.\n- ``=``: key-value pairs for the node labels, which correspond to the selectors that you specify in your workload manifests.## Add a toleration and a node affinity rule to your workloads\nAfter you taint the dedicated node pool, no workloads can schedule on it unless they have a toleration corresponding to the taint you added. Add the toleration to the specification for your workloads to let those Pods schedule on your tainted node pool.\nIf you labelled the dedicated node pool, you can also add a node affinity rule to tell GKE to only schedule your workloads on that node pool.\nThe following example adds a toleration for the `workloadType=untrusted:NoExecute` taint and a node affinity rule for the `workloadType=untrusted` node label.\n```\nkind: DeploymentapiVersion: apps/v1metadata:\u00a0 name: my-app\u00a0 namespace: default\u00a0 labels:\u00a0 \u00a0 app: my-appspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: my-app\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: my-app\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: TAINT_KEY\u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 value: TAINT_VALUE\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: LABEL_KEY\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"LABEL_VALUE\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: sleep\u00a0 \u00a0 \u00a0 \u00a0 image: ubuntu\u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sleep\", \"inf\"]\n```\nReplace the following:\n- ``: the taint key that you applied to your dedicated node pool.\n- ``: the taint value that you applied to your dedicated node pool.\n- ``: the node label key that you applied to your dedicated node pool.\n- ``: the node label value that you applied to your dedicated node pool.\nWhen you update your Deployment with `kubectl apply` , GKE recreates the affected Pods. The node affinity rule forces the Pods onto the dedicated node pool that you created. The toleration allows only those Pods to be placed on the nodes.\n## Verify that the separation works\nTo verify that the scheduling works correctly, run the following command and check whether your workloads are on the dedicated node pool:\n```\nkubectl get pods -o=wide\n```\n## Recommendations and best practices\nAfter setting up node isolation, we recommend that you do the following:\n- Restrict specific node pools to GKE-managed workloads only by adding the`components.gke.io/gke-managed-components`taint. Adding this taint prevents your own Pods from scheduling on those nodes, improving the isolation.\n- When creating new node pools, prevent most GKE-managed workloads from running on those nodes by adding your own taint to those node pools.\n- Whenever you deploy new workloads to your cluster, such as when installing third-party tooling, audit the permissions that the Pods require. When possible, avoid deploying workloads that use elevated permissions to shared nodes.## What's next\n- [Learn more about GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\n- [Learn about GKE Autopilot, which implements manyGKE security features by default](/kubernetes-engine/docs/concepts/autopilot-overview) .", "guide": "Google Kubernetes Engine (GKE)"}