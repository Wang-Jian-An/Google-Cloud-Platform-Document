{"title": "Google Kubernetes Engine (GKE) - Train a model with GPUs on GKE Standard mode", "url": "https://cloud.google.com/kubernetes-engine/docs/quickstarts/train-model-gpus-standard", "abstract": "# Google Kubernetes Engine (GKE) - Train a model with GPUs on GKE Standard mode\n# Train a model with GPUs on GKE Standard modeThis quickstart shows you how to deploy a training model with GPUs in Google Kubernetes Engine (GKE) and store the predictions in Cloud Storage. This document is intended for GKE administrators who have existing Standard mode clusters and want to run GPU workloads for the first time.\nYou can also run these workloads on Autopilot mode clusters with fewer setup steps.", "content": "## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.## Clone the sample repositoryIn Cloud Shell, run the following command:\n```\ngit clone https://github.com/GoogleCloudPlatform/ai-on-gke/ ai-on-gkecd ai-on-gke/training-single-gpu\n```## Create a Standard mode cluster and a GPU node poolUse Cloud Shell to do the following:- Create a Standard cluster that uses [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) and installs the [Cloud Storage FUSE driver](/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver) :```\ngcloud container clusters create gke-gpu-cluster \\\u00a0 \u00a0 --addons GcsFuseCsiDriver \\\u00a0 \u00a0 --location=us-central1 \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --workload-pool=PROJECT_ID.svc.id.goog\n```Replace `` with your Google Cloud project ID.Cluster creation might take several minutes.\n- Create a GPU node pool:```\ngcloud container node-pools create gke-gpu-pool-1 \\\u00a0 \u00a0 --accelerator=type=nvidia-tesla-t4,count=1,gpu-driver-version=default \\\u00a0 \u00a0 --machine-type=n1-standard-16 --num-nodes=1 \\\u00a0 \u00a0 --location=us-central1 \\\u00a0 \u00a0 --cluster=gke-gpu-cluster\n```\n## Create a Cloud Storage bucket\n- In the Google Cloud console, go to the **Create a bucket** page: [Go to Create a bucket](https://console.cloud.google.com/storage/create-bucket) \n- In the **Name your bucket** field, enter the following name:```\nPROJECT_ID-gke-gpu-bucket\n```\n- Click **Continue** .\n- For **Location type** , select **Region** .\n- In the **Region** list, select `us-central1 (Iowa)` and click **Continue** .\n- In the **Choose a storage class for your data** section, click **Continue** .\n- In the **Choose how to control access to objects** section, for **Access control** , select **Uniform** .\n- Click **Create** .\n- In the **Public access will be prevented** dialog, ensure that the **Enforce public access prevention on this bucket** checkbox is selected, and click **Confirm** .\n## Configure your cluster to access the bucket using workload identity federation for GKETo let your cluster access the Cloud Storage bucket, you do the following:- Create a Google Cloud service account.\n- Create a Kubernetes ServiceAccount in your cluster.\n- Bind the Kubernetes ServiceAccount to the Google Cloud service account.\n### Create a Google Cloud service account\n- In the Google Cloud console, go to the **Create service account** page: [Go to Create service account](https://console.cloud.google.com/iam-admin/serviceaccounts/create) \n- In the **Service account ID** field, enter `gke-ai-sa` .\n- Click **Create and continue** .\n- In the **Role** list, select the **Cloud Storage > Storage Insights Collector Service** role.\n- Click add **Add another role** .\n- In the **Select a role** list, select the **Cloud Storage > Storage Object Admin** role.\n- Click **Continue** , and then click **Done** .\n### Create a Kubernetes ServiceAccount in your clusterIn Cloud Shell, do the following:- Create a Kubernetes namespace:```\nkubectl create namespace gke-ai-namespace\n```\n- Create a Kubernetes ServiceAccount in the namespace:```\nkubectl create serviceaccount gpu-k8s-sa --namespace=gke-ai-namespace\n```\n### Bind the Kubernetes ServiceAccount to the Google Cloud service accountIn Cloud Shell, run the following commands:- Add an IAM binding to the Google Cloud service account:```\ngcloud iam service-accounts add-iam-policy-binding gke-ai-sa@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[gke-ai-namespace/gpu-k8s-sa]\"\n```The `--member` flag provides the full identity of the Kubernetes ServiceAccount in Google Cloud.\n- Annotate the Kubernetes ServiceAccount:```\nkubectl annotate serviceaccount gpu-k8s-sa \\\u00a0 \u00a0 --namespace gke-ai-namespace \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=gke-ai-sa@PROJECT_ID.iam.gserviceaccount.com\n```\n## Verify that Pods can access the Cloud Storage bucket\n- In Cloud Shell, create the following environment variables:```\nexport K8S_SA_NAME=gpu-k8s-saexport BUCKET_NAME=PROJECT_ID-gke-gpu-bucket\n```Replace `` with your Google Cloud project ID.\n- Create a Pod that has a [TensorFlow](https://www.tensorflow.org/) container:```\nenvsubst < src/gke-config/standard-tensorflow-bash.yaml | kubectl --namespace=gke-ai-namespace apply -f ```This command substitutes the environment variables that you created into the corresponding references in the manifest. You can also open the manifest in a text editor and replace `$K8S_SA_NAME` and `$BUCKET_NAME` with the corresponding values.\n- Create a sample file in the bucket:```\ntouch sample-filegsutil cp sample-file gs://PROJECT_ID-gke-gpu-bucket\n```\n- Wait for your Pod to become ready:```\nkubectl wait --for=condition=Ready pod/test-tensorflow-pod -n=gke-ai-namespace --timeout=180s\n```When the Pod is ready, the output is the following:```\npod/test-tensorflow-pod condition met\n```\n- Open a shell in the Tensorflow container:```\nkubectl -n gke-ai-namespace exec --stdin --tty test-tensorflow-pod --container tensorflow -- /bin/bash\n```\n- Try to read the sample file that you created:```\nls /data\n```The output shows the sample file.\n- Check the logs to identify the GPU attached to the Pod:```\npython -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```The output shows the GPU attached to the Pod, similar to the following:```\n...\nPhysicalDevice(name='/physical_device:GPU:0',device_type='GPU')\n```\n- Exit the container:```\nexit\n```\n- Delete the sample Pod:```\nkubectl delete -f src/gke-config/standard-tensorflow-bash.yaml \\\u00a0 \u00a0 --namespace=gke-ai-namespace\n```\n **Success:** At this point, your cluster runs a GPU node pool and can communicate with the Cloud Storage bucket using the Kubernetes ServiceAccount.## Train and predict using the MNIST datasetIn this section, you run a training workload on the `MNIST` example dataset.- Copy the example data to the Cloud Storage bucket:```\ngsutil -m cp -R src/tensorflow-mnist-example gs://PROJECT_ID-gke-gpu-bucket/\n```\n- Create the following environment variables:```\nexport K8S_SA_NAME=gpu-k8s-saexport BUCKET_NAME=PROJECT_ID-gke-gpu-bucket\n```\n- Review the training Job: [  training-single-gpu/src/gke-config/standard-tf-mnist-train.yaml ](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/HEAD/training-single-gpu/src/gke-config/standard-tf-mnist-train.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/HEAD/training-single-gpu/src/gke-config/standard-tf-mnist-train.yaml) ```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: mnist-training-jobspec:\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 name: mnist\u00a0 \u00a0 \u00a0 annotations:\u00a0 \u00a0 \u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-tesla-t4\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"nvidia.com/gpu\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: tensorflow\u00a0 \u00a0 \u00a0 \u00a0 image: tensorflow/tensorflow:latest-gpu \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/bash\", \"-c\", \"--\"]\u00a0 \u00a0 \u00a0 \u00a0 args: [\"cd /data/tensorflow-mnist-example; pip install -r requirements.txt; python tensorflow_mnist_train_distributed.py\"]\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 3Gi\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-vol\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: false\u00a0 \u00a0 \u00a0 serviceAccountName: $K8S_SA_NAME\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-vol\u00a0 \u00a0 \u00a0 \u00a0 csi:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 driver: gcsfuse.csi.storage.gke.io\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bucketName: $BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountOptions: \"implicit-dirs\"\u00a0 \u00a0 \u00a0 restartPolicy: \"Never\"\n```\n- Deploy the training Job:```\nenvsubst < src/gke-config/standard-tf-mnist-train.yaml | kubectl -n gke-ai-namespace apply -f ```This command substitutes the environment variables that you created into the corresponding references in the manifest. You can also open the manifest in a text editor and replace `$K8S_SA_NAME` and `$BUCKET_NAME` with the corresponding values.\n- Wait until the Job has the `Completed` status:```\nkubectl wait -n gke-ai-namespace --for=condition=Complete job/mnist-training-job --timeout=180s\n```The output is similar to the following:```\njob.batch/mnist-training-job condition met\n```\n- Check the logs from the Tensorflow container:```\nkubectl logs -f jobs/mnist-training-job -c tensorflow -n gke-ai-namespace\n```The output shows the following events occur:- Install required Python packages\n- Download the MNIST dataset\n- Train the model using a GPU\n- Save the model\n- Evaluate the model\n```\n...\nEpoch 12/12\n927/938 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9954\nLearning rate for epoch 12 is 9.999999747378752e-06\n938/938 [==============================] - 5s 6ms/step - loss: 0.0187 - accuracy: 0.9954 - lr: 1.0000e-05\n157/157 [==============================] - 1s 4ms/step - loss: 0.0424 - accuracy: 0.9861\nEval loss: 0.04236088693141937, Eval accuracy: 0.9861000180244446\nTraining finished. Model saved\n```\n- Delete the training workload:```\nkubectl -n gke-ai-namespace delete -f src/gke-config/standard-tf-mnist-train.yaml\n```\n## Deploy an inference workloadIn this section, you deploy an inference workload that takes a sample dataset as input and returns predictions.- Copy the images for prediction to the bucket:```\ngsutil -m cp -R data/mnist_predict gs://PROJECT_ID-gke-gpu-bucket/\n```\n- Review the inference workload: [  training-single-gpu/src/gke-config/standard-tf-mnist-batch-predict.yaml ](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/HEAD/training-single-gpu/src/gke-config/standard-tf-mnist-batch-predict.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/HEAD/training-single-gpu/src/gke-config/standard-tf-mnist-batch-predict.yaml) ```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: mnist-batch-prediction-jobspec:\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 name: mnist\u00a0 \u00a0 \u00a0 annotations:\u00a0 \u00a0 \u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-tesla-t4\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"nvidia.com/gpu\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: tensorflow\u00a0 \u00a0 \u00a0 \u00a0 image: tensorflow/tensorflow:latest-gpu \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/bash\", \"-c\", \"--\"]\u00a0 \u00a0 \u00a0 \u00a0 args: [\"cd /data/tensorflow-mnist-example; pip install -r requirements.txt; python tensorflow_mnist_batch_predict.py\"]\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 3Gi\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-vol\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: false\u00a0 \u00a0 \u00a0 serviceAccountName: $K8S_SA_NAME\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-vol\u00a0 \u00a0 \u00a0 \u00a0 csi:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 driver: gcsfuse.csi.storage.gke.io\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bucketName: $BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountOptions: \"implicit-dirs\"\u00a0 \u00a0 \u00a0 restartPolicy: \"Never\"\n```\n- Deploy the inference workload:```\nenvsubst < src/gke-config/standard-tf-mnist-batch-predict.yaml | kubectl -n gke-ai-namespace apply -f ```This command substitutes the environment variables that you created into the corresponding references in the manifest. You can also open the manifest in a text editor and replace `$K8S_SA_NAME` and `$BUCKET_NAME` with the corresponding values.\n- Wait until the Job has the `Completed` status:```\nkubectl wait -n gke-ai-namespace --for=condition=Complete job/mnist-batch-prediction-job --timeout=180s\n```The output is similar to the following:```\njob.batch/mnist-batch-prediction-job condition met\n```\n- Check the logs from the Tensorflow container:```\nkubectl logs -f jobs/mnist-batch-prediction-job -c tensorflow -n gke-ai-namespace\n```The output is the prediction for each image and the model's confidence in the prediction, similar to the following:```\nFound 10 files belonging to 1 classes.\n1/1 [==============================] - 2s 2s/step\nThe image /data/mnist_predict/0.png is the number 0 with a 100.00 percent confidence.\nThe image /data/mnist_predict/1.png is the number 1 with a 99.99 percent confidence.\nThe image /data/mnist_predict/2.png is the number 2 with a 100.00 percent confidence.\nThe image /data/mnist_predict/3.png is the number 3 with a 99.95 percent confidence.\nThe image /data/mnist_predict/4.png is the number 4 with a 100.00 percent confidence.\nThe image /data/mnist_predict/5.png is the number 5 with a 100.00 percent confidence.\nThe image /data/mnist_predict/6.png is the number 6 with a 99.97 percent confidence.\nThe image /data/mnist_predict/7.png is the number 7 with a 100.00 percent confidence.\nThe image /data/mnist_predict/8.png is the number 8 with a 100.00 percent confidence.\nThe image /data/mnist_predict/9.png is the number 9 with a 99.65 percent confidence.\n```\n **Success:** You've successfully trained a model and used it to evaluate new data.## Clean upTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, do one of the following:- **Keep the GKE cluster:** Delete the Kubernetes resources in the cluster and the Google Cloud resources\n- **Keep the Google Cloud project:** Delete the GKE cluster and the Google Cloud resources\n- **Delete the project** \n### Delete the Kubernetes resources in the cluster and the Google Cloud resources\n- Delete the Kubernetes namespace and the workloads that you deployed:```\nkubectl -n gke-ai-namespace delete -f src/gke-config/standard-tf-mnist-batch-predict.yamlkubectl delete namespace gke-ai-namespace\n```\n- Delete the Cloud Storage bucket:- Go to the **Buckets** page: [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Select the checkbox for `` `-gke-gpu-bucket` .\n- Click delete **Delete** .\n- To confirm deletion, type `DELETE` and click **Delete** .\n- Delete the Google Cloud service account:- Go to the **Service accounts** page: [Go to Service accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Select the checkbox for `gke-ai-sa@` `` `.iam.gserviceaccount.com` .\n- Click delete **Delete** .\n- To confirm deletion, click **Delete** .\n### Delete the GKE cluster and the Google Cloud resources\n- Delete the GKE cluster:- Go to the **Clusters** page: [Go to Clusters](https://console.cloud.google.com/kubernetes/list/overview) \n- Select the checkbox for `gke-gpu-cluster` .\n- Click delete **Delete** .\n- To confirm deletion, type `gke-gpu-cluster` and click **Delete** .\n- Delete the Cloud Storage bucket:- Go to the **Buckets** page: [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Select the checkbox for `` `-gke-gpu-bucket` .\n- Click delete **Delete** .\n- To confirm deletion, type `DELETE` and click **Delete** .\n- Delete the Google Cloud service account:- Go to the **Service accounts** page: [Go to Service accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Select the checkbox for `gke-ai-sa@` `` `.iam.gserviceaccount.com` .\n- Click delete **Delete** .\n- To confirm deletion, click **Delete** .\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- [Learn more about using GPUs in GKE](/kubernetes-engine/docs/concepts/gpus)", "guide": "Google Kubernetes Engine (GKE)"}