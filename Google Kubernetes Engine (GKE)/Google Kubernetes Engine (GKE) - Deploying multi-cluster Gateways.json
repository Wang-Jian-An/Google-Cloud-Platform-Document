{"title": "Google Kubernetes Engine (GKE) - Deploying multi-cluster Gateways", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways", "abstract": "# Google Kubernetes Engine (GKE) - Deploying multi-cluster Gateways\nThis page describes how to deploy Kubernetes [Gateway resources](/kubernetes-engine/docs/concepts/gateway-api) for load balancing ingress traffic across multiple Google Kubernetes Engine (GKE) clusters (or fleet). Before deploying multi-cluster Gateways, see [Enabling multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) to prepare your environment.\nFor deploying Gateways to load balance ingress traffic to just a single GKE cluster, see [Deploying Gateways](/kubernetes-engine/docs/how-to/deploying-gateways) .\n", "content": "## Multi-cluster Gateways\nA multi-cluster Gateway is a Gateway resource that load balances traffic across multiple Kubernetes clusters. In GKE the `gke-l7-global-external-managed-mc` , `gke-l7-regional-external-managed-mc` , `gke-l7-rilb-mc` , and `gke-l7-gxlb-mc` GatewayClasses deploy multi-cluster Gateways that provide HTTP routing, traffic splitting, traffic mirroring, health-based failover, and more across different GKE clusters, Kubernetes Namespaces, and across different regions. Multi-cluster Gateways make managing application networking across many clusters and teams easy, secure, and scalable for infrastructure administrators.\nThis page introduces three examples to teach you how to deploy multi-cluster Gateways using the [GKE Gateway controller](/kubernetes-engine/docs/concepts/gateway-api#gateway_controller) :\n- [Example 1](#external-gateway) : An external, multi-cluster Gateway providing load balancing across two GKE clusters for internet traffic.\n- [Example 2](#blue-green) : Blue-green, weight-based traffic splitting and traffic mirroring across two GKE clusters for internal VPC traffic.\n- [Example 3](#capacity-load-balancing) : A capacity-based Gateway to load-balance request to different backends based on their max capacity.\nEach of the examples will use the same and applications to model a real-world scenario where an online shopping service and a website service are owned and operated by separate teams and deployed across a [fleet](/anthos/fleet-management/docs) of shared GKE clusters. Each of the examples highlights different topologies and use cases enabled by multi-cluster Gateways.\nMulti-cluster Gateways require some environmental preparation before they can be deployed. Before proceeding, follow the steps in [Enabling multi-clusterGateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) :\n- [Deploy](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways#deploy_clusters) GKE clusters.\n- [Register](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways#register_with) your clusters to a fleet.\n- [Enable](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways#enable_multi-cluster_services) the multi-cluster Service and multi-cluster Gateway controllers.\nLastly, review the GKE Gateway controller [limitations and known issues](/kubernetes-engine/docs/how-to/deploying-gateways#limitations) before using it in your environment.\n## Multi-cluster, multi-region, external Gateway\nIn this tutorial, you will create an external multi-cluster Gateway that serves external traffic across an application running in two GKE clusters.\nIn the following steps you:\n- [Deploy the sample store application](#demo-app) to the`gke-west-1`and`gke-east-1`clusters.\n- [Configure Services on each cluster to be exported](#service-export) into your fleet (multi-cluster Services).\n- [Deploy an external multi-cluster Gateway and an HTTPRoute](#deploy-gateway) to your config cluster (`gke-west-1`).\nAfter the application and Gateway resources are deployed, you can control traffic across the two GKE clusters using path-based routing:\n- Requests to`/west`are routed to`store`Pods in the`gke-west-1`cluster.\n- Requests to`/east`are routed to`store`Pods in the`gke-east-1`cluster.\n- Requests to any other path are routed to either cluster, according to its health, capacity, and proximity to the requesting client.\n### Deploying the demo application\n- Create the `store` Deployment and Namespace in all three of the clusters that were deployed in [Enabling multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways#deploy_clusters) :```\nkubectl apply --context gke-west-1 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/main/gateway/gke-gateway-controller/multi-cluster-gateway/store.yamlkubectl apply --context gke-west-2 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/main/gateway/gke-gateway-controller/multi-cluster-gateway/store.yamlkubectl apply --context gke-east-1 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/main/gateway/gke-gateway-controller/multi-cluster-gateway/store.yaml\n```It deploys the following resources to each cluster:```\nnamespace/store created\ndeployment.apps/store created\n```All examples in this page use the app deployed in this step. Make sure that the app is deployed across all three clusters before trying any of the remaining steps. This example uses only clusters `gke-west-1` and `gke-east-1` , and `gke-west-2` is used in another example.\n### Multi-cluster Services\nServices are how Pods are exposed to clients. Because the GKE [Gateway controller](/kubernetes-engine/docs/concepts/gateway-api#gateway_controller) uses container-native load balancing, it does not use the ClusterIP or Kubernetes load balancing to reach Pods. Traffic is sent directly from the load balancer to the Pod IP addresses. However, Services still play a critical role as a logical identifier for Pod grouping.\n[Multi-cluster Services (MCS)](/kubernetes-engine/docs/concepts/multi-cluster-services) is an API standard for Services that span clusters and its GKE controller provides service discovery across GKE clusters. The multi-cluster Gateway controller uses MCS API resources to group Pods into a Service that is addressable across or spans multiple clusters.\nThe multi-cluster Services API defines the following custom resources:\n- **ServiceExports** map to a Kubernetes Service, exporting the endpoints of that Service to all clusters registered to the fleet. When a Service has a corresponding ServiceExport it means that the Service can be addressed by a multi-cluster Gateway.\n- **ServiceImports** are automatically generated by the multi-cluster Service controller. ServiceExport and ServiceImport come in pairs. If a ServiceExport exists in the fleet, then a corresponding ServiceImport is created to allow the Service mapped to the ServiceExport to be accessed from across clusters.\nExporting Services works in the following way. A store Service exists in `gke-west-1` which selects a group of Pods in that cluster. A ServiceExport is created in the cluster which allows the Pods in `gke-west-1` to become accessible from the other clusters in the fleet. The ServiceExport will map to and expose Services that have the same name and Namespace as the ServiceExport resource.\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 namespace: storespec:\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store\u00a0 namespace: store\n```\nThe following diagram shows what happens after a ServiceExport is deployed. If a ServiceExport and Service pair exist then the multi-cluster Service controller deploys a corresponding ServiceImport to every GKE cluster in the fleet. The ServiceImport is the local representation of the `store` Service in every cluster. This enables the `client` Pod in `gke-east-1` to use ClusterIP or headless Services to reach the `store` Pods in `gke-west-1` . When used in this manner multi-cluster Services provide east-west load balancing between clusters without requiring an internal LoadBalancer Service. To use multi-cluster Services for cluster-to-cluster load balancing, see [Configuring multi-cluster Services](/kubernetes-engine/docs/how-to/multi-cluster-services) .\nMulti-cluster Gateways also use ServiceImports, but not for cluster-to-cluster load balancing. Instead, Gateways use ServiceImports as logical identifiers for a Service that exists in another cluster or that stretches across multiple clusters. The following HTTPRoute references a ServiceImport instead of a Service resource. By referencing a ServiceImport, this indicates that it is forwarding traffic to a group of backend Pods that run across one or more clusters.\n```\nkind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: multi-cluster-gatewayspec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: store\u00a0 \u00a0 name: external-http\u00a0 hostnames:\u00a0 - \"store.example.com\"\u00a0 rules:\u00a0 - backendRefs:\u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 name: store\u00a0 \u00a0 \u00a0 port: 8080\n```\nThe following diagram shows how the HTTPRoute routes `store.example.com` traffic to `store` Pods on `gke-west-1` and `gke-east-1` . The load balancer treats them as one pool of backends. If the Pods from one of the clusters becomes unhealthy, unreachable, or has no traffic capacity, then traffic load is balanced to the remaining Pods on the other cluster. New clusters can be added or removed with the `store` Service and ServiceExport. This will transparently add or remove backend Pods without any explicit routing configuration changes.### Exporting Services\nAt this point, the application is running across both clusters. Next, you will expose and export the applications by deploying Services and ServiceExports to each cluster.\n- Apply the following manifest to the `gke-west-1` cluster to create your `store` and `store-west-1` Services and ServiceExports:```\ncat << EOF | kubectl apply --context gke-west-1 -f -apiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 namespace: storespec:\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store\u00a0 namespace: store---apiVersion: v1kind: Servicemetadata:\u00a0 name: store-west-1\u00a0 namespace: storespec:\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store-west-1\u00a0 namespace: storeEOF\n```\n- Apply the following manifest to the `gke-east-1` cluster to create your `store` and `store-east-1` Services and ServiceExports:```\ncat << EOF | kubectl apply --context gke-east-1 -f -apiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 namespace: storespec:\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store\u00a0 namespace: store---apiVersion: v1kind: Servicemetadata:\u00a0 name: store-east-1\u00a0 namespace: storespec:\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store-east-1\u00a0 namespace: storeEOF\n```\n- Verify that the correct ServiceExports have been created in the clusters.```\nkubectl get serviceexports --context CLUSTER_NAME --namespace store\n```Replace with `gke-west-1` and `gke-east-1` . The output should resemble the following:```\n# gke-west-1NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AGEstore \u00a0 \u00a0 \u00a0 \u00a0 \u00a02m40sstore-west-1 \u00a0 2m40s# gke-east-1NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AGEstore \u00a0 \u00a0 \u00a0 \u00a0 \u00a02m25sstore-east-1 \u00a0 2m25s\n```This demonstrates that the `store` Service contains `store` Pods across both clusters while the `store-west-1` and `store-east-1` Services only contain `store` Pods on their respective clusters. These overlapping Services are used to target the Pods across multiple clusters or a subset of Pods on a single cluster.\n- After a few minutes verify that the accompanying `ServiceImports` have been automatically created by the multi-cluster Services controller across all clusters in the fleet. **Note:** The first MCS you create in your fleet can take up to 20 min to be fully operational. Exporting new services after the first one is created or adding endpoints to existing Multi-cluster Services is faster (up to a few minutes in some cases).```\nkubectl get serviceimports --context CLUSTER_NAME --namespace store\n```Replace with `gke-west-1` and `gke-east-1` . The output should resemble the following:```\n# gke-west-1NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TYPE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 IP \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AGEstore \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ClusterSetIP \u00a0 [\"10.112.31.15\"] \u00a0 \u00a06m54sstore-east-1 \u00a0 ClusterSetIP \u00a0 [\"10.112.26.235\"] \u00a0 5m49sstore-west-1 \u00a0 ClusterSetIP \u00a0 [\"10.112.16.112\"] \u00a0 6m54s# gke-east-1NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TYPE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 IP \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AGEstore \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ClusterSetIP \u00a0 [\"10.72.28.226\"] \u00a0 \u00a05d10hstore-east-1 \u00a0 ClusterSetIP \u00a0 [\"10.72.19.177\"] \u00a0 \u00a05d10hstore-west-1 \u00a0 ClusterSetIP \u00a0 [\"10.72.28.68\"] \u00a0 \u00a0 4h32m\n```This demonstrates that all three Services are accessible from both clusters in the fleet. However, because there is only a single active config cluster per fleet, you can only deploy Gateways and HTTPRoutes that reference these ServiceImports in `gke-west-1` . When an HTTPRoute in the config cluster references these ServiceImports as backends, the Gateway can forward traffic to these Services no matter which cluster they are exported from.\n**Note:** The Gateway controller does not use ServiceImports or the ClusterSetIP to perform routing, even though the ClusterSetIP can be used to load balance traffic directly between clusters for east-west use cases. ServiceImports are only used by Gateways for their service discovery information. All Gateway traffic flows directly from the load balancer to the Pod IP addresses without any kube-proxy load balancing.\n### Deploying the Gateway and HTTPRoute\nOnce the applications have been deployed, you can then configure a Gateway using the `gke-l7-global-external-managed-mc` GatewayClass. This Gateway creates an external Application Load Balancer configured to distribute traffic across your target clusters.\n- Apply the following `Gateway` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: GatewayapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: external-http\u00a0 namespace: storespec:\u00a0 gatewayClassName: gke-l7-global-external-managed-mc\u00a0 listeners:\u00a0 - name: http\u00a0 \u00a0 protocol: HTTP\u00a0 \u00a0 port: 80\u00a0 \u00a0 allowedRoutes:\u00a0 \u00a0 \u00a0 kinds:\u00a0 \u00a0 \u00a0 - kind: HTTPRouteEOF\n``` **Note:** It might take several minutes (up to 10) for the Gateway to fully deploy and serve traffic.This Gateway configuration deploys external Application Load Balancer resources with the following namging convention: `gkemcg1-` `` `-` `` `-` `` .The default resources created with this configuration are:- 1 load balancer:`gkemcg1-store-external-http-` ``\n- 1 public IP address:`gkemcg1-store-external-http-` ``\n- 1 forwarding rule:`gkemcg1-store-external-http-` ``\n- 2 backend services:- Default 404 backend service:`gkemcg1-store-gw-serve404-` ``\n- Default 500 backend service:`gkemcg1-store-gw-serve500-` ``\n- 1 Health check:- Default 404 health check:`gkemcg1-store-gw-serve404-` ``\n- 0 routing rules (URLmap is empty)\nAt this stage, any request to the :80 will result in a default page displaying the following message: `fault filter abort` . **Warning:** In this example, we are deploying an external multi-cluster Gateway listening on port 80. If you are deploying an external multi-cluster Gateway for a production environment, you should ensure that you configure a set of features to properly secure your Gateway, depending on your context and your environment. To learn more about how to secure a multi-cluster Gateway, see [Secure a Gateway](/kubernetes-engine/docs/how-to/secure-gateway) , or how to apply Policies to a multi-cluster Gateway, see [Configure Gateway resources using Policies](/kubernetes-engine/docs/how-to/configure-gateway-resources) .\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: public-store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: external-httpspec:\u00a0 hostnames:\u00a0 - \"store.example.com\"\u00a0 parentRefs:\u00a0 - name: external-http\u00a0 rules:\u00a0 - matches:\u00a0 \u00a0 - path:\u00a0 \u00a0 \u00a0 \u00a0 type: PathPrefix\u00a0 \u00a0 \u00a0 \u00a0 value: /west\u00a0 \u00a0 backendRefs:\u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 name: store-west-1\u00a0 \u00a0 \u00a0 port: 8080\u00a0 - matches:\u00a0 \u00a0 - path:\u00a0 \u00a0 \u00a0 \u00a0 type: PathPrefix\u00a0 \u00a0 \u00a0 \u00a0 value: /east\u00a0 \u00a0 backendRefs:\u00a0 \u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 \u00a0 name: store-east-1\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 - backendRefs:\u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 name: store\u00a0 \u00a0 \u00a0 port: 8080EOF\n```At this stage, any request to the :80 will result in a default page displaying the following message: `fault filter abort` .Once deployed, this HTTPRoute will configure the following routing behavior:- Requests to`/west`are routed to`store`Pods in the`gke-west-1`cluster, because Pods selected by the`store-west-1`ServiceExport only exist in the`gke-west-1`cluster.\n- Requests to`/east`are routed to`store`Pods in the`gke-east-1`cluster, because Pods selected by the`store-east-1`ServiceExport only exist in the`gke-east-1`cluster.\n- Requests to any other path are routed to`store`Pods in either cluster, according to its health, capacity, and proximity to the requesting client.\n- Requests to the:80 will result in a default page displaying the following message:`fault filter abort`.\nNote that if all the Pods on a given cluster are unhealthy (or don't exist) then traffic to the `store` Service would only be sent to clusters that actually have `store` Pods. The existence of a ServiceExport and Service on a given cluster does not guarantee that traffic will be sent to that cluster. Pods must exist and be responding affirmatively to the load balancer health check or else the load balancer will just send traffic to healthy `store` Pods in other clusters.New resources are created with this configuration:- 3 backend services:- The`store`backend service:`gkemcg1-store-store-8080-` ``\n- The`store-east-1`backend service:`gkemcg1-store-store-east-1-8080-` ``\n- The`store-west-1`backend service:`gkemcg1-store-store-west-1-8080-` ``\n- 3 Health checks:- The`store`health check:`gkemcg1-store-store-8080-` ``\n- The`store-east-1`health check:`gkemcg1-store-store-east-1-8080-` ``\n- The`store-west-1`health check:`gkemcg1-store-store-west-1-8080-` ``\n- 1 routing rule in the URLmap:- The`store.example.com`routing rule:\n- 1 Host:`store.example.com`\n- Multiple`matchRules`to route to the new backend services\nThe following diagram shows the resources you've deployed across both clusters. Because `gke-west-1` is the Gateway config cluster, it is the cluster in which our Gateway, HTTPRoutes, and ServiceImports are watched by the Gateway controller. Each cluster has a `store` ServiceImport and another ServiceImport specific to that cluster. Both point at the same Pods. This lets the HTTPRoute to specify exactly where traffic should go - to the `store` Pods on a specific cluster or to the `store` Pods across all clusters.\nNote that this is a logical resource model, not a depiction of the traffic flow. The traffic path goes directly from the load balancer to backend Pods and has no direct relation to whichever cluster is the config cluster.\n### Validating deployment\nYou can now issue requests to our multi-cluster Gateway and distribute traffic across both GKE clusters.\n- Validate that the Gateway and HTTPRoute have been deployed successfully by inspecting the Gateway status and events.```\nkubectl describe gateways.gateway.networking.k8s.io external-http --context gke-west-1 --namespace store\n```Your output should look similar to the following:```\nName:   external-http\nNamespace: store\nLabels:  <none>\nAnnotations: networking.gke.io/addresses: /projects/PROJECT_NUMBER/global/addresses/gkemcg1-store-external-http-laup24msshu4\n    networking.gke.io/backend-services:\n    /projects/PROJECT_NUMBER/global/backendServices/gkemcg1-store-gw-serve404-80-n65xmts4xvw2, /projects/PROJECT_NUMBER/global/backendServices/gke...\n    networking.gke.io/firewalls: /projects/PROJECT_NUMBER/global/firewalls/gkemcg1-l7-default-global\n    networking.gke.io/forwarding-rules: /projects/PROJECT_NUMBER/global/forwardingRules/gkemcg1-store-external-http-a5et3e3itxsv\n    networking.gke.io/health-checks:\n    /projects/PROJECT_NUMBER/global/healthChecks/gkemcg1-store-gw-serve404-80-n65xmts4xvw2, /projects/PROJECT_NUMBER/global/healthChecks/gkemcg1-s...\n    networking.gke.io/last-reconcile-time: 2023-10-12T17:54:24Z\n    networking.gke.io/ssl-certificates: \n    networking.gke.io/target-http-proxies: /projects/PROJECT_NUMBER/global/targetHttpProxies/gkemcg1-store-external-http-94oqhkftu5yz\n    networking.gke.io/target-https-proxies: \n    networking.gke.io/url-maps: /projects/PROJECT_NUMBER/global/urlMaps/gkemcg1-store-external-http-94oqhkftu5yz\nAPI Version: gateway.networking.k8s.io/v1beta1\nKind:   Gateway\nMetadata:\n Creation Timestamp: 2023-10-12T06:59:32Z\n Finalizers:\n gateway.finalizer.networking.gke.io\n Generation:  1\n Resource Version: 467057\n UID:    1dcb188e-2917-404f-9945-5f3c2e907b4c\nSpec:\n Gateway Class Name: gke-l7-global-external-managed-mc\n Listeners:\n Allowed Routes:\n  Kinds:\n  Group: gateway.networking.k8s.io\n  Kind: HTTPRoute\n  Namespaces:\n  From: Same\n Name:  http\n Port:  80\n Protocol: HTTP\nStatus:\n Addresses:\n Type: IPAddress\n Value: 34.36.127.249\n Conditions:\n Last Transition Time: 2023-10-12T07:00:41Z\n Message:    The OSS Gateway API has deprecated this condition, do not depend on it.\n Observed Generation: 1\n Reason:    Scheduled\n Status:    True\n Type:     Scheduled\n Last Transition Time: 2023-10-12T07:00:41Z\n Message:    \n Observed Generation: 1\n Reason:    Accepted\n Status:    True\n Type:     Accepted\n Last Transition Time: 2023-10-12T07:00:41Z\n Message:    \n Observed Generation: 1\n Reason:    Programmed\n Status:    True\n Type:     Programmed\n Last Transition Time: 2023-10-12T07:00:41Z\n Message:    The OSS Gateway API has altered the \"Ready\" condition semantics and reservedit for future use. GKE Gateway will stop emitting it in a future update, use \"Programmed\" instead.\n Observed Generation: 1\n Reason:    Ready\n Status:    True\n Type:     Ready\n Listeners:\n Attached Routes: 1\n Conditions:\n  Last Transition Time: 2023-10-12T07:00:41Z\n  Message:    \n  Observed Generation: 1\n  Reason:    Programmed\n  Status:    True\n  Type:     Programmed\n  Last Transition Time: 2023-10-12T07:00:41Z\n  Message:    The OSS Gateway API has altered the \"Ready\" condition semantics and reservedit for future use. GKE Gateway will stop emitting it in a future update, use \"Programmed\" instead.\n  Observed Generation: 1\n  Reason:    Ready\n  Status:    True\n  Type:     Ready\n Name:     http\n Supported Kinds:\n  Group: gateway.networking.k8s.io\n  Kind: HTTPRoute\nEvents:\n Type Reason Age     From     Message\n ---- ------ ----     ----     ------ Normal UPDATE 35m (x4 over 10h)  mc-gateway-controller store/external-http\n Normal SYNC 4m22s (x216 over 10h) mc-gateway-controller SYNC on store/external-http was a success\n```\n- Once the Gateway has deployed successfully retrieve the external IP address from `external-http` Gateway.```\nkubectl get gateways.gateway.networking.k8s.io external-http -o=jsonpath=\"{.status.addresses[0].value}\" --context gke-west-1 --namespace store\n```Replace `` in the following steps with the IP address you receive as output.\n- Send traffic to the root path of the domain. This load balances traffic to the `store` ServiceImport which is across cluster `gke-west-1` and `gke-east-1` . The load balancer sends your traffic to the closest region to you and you might not see responses from the other region.```\ncurl -H \"host: store.example.com\" http://VIP\n```The output confirms that the request was served by Pod from the `gke-east-1` cluster:```\n{\u00a0 \"cluster_name\": \"gke-east-1\",\u00a0 \"zone\": \"us-east1-b\",\u00a0 \"host_header\": \"store.example.com\",\u00a0 \"node_name\": \"gke-gke-east-1-default-pool-7aa30992-t2lp.c.agmsb-k8s.internal\",\u00a0 \"pod_name\": \"store-5f5b954888-dg22z\",\u00a0 \"pod_name_emoji\": \"\u23ed\",\u00a0 \"project_id\": \"agmsb-k8s\",\u00a0 \"timestamp\": \"2021-06-01T17:32:51\"}\n```\n- Next send traffic to the `/west` path. This routes traffic to the `store-west-1` ServiceImport which only has Pods running on the `gke-west-1` cluster. A cluster-specific ServiceImport, like `store-west-1` , enables an application owner to explicitly send traffic to a specific cluster, rather than letting the load balancer make the decision.```\ncurl -H \"host: store.example.com\" http://VIP/west\n```The output confirms that the request was served by Pod from the `gke-west-1` cluster:```\n{\u00a0 \"cluster_name\": \"gke-west-1\", \u00a0 \"zone\": \"us-west1-a\", \u00a0 \"host_header\": \"store.example.com\",\u00a0 \"node_name\": \"gke-gke-west-1-default-pool-65059399-2f41.c.agmsb-k8s.internal\",\u00a0 \"pod_name\": \"store-5f5b954888-d25m5\",\u00a0 \"pod_name_emoji\": \"\ud83c\udf7e\",\u00a0 \"project_id\": \"agmsb-k8s\",\u00a0 \"timestamp\": \"2021-06-01T17:39:15\",}\n```\n- Finally, send traffic to the `/east` path.```\ncurl -H \"host: store.example.com\" http://VIP/east\n```The output confirms that the request was served by Pod from the `gke-east-1` cluster:```\n{\u00a0 \"cluster_name\": \"gke-east-1\",\u00a0 \"zone\": \"us-east1-b\",\u00a0 \"host_header\": \"store.example.com\",\u00a0 \"node_name\": \"gke-gke-east-1-default-pool-7aa30992-7j7z.c.agmsb-k8s.internal\",\u00a0 \"pod_name\": \"store-5f5b954888-hz6mw\",\u00a0 \"pod_name_emoji\": \"\ud83e\udddc\ud83c\udffe\",\u00a0 \"project_id\": \"agmsb-k8s\",\u00a0 \"timestamp\": \"2021-06-01T17:40:48\"}\n```## Blue-green, multi-cluster routing with Gateway\nThe `gke-l7-global-external-managed-*` , `gke-l7-regional-external-managed-*` , and `gke-l7-rilb-*` GatewayClasses have many advanced traffic routing capabilities including traffic splitting, header matching, header manipulation, traffic mirroring, and more. In this example, you'll demonstrate how to use weight-based traffic splitting to explicitly control the traffic proportion across two GKE clusters.\nThis example goes through some realistic steps that a service owner would take in moving or expanding their application to a new GKE cluster. The goal of blue-green deployments is to reduce risk through multiple validation steps which confirm that the new cluster is operating correctly. This example walks through four stages of deployment:\n- **100%-Header-based canary** : [Use HTTP header routing](#header-canary) to send only test or synthetic traffic to the new cluster.\n- **100%-Mirror traffic** : [Mirror user traffic](#traffic-mirror) to the canary cluster. This tests the capacity of the canary cluster by copying 100% of the user traffic to this cluster.\n- **90%-10%** : [Canary a traffic split](#traffic-split) of 10% to slowly expose the new cluster to live traffic.\n- **0%-100%** : [Cutover fully to the new cluster](#cut-over) with the option of switching back if any errors are observed.This example is similar to the previous one, except it deploys an internal multi-cluster Gateway instead. This deploys an internal Application Load Balancer which is only privately accessible from within the VPC. You will use the clusters and same application that you deployed in the previous steps, except deploy them through a different Gateway.\n**Note:** The same configurations could be applied to Gateways using external GatewayClasses (gke-l7-global-external-managed- ), except the gke-l7-gxlb GatewayClass that does not support advanced traffic management capabilities. To learn more about the different features supported with each GatewayClass, see [GatewayClass capabilities](/kubernetes-engine/docs/how-to/gatewayclass-capabilities) .\n### Prerequisites\nThe following example builds on some of the steps in [Deploying an external multi-cluster Gateway](#external-gateway) . Ensure that you have done the following steps before proceeding with this example:\n- [Enabling multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways) \n- [Deploying a demo application](#demo-app) This example uses the `gke-west-1` and `gke-west-2` clusters that you already set up. These clusters are in the same region because the `gke-l7-rilb-mc` GatewayClass is regional and only supports cluster backends in the same region.\n- Deploy the Service and ServiceExports needed on each cluster. If you deployed Services and ServiceExports from the previous example then you already deployed some of these.```\nkubectl apply --context gke-west-1 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/main/gateway/gke-gateway-controller/multi-cluster-gateway/store-west-1-service.yamlkubectl apply --context gke-west-2 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/main/gateway/gke-gateway-controller/multi-cluster-gateway/store-west-2-service.yaml\n```It deploys a similar set of resources to each cluster:```\nservice/store created\nserviceexport.net.gke.io/store created\nservice/store-west-2 created\nserviceexport.net.gke.io/store-west-2 created\n```\n### Configuring a proxy-only subnet\nIf you have not already done so, [configure a proxy-onlysubnet](/load-balancing/docs/proxy-only-subnets#proxy_only_subnet_create) for each region in which you are deploying internal Gateways. This subnet is used to provide internal IP addresses to the load balancer proxies and must be configured with a `--purpose` set to `REGIONAL_MANAGED_PROXY` only.\nYou must create a proxy-only subnet before you create Gateways that manage internal Application Load Balancers. Each region of a Virtual Private Cloud (VPC) network in which you use internal Application Load Balancers must have a proxy-only subnet.\nThe [gcloud compute networks subnets create](/sdk/gcloud/reference/compute/networks/subnets/create) command creates a proxy-only a subnet.\n```\ngcloud compute networks subnets create SUBNET_NAME \\\u00a0 \u00a0 --purpose=REGIONAL_MANAGED_PROXY \\\u00a0 \u00a0 --role=ACTIVE \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --network=VPC_NETWORK_NAME \\\u00a0 \u00a0 --range=CIDR_RANGE\n```\nReplace the following:\n- ``: the name of the proxy-only subnet.\n- ``: the region of the proxy-only subnet.\n- ``: the name of the VPC network that contains the subnet.\n- ``: the primary IP address range of the subnet. You must use a subnet mask no larger than`/26`so that at least 64 IP addresses are available for proxies in the region. The recommended subnet mask is`/23`.\n### Deploying the Gateway\nThe following Gateway is created from the `gke-l7-rilb-mc` GatewayClass. This is a regional internal Gateway which can only target GKE clusters in the same region.\n- Apply the following `Gateway` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: GatewayapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: internal-http\u00a0 namespace: storespec:\u00a0 gatewayClassName: gke-l7-rilb-mc\u00a0 listeners:\u00a0 - name: http\u00a0 \u00a0 protocol: HTTP\u00a0 \u00a0 port: 80\u00a0 \u00a0 allowedRoutes:\u00a0 \u00a0 \u00a0 kinds:\u00a0 \u00a0 \u00a0 - kind: HTTPRouteEOF\n``` **Note:** It might take several minutes (up to 10) for the Gateway to fully deploy and serve traffic.\n- Validate that the Gateway has come up successfully. You can filter for just the events from this Gateway with the following command:```\nkubectl get events --field-selector involvedObject.kind=Gateway,involvedObject.name=internal-http --context=gke-west-1 --namespace store\n```The Gateway deployment was successful if the output resembles the following:```\nLAST SEEN TYPE  REASON OBJECT     MESSAGE\n5m18s  Normal ADD  gateway/internal-http store/internal-http\n3m44s  Normal UPDATE gateway/internal-http store/internal-http\n3m9s  Normal SYNC  gateway/internal-http SYNC on store/internal-http was a success\n```\n### Header-based canary\nHeader-based canarying lets the service owner match synthetic test traffic that does not come from real users. This is an easy way of validating that the basic networking of the application is functioning without exposing users directly.\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: internal-store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: internal-httpspec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: store\u00a0 \u00a0 name: internal-http\u00a0 hostnames:\u00a0 - \"store.example.internal\"\u00a0 rules:\u00a0 # Matches for env=canary and sends it to store-west-2 ServiceImport\u00a0 - matches:\u00a0 \u00a0 - headers:\u00a0 \u00a0 \u00a0 - name: env\u00a0 \u00a0 \u00a0 \u00a0 value: canary\u00a0 \u00a0 backendRefs:\u00a0 \u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 \u00a0 name: store-west-2\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 # All other traffic goes to store-west-1 ServiceImport\u00a0 - backendRefs:\u00a0 \u00a0 - group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 name: store-west-1\u00a0 \u00a0 \u00a0 port: 8080EOF\n```Once deployed, this HTTPRoute configures the following routing behavior:- Internal requests to`store.example.internal` **without** the`env: canary`HTTP header are routed to`store`Pods on the`gke-west-1`cluster\n- Internal requests to`store.example.internal` **with** the`env: canary`HTTP header are routed to`store`Pods on the`gke-west-2`cluster\nValidate that the HTTPRoute is functioning correctly by sending traffic to the Gateway IP address.\n- Retrieve the internal IP address from `internal-http` .```\nkubectl get gateways.gateway.networking.k8s.io internal-http -o=jsonpath=\"{.status.addresses[0].value}\" --context gke-west-1 --namespace store\n```Replace in the following steps with the IP address you receive as output. **Note:** In the following steps send all requests to from a client that has internal VPC connectivity and is in the same region as the GKE cluster (unless you have configured global access on your Gateway). You can create a VM in `us-west1` and SSH to it for this purpose. This is necessary because the `internal-http` Gateway is an internal, regional load balancer. Also set the host header with `store.example.internal` so that DNS does not have to be configured for this example to work.\n- Send a request to the Gateway using the `env: canary` HTTP header. This will confirm that traffic is being routed to `gke-west-2` . Use a private client in the same VPC as the GKE clusters to confirm that requests are being routed correctly. The following command must be run on a machine that has private access to the Gateway IP address or else it will not function.```\ncurl -H \"host: store.example.internal\" -H \"env: canary\" http://VIP\n```The output confirms that the request was served by a Pod from the `gke-west-2` cluster:```\n{\u00a0 \u00a0 \"cluster_name\": \"gke-west-2\", \u00a0 \u00a0 \"host_header\": \"store.example.internal\",\u00a0 \u00a0 \"node_name\": \"gke-gke-west-2-default-pool-4cde1f72-m82p.c.agmsb-k8s.internal\",\u00a0 \u00a0 \"pod_name\": \"store-5f5b954888-9kdb5\",\u00a0 \u00a0 \"pod_name_emoji\": \"\ud83d\ude02\",\u00a0 \u00a0 \"project_id\": \"agmsb-k8s\",\u00a0 \u00a0 \"timestamp\": \"2021-05-31T01:21:55\",\u00a0 \u00a0 \"zone\": \"us-west1-a\"}\n```\n### Traffic mirror\nThis stage sends traffic to the intended cluster but also mirrors that traffic to the canary cluster.\nUsing mirroring is helpful to determine how traffic load will impact application performance without impacting responses to your clients in any way. It may not be necessary for all kinds of rollouts, but can be useful when rolling out large changes that could impact performance or load.\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: internal-store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: internal-httpspec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: store\u00a0 \u00a0 name: internal-http\u00a0 hostnames:\u00a0 - \"store.example.internal\"\u00a0 rules:\u00a0 # Sends all traffic to store-west-1 ServiceImport\u00a0 - backendRefs:\u00a0 \u00a0 - name: store-west-1\u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 # Also mirrors all traffic to store-west-2 ServiceImport\u00a0 \u00a0 filters:\u00a0 \u00a0 - type: RequestMirror\u00a0 \u00a0 \u00a0 requestMirror:\u00a0 \u00a0 \u00a0 \u00a0 backendRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: store-west-2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: 8080EOF\n```\n- Using your private client, send a request to the `internal-http` Gateway. Use the `/mirror` path so you can uniquely identify this request in the application logs in a later step.```\ncurl -H \"host: store.example.internal\" http://VIP/mirror\n```\n- The output confirms that the client received a response from a Pod in the `gke-west-1` cluster:```\n{\u00a0 \u00a0 \"cluster_name\": \"gke-west-1\", \u00a0 \u00a0 \"host_header\": \"store.example.internal\",\u00a0 \u00a0 \"node_name\": \"gke-gke-west-1-default-pool-65059399-ssfq.c.agmsb-k8s.internal\",\u00a0 \u00a0 \"pod_name\": \"store-5f5b954888-brg5w\",\u00a0 \u00a0 \"pod_name_emoji\": \"\ud83c\udf96\",\u00a0 \u00a0 \"project_id\": \"agmsb-k8s\",\u00a0 \u00a0 \"timestamp\": \"2021-05-31T01:24:51\",\u00a0 \u00a0 \"zone\": \"us-west1-a\"}\n```This confirms that the primary cluster is responding to traffic. You still need to confirm that the cluster you are migrating to is receiving mirrored traffic.\n- Check the application logs of a `store` Pod on the `gke-west-2` cluster. The logs should confirm that the Pod received mirrored traffic from the load balancer.```\nkubectl logs deployment/store --context gke-west-2 -n store | grep /mirror\n```\n- This output confirms that Pods on the `gke-west-2` cluster are also receiving the same requests, however their responses to these requests are not sent back to the client. The IP addresses seen in the logs are that of the load balancer's internal IP addresses which are communicating with your Pods.```\nFound 2 pods, using pod/store-5c65bdf74f-vpqbs[2023-10-12 21:05:20,805] INFO in _internal: 192.168.21.3 - - [12/Oct/2023 21:05:20] \"GET /mirror HTTP/1.1\" 200 -[2023-10-12 21:05:27,158] INFO in _internal: 192.168.21.3 - - [12/Oct/2023 21:05:27] \"GET /mirror HTTP/1.1\" 200 -[2023-10-12 21:05:27,805] INFO in _internal: 192.168.21.3 - - [12/Oct/2023 21:05:27] \"GET /mirror HTTP/1.1\" 200 ```\n### Traffic split\nTraffic splitting is one of the most common methods of rolling out new code or deploying to new environments safely. The service owner sets an explicit percentage of traffic that is sent to the canary backends that is typically a very small amount of the overall traffic so that the success of the rollout can be determined with an acceptable amount of risk to real user requests.\nDoing a traffic split with a minority of the traffic enables the service owner to inspect the health of the application and the responses. If all the signals look healthy, then they may proceed to the full cutover.\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: internal-store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: internal-httpspec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: store\u00a0 \u00a0 name: internal-http\u00a0 hostnames:\u00a0 - \"store.example.internal\"\u00a0 rules:\u00a0 - backendRefs:\u00a0 \u00a0 # 90% of traffic to store-west-1 ServiceImport\u00a0 \u00a0 - name: store-west-1\u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 weight: 90\u00a0 \u00a0 # 10% of traffic to store-west-2 ServiceImport\u00a0 \u00a0 - name: store-west-2\u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 weight: 10EOF\n```\n- Using your private client, send a continuous curl request to the `internal- http` Gateway.```\nwhile true; do curl -H \"host: store.example.internal\" -s VIP | grep \"cluster_name\"; sleep 1; done\n```The output will be similar to this, indicating that a 90/10 traffic split is occurring.```\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-2\",\n\"cluster_name\": \"gke-west-1\",\n\"cluster_name\": \"gke-west-1\",\n...\n```\n### Traffic cut over\nThe last stage of the blue-green migration is to fully cut over to the new cluster and remove the old cluster. If the service owner was actually onboarding a second cluster to an existing cluster then this last step would be different as the final step would have traffic going to both clusters. In that scenario a single `store` ServiceImport is recommended that has Pods from both `gke-west-1` and `gke-west-2` clusters. This allows the load balancer to make the decision of where traffic should go for an active-active application, based on proximity, health, and capacity.\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: internal-store-route\u00a0 namespace: store\u00a0 labels:\u00a0 \u00a0 gateway: internal-httpspec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: store\u00a0 \u00a0 name: internal-http\u00a0 hostnames:\u00a0 - \"store.example.internal\"\u00a0 rules:\u00a0 \u00a0 - backendRefs:\u00a0 \u00a0 \u00a0 # No traffic to the store-west-1 ServiceImport\u00a0 \u00a0 \u00a0 - name: store-west-1\u00a0 \u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 \u00a0 weight: 0\u00a0 \u00a0 \u00a0 # All traffic to the store-west-2 ServiceImport\u00a0 \u00a0 \u00a0 - name: store-west-2\u00a0 \u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 \u00a0 weight: 100EOF\n```\n- Using your private client, send a continuous curl request to the `internal- http` Gateway.```\nwhile true; do curl -H \"host: store.example.internal\" -s VIP | grep \"cluster_name\"; sleep 1; done\n```The output will be similar to this, indicating that all traffic is now going to `gke-west-2` .```\n\"cluster_name\": \"gke-west-2\",\n\"cluster_name\": \"gke-west-2\",\n\"cluster_name\": \"gke-west-2\",\n\"cluster_name\": \"gke-west-2\",\n...\n```\nThis final step completes a full blue-green application migration from one GKE cluster to another GKE cluster.\n## Deploy capacity-based load balancing\nThe exercise in this section demonstrates global load balancing and Service capacity concepts by deploying an application across two GKE clusters in different regions. Generated traffic is sent at various request per second (RPS) levels to show how traffic is load balanced across clusters and regions.\nThe following diagram shows the topology that you will deploy and how traffic overflows between clusters and regions when traffic has exceeded Service capacity:\nTo learn more about traffic management, see [GKE traffic management](/kubernetes-engine/docs/concepts/traffic-management) .\n### Prepare your environment\n- Follow [Enabling multi-cluster Gateways](/kubernetes-engine/docs/how-to/enabling-multi-cluster-gateways#deploy-clusters) to prepare your environment.\n- Confirm that the GatewayClass resources are installed on the config cluster:```\nkubectl get gatewayclasses --context=gke-west-1\n```The output is similar to the following:```\nNAME         CONTROLLER     ACCEPTED AGE\ngke-l7-global-external-managed  networking.gke.io/gateway True  16h\ngke-l7-global-external-managed-mc  networking.gke.io/gateway True  14h\ngke-l7-gxlb       networking.gke.io/gateway True  16h\ngke-l7-gxlb-mc      networking.gke.io/gateway True  14h\ngke-l7-regional-external-managed  networking.gke.io/gateway True  16h\ngke-l7-regional-external-managed-mc networking.gke.io/gateway True  14h\ngke-l7-rilb       networking.gke.io/gateway True  16h\ngke-l7-rilb-mc      networking.gke.io/gateway True  14h\n```\n### Deploy an application\nDeploy the sample web application server to both clusters:\n```\nkubectl apply --context gke-west-1 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/master/gateway/docs/store-traffic-deploy.yamlkubectl apply --context gke-east-1 -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/master/gateway/docs/store-traffic-deploy.yaml\n```\nThe output is similar to the following:\n```\nnamespace/store created\ndeployment.apps/store created\n```\n### Deploy a Service, Gateway, and HTTPRoute\n- Apply the following `Service` manifest to both `gke-west-1` and `gke-east-1` clusters:```\ncat << EOF | kubectl apply --context gke-west-1 -f -apiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 namespace: traffic-test\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/max-rate-per-endpoint: \"10\"spec:\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080\u00a0 \u00a0 name: http\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 type: ClusterIP---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store\u00a0 namespace: traffic-testEOF\n``````\ncat << EOF | kubectl apply --context gke-east-1 -f -apiVersion: v1kind: Servicemetadata:\u00a0 name: store\u00a0 namespace: traffic-test\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/max-rate-per-endpoint: \"10\"spec:\u00a0 ports:\u00a0 - port: 8080\u00a0 \u00a0 targetPort: 8080\u00a0 \u00a0 name: http\u00a0 selector:\u00a0 \u00a0 app: store\u00a0 type: ClusterIP---kind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0 name: store\u00a0 namespace: traffic-testEOF\n```The Service is annotated with `max-rate-per-endpoint` set to 10 requests per seconds. With 2 replicas per cluster, each Service has 20 RPS of capacity per cluster.For more information on how to choose a Service capacity level for your Service, see [Determine your Service's capacity](/kubernetes-engine/docs/concepts/traffic-management#capacity) .\n- Apply the following `Gateway` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: GatewayapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: store\u00a0 namespace: traffic-testspec:\u00a0 gatewayClassName: gke-l7-global-external-managed-mc\u00a0 listeners:\u00a0 - name: http\u00a0 \u00a0 protocol: HTTP\u00a0 \u00a0 port: 80\u00a0 \u00a0 allowedRoutes:\u00a0 \u00a0 \u00a0 kinds:\u00a0 \u00a0 \u00a0 - kind: HTTPRouteEOF\n```The manifest describes an external, global, multi-cluster Gateway that deploys an external Application Load Balancer with a publicly accessible IP address.\n- Apply the following `HTTPRoute` manifest to the config cluster, `gke-west-1` in this example:```\ncat << EOF | kubectl apply --context gke-west-1 -f -kind: HTTPRouteapiVersion: gateway.networking.k8s.io/v1beta1metadata:\u00a0 name: store\u00a0 namespace: traffic-test\u00a0 labels:\u00a0 \u00a0 gateway: storespec:\u00a0 parentRefs:\u00a0 - kind: Gateway\u00a0 \u00a0 namespace: traffic-test\u00a0 \u00a0 name: store\u00a0 rules:\u00a0 - backendRefs:\u00a0 \u00a0 - name: store\u00a0 \u00a0 \u00a0 group: net.gke.io\u00a0 \u00a0 \u00a0 kind: ServiceImport\u00a0 \u00a0 \u00a0 port: 8080EOF\n```The manifest describes an HTTPRoute that configures the Gateway with a routing rule that directs all traffic to the store ServiceImport. The `store` ServiceImport groups the `store` Service Pods across both clusters and allows them to be addressed by the load balancer as a single Service. **Note:** It might take several minutes (up to 10) for the Gateway to fully deploy and serve traffic.You can check the Gateway's events after a few minutes to see if it has finished deploying:```\nkubectl describe gateway store -n traffic-test --context gke-west-1\n```The output is similar to the following:```\n...\nStatus:\n Addresses:\n Type: IPAddress\n Value: 34.102.159.147\n Conditions:\n Last Transition Time: 2023-10-12T21:40:59Z\n Message:    The OSS Gateway API has deprecated this condition, do not depend on it.\n Observed Generation: 1\n Reason:    Scheduled\n Status:    True\n Type:     Scheduled\n Last Transition Time: 2023-10-12T21:40:59Z\n Message:    \n Observed Generation: 1\n Reason:    Accepted\n Status:    True\n Type:     Accepted\n Last Transition Time: 2023-10-12T21:40:59Z\n Message:    \n Observed Generation: 1\n Reason:    Programmed\n Status:    True\n Type:     Programmed\n Last Transition Time: 2023-10-12T21:40:59Z\n Message:    The OSS Gateway API has altered the \"Ready\" condition semantics and reservedit for future use. GKE Gateway will stop emitting it in a future update, use \"Programmed\" instead.\n Observed Generation: 1\n Reason:    Ready\n Status:    True\n Type:     Ready\n Listeners:\n Attached Routes: 1\n Conditions:\n  Last Transition Time: 2023-10-12T21:40:59Z\n  Message:    \n  Observed Generation: 1\n  Reason:    Programmed\n  Status:    True\n  Type:     Programmed\n  Last Transition Time: 2023-10-12T21:40:59Z\n  Message:    The OSS Gateway API has altered the \"Ready\" condition semantics and reservedit for future use. GKE Gateway will stop emitting it in a future update, use \"Programmed\" instead.\n  Observed Generation: 1\n  Reason:    Ready\n  Status:    True\n  Type:     Ready\n Name:     http\n Supported Kinds:\n  Group: gateway.networking.k8s.io\n  Kind: HTTPRoute\nEvents:\n Type Reason Age     From     Message\n ---- ------ ----     ----     ------ Normal ADD  12m     mc-gateway-controller traffic-test/store\n Normal SYNC 6m43s    mc-gateway-controller traffic-test/store\n Normal UPDATE 5m40s (x4 over 12m) mc-gateway-controller traffic-test/store\n Normal SYNC 118s (x6 over 10m) mc-gateway-controller SYNC on traffic-test/store was a success\n```This output shows that the Gateway has deployed successfully. It might still take a few minutes for traffic to start passing after the Gateway has deployed. Take note of the IP address in this output, as it is used in a following step.\n### Confirm traffic\nConfirm that traffic is passing to the application by testing the Gateway IP address with a curl command:\n```\ncurl GATEWAY_IP_ADDRESS\n```\nThe output is similar to the following:\n```\n{\n \"cluster_name\": \"gke-west-1\",\n \"host_header\": \"34.117.182.69\",\n \"pod_name\": \"store-54785664b5-mxstv\",\n \"pod_name_emoji\": \"\ud83d\udc73\ud83c\udfff\",\n \"project_id\": \"project\",\n \"timestamp\": \"2021-11-01T14:06:38\",\n \"zone\": \"us-west1-a\"\n}\n```\nThis output shows the Pod metadata, which indicates the region where the request was served from.\n### Verify traffic using load testing\nTo verify the load balancer is working, you can deploy a traffic generator in your `gke-west-1` cluster. The traffic generator generates traffic at different levels of load to demonstrate the capacity and overflow capabilities of the load balancer. The following steps demonstrate three levels of load:\n- 10 RPS, which is under the capacity for the store Service in`gke-west-1`.\n- 30 RPS, which is over capacity for the`gke-west-1`store Service and causes traffic overflow to`gke-east-1`.\n- 60 RPS, which is over capacity for the Services in both clusters.- Get the name of the underying URLmap for your Gateway:```\nkubectl get gateway store -n traffic-test --context=gke-west-1 -o=jsonpath=\"{.metadata.annotations.networking\\.gke\\.io/url-maps}\"\n```The output is similar to the following:```\n/projects/PROJECT_NUMBER/global/urlMaps/gkemcg1-traffic-test-store-armvfyupay1t\n```\n- In the Google Cloud console, go to the **Metrics explorer** page. [Go to Metrics explorer](https://console.cloud.google.com/monitoring/metrics-explorer) \n- Under **Select a metric** , click **CODE: MQL** .\n- Enter the following query to observe traffic metrics for the store Service across your two clusters:```\nfetch https_lb_rule| metric 'loadbalancing.googleapis.com/https/backend_request_count'| filter (resource.url_map_name == 'GATEWAY_URL_MAP')| align rate(1m)| every 1m| group_by [resource.backend_scope],\u00a0 \u00a0 [value_backend_request_count_aggregate:\u00a0 \u00a0 \u00a0 \u00a0 aggregate(value.backend_request_count)]\n```Replace `` with the URLmap name from the previous step.\n- Click **Run query** . Wait at least 5 minutes after deploying the load generator in the next section for the metrics to display in the chart.- Deploy a Pod to your `gke-west-1` cluster:```\nkubectl run --context gke-west-1 -i --tty --rm loadgen \u00a0\\\u00a0 \u00a0 --image=cyrilbkr/httperf \u00a0\\\u00a0 \u00a0 --restart=Never \u00a0\\\u00a0 \u00a0 -- /bin/sh -c 'httperf \u00a0\\\u00a0 \u00a0 --server=GATEWAY_IP_ADDRESS \u00a0\\\u00a0 \u00a0 --hog --uri=\"/zone\" --port 80 \u00a0--wsess=100000,1,1 --rate 10'\n```Replace `` with the Gateway IP address from the previous step.The output is similar to the following, indicating that the traffic generator is sending traffic:```\nIf you don't see a command prompt, try pressing enter.\n```The load generator continuously sends 10 RPS to the Gateway. Even though traffic is coming from inside a Google Cloud region, the load balancer treats it as client traffic coming from the US West Coast. To simulate realistic client diversity, the load generator sends each HTTP request as a new TCP connection, which means traffic is distributed across backend Pods more evenly.The generator takes up to 5 minutes to generate traffic for the dashboard.\n- View your Metrics explorer dashboard. Two lines appear, indiciating how much traffic is load balanced to each of the clusters:You should see that `us-west1-a` is receiving approximately 10 RPS of traffic while `us-east1-b` is not receiving any traffic. Because the traffic generator is running in `us-west1` , all traffic is sent to the Service in the `gke-west-1` cluster.\n- Stop the load generator using **Ctrl+C** , then delete the pod:```\nkubectl delete pod loadgen --context gke-west-1\n```- Deploy the load generator again, but configured to send 30 RPS:```\nkubectl run --context gke-west-1 -i --tty --rm loadgen \u00a0\\\u00a0 \u00a0 --image=cyrilbkr/httperf \u00a0\\\u00a0 \u00a0 --restart=Never \u00a0\\\u00a0 \u00a0 -- /bin/sh -c 'httperf \u00a0\\\u00a0 \u00a0 --server=GATEWAY_IP_ADDRESS \u00a0\\\u00a0 \u00a0 --hog --uri=\"/zone\" --port 80 \u00a0--wsess=100000,1,1 --rate 30'\n```The generator takes up to 5 minutes to generate traffic for the dashboard.\n- View your Cloud Ops dashboard.You should see that approximately 20 RPS is being sent to `us-west1-a` and 10 RPS to `us-east1-b` . This indicates that the Service in `gke-west-1` is fully utilized and is overflowing 10 RPS of traffic to the Service in `gke-east-1` .\n- Stop the load generator using **Ctrl+C** , then delete the Pod:```\nkubectl delete pod loadgen --context gke-west-1\n```- Deploy the load generator configured to send 60 RPS:```\nkubectl run --context gke-west-1 -i --tty --rm loadgen \u00a0\\\u00a0 \u00a0 --image=cyrilbkr/httperf \u00a0\\\u00a0 \u00a0 --restart=Never \u00a0\\\u00a0 \u00a0 -- /bin/sh -c 'httperf \u00a0\\\u00a0 \u00a0 --server=GATEWAY_IP_ADDRESS \u00a0\\\u00a0 \u00a0 --hog --uri=\"/zone\" --port 80 \u00a0--wsess=100000,1,1 --rate 60'\n```\n- Wait 5 minutes and view your Cloud Ops dashboard. It should now show that both clusters are receiving roughly 30 RPS. Since all Services are overutilized globally, there is no traffic spillover and Services absorb all the traffic they can.\n- Stop the load generator using **Ctrl+C** , then delete the Pod:```\nkubectl delete pod loadgen --context gke-west-1\n```## Clean up\nAfter completing the exercises on this page, follow these steps to remove resources and prevent unwanted charges incurring on your account:\n- [Delete the clusters](/kubernetes-engine/docs/how-to/deleting-a-cluster) .\n- [Unregister the clusters](/anthos/fleet-management/docs/unregister) from the fleet if they don't need to be registered for another purpose.\n- Disable the `multiclusterservicediscovery` feature:```\ngcloud container fleet multi-cluster-services disable\n```\n- Disable Multi Cluster Ingress:```\ngcloud container fleet ingress disable\n```\n- Disable the APIs:```\ngcloud services disable \\\u00a0 \u00a0 multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 multiclusteringress.googleapis.com \\\u00a0 \u00a0 trafficdirector.googleapis.com \\\u00a0 \u00a0 --project=PROJECT_ID\n```## Use multi-cluster Gateway with Shared VPC\nA multi-cluster Gateway can also be deployed in a Shared VPC environment, with different topologies, depending on the use case.\nThe following table describes the supported multi-cluster Gateway topologies within a Shared VPC environment:\n| Scenario | Fleet host project   | Config cluster            | Workload clusters           |\n|-----------:|:---------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------|\n|   1 | Shared VPC host project | Shared VPC host project         | Shared VPC host project         |\n|   2 | Shared VPC service project | Shared VPC service project (Same as fleet service project) | Shared VPC service project (Same as fleet service project) |\n**Note:** Multi-cluster Services can be deployed in [different scenarios](/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks#scenarios) with a mix of Shared VPC host project and service projects. Multi-cluster Gateway does not support all topologies and require all clusters to be created in the same Shared VPC host project or service project.\nTo create multi-cluster Gateways in a Shared VPC environment, use the following steps:\n- Follow the steps to set up your [multi-cluster Services with Shared VPC](/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks) \n- [Create your services and export them](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#service-export) to the config cluster\n- If you plan to use an multi-cluster internal Gateway, [create a proxy-only subnet](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#configuring_a_proxy-only_subnet) \n- Create your multi-cluster [external](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#deploy-gateway) or [internal](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#deploying_the_gateway) Gateway and HTTPRoutes\nOnce you are done with these steps, you can validate your deployment, depending on your topology.\n## Troubleshooting\n### Proxy-only subnet for internal Gateway does not exist\nIf the following event appears on your internal Gateway, a proxy-only subnet does not exist for that region. To resolve this issue, deploy a proxy-only subnet.\n```\ngeneric::invalid_argument: error ensuring load balancer: Insert: Invalid value for field 'resource.target': 'regions/us-west1/targetHttpProxies/gkegw-x5vt-default-internal-http-2jzr7e3xclhj'. A reserved and active subnetwork is required in the same region and VPC as the forwarding rule.\n```\n## What's next\n- Learn more about the [Gateway controller](/kubernetes-engine/docs/concepts/gateway-api) .", "guide": "Google Kubernetes Engine (GKE)"}