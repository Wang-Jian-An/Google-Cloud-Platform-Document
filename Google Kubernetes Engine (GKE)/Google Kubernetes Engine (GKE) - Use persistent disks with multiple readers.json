{"title": "Google Kubernetes Engine (GKE) - Use persistent disks with multiple readers", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/readonlymany-disks", "abstract": "# Google Kubernetes Engine (GKE) - Use persistent disks with multiple readers\nThis page explains how to add a [persistent disk](/kubernetes-engine/docs/concepts/persistent-volumes) to your cluster using the ReadOnlyMany access mode. This mode allows multiple [Pods](https://kubernetes.io/docs/concepts/workloads/pods/) on different [nodes](/kubernetes-engine/docs/concepts/cluster-architecture#nodes) to mount the disk for reading.\nFor more information on this mode, refer to [persistent volume accessmodes](/kubernetes-engine/docs/concepts/persistent-volumes#access_modes) .\n", "content": "## Requirements\n- Your cluster must run GKE version 1.22 or later.\n- Your cluster must use the [Compute Engine persistent disk CSI driver](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver) .## Format and populate a source persistent disk\nTo use a persistent disk in read-only mode, you must populate a source persistent disk with data, then either clone the volume or use a volume snapshot to move the data into a new ReadOnlyMany PersistentVolume.\n- Create a PersistentVolume using an [existing persistent disk](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) or using [dynamic provisioning](/kubernetes-engine/docs/concepts/persistent-volumes#dynamic_provisioning) .\n- Format the disk and populate it with data. To format the disk, reference the disk as a ReadWriteOnce PersistentVolume in a Pod. GKE automatically formats the underlying disk, and lets the Pod write data to the disk. When the Pod starts, make sure the Pod writes the data you want to the disk.## Create a ReadOnlyMany PersistentVolume\nYou can create a `ReadOnlyMany` PersistentVolume using one of the following methods:\n- A volume snapshot of a source PersistentVolume that you populate with data.\n- A [volume clone](/kubernetes-engine/docs/how-to/persistent-volumes/volume-cloning) of a source PersistentVolume that you populate with data.\n- A pre-existing persistent disk that was already populated with data.\n- [Create a volume snapshot of the sourcePersistentVolume](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots#create-snapshotclass) .\n- Save the following PersistentVolumeClaim manifest as `snapshot-pvc.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 namespace: PVC_NAMESPACE\u00a0 name: PVC_NAMEspec:\u00a0 dataSource:\u00a0 \u00a0 apiGroup: snapshot.storage.k8s.io\u00a0 \u00a0 kind: VolumeSnapshot\u00a0 \u00a0 name: SNAPSHOT_NAME\u00a0 accessModes:\u00a0 \u00a0 - ReadOnlyMany\u00a0 storageClassName: premium-rwo\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: STORAGE_SIZE\n```Replace the following:- ``: the namespace of the new PersistentVolumeClaim.\n- ``: the name of the new PersistentVolumeClaim.\n- ``:the name of your`VolumeSnapshot`object.\n- ``: the amount of storage to request. This must be the same as the amount requested in the source PersistentVolumeClaim.\n- Apply the manifest to your cluster:```\nkubectl apply -f snapshot-pvc.yaml\n```\nThis creates a new PersistentVolumeClaim named `` in your cluster, which GKE uses to dynamically provision a new PersistentVolume that contains the data from the snapshot of the source persistent disk.\nBefore using volume cloning, make sure to familiarize yourself with the [limitations of this approach](/kubernetes-engine/docs/how-to/persistent-volumes/volume-cloning) .- Save the following manifest as `cloning-pvc.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 namespace: PVC_NAMESPACE\u00a0 name: PVC_NAMEspec:\u00a0 dataSource:\u00a0 \u00a0 name: SOURCE_PVC\u00a0 \u00a0 kind: PersistentVolumeClaim\u00a0 accessModes:\u00a0 \u00a0 - ReadOnlyMany\u00a0 storageClassName: STORAGE_CLASS_NAME\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: STORAGE_SIZE\n```Replace the following:- ``: the namespace of the new PersistentVolumeClaim.\n- ``: the name of the new PersistentVolumeClaim.\n- ``: the name of the source PersistentVolumeClaim populated with data.\n- ``: the storage class for the new PersistentVolumeClaim. This must be the same as the storage class of the source PersistentVolumeClaim.\n- ``: the amount of storage for the new PersistentVolumeClaim. This must be the same amount requested by the source PersistentVolumeClaim.\n- Apply the manifest to your cluster:```\nkubectl apply -f cloning-pvc.yaml\n```\nThis creates a PersistentVolumeClaim named `` that GKE uses to create a new PersistentVolume in ReadOnlyMany mode with the data in the source PersistentVolume.\nFor more detailed information about Volume Cloning, see [Create clones of persistent volumes](/kubernetes-engine/docs/how-to/persistent-volumes/volume-cloning) .- Save the following manifest as `preexisting-disk-pv-pvc.yaml` :```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 name: PV_NAMEspec:\u00a0 storageClassName: \"STORAGE_CLASS_NAME\"\u00a0 capacity:\u00a0 \u00a0 storage: DISK_SIZE\u00a0 accessModes:\u00a0 \u00a0 - ReadOnlyMany\u00a0 claimRef:\u00a0 \u00a0 namespace: PVC_NAMESPACE\u00a0 \u00a0 name: PVC_NAME\u00a0 csi:\u00a0 \u00a0 driver: pd.csi.storage.gke.io\u00a0 \u00a0 volumeHandle: DISK_ID\u00a0 \u00a0 fsType: FS_TYPE\u00a0 \u00a0 readOnly: true---apiVersion: v1kind: PersistentVolumeClaimmetadata:\u00a0 namespace: PVC_NAMESPACE\u00a0 name: PVC_NAMEspec:\u00a0 storageClassName: \"STORAGE_CLASS_NAME\"\u00a0 volumeName: PV_NAME\u00a0 accessModes:\u00a0 \u00a0 - ReadOnlyMany\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: DISK_SIZE\n```Replace the following:- ``: the name of your new PersistentVolume.\n- ``: the name of your new StorageClass.\n- ``: the size of your pre-existing persistent disk. For example,`500G`.\n- ``: the namespace of the new PersistentVolumeClaim.\n- ``: the name of your new PersistentVolumeClaim.\n- ``: the identifier of your pre-existing persistent disk. The format is`projects/{project_id}/zones/{zone_name}/disks/{disk_name}`for [Zonal persistent disks](/kubernetes-engine/docs/concepts/persistent-volumes#pd-zones) , or`projects/{project_id}/regions/{region_name}/disks/{disk_name}`for [Regional persistent disks](/kubernetes-engine/docs/concepts/persistent-volumes#regional_persistent_disks) .\n- ``: the filesystem type. You can leave this as the default (`ext4`), or use`xfs`. If your clusters use a Windows Server node pool, you must change this to`NTFS`.\n- Apply the manifest to your cluster:```\nkubectl apply -f preexisting-disk-pv-pvc.yaml\n```\nThis creates a PersistentVolumeClaim named `` and a PersistentVolume named `` in ReadOnlyMany mode.\nFor more detailed information about using a pre-existing persistent disk, see [existing persistent disk](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) .\n## Use the PersistentVolumeClaim in a Pod\nYou can now reference the new PersistentVolumeClaim in read-only mode in multiple Pods on multiple nodes at the same time. You cannot attach persistent disks in ReadWriteOnce mode to multiple nodes at the same time. For more information, refer to [Deployments vs. StatefulSets](/kubernetes-engine/docs/concepts/persistent-volumes#deployments_vs_statefulsets) .\nIn your Pod specification, you must specify `readOnly: true` in the `volumeMounts` section and the `volumes` section, such as in the following example:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: multi-read\u00a0 labels:\u00a0 \u00a0 app: web-serverspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: web-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: web-server\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: web-server\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /test-mnt\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: my-volume\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: PVC_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\n```\n**Note:** Read-only ext3 and ext4 filesystems do not support journal playback. If the source persistent disk filesystem is marked as dirty when you clone or snapshot it, the data in your read-only persistent disk might be inconsistent or corrupt. The Compute Engine persistent disk CSI driver attempts to mount the disk normally. If the inability to playback the journal causes the volume mount to fail, the CSI driver automatically retries the mount with the `noload` option, which disables playback attempts on ext3 and ext4 filesystems. If the source filesystem was actually inconsistent at the moment it was cloned or snapshotted, the read-only mount will also be inconsistent.\n## What's next\n- Learn more about [using pre-existing persistent disks as PersistentVolumes](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) .\n- Learn more about [Compute Engine persistent disks](/compute/docs/disks#pdspecs) .", "guide": "Google Kubernetes Engine (GKE)"}