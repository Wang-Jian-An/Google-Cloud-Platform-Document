{"title": "Google Kubernetes Engine (GKE) - Regional clusters", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters", "abstract": "# Google Kubernetes Engine (GKE) - Regional clusters\nThis page explains how regional clusters work in Google Kubernetes Engine (GKE).\nRegional clusters increase the availability of a cluster by replicating the control plane and nodes across multiple zones in a [region](/compute/docs/regions-zones/regions-zones#available) .\nRegional clusters provide all of the advantages of [multi-zonal clusters](/kubernetes-engine/docs/concepts/types-of-clusters#multi-zonal_clusters) with the following additional benefits:\n- **Resilience from single zone failure:** Regional clusters are available across arather than a single zone within a region. If a single zone becomes unavailable, your control plane and your resources are not impacted.\n- **Continuous control plane upgrades, control plane resizes, and reduced downtime from control plane failures** . With redundant replicas of the control plane, regional clusters provide higher availability of the Kubernetes API, so you can access your control plane even during upgrades.\nGKE Autopilot clusters are always regional. If you use GKE Standard, you can choose to create regional, zonal, or multi-zonal clusters. To learn about the different cluster availability types, see [About cluster configuration choices](/kubernetes-engine/docs/concepts/types-of-clusters#availability) .\nIn regional clusters, including Autopilot clusters, the control plane is replicated across three zones of a region. GKE automatically replicates nodes across zones in the same region. In Standard clusters and node pools, you can optionally manually specify the zone(s) in which the nodes run. All zones must be within the same region as the control plane.\n**Note:** Use regional clusters to run your production workloads, as they offer higher availability than zonal clusters.\nAfter creating a regional cluster, you cannot change it to a zonal cluster.\n", "content": "## How regional clusters work\nRegional clusters replicate the cluster's control plane and nodes across multiple zones within a single [region](/compute/docs/regions-zones/regions-zones#available) . For example, using the default configuration, a regional cluster in the `us-east1` region creates replicas of the control plane and nodes in three `us-east1` zones: `us-east1-b` , `us-east1-c` , and `us-east1-d` . In the event of an infrastructure outage, Autopilot workloads continue to run and GKE automatically rebalances nodes. If you use Standard clusters, you must rebalance nodes manually or by using the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n### Limitations\n- The default node pool created for regional Standard clusters consists of nine nodes (three per zone) spread evenly across three zones in a region. For public clusters, this consumes nine IP addresses. You can reduce the number of nodes down to one per zone, if desired. Newly created Cloud Billing accounts are granted only eight IP addresses per region, so you may need to [request an increase in your quotas](/compute/quotas) for regional in-use IP addresses, depending on the size of your regional cluster. If you have too few available in-use IP addresses, cluster creation fails.\n- To run [GPUs](/kubernetes-engine/docs/concepts/gpus) in your regional cluster, choose a region that has at least one zone where the requested GPUs are available. You must use the [--node-locations](/kubernetes-engine/docs/how-to/gpus#create) flag when creating the node pool to specify the zone or zones containing the requested GPUs.If the region you choose doesn't have at least one zone where the requested GPUs are available, you might see an error like the following:```\nERROR: (gcloud.container.clusters.create) ResponseError: code=400, message=\n (1) accelerator type \"nvidia-tesla-k80\" does not exist in zone us-west1-c.\n (2) accelerator type \"nvidia-tesla-k80\" does not exist in zone us-west1-a.\n```For a complete list of regions and zones where GPUs are available, see [GPUs on Compute Engine](/compute/docs/gpus) .\n- Zones for Standard mode node pools must be in the same region as the cluster's control plane. If you need to, you can [change a cluster's zones](/kubernetes-engine/docs/how-to/managing-clusters#add_or_remove_zones) , which causes all new and existing nodes to span those zones.\n### Pricing\nAll Autopilot clusters are regional, and are subject to the [Autopilot pricing model](/kubernetes-engine/pricing#autopilot_mode) .\nIn Standard mode, regional clusters require more of your project's [regional quotas](/kubernetes-engine/quotas) than a similar zonal or multi-zonal cluster. Ensure that you understand your quotas and [Standard pricing](/kubernetes-engine/pricing#standard_mode) before using regional clusters. If you encounter an `Insufficient regional quota to satisfy request for resource` error, your request exceeds your available quota in the current region.\nAlso, you are charged for node-to-node traffic across zones. For example, if a workload running in one zone needs to communicate with a workload in a different zone, the cross-zone traffic incurs cost. For more information, see [Egress between zones in the same region (per GB)](/vpc/network-pricing#general) in the Compute Engine pricing page.\n## Persistent storage in regional clusters\nZonal persistent disks are zonal resources and regional persistent disks are multi-zonal resources. When adding [persistent storage](/kubernetes-engine/docs/how-to/stateful-apps#requesting_persistent_storage_in_a_statefulset) unless a zone is specified, GKE assigns the disk to a single, random zone. To learn how to control the zones, see [Zones in persistent disks](/kubernetes-engine/docs/concepts/persistent-volumes#pd-zones) .\n## Autoscaling regional clusters\nKeep the following considerations in mind when using the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) to automatically scale node pools in regional Standard mode clusters.\nYou can also learn more about [Autoscaling limits](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_limits) for regional clusters or about how Cluster Autoscaler [balances across zones](/kubernetes-engine/docs/concepts/cluster-autoscaler#balancing_across_zones) .\nThese considerations only apply to Standard mode clusters with the cluster autoscaler.\n### Overprovisioning scaling limits\nTo maintain capacity in the unlikely event of zonal failure, you can allow GKE to overprovision your scaling limits, to guarantee a minimum level of availability even when some zones are unavailable.\nFor example, if you overprovision a three-zone cluster to 150% (50% excess capacity), you can ensure that 100% of traffic is routed to available zones if one-third of the cluster's capacity is lost. In the preceding example, you would accomplish this by specifying a maximum of six nodes per zone rather than four. If one zone fails, the cluster scales to 12 nodes in the remaining zones.\nSimilarly, if you overprovision a two-zone cluster to 200%, you can ensure that 100% of traffic is rerouted if half of the cluster's capacity is lost.\nYou can learn more about the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) or read the [FAQ for autoscaling](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) in the Kubernetes documentation.\n## What's next\n- [Create a regional cluster](/kubernetes-engine/docs/how-to/creating-a-regional-cluster) .\n- Learn more about the different [types of clusters](/kubernetes-engine/docs/concepts/types-of-clusters) .\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn more about cluster architecture](/kubernetes-engine/docs/concepts/cluster-architecture) .", "guide": "Google Kubernetes Engine (GKE)"}