{"title": "Google Kubernetes Engine (GKE) - Serve an LLM using multi-host TPUs on GKE with Saxml", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/tpu-multihost-saxml", "abstract": "# Google Kubernetes Engine (GKE) - Serve an LLM using multi-host TPUs on GKE with Saxml\nThis tutorial shows you how to serve a large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with [Saxml](https://github.com/google/saxml#saxml-aka-sax) .\n#", "content": "## BackgroundSaxml is an experimental system that serves [Paxml](https://github.com/google/paxml) , [JAX](https://github.com/google/jax) , and [PyTorch](https://pytorch.org/) frameworks. You can use TPUs to accelerate data processing with these frameworks. To demo the deployment of TPUs in GKE, this tutorial serves the 175B [LmCloudSpmd175B32Test](https://github.com/google/saxml/blob/7caf0870b6f9bacab4d255abe278833d642e7bd5/saxml/server/pax/lm/params/lm_cloud.py#L585C9-L585C9) test model. GKE deploys this test model on two v5e TPU node pools with `4x8` topology respectively.\nTo properly deploy the test model, the TPU topology has been defined based on the size of the model. Given that the N billion 16\u00a0bit model approximately requires around 2 times (2xN) GB of memory, the 175B [LmCloudSpmd175B32Test](https://github.com/google/saxml/blob/7caf0870b6f9bacab4d255abe278833d642e7bd5/saxml/server/pax/lm/params/lm_cloud.py#L585C9-L585C9) model requires about 350\u00a0GB of memory. The TPU v5e single chip has 16\u00a0GB. To support 350 GB, GKE needs 21 v5e chips (350/16= 21). Based on the [mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) , the proper TPU configuration for this tutorial is:- Machine type:`ct5lp-hightpu-4t`\n- Topology:`4x8`(32 number of TPU chips)\nSelecting the right TPU topology for serving a model is important when deploying TPUs in GKE. To learn more, see [Plan your TPU configuration](/kubernetes-engine/docs/how-to/tpus#plan-tpu) .## ObjectivesThis tutorial is intended for MLOps or DevOps engineers or platform administrators that want to use GKE orchestration capabilities for serving data models.\nThis tutorial covers the following steps:- Prepare your environment with a GKE Standard cluster. The cluster has two v5e TPU node pools with`4x8`topology.\n- Deploy Saxml. Saxml needs an administrator server, a group of Pods that work as the model server, a prebuilt HTTP server, and a load balancer.\n- Use the Saxml to serve the LLM.\nThe following diagram shows the architecture that the following tutorial implements:## Before you begin- Make sure that you have the following role or roles on the project:      roles/container.admin, roles/iam.serviceAccountAdmin\n- [Ensure your project has sufficient quota](/kubernetes-engine/docs/how-to/tpus#ensure-quota) for Cloud TPU in GKE.\n## Prepare the environment\n- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the default environment variables:```\n\u00a0 gcloud config set project PROJECT_ID\u00a0 export PROJECT_ID=$(gcloud config get project)\u00a0 export REGION=COMPUTE_REGION\u00a0 export ZONE=COMPUTE_ZONE\u00a0 export GSBUCKET=PROJECT_ID-gke-bucket\n```Replace the following values:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : The [Compute Engine region](/compute/docs/regions-zones#available) .\n- : The zone where the [ct5lp-hightpu-4t is available](/kubernetes-engine/docs/concepts/tpus#availability) .\n### Create a GKE Standard clusterUse Cloud Shell to do the following:- Create a Standard cluster that uses [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) :```\ngcloud container clusters create saxml \\\u00a0 \u00a0 --zone=${ZONE} \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --cluster-version=VERSION \\\u00a0 \u00a0 --num-nodes=4\n```Replace the `` with the GKE version number. GKE supports TPU v5e in version 1.27.2-gke.2100 and later. For more information, see [TPU availability in GKE](/kubernetes-engine/docs/concepts/tpus) .The cluster creation might take several minutes.\n- Create the first node pool named `tpu1` :```\ngcloud container node-pools create tpu1 \\\u00a0 \u00a0 --zone=${ZONE} \\\u00a0 \u00a0 --num-nodes=8 \\\u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 --tpu-topology=4x8 \\\u00a0 \u00a0 --cluster=saxml\n```\n- Create the second node pool named `tpu2` :```\ngcloud container node-pools create tpu2 \\\u00a0 \u00a0 --zone=${ZONE} \\\u00a0 \u00a0 --num-nodes=8 \\\u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 --tpu-topology=4x8 \\\u00a0 \u00a0 --cluster=saxml\n```\nYou have created the following resources:- A Standard cluster with four CPU nodes.\n- Two v5e TPU node pools with`4x8`topology. Each node pools represent eight TPU nodes with 4 chips each.\nThe 175B model has to be served on a multi-host v5e TPU slice with `4x8` topology slice (32 v5e TPU chips) at minimum.\n### Create a Cloud Storage bucketCreate a Cloud Storage bucket to store Saxml administrator server configurations. A running administrator server periodically saves its state and the details of the published models.\nIn Cloud Shell, run the following:\n```\ngcloud storage buckets create gs://${GSBUCKET}\n```\n### Configure your workloads access using workload identity federation for GKEAssign a Kubernetes ServiceAccount to the application and configure that Kubernetes ServiceAccount to act as an IAM service account.- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials saxml --zone=${ZONE}\n```\n- Create a Kubernetes ServiceAccount for your application to use:```\nkubectl create serviceaccount sax-sa --namespace default\n```\n- Create an IAM service account for your application:```\ngcloud iam service-accounts create sax-iam-sa\n```\n- Add an [IAM policy binding](/sdk/gcloud/reference/iam/service-accounts/add-iam-policy-binding) for your IAM service account to read and write to Cloud Storage:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 --member \"serviceAccount:sax-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\u00a0 --role roles/storage.admin\n```\n- Allow the Kubernetes ServiceAccount to [impersonate the IAM service account](/iam/docs/service-account-overview#impersonation) by adding an IAM policy binding between the two service accounts. This binding allows the Kubernetes ServiceAccount to act as the IAM service account, so that the Kubernetes ServiceAccount can read and write to Cloud Storage.```\ngcloud iam service-accounts add-iam-policy-binding sax-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/sax-sa]\"\n```\n- [Annotate](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) the Kubernetes service account with the email address of the IAM service account. This lets your sample app know which service account to use to access Google Cloud services. So when the app uses any standard Google API Client Libraries to access Google Cloud services, it uses that IAM service account.```\nkubectl annotate serviceaccount sax-sa \\\u00a0 iam.gke.io/gcp-service-account=sax-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n```\n## Deploy SaxmlIn this section, you deploy the Saxml administrator server and the Saxml model server.\n### Deploy the Saxml administrator server\n- Create the following `sax-admin-server.yaml` manifest: [  ai-ml/llm-multihost-tpus-saxml/sax-admin-server.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-admin-server.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-admin-server.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-admin-serverspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: sax-admin-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: sax-admin-server\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 serviceAccountName: sax-sa\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: sax-admin-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-admin-server:v1.1.0\u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 10000\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: GSBUCKET\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: BUCKET_NAME\n```Replace the `` with the name of your Cloud Storage bucket name.\n- Apply the manifest:```\nkubectl apply -f sax-admin-server.yaml\n```\n- Verify that the administrator server Pod is up and running:```\nkubectl get deployment\n```The output is similar to the following:```\nNAME    READY UP-TO-DATE AVAILABLE AGE\nsax-admin-server 1/1  1   1   52s\n```\n### Deploy Saxml model serverWorkloads running in multi-host TPU slices require a stable network identifier for each Pod to discover peers in the same TPU slice. To define these identifiers, use [IndexedJob](https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/) , [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-identity) with a headless Service or [JobSet](https://github.com/kubernetes-sigs/jobset) which automatically creates a headless Service for all the Jobs that belong to the JobSet. The following section shows how to manage multiple groups of model server Pods with JobSet.- [Install JobSet](https://github.com/kubernetes-sigs/jobset/blob/main/docs/setup/install.md) v0.2.3 or later.```\nkubectl apply --server-side -f https://github.com/kubernetes-sigs/jobset/releases/download/JOBSET_VERSION/manifests.yaml\n```Replace the `` with the JobSet version. For example, `v0.2.3` .\n- Validate JobSet controller is running in the `jobset-system` namespace:```\nkubectl get pod -n jobset-system\n```The output is similar to the following:```\nNAME          READY STATUS RESTARTS AGE\njobset-controller-manager-69449d86bc-hp5r6 2/2  Running 0   2m15s\n```\n- Deploy two model servers in two TPU node pools. Save the following `sax-model-server-set` manifest: [  ai-ml/llm-multihost-tpus-saxml/sax-model-server-set.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-model-server-set.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-model-server-set.yaml) ```\napiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: sax-model-server-set\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: sax-model-server\u00a0 \u00a0 \u00a0 replicas: 2\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: 8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: 8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceAccountName: sax-sa\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 4x8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: sax-model-server\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-model-server:v1.1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: [\"--port=10001\",\"--sax_cell=/sax/test\", \"--platform_chip=tpuv5e\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 10001\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: SAX_ROOT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gs://BUCKET_NAME/sax-root\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: MEGASCALE_NUM_SLICES\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\n```Replace the `` with the name of your Cloud Storage bucket name.In this manifest:- `replicas: 2`is the number of Job replicas. Each job represents a model server. Therefore, a group of 8 Pods.\n- `parallelism: 8`and`completions: 8`are equal to the number of nodes in each node pool.\n- `backoffLimit: 0`must be zero to mark the Job as failed if any Pod fails.\n- `ports.containerPort: 8471`is the default port for the TPU VMs communication\n- `name: MEGASCALE_NUM_SLICES`unsets the environment variable because GKE isn't running Multislice training.\n- Apply the manifest:```\nkubectl apply -f sax-model-server-set.yaml\n```\n- Verify the status of the Saxml Admin Server and Model Server Pods:```\nkubectl get pods\n```The output is similar to the following:```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server-557c85f488-lnd5d     1/1  Running 0   35h\nsax-model-server-set-sax-model-server-0-0-nj4sm 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-1-sl8w4 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-2-hb4rk 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-3-qv67g 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-4-pzqz6 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-5-nm7mz 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-6-7br2x 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-7-4pw6z 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-0-8mlf5 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-1-h6z6w 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-2-jggtv 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-3-9v8kj 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-4-6vlb2 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-5-h689p 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-6-bgv5k 1/1  Running 0   24m\nsax-model-server-set-sax-model-server-1-7-cd6gv 1/1  Running 0   24m\n```\nIn this example, there are 16 model server containers: `sax-model-server-set-sax-model-server-0-0-nj4sm` and `sax-model-server-set-sax-model-server-1-0-8mlf5` are the two primary model servers in each group.\nYour Saxml cluster has two model servers deployed on two v5e TPU node pools with `4x8` topology respectively.\n### Deploy Saxml HTTP Server and load balancer\n- Use the following prebuilt image HTTP server image. Save the following `sax-http.yaml` manifest: [  ai-ml/llm-multihost-tpus-saxml/sax-http.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-http.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multihost-tpus-saxml/sax-http.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-httpspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: sax-http\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: sax-http\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 serviceAccountName: sax-sa\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: sax-http\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-http:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8888\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: SAX_ROOT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gs://BUCKET_NAME/sax-root\"---apiVersion: v1kind: Servicemetadata:\u00a0 name: sax-http-lbspec:\u00a0 selector:\u00a0 \u00a0 app: sax-http\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 8888\u00a0 \u00a0 targetPort: 8888\u00a0 type: LoadBalancer\n```Replace the `` with the name of your Cloud Storage bucket name.\n- Apply the `sax-http.yaml` manifest:```\nkubectl apply -f sax-http.yaml\n``` **Note:** You can use your own HTTP server built for Saxml. To learn more, see [Inferencing using Saxml and an HTTP Server](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/saxml-on-gke/httpserver) .\n- Wait for the HTTP Server container to finish creating:```\nkubectl get pods\n```The output is similar to the following:```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server-557c85f488-lnd5d     1/1  Running 0   35h\nsax-http-65d478d987-6q7zd       1/1  Running 0   24m\nsax-model-server-set-sax-model-server-0-0-nj4sm 1/1  Running 0   24m\n...\n```\n- Wait for the Service to have an external IP address assigned:```\nkubectl get svc\n```The output is similar to the following:```\nNAME   TYPE   CLUSTER-IP EXTERNAL-IP PORT(S)   AGE\nsax-http-lb LoadBalancer 10.48.11.80 10.182.0.87 8888:32674/TCP 7m36s\n```\n## Use SaxmlLoad, deploy, and serve the model on the Saxml in the v5e TPU multihost slice:\n### Load the model\n- Retrieve the load balancer IP address for Saxml.```\nLB_IP=$(kubectl get svc sax-http-lb -o jsonpath='{.status.loadBalancer.ingress[*].ip}')PORT=\"8888\"\n```\n- Load the `LmCloudSpmd175B` test model in two v5e TPU node pools:```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s ${LB_IP}:${PORT}/publish --data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/spmd\",\u00a0 \u00a0 \"model_path\": \"saxml.server.pax.lm.params.lm_cloud.LmCloudSpmd175B32Test\",\u00a0 \u00a0 \"checkpoint\": \"None\",\u00a0 \u00a0 \"replicas\": 2}'\n```The test model does not have a fine-tuned checkpoint, the weights are randomly generated. The model loading could take up to 10 minutes.The output is similar to the following:```\n{\n \"model\": \"/sax/test/spmd\",\n \"path\": \"saxml.server.pax.lm.params.lm_cloud.LmCloudSpmd175B32Test\",\n \"checkpoint\": \"None\",\n \"replicas\": 2\n}\n```\n- Check the model readiness:```\nkubectl logs sax-model-server-set-sax-model-server-0-0-nj4sm\n```The output is similar to the following:```\n...\nloading completed.\nSuccessfully loaded model for key: /sax/test/spmd\n```The model is fully loaded.\n- Get information about the model:```\ncurl --request GET \\--header \"Content-type: application/json\" \\-s ${LB_IP}:${PORT}/listcell --data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/spmd\"}'\n```The output is similar to the following:```\n{\n\"model\": \"/sax/test/spmd\",\n\"model_path\": \"saxml.server.pax.lm.params.lm_cloud.LmCloudSpmd175B32Test\",\n\"checkpoint\": \"None\",\n\"max_replicas\": 2,\n\"active_replicas\": 2\n}\n```\n### Serve the modelServe a prompt request:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s ${LB_IP}:${PORT}/generate --data \\'{\u00a0 \"model\": \"/sax/test/spmd\",\u00a0 \"query\": \"How many days are in a week?\"}'\n```\nThe output shows an example of the model response. This response might not be meaningful because the test model has random weights.\n **Success:** You have successfully served an LLM using multi-host TPUs on GKE.\n### Unpublish the modelRun the following command to unpublish the model:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s ${LB_IP}:${PORT}/unpublish --data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/spmd\"}'\n```\nThe output is similar to the following:\n```\n{\n \"model\": \"/sax/test/spmd\"\n}\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the deployed resources\n- Delete the cluster you created for this tutorial:```\ngcloud container clusters delete saxml --zone ${ZONE}\n```\n- Delete the service account:```\ngcloud iam service-accounts delete sax-iam-sa@${PROJECT_ID}.iam.gserviceaccount.com\n```\n- Delete the Cloud Storage bucket:```\ngcloud storage rm -r gs://${GSBUCKET}\n```\n## What's next\n- Learn about current TPU versions with the [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) .\n- Learn more about [TPUs in GKE](/kubernetes-engine/docs/concepts/tpus) .", "guide": "Google Kubernetes Engine (GKE)"}