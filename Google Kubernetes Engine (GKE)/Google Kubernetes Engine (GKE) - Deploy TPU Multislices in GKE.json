{"title": "Google Kubernetes Engine (GKE) - Deploy TPU Multislices in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/tpu-multislice", "abstract": "# Google Kubernetes Engine (GKE) - Deploy TPU Multislices in GKE\nThis page introduces the TPU Multislice configuration in Google Kubernetes Engine (GKE). Before you configure Multislice in GKE, you should be familiar with the following concepts:\n- [Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) \n- [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) \n- [About TPUs in GKE](/kubernetes-engine/docs/concepts/tpus#workload-scheduling) \n**Key Term:** This page introduces the following terms: - - Data Center Networking (DCN): A higher capacity, low latency, and lower-throughput network that connects Cloud TPU slices in a Multislice configuration.- - TPU slice (or slice): A subset of a full group of connected TPU devices. Each VM in a TPU slice may contain one, four, or eight chips. To learn more, see [System Architecture](/tpu/docs/system-architecture-tpu-vm#tpu_slices) .- - Multislice workload: A workload requiring multiple TPU slices.- - TPU Topology: It defines the number and physical arrangement of TPU chips within a TPU slice. To learn more, see [About TPUs in GKE](/kubernetes-engine/docs/concepts/tpus#topology) .\n", "content": "## What's TPU Multislice\nTPU is the architectural organization of TPU VMs where two or more Cloud TPU slices communicate over the Data Center Network (DCN). Multislice enables full-stack, cost effective, large scale training with near-linear scaling up to tens of thousands of TPUs chips. In a Multislice configuration, GKE deploys a Multislice workload on multiple TPU slices. The communication between chips within a slice happens over inter chip interconnects (ICI). The communication between slices happens over the DCN.\nWe recommend that you use the Multislice if your Job is too big to fit on a single TPU slice.\n## Multislice availability in GKE\n- GKE supports Multislice in version 1.27.4-gke.900 and later.\n- Multislice supports JAX and PyTorch frameworks. The minimum supported JAX version is 2.1.\n- Multislice only supports [multi-host TPU slice nodepools](/tpu/docs/tpus-in-gke#multi-host) . For example, you cannot use Multislice with a`ct4p-hightpu-4t`with a`2x2x1`topology or a`ct5lp-hightpu-4t`with a`2x2`topology, because these are single-host TPU slice node pools.\n- Multislice only supports synchronous multicontroller training.\n- Multislice workloads can only run across TPU slices that share the same TPU type, size, and topology.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Create a [GKE Standard](/kubernetes-engine/docs/how-to/creating-a-regional-cluster) cluster with a version 1.27.4-gke.900 or later.\n- [Ensure your project has sufficient quota](/kubernetes-engine/docs/how-to/tpus#ensure-quota) for Cloud TPU in GKE.\n- [Install JobSet](https://github.com/kubernetes-sigs/jobset/blob/main/docs/setup/install.md) v0.2.3 or later.## Run a workload on a Multislice\nThis section covers the following steps:\n- Createmulti-host TPU slice node pools.\n- Run a workload in a Multislice using a JobSet.\n### Create the TPU node pools\nYou can create more than one multi-host TPU node pool. For the purpose of this guide, create multi-host TPU node pools to run a Multislice workload. You can create a [multi-host TPU slice](/kubernetes-engine/docs/concepts/tpus#node_pool) node pool using the Google Cloud CLI, Terraform, or the Google Cloud console.\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --node-locations=NODE_ZONE \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --tpu-topology=TPU_TOPOLOGY \\\u00a0 \u00a0 --num-nodes=NUM_NODES \\\u00a0 \u00a0 [--spot \\]\u00a0 \u00a0 [--enable-autoscaling \\\u00a0 \u00a0 \u00a0 --max-nodes MAX_NODES]\u00a0 \u00a0 [--reservation-affinity=specific \\\u00a0 \u00a0 --reservation=RESERVATION_NAME]\n```\nReplace the following:- ``: The name of the new node pool.\n- ``: The name of the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- TPU v5e machine types beginning with`ct5l-`are never multi-host.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p machine types beginning with`ct5p-`, use`us-east1-d`,`us-east5-a`, or`us-east5-c`. To learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- ``: The name of the cluster.\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The type of machine to use for nodes. To learn more about the available machine types, see [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\n- ``: The physical topology for the TPU slice. The format of the topology depends on the TPU version as follows:- TPU v4: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.\n- TPU v5e: Define the topology in 2-tuples (`{A}x{B}`), for example`2x2`.\n- ``: The number of nodes in the node pool. It must be zero or the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM. For multi-host TPU v4 and TPU v5e, the number of chips in each VM is four. Therefore, if your``is`2x4x4`(TPU v4 with four chips in each VM), then the``is 32/4 which equals to 8.\nOptionally, you can also use the following flags:- ``: The name of the reservation GKE uses when creating the node pool. If you omit this flag, GKE uses available TPU node pools. To learn more about TPU reservations, see [TPU reservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) .\n- `--spot`: Sets the node pool to use Spot VMs for the TPU nodes. This cannot be changed after node pool creation. For more information, see [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\n- `--enable-autoscaling`: Create a node pool with autoscaling enabled. When GKE scales a multi-host TPU slice node pool, it [atomically](/kubernetes-engine/docs/concepts/tpus#terminology) scales up the node pool from zero to the maximum size.- ``: The maximum size of the node pool. The`--max-nodes`flag is required if`--enable-autoscaling`is supplied and must be equal to the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM.- Ensure that you use the version 4.84.0 or later of the [google](https://registry.terraform.io/providers/hashicorp/google/latest) provider.\n- Add the following block to your Terraform configuration:```\nresource \"google_container_node_pool\" \"NODE_POOL_RESOURCE_NAME\" {\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = google\u00a0 project \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= PROJECT_ID\u00a0 cluster \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= CLUSTER_NAME\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = POOL_NAME\u00a0 location \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = CLUSTER_LOCATION\u00a0 node_locations \u00a0 \u00a0 = [NODE_ZONES]\u00a0 initial_node_count = NUM_NODES\u00a0 autoscaling {\u00a0 \u00a0 max_node_count = MAX_NODES\u00a0 \u00a0 location_policy \u00a0 \u00a0 \u00a0= \"ANY\"\u00a0 }\u00a0 node_config {\u00a0 \u00a0 machine_type = MACHINE_TYPE\u00a0 \u00a0 reservation_affinity {\u00a0 \u00a0 \u00a0 consume_reservation_type = \"SPECIFIC_RESERVATION\"\u00a0 \u00a0 \u00a0 key = \"compute.googleapis.com/reservation-name\"\u00a0 \u00a0 \u00a0 values = [RESERVATION_LABEL_VALUES]\u00a0 \u00a0 }\u00a0 \u00a0 spot = true\u00a0 }\u00a0 placement_policy {\u00a0 \u00a0 type = \"COMPACT\"\u00a0 \u00a0 tpu_topology = TPU_TOPOLOGY\u00a0 }}\n```Replace the following:- ``: The name of the node pool resource in the Terraform template.\n- ``: Your project ID.\n- ``: The name of the existing cluster to add the node pool to.\n- ``: The name of the node pool to create.\n- ``: Compute location for the cluster. We recommend having a regional cluster for higher reliability of the Kubernetes control plane. You can also use a zonal cluster. To learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The number of nodes in the node pool. It must be zero or the product of the number of the TPU chips divided by four, because in multi-host TPU slices each TPU node has 4 chips. For example, if``is`4x8`, then there are 32 chips which means``must be 8. To learn more about TPU topologies, use the table in [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\n- ``: This indicates the desired physical topology for the TPU slice. The format of the topology depends on the TPU version you are using:- For TPU v4: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.\n- For TPU v5e: Define the topology in 2-tuples (`{A}x{B}`), for example`2x2`.Optionally, you can also use the following variables:- ``: If you use [TPUreservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) , this is the list of labels of the reservation resources to use when creating the node pool. To learn more about how to populate the``in the`reservation_affinity`field, see [Terraform Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster#reservation_affinity) .\n- `autoscaling`: Create a node pool with autoscaling enabled. When GKE scales a multi-host TPU slice node pool, it [atomically](/kubernetes-engine/docs/concepts/tpus#terminology) scales up the node pool from zero to the maximum size.- ``: It is the maximum size of the node pool. It must be equal to the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM.\n- `spot`: Lets the node pool to use Spot VMs for the TPU nodes. This cannot be changed after node pool creation. For more information, see [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\nTo create a node pool with TPUs:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add node pool** .\n- In the **Node pool details** section, check the **Specify node locations** box.\n- Select the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- TPU v5e machine types beginning with`ct5l-`are never multi-host.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p machine types beginning with`ct5p-`, use`us-east1-d`,`us-east5-a`, or`us-east5-c`.\n- From the navigation pane, click **Nodes** .\n- In the **Machine Configuration** section, select **TPUs** .\n- In the **Series** drop-down menu, select one of the following:- **CT4P** : For TPU v4.\n- **CT5LP** : For TPU v5e.\n- In the **Machine type** drop-down menu, select the name of the machine to use for nodes. Use the [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) table to learn how to define the machine type and TPU topology that create a TPU node pool.\n- In the **TPU Topology** drop-down menu, select the physical topology for the TPU slice.\n- In the **Changes needed** dialog, click **Make changes** .\n- Ensure that **Boot disk type** is either **Standard persistent disk** or **SSD persistent disk** .\n- Optionally, select the **Enable nodes on spot VMs** checkbox to use Spot VMs for the nodes in the node pool.\n- Click **Create** .\n**Important:** Repeat the preceding steps to create two additional TPU node  pools in the cluster. Change the name of the node pool in every iteration.\n### Verify the node pool status\n- Get credentials, so that you can use `kubectl` to access the cluster:```\ngcloud container clusters get-credentials CLUSTER_NAME \\\u00a0 \u00a0 --project=PROJECT_ID\n```Replace the following:- ``: The name of the cluster.\n- ``: Your project ID.\n- Use `kubectl` , in Cloud Shell, to see your TPU nodes:```\nkubectl get nodes -l cloud.google.com/gke-tpu-accelerator=TPU_ACCELERATOR \\\u00a0 \u00a0-l cloud.google.com/gke-tpu-topology=TPU_TOPOLOGY\n```Replace the following:- ``: The type of [TPU accelerator](/kubernetes-engine/docs/concepts/tpus#configuration) you used when you created the node pools. For example,`tpu-v4-podslice`,`tpu-v5-lite-device`, or`tpu-v5-lite-podslice`.\n- ``: The [physical topology](/kubernetes-engine/docs/concepts/tpus#configuration) for the TPU slice.\nThe output is similar to the following:```\n NAME         STATUS ROLES AGE VERSION\n gke-tpu-20ee2cce-5tv6     Ready <none> 34h  v1.28.1-gke.1066000\n```\n### Run a Multislice workload\nIn this section you run a JAX workload which shows the global number of TPU chips in the TPU slice and then exits.\nTo run a JAX workload do the following:\n- Create the following `tpu-multislice.yaml` manifest:```\napiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: multislice-job\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: slice\u00a0 \u00a0 \u00a0 replicas: NUM_SLICES\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: NUM_NODES\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: NUM_NODES\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: ACCELERATOR_TYPE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: TPU_TOPOLOGY\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: jax-tpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: python:3.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8431\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -c 'import jax; print(\"Global device count:\", jax.device_count())'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sleep 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: NUM_CHIPS\n```Replace the following:- ``: The number of TPU node pools. In this case, the``equals`3`.\n- ``: The type of [TPU accelerator](/kubernetes-engine/docs/concepts/tpus#configuration) you used when you created the node pools. For example,`tpu-v4-podslice`,`tpu-v5-lite-device`, or`tpu-v5-lite-podslice`.\n- ``: The [physical topology](/kubernetes-engine/docs/concepts/tpus#configuration) for the TPU slice. For example`4x4x4`or`2x2`depending on the TPU version.\n- ``: The number of nodes in the node pool. It must be zero or the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM. For multi-host TPU v4, the number of chips in each VM is four. For multi-host TPU v5e, the number of chips in each VM is one, four, or eight. Therefore, if your``is`2x4x4`(TPU v4 with four chips in each VM), then the``is 32/4 which equals to 8.\n- ``: For multi-host TPU v4, the number of chips in each VM is four. For multi-host TPU v5e, the number of chips in each VM is one, four, or eight. To learn more, see [TPU chips on the TPU VM](/kubernetes-engine/docs/concepts/tpus#configuration) .\nIn this manifest:- The JobSet is a Headless Service with the same name as the JobSet name, in this case it is`multislice-job`.\n- The`maxRestarts: 4`indicates the maximum number of times that GKE restarts the JobSet when a child Job fails. If the JobSet restarts reaches the maximum defined, then the JobSet is marked as failed.\n- The`parallelism`and`completions`fields equal the number of nodes in each node pool.\n- The`backoff`is 0 because Multislice only supports synchronous multi-controller training. Must be set to 0. Fail the job when any pod fails.\n- The values in the affinity section ensure that there is only one TPU Multislice workload running in a group of Multislices.\n- The`containerPort: 8080`is the port for MXLA coordinator\n- The`containerPort: 8431`is the port to export the TPU usage metrics\n- The`securityContext: privileged: true`indicates that nodes have privileged mode enabled to access TPUs. Nodes in GKE version 1.28 or later don't need to have privileged mode enabled to access TPUs. To learn more, see [Run containers without privileged mode](/kubernetes-engine/docs/how-to/tpus#privileged-mode) .\n- Apply the manifest:```\nkubectl apply -f tpu-multislice.yaml\n```\n- Confirm that the workload is admitted:```\nkubectl get jobsets\n```The output is similar to the following:```\nNAME   RESTARTS COMPLETED AGE\nmultislice-job       3s\n```\n- Monitor the status of the provisioned Pods:```\nkubectl get pods\n```The output is similar to the following:```\n NAME        READY STATUS  RESTARTS AGE\n multislice-job-slice-0-0-wzq9t  0/1  Completed 0   2m31s\n multislice-job-slice-0-1-zf4dp  0/1  Completed 0   2m30s\n multislice-job-slice-1-0-hbfn5  0/1  Completed 0   2m31s\n multislice-job-slice-1-1-45fgl  0/1  Completed 0   2m30s\n multislice-job-slice-2-0-wjbp4  0/1  Completed 0   2m30s\n multislice-job-slice-2-1-lwnvs  0/1  Completed 0   2m30s\n```\nThe `multislice-job` JobSet schedules, creates, then runs the Pods to completion. The Pod names are in the format `<jobsetName>-<jobName>-<jobReplicaIndex>-<randomSuffix>` . The `jobsetName` prefix determines the JobSet the Pod belongs to.\n## Additional configurations\nThe following sections describe the additional configurations you can apply to your Multislice.\n### Turn on hostNetwork on your GKE Pods\nTo improve network performance between TPU slices, we recommend you turn on `hostNetworking` . Use `hostNetwork: true` in your Pod spec to skip all the Kubernetes networking stack and let your Kubernetes Pods use the host network directly for VM-to-VM communication.\nTo turn on `hostNetworking` , remove the following two lines from your Pod spec:\n```\nhostNetwork: true\ndnsPolicy: ClusterFirstWithHostNet\n```\nTo keep using the `podHostnames` for worker node discovery with `hostNetwork` , set `dnsPolicy: ClusterFirstWithHostNet` . This is important when you are running auto-resuming training Jobs and you need to have the same names for reloading the same checkpoints.\n**Note:** If you don't turn on `hostNetworking` , the Multislice workload might have slower networking between slices.\n### Logging\nLogs emitted by containers running on GKE nodes, including TPU VMs, are visible in the [Logs Explorer](/logging/docs/view/logs-explorer-interface) , if you have [GKE system loggingenabled](/stackdriver/docs/solutions/gke/installing#available-logs) in your cluster.\nYou can [view your logs from GKE](/stackdriver/docs/solutions/gke/using-logs) using the Logs Explorer with the following filter to view the container logs for your workload:\n```\nresource.type=\"k8s_container\"resource.labels.cluster_name=CLUSTER_NAMElabels.\"k8s-pod/jobset_sigs_k8s_io/jobset-name\"=JOBSET_NAME\n```\nUse the following filter for TPU slice and workers:\n```\nresource.type=\"k8s_container\"resource.labels.cluster_name=CLUSTER_NAMElabels.\"k8s-pod/jobset_sigs_k8s_io/jobset-name\"=JOBSET_NAMEresource.labels.pod_name:<jobSetName>-<replicateJobName>-<job-index>-<worker-index>\n```\n### Observability and metrics\nIn addition to the general [TPU metrics](/kubernetes-engine/docs/how-to/tpus#metrics) , there are 4 additional multislice specific TPU runtime metrics. These metrics are available in GKE version 1.29.1-gke.1016000 or later. TPU workload must use JAX version [0.4.24](https://github.com/google/jax/releases/tag/jaxlib-v0.4.24)\nThe following are the available multislice metrics:\n- **DCN (Data Center Network) transfer latencies** : Distribution of network transfer latencies for multislice traffic.\n- **Collective latencies** : Distribution of end to end collective latency for multislice traffic.\n- **Host-to-Device transfer latencies** : Distribution of host to device transfer latency for each chunk of data for multislice traffic.\n- **Device-to-Host transfer latencies** : Distribution of device to host transfer latency for each chunk of data for multislice traffic.\nThese metrics are located in the Kubernetes container ( `k8s_container` ) schema:\n- `kubernetes.io/container/multislice/network/dcn_transfer_latencies`\n- `kubernetes.io/container/multislice/network/collective_end_to_end_latencies`\n- `kubernetes.io/container/multislice/accelerator/host_to_device_transfer_latencies`\n- `kubernetes.io/container/multislice/accelerator/device_to_host_transfer_latencies`\n### TPU slice versus Multislice\nThe following table differentiates the architectural organization of a TPU slice and a Multislice:\n| Unnamed: 0    | TPU slice                     | Multislice                                |\n|:--------------------------|:------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n| Interconnectivity   | The workload runs on a single TPU slice. All TPU chips in a slice are connected with ICI. | The workload runs on multiple TPU slices. Communication within a slice happens over ICI. Communication between slices occurs over DCN. |\n| Supported node pools  | Single-host TPU slice and multi-host TPU slice           | Groups of multi-host TPU slices                          |\n| Recommended workload type | IndexedJob or JobSet                  | JobSet                                 |\n## What's next\n- Learn how to [Orchestrate Multislice workloads with TPU slices](/kubernetes-engine/docs/tutorials/tpu-multislice-kueue)", "guide": "Google Kubernetes Engine (GKE)"}