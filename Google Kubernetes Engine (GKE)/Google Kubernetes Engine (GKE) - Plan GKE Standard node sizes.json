{"title": "Google Kubernetes Engine (GKE) - Plan GKE Standard node sizes", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes", "abstract": "# Google Kubernetes Engine (GKE) - Plan GKE Standard node sizes\nThis page describes how to plan the size of nodes in Google Kubernetes Engine (GKE) Standard node pools to reduce the risk of workload disruptions and out-of-resource terminations. This planning is not required in GKE Autopilot because Google manages the nodes for you.\n", "content": "## Benefits of right-sized nodes\nEnsuring that your nodes are correctly sized to accommodate your workloads and to handle spikes in activity provides benefits such as the following:\n- Better workload reliability because of a reduced risk of out-of-resource eviction.\n- Improved scalability for scaling workloads during high-traffic periods.\n- Lower costs because nodes aren't too large for your needs, which might result in wasted resources.## Node allocatable resources\nGKE nodes run system components that let the node function as a part of your cluster. These components use node resources, such as CPU and memory. You might notice a difference between your node's total resources, which are based on the size of the underlying Compute Engine virtual machine (VM), and the resources that are for your GKE workloads to request. This difference is because GKE reserves a pre-defined quantity of resources for system functionality and node reliability. The disk space that GKE reserves for system resources differs based on the [nodeimage](/kubernetes-engine/docs/concepts/node-images#available_node_images) . The remaining resources that are available for your workloads are called .\nWhen you define Pods in a manifest, you can specify resource and in the Pod specification. When GKE places the Pods on a node, the Pod requests those specified resources from the allocatable resources on the node. When planning the size of the nodes in your node pools, you should consider how many resources your workloads need to function correctly.\n### Check allocatable resources on a node\nTo inspect the allocatable resources on an existing node, run the following command:\n```\nkubectl get node NODE_NAME \\\u00a0 \u00a0 -o=yaml | grep -A 7 -B 7 capacity\n```\nReplace `` with the name of the node.\nThe output is similar to the following:\n```\nallocatable:\n attachable-volumes-gce-pd: \"127\"\n cpu: 3920m\n ephemeral-storage: \"47060071478\"\n hugepages-1Gi: \"0\"\n hugepages-2Mi: \"0\"\n memory: 13498416Ki\n pods: \"110\"\ncapacity:\n attachable-volumes-gce-pd: \"127\"\n cpu: \"4\"\n ephemeral-storage: 98831908Ki\n hugepages-1Gi: \"0\"\n hugepages-2Mi: \"0\"\n memory: 16393264Ki\n pods: \"110\"\n```\nIn this output, the values in the `allocatable` section are the allocatable resources on the node. The values in the `capacity` section are the total resources on the node. The units of ephemeral storage are bytes.\n## GKE resource reservations\nGKE reserves specific amounts of memory and CPU resources on nodes based on the total size of the resource available on the node. Larger machine types run more containers and Pods, so the amount of resources that GKE reserves scales up for larger machines. Windows Server nodes also require more resources than equivalent Linux nodes, to account for running the Windows OS and for the Windows Server components that can't run in containers.\n### Memory and CPU reservations\nThe following sections describe the default memory and CPU reservations based on the machine type.\n**Note:** This document uses gibibytes (GiB) and mebibytes (MiB) for memory and storage values. However, the Google Cloud console uses gigabytes (GB) and megabytes (MB). This difference in units means that the allocatable capacity values that you see in the Google Cloud console might be different than the values that you calculate using this document.\nFor memory resources, GKE reserves the following:\n- 255 MiB of memory for machines with less than 1 GiB of memory\n- 25% of the first 4 GiB of memory\n- 20% of the next 4 GiB of memory (up to 8 GiB)\n- 10% of the next 8 GiB of memory (up to 16 GiB)\n- 6% of the next 112 GiB of memory (up to 128 GiB)\n- 2% of any memory above 128 GiB\nGKE also reserves an additional 100 MiB of memory on every node to handle Pod eviction.\n**Note:** If you enable Image streaming, GKE reserves extra memory. For details, refer to [Memory reservation for Image streaming](/kubernetes-engine/docs/how-to/image-streaming#memory_reservation_for) .\nFor CPU resources, GKE reserves the following:\n- 6% of the first core\n- 1% of the next core (up to 2 cores)\n- 0.5% of the next 2 cores (up to 4 cores)\n- 0.25% of any cores above 4 cores\n**Note:** If you increase the maximum number of Pods per node beyond the default of 110, GKE reserves an extra 400 mCPU in addition to the preceding reservations.\nFor [shared-core E2 machine types](/compute/docs/general-purpose-machines#sharedcore) , GKE reserves a total of 1060 millicores.\n### Local ephemeral storage reservation\nGKE provides nodes with local , backed by locally attached devices such as the node's boot disk or local SSDs. Ephemeral storage has no guarantee of availability, and data in ephemeral storage could be lost if a node fails and is deleted.\nGKE reserves a portion of the node's total ephemeral storage as a single file system for the kubelet to use during Pod eviction, and for other system components running on the node. You can allocate the remaining ephemeral storage to your Pods to use for purposes such as logs. To learn how to specify ephemeral storage requests and limits in your Pods, refer to [Local ephemeral storage](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage) .\nGKE calculates the local ephemeral storage reservation as follows:\n```\nEVICTION_THRESHOLD + SYSTEM_RESERVATION\n```\nThe actual values vary based on the size and type of device that backs the storage.\nBy default, ephemeral storage is backed by the node boot disk. In this case, GKE determines the value of the eviction threshold as follows:\n```\nEVICTION_THRESHOLD = 10% * BOOT_DISK_CAPACITY\n```\nThe eviction threshold is always 10% of the total boot disk capacity.\nGKE determines the value of the system reservation as follows:\n```\nSYSTEM_RESERVATION = Min(50% * BOOT_DISK_CAPACITY, 6GiB + 35% * BOOT_DISK_CAPACITY, 100 GiB)\n```\nThe system reservation amount is the **lowest** of the following:\n- 50% of the boot disk capacity\n- 35% of the boot disk capacity + 6 GiB\n- 100 GiB\nFor example, if your boot disk is 300 GiB, the following values apply:\n- 50% of capacity: 150 GiB\n- 35% of capacity + 6 GiB: 111 GiB\n- 100 GiB\nGKE would reserve the following:\n- System reservation: 100 GiB (the lowest value)\n- Eviction threshold: 30 GiB\nThe total reserved ephemeral storage is 130 GiB. The remaining capacity, 170 GiB, is allocatable ephemeral storage.\nIf your ephemeral storage is backed by [local SSDs](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd) , GKE calculates the eviction threshold as follows:\n```\nEVICTION_THRESHOLD = 10% * SSD_NUMBER * 375 GB\n```\nIn this calculation, `SSD_NUMBER` is the number of attached local SSDs. All local SSDs are 375 GB in size, so the eviction threshold is 10% of the total ephemeral storage capacity.\nGKE calculates the system reservation depending on the number of attached SSDs, as follows:\n| Number of local SSDs | System reservation (GiB) |\n|:-----------------------|:---------------------------|\n| 1      | 50 GiB      |\n| 2      | 75 GiB      |\n| 3 or more    | 100 GiB     |\n## Use resource reservations to plan node sizes\n- Consider the resource requirements of your workloads at deploy time and under load. This includes the requests and planned limits for the workloads, as well as overhead to accommodate scaling up.\n- Consider whether you want a small number of large nodes or a large number of small nodes to run your workloads.- A small number of large nodes works well for resource-intensive workloads that don't require high availability. Node autoscaling is **less agile** because more Pods must be evicted for a scale-down to occur.\n- A large number of small nodes works well for highly-available workloads that aren't resource intensive. Node autoscaling is **more agile** because fewer Pods must be evicted for a scale-down to occur.\n- Use the [Compute Engine machine family comparison guide](/compute/docs/machine-resource#machine_type_comparison) to determine the machine series and family that you want for your nodes.\n- Consider the ephemeral storage requirements of your workloads. Is the node boot disk enough? Do you need local SSDs?\n- Calculate the on your chosen machine type using the information in the previous sections. Compare this to the resources and overhead that you need.- If your chosen machine type is too large, consider a smaller machine to avoid paying for the extra resources.\n- If your chosen machine type is too small, consider a larger machine to reduce the risk of workload disruptions.\n## What's next\n- [Choose a machine type for a node pool](/sdk/gcloud/reference/container/node-pools/create#--machine-type) \n- [Schedule workloads on specific node pools using node taints](/kubernetes-engine/docs/how-to/node-taints#create_a_node_pool_with_node_taints)", "guide": "Google Kubernetes Engine (GKE)"}