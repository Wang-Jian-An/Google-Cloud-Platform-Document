{"title": "Google Kubernetes Engine (GKE) - Automatically bootstrap GKE nodes with DaemonSets", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/automatically-bootstrapping-gke-nodes-with-daemonsets", "abstract": "# Google Kubernetes Engine (GKE) - Automatically bootstrap GKE nodes with DaemonSets\nThis tutorial shows how to customize the nodes of a [Google Kubernetes Engine (GKE)](/kubernetes-engine) cluster by using [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) . A DaemonSet ensures that all (or selected) nodes run a copy of a [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) . This approach lets you use the same tools to orchestrate your workloads that you use to [modify your GKE nodes](/kubernetes-engine/docs/concepts/node-images#modifications) .\nIf the tools and systems you use to initialize your clusters are different from the tools and systems you use to run your workloads, you increase the effort it takes to manage your environment. For example, if you use a configuration management tool to initialize the cluster nodes, you're relying on a procedure that's outside the runtime environment where the rest of your workloads run.\nThe goal of this tutorial is to help system administrators, system engineers, or infrastructure operators streamline the initialization of Kubernetes clusters.\n **Caution:** Customizing your GKE nodes can lead to unintended behavior that might negatively affect the health of your workloads and nodes. Not all customizations are supported. Ensure that you implement extensive testing of any customizations before deploying them on clusters running production workloads.\nFor this tutorial, you need to be familiar with the following tools:- [Kubernetes](https://kubernetes.io/) \n- [GKE](/kubernetes-engine) \n- [Docker](https://www.docker.com/) \n- [chroot](https://wikipedia.org/wiki/Chroot) \nIn this tutorial, you learn to use [Kubernetes labels and selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) to choose which initialization procedure to run based on the labels that are applied to a node. In these steps, you deploy a DaemonSet to run only on nodes that have the `default-init` label applied. However, to demonstrate the flexibility of this mechanism, you could create another node pool and apply the `alternative-init` label to the nodes in this new pool. In the cluster, you could then deploy another DaemonSet that is configured to run only on nodes that have the `alternative-init` label.\nAlso, you could run multiple initialization procedures on each node, not just one. You can leverage this mechanism to better structure your initialization procedures, clearly separating the concerns of each one.\nIn this tutorial, as an example, the initialization procedure performs the following actions on each node that is labeled with the `default-init` label:- Attaches an additional [disk](/compute/docs/disks) to the node.\n- Installs a set of packages and libraries by using the node's operating system package manager.\n- Loads a set of Linux kernel modules.\n", "content": "## ObjectivesIn this tutorial you do the following:- Provision and configure a GKE cluster.\n- Prepare a DaemonSet descriptor to initialize the nodes in the cluster.\n- Deploy the DaemonSet in the cluster.\n- Verify that the cluster nodes have been initialized.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n## Bootstrap the environmentIn this section, you do the following:- Enable the necessary [Cloud APIs](/apis) .\n- Provision a [service account](/compute/docs/access/service-accounts) with limited privileges for the nodes in the GKE cluster.\n- Prepare the GKE cluster.\n- Grant the user cluster administration privileges.\n### Enable Cloud APIs\n- Open Cloud Shell. [OPEN Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Select the Google Cloud project:```\ngcloud config set project project-id\n```Replace `` with the ID of the Google Cloud project that you created or selected for this tutorial.\n- Enable the Google Kubernetes Engine API:```\ngcloud services enable container.googleapis.com\n```\n### Provision a service account to manage GKE clustersIn this section, you create a [service account](/compute/docs/access/service-accounts) that is associated with the nodes in the cluster. In this tutorial, GKE nodes use this service account instead of the default service account. As a best practice, [grant the service account just the roles and access permissions that are required](/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa) to run the application.\nThe roles required for the service account are as follows:- Monitoring Viewer role (`roles/monitoring.viewer`). This role gives read-only access to the Cloud Monitoring console and API.\n- Monitoring Metric Writer role (`roles/monitoring.metricWriter`). This role permits writing monitoring data.\n- Logs Writer role (`roles/logging.logWriter`). This role gives just enough permissions to write logs.\n- Service Account User role (`roles/iam.serviceAccountUser`). This role gives access to service accounts in a project. In this tutorial, the initialization procedure impersonates the service account to run privileged operations.\n- Compute Admin role (`roles/compute.admin`). This role provides full control of all Compute Engine resources. In this tutorial, the service account needs this role to attach additional disks to cluster nodes.\nTo provision a service account, follow these steps:- In Cloud Shell, initialize an environment variable that stores the service account name:```\nGKE_SERVICE_ACCOUNT_NAME=ds-init-tutorial-gke\n```\n- Create a service account:```\ngcloud iam service-accounts create \"$GKE_SERVICE_ACCOUNT_NAME\" \\\u00a0 --display-name=\"$GKE_SERVICE_ACCOUNT_NAME\"\n```\n- Initialize an environment variable that stores the service account email account name:```\nGKE_SERVICE_ACCOUNT_EMAIL=\"$(gcloud iam service-accounts list \\\u00a0 \u00a0 --format='value(email)' \\\u00a0 \u00a0 --filter=displayName:\"$GKE_SERVICE_ACCOUNT_NAME\")\"\n```\n- Bind the Identity and Access Management (IAM) roles to the service account:```\ngcloud projects add-iam-policy-binding \\\u00a0 \u00a0 \"$(gcloud config get-value project 2> /dev/null)\" \\\u00a0 \u00a0 --member serviceAccount:\"$GKE_SERVICE_ACCOUNT_EMAIL\" \\\u00a0 \u00a0 --role roles/compute.admingcloud projects add-iam-policy-binding \\\u00a0 \u00a0 \"$(gcloud config get-value project 2> /dev/null)\" \\\u00a0 \u00a0 --member serviceAccount:\"$GKE_SERVICE_ACCOUNT_EMAIL\" \\\u00a0 \u00a0 --role roles/monitoring.viewergcloud projects add-iam-policy-binding \\\u00a0 \u00a0 \"$(gcloud config get-value project 2> /dev/null)\" \\\u00a0 \u00a0 --member serviceAccount:\"$GKE_SERVICE_ACCOUNT_EMAIL\" \\\u00a0 \u00a0 --role roles/monitoring.metricWritergcloud projects add-iam-policy-binding \\\u00a0 \u00a0 \"$(gcloud config get-value project 2> /dev/null)\" \\\u00a0 \u00a0 --member serviceAccount:\"$GKE_SERVICE_ACCOUNT_EMAIL\" \\\u00a0 \u00a0 --role roles/logging.logWritergcloud projects add-iam-policy-binding \\\u00a0 \u00a0 \"$(gcloud config get-value project 2> /dev/null)\" \\\u00a0 \u00a0 --member serviceAccount:\"$GKE_SERVICE_ACCOUNT_EMAIL\" \\\u00a0 \u00a0 --role roles/iam.serviceAccountUser\n```\n### Prepare the GKE clusterIn this section, you launch the GKE cluster, grant permissions, and finish the cluster configuration.\nFor this tutorial, a cluster with a relatively low number of small, [general purpose](/compute/docs/machine-types#general_purpose) nodes is enough to demonstrate the concept of this tutorial. You create a cluster with [one node pool](/kubernetes-engine/docs/concepts/node-pools) (the default one). Then you label all the nodes in the default node pool with the `default-init` label.- In Cloud Shell, create and launch a [regional](/kubernetes-engine/docs/concepts/regional-clusters) GKE cluster:```\ngcloud container clusters create ds-init-tutorial \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --image-type=ubuntu_containerd \\\u00a0 \u00a0 --machine-type=n1-standard-2 \\\u00a0 \u00a0 --metadata disable-legacy-endpoints=true \\\u00a0 \u00a0 --node-labels=app=default-init \\\u00a0 \u00a0 --node-locations us-central1-a,us-central1-b,us-central1-c \\\u00a0 \u00a0 --no-enable-basic-auth \\\u00a0 \u00a0 --no-issue-client-certificate \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --region us-central1 \\\u00a0 \u00a0 --service-account=\"$GKE_SERVICE_ACCOUNT_EMAIL\"\n```\n **Note:** The initialization procedure of your nodes should take into account the tools that are available in the underlying operating system. In this case, for example, the GKE cluster nodes run Ubuntu, letting you use all the installed tools for Ubuntu, like the [APT package manager](https://wikipedia.org/wiki/APT_(Package_Manager)) .## Deploy the DaemonSetIn this section, you do the following:- Create the [ConfigMap](/kubernetes-engine/docs/concepts/configmap) that stores the initialization procedure.\n- Deploy the DaemonSet that schedules and executes the initialization procedure.\nThe DaemonSet does the following:- Configures a volume that makes the contents of the ConfigMap available to the containers that the DaemonSet handles.\n- Configures the volumes for privileged file system areas of the underlying cluster node. These areas let the containers that the DaemonSet schedules directly interact with the node that runs them.\n- Schedules and runs an [init container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) that executes the initialization procedure and then is terminated upon completion.\n- Schedules and runs a container that stays idle and consumes no resources.\nThe idle container ensures that a node is initialized only once. DaemonSets are designed so [that all eligible nodes run a copy of a Pod](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler-enabled-by-default-since-1-12) . If you use a regular container, that container runs the initialization procedure and is then terminated upon completion. By design, the DaemonSet reschedules the Pod. To avoid \"continuous rescheduling,\" the DaemonSet first executes the initialization procedure in an init container, and then leaves a container running.\n **Note:** In this tutorial, you deploy a \"one-shot\" initialization procedure. You might implement another procedure that continuously monitors the state of the node and acts accordingly to achieve a \"self-healing\" solution.\nThe following initialization procedure contains privileged and unprivileged operations. By using `chroot` , you can run commands as if you were executing them directly on the node, not just inside a container.\n [  cm-entrypoint.yaml ](https://github.com/GoogleCloudPlatform/solutions-gke-init-daemonsets-tutorial/blob/HEAD/cm-entrypoint.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/solutions-gke-init-daemonsets-tutorial/blob/HEAD/cm-entrypoint.yaml) \n```\n# Copyright 2019 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.---apiVersion: v1kind: ConfigMapmetadata:\u00a0 name: entrypoint\u00a0 labels:\u00a0 \u00a0 app: default-initdata:\u00a0 entrypoint.sh: |\u00a0 \u00a0 #!/usr/bin/env bash\u00a0 \u00a0 set -euo pipefail\u00a0 \u00a0 DEBIAN_FRONTEND=noninteractive\u00a0 \u00a0 ROOT_MOUNT_DIR=\"${ROOT_MOUNT_DIR:-/root}\"\u00a0 \u00a0 echo \"Installing dependencies\"\u00a0 \u00a0 apt-get update\u00a0 \u00a0 apt-get install -y apt-transport-https curl gnupg lsb-release\u00a0 \u00a0 echo \"Installing gcloud SDK\"\u00a0 \u00a0 export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\"\u00a0 \u00a0 echo \"deb https://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\u00a0 \u00a0 curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\u00a0 \u00a0 apt-get update\u00a0 \u00a0 apt-get install -y google-cloud-sdk\u00a0 \u00a0 echo \"Getting node metadata\"\u00a0 \u00a0 NODE_NAME=\"$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google')\"\u00a0 \u00a0 ZONE=\"$(curl -sS http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google' | awk -F \u00a0\"/\" '{print $4}')\"\u00a0 \u00a0 echo \"Setting up disks\"\u00a0 \u00a0 DISK_NAME=\"$NODE_NAME-additional\"\u00a0 \u00a0 if ! gcloud compute disks list --filter=\"name:$DISK_NAME\" | grep \"$DISK_NAME\" > /dev/null; then\u00a0 \u00a0 \u00a0 \u00a0 echo \"Creating $DISK_NAME\"\u00a0 \u00a0 \u00a0 \u00a0 gcloud compute disks create \"$DISK_NAME\" --size=1024 --zone=\"$ZONE\"\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 echo \"$DISK_NAME already exists\"\u00a0 \u00a0 fi\u00a0 \u00a0 if ! gcloud compute instances describe \"$NODE_NAME\" --zone \"$ZONE\" --format '(disks[].source)' | grep \"$DISK_NAME\" > /dev/null; then\u00a0 \u00a0 \u00a0 \u00a0 echo \"Attaching $DISK_NAME to $NODE_NAME\"\u00a0 \u00a0 \u00a0 \u00a0 gcloud compute instances attach-disk \"$NODE_NAME\" --device-name=sdb --disk \"$DISK_NAME\" --zone \"$ZONE\"\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 echo \"$DISK_NAME is already attached to $NODE_NAME\"\u00a0 \u00a0 fi\u00a0 \u00a0 # We use chroot to run the following commands in the host root (mounted as the /root volume in the container)\u00a0 \u00a0 echo \"Installing nano\"\u00a0 \u00a0 chroot \"${ROOT_MOUNT_DIR}\" apt-get update\u00a0 \u00a0 chroot \"${ROOT_MOUNT_DIR}\" apt-get install -y nano\u00a0 \u00a0 echo \"Loading Kernel modules\"\u00a0 \u00a0 # Load the bridge kernel module as an example\u00a0 \u00a0 chroot \"${ROOT_MOUNT_DIR}\" modprobe bridge...\n```\n **Note:** The commands you intend to run as part of the initialization procedure must be available in the containers that the DaemonSet runs. We recommend that you install them in the container or provide them by mounting the necessary volumes from the cluster node.\nWe recommend that you carefully review each initialization procedure, because the procedure could alter the state of the nodes of your cluster. Only a small group of individuals should have the right to modify those procedures, because those procedures can greatly affect the availability and the security of your clusters.\n **Caution:** In this tutorial, you deploy the ConfigMap and the DaemonSet in the default namespace. In a production environment, we recommend that you use a dedicated namespace to separate these initialization tasks from the rest of your workloads. Also, we recommend that you use [role-based access control](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) to help protect the resources in this dedicated namespace.\nTo deploy the ConfigMap and the DaemonSet, do the following:- In Cloud Shell, change the working directory to the `$HOME` directory:```\ncd \"$HOME\"\n```\n- Clone the Git repository that contains the scripts and the manifest files to deploy and configure the initialization procedure:```\ngit clone https://github.com/GoogleCloudPlatform/solutions-gke-init-daemonsets-tutorial\n```\n- Change the working directory to the newly cloned repository directory:```\ncd \"$HOME\"/solutions-gke-init-daemonsets-tutorial\n```\n- Create a ConfigMap to hold the node initialization script:```\nkubectl apply -f cm-entrypoint.yaml\n```\n- Deploy the DaemonSet:```\nkubectl apply -f daemon-set.yaml\n``` **Caution:** This DaemonSet deploys a privileged init container. For this reason, you should use a non-production cluster for this tutorial.\n- Verify that the node initialization is completed:```\nkubectl get ds --watch\n```Wait for the DaemonSet to be reported as ready and up to date, as indicated by output similar to the following:```\nNAME    DESIRED CURRENT READY  UP-TO-DATE AVAILABLE NODE SELECTOR AGE\nnode-initializer 3   3   3   3   3   <none> 2h\n```\n## Validate and verify the initialization procedureAfter each node of the cluster marked with the `default-init` label executes the initialization procedure, you can verify the results.\n **Note:** In this example, you verify the results manually. In a production environment, we recommend that you configure your monitoring system to continuously monitor your cluster to verify that the initialization procedure runs correctly on each newly added node, instead of relying on manual verification.\nFor each node, the verification procedure checks for the following:- An additional disk is attached and ready to be used.\n- The node's operating system package manager installed packages and libraries.\n- Kernel modules are loaded.\nExecute the verification procedure:- In Cloud Shell, run the verification script:```\nkubectl get nodes -o=jsonpath='{range .items[?(@.metadata.labels.app==\"default-init\")]}{.metadata.name}{\" \"}{.metadata.labels.failure-domain\\.beta\\.kubernetes\\.io/zone}{\"\\n\"}{end}' | while IFS= read -r line ; do ./verify-init.sh $line < /dev/null; done\n```Wait for the script to run and check that each node has been correctly initialized, as indicated by output like the following:```\nVerifying gke-ds-init-tutorial-default-pool-5464b7e3-nzjm (us-central1-c) configuration\nDisk configured successfully on gke-ds-init-tutorial-default-pool-5464b7e3-nzjm (us-central1-c)\nPackages installed successfully in gke-ds-init-tutorial-default-pool-5464b7e3-nzjm (us-central1-c)\nKernel modules loaded successfully on gke-ds-init-tutorial-default-pool-5464b7e3-nzjm (us-central1-c)\nVerifying gke-ds-init-tutorial-default-pool-65baf745-0gwt (us-central1-a) configuration\nDisk configured successfully on gke-ds-init-tutorial-default-pool-65baf745-0gwt (us-central1-a)\nPackages installed successfully in gke-ds-init-tutorial-default-pool-65baf745-0gwt (us-central1-a)\nKernel modules loaded successfully on gke-ds-init-tutorial-default-pool-65baf745-0gwt (us-central1-a)\nVerifying gke-ds-init-tutorial-default-pool-6b125c50-3xvl (us-central1-b) configuration\nDisk configured successfully on gke-ds-init-tutorial-default-pool-6b125c50-3xvl (us-central1-b)\nPackages installed successfully in gke-ds-init-tutorial-default-pool-6b125c50-3xvl (us-central1-b)\nKernel modules loaded successfully on gke-ds-init-tutorial-default-pool-6b125c50-3xvl (us-central1-b)\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this tutorial, you could delete the project you created for this tutorial. If you created a project dedicated to this tutorial, you can [delete it entirely](#delete-project) . If you used an existing project but don't want to delete it, use the following steps for [cleaning up the project](#clean-project) .\n### Clean up the projectTo clean up a project without deleting it, you need to remove the resources that you created in this tutorial.- In Cloud Shell, delete the GKE cluster:```\ngcloud container clusters delete ds-init-tutorial --quiet --region us-central1\n```\n- Delete the additional disks that you created as part of this example initialization procedure:```\ngcloud compute disks list --filter=\"name:additional\" --format=\"csv[no-heading](name,zone)\" | while IFS= read -r line ; do DISK_NAME=\"$(echo $line | cut -d',' -f1)\"; ZONE=\"$(echo $line | cut -d',' -f2)\"; gcloud compute disks delete \"$DISK_NAME\" --quiet --zone \"$ZONE\" < /dev/null; done\n```\n- Delete the service account:```\ngcloud iam service-accounts delete \"$GKE_SERVICE_ACCOUNT_EMAIL\" --quiet\n```\n- Delete the cloned repository directory:```\nrm -rf \"$HOME\"/solutions-gke-init-daemonsets-tutorial\n```\n### Delete the projectThe easiest way to eliminate billing is to delete the project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read about [GKE](/kubernetes-engine) .\n- Implement a [secure software supply chain](/software-supply-chain-security/docs/overview) .\n- Learn how you can [harden your GKE cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}