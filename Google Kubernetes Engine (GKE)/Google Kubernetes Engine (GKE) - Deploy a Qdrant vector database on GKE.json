{"title": "Google Kubernetes Engine (GKE) - Deploy a Qdrant vector database on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/deploy-qdrant", "abstract": "# Google Kubernetes Engine (GKE) - Deploy a Qdrant vector database on GKE\nThis guide shows you how to deploy a [Qdrant](https://qdrant.tech/documentation/) vector database cluster on Google Kubernetes Engine (GKE).\n [Vector databases](/discover/what-is-a-vector-database) are data stores specifically designed to manage and search through large collections of high-dimensional vectors. These vectors represent data like text, images, audio, video or any data that can be numerically encoded. Unlike traditional databases that rely on exact matches, vector databases specialize in finding similar items or identifying patterns within massive datasets. These characteristics make Qdrant a suitable choice for a variety of applications, including neural network or semantic-based matching, faceted search, and more. Qdrant not only functions as a vector database but also as a vector similarity search engine.\nThis tutorial is intended for cloud platform administrators, cloud architects, ML engineers, and MLOps (DevOps) professionals interested in deploying Qdrant database clusters on GKE.", "content": "## BenefitsQdrant offers the following benefits:- Wide range of libraries for various programming languages and open API to integrate with other services.\n- Horizontal scaling, and support for sharding and replication that simplifies scaling and high availability.\n- Container and Kubernetes support that enables deployment and management in modern cloud-native environments.\n- Flexible payloads with [advanced filtering](https://qdrant.tech/documentation/concepts/filtering/) to tailor search criteria precisely.\n- [Different quantization options](https://qdrant.tech/documentation/guides/quantization/) and other optimizations to reduce infrastructure costs and improve performance.\n## ObjectivesIn this tutorial, you learn how to:- Plan and deploy GKE infrastructure for Qdrant.\n- Deploy the [StatefulHA](/kubernetes-engine/docs/how-to/stateful-ha) operator to ensure Qdrant high availability.\n- Deploy and configure the Qdrant cluster.\n- Upload a demo dataset and run a simple search query.\n- Collect metrics and run a dashboard.\n## Deployment architectureIn this tutorial, you deploy a highly available regional GKE cluster for Qdrant, with multiple Kubernetes nodes spread across several availability zones. This setup helps ensure fault tolerance, scalability, and geographic redundancy. It allows for rolling updates and maintenance while providing SLAs for uptime and availability. For more information, see [Regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) .\nThis tutorial employs the regional persistent disk (RePD) storage type offered by Qdrant. RePD enables synchronous replication of your data across two zones within a region. By utilizing persisted data, systems can substantially reduce their Recovery Time Objective (RTO).\nWhen a node becomes unreachable, a Pod on that node is not rescheduled immediately. With Pods using a StatefulSet, it can take more than eight minutes for application Pods to be deleted and rescheduled to new nodes.\nTo address this issue, the StatefulHA operator does the following:- Handles failover settings and shortens recovery time by using`.forceDeleteStrategy`:`AfterNodeUnreachable`settings.\n- Ensures that the StatefulSet application is using RePD.\n- Extends GKE with a custom [HighAvailabilityApplication](/kubernetes-engine/docs/reference/crds/highavailabilityapplication) resource that should be deployed in the same namespace as Qdrant. This enables the StatefulHA operator to monitor and respond to failover events.\nQdrant stores data in [collections](https://qdrant.tech/documentation/concepts/collections/) that consist of [Shards](https://qdrant.tech/documentation/guides/distributed_deployment/#sharding) and [Replication factor](https://qdrant.tech/documentation/guides/distributed_deployment/#replication) . By default, Qdrant creates collections with a replication factor of one and a number of shards equal to the node count.\nThis guide walks you through creating a collection with a higher number of shards and a replication factor of two to enhance reliability and fault tolerance. For example, if you set the shard number to 6 and replication factor to 2, you get 12 physical shards distributed across Qdrant nodes in different zones.\n### Architecture diagramThe following diagram shows a Qdrant cluster running on multiple nodes and zones in a GKE cluster:In this architecture, the Qdrant `StatefulSet` is deployed across three nodes in three different zones.- You can control how GKE distributes Pods across nodes by configuring the required Pod [affinity rules](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and [topology spread constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/) in the Helm chart values file.\n- If one zone fails, GKE reschedules Pods on new nodes based on the recommended configuration.\nFor data persistence, the architecture in this tutorial has the following characteristics:- It uses [regional](/compute/docs/disks/about-regional-persistent-disk) SSD disks (custom`regional-pd` [StorageClass](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver#create_a_storageclass) ) for persisting data. We [recommend](/compute/docs/disks/performance) regional SSD disks for databases due to their low latency and high IOPS.\n- All disk data is replicated between primary and secondary zones in the region, increasing tolerance to potential zone failures.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/pricing#mgd-prometheus-pricing-summary) \n- [Backup for GKE](/backup-disaster-recovery/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you beginIn this tutorial, you use [Cloud Shell](/shell/docs) to run commands. Cloud Shell is a shell environment for managing resources hosted on Google Cloud. It comes preinstalled with the [Google Cloud CLI](/sdk/gcloud) , [kubectl](https://kubernetes.io/docs/reference/kubectl/) , [Helm](https://helm.sh/) and [ Terraform](/docs/terraform) command-line tools. If you don't use Cloud Shell, you must install the Google Cloud CLI.- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `role/storage.objectViewer, roles/container.admin, roles/iam.serviceAccountAdmin, roles/compute.admin, roles/gkebackup.admin, roles/monitoring.viewer` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.## Set up your environmentTo set up your environment with Cloud Shell, follow these steps:- Set environment variables for your project, region, and a Kubernetes cluster resource prefix:For the purpose of this tutorial, use `us-central1` region to create your deployment resources.```\nexport PROJECT_ID=PROJECT_IDexport KUBERNETES_CLUSTER_PREFIX=qdrantexport REGION=us-central1\n```- Replace``with your Google Cloud project ID.\n- Check the version of Helm:```\nhelm version\n```Update the version if it's older than 3.13:```\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n```\n- Clone the sample code repository from GitHub:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Navigate to the `qdrant` directory to start creating deployment resources:```\ncd kubernetes-engine-samples/databases/qdrant\n```\n## Create your cluster infrastructureThis section involves running a Terraform script to create a private, highly-available, regional GKE cluster to deploy your Qdrant database.\nYou can choose to deploy Qdrant using a [Standard or Autopilot cluster](/kubernetes-engine/docs/concepts/choose-cluster-mode) . Each has its own advantages and different pricing models. Standard clusters offer fully customized nodes with SSH access and Cloud TPU support, while Autopilot clusters are auto-provisioned.\nThe following diagram shows an Autopilot regional GKE cluster deployed across three different zones.To deploy the cluster infrastructure, run the following commands in the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply \\-var project_id=${PROJECT_ID} \\-var region=${REGION} \\-var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nThe following variables are replaced at runtime:- `GOOGLE_OAUTH_ACCESS_TOKEN`: Replaced by an access token retrieved by`gcloud auth print-access-token`command to authenticate interactions with various Google Cloud APIs\n- `PROJECT_ID`,`REGION`, and`KUBERNETES_CLUSTER_PREFIX`are the environment variables defined in [Set up your environment](#environment) section and assigned to the new relevant variables for the Autopilot cluster you are creating.\nWhen prompted, type `yes` .\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 9 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials qdrant-cluster --region us-central1\"\n```\nTerraform creates the following resources:- A custom VPC network and private subnet for the Kubernetes nodes.\n- A Cloud Router to access the internet through Network Address Translation (NAT).\n- A private GKE cluster in the`us-central1`region.\n- A`ServiceAccount`with logging and monitoring permissions for the cluster.\n- Google Cloud Google Cloud Managed Service for Prometheus configuration for cluster monitoring and alerting.\nThe following diagram shows a Standard private regional GKE cluster deployed across three different zones.To deploy the cluster infrastructure, run the following commands in the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply \\-var project_id=${PROJECT_ID} \\-var region=${REGION} \\-var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nThe following variables are replaced at runtime:- `GOOGLE_OAUTH_ACCESS_TOKEN`is replaced by an access token retrieved by`gcloud auth print-access-token`command to authenticate interactions with various Google Cloud APIs.\n- `PROJECT_ID`,`REGION`, and`KUBERNETES_CLUSTER_PREFIX`are the environment variables defined in [Set up your environment](#environment) section and assigned to the new relevant variables for the Standard cluster that you are creating.\nWhen prompted, type `yes` . It might take several minutes for these commands to complete and for the cluster to show a ready status.\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\nOutputs:\nkubectl_connection_command = \"gcloud container clusters get-credentials qdrant-cluster --region us-central1\"\n```\nTerraform creates the following resources:- A custom VPC network and private subnet for the Kubernetes nodes.\n- A Cloud Router to access the internet through Network Address Translation (NAT).\n- A private GKE cluster in the`us-central1`region with autoscaling enabled (one to two nodes per zone).\n- A`ServiceAccount`with logging and monitoring permissions for the cluster.\n- Google Cloud Google Cloud Managed Service for Prometheus configuration for cluster monitoring and alerting.### Connect to the clusterConfigure `kubectl` to fetch credentials and communicate with your new GKE cluster:\n```\ngcloud container clusters get-credentials \\\u00a0 \u00a0 ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the Qdrant database to your clusterIn this tutorial, you deploy the Qdrant database (in [distributed mode](https://qdrant.tech/documentation/guides/distributed_deployment/) ) and the Stateful HA operator to your GKE cluster cluster using the [Helm chart](https://github.com/qdrant/qdrant-helm) .\nThe deployment creates a GKE cluster with the following configuration:- Three replicas of the Qdrant nodes.\n- Tolerations, node affinities, and topology spread constraints are configured to ensure proper distribution across Kubernetes nodes. This leverages the node pools and different availability zones.\n- A RePD volume with the SSD disk type is provisioned for data storage.\n- A Stateful HA operator is used to manage failover processes and ensure high availability.\n- For authentication, the database creates a Kubernetes secret containing the API key.\nTo use the Helm chart to deploy Qdrant database, follow these steps:- Deploy Stateful HA operator with helm chart:```\nVERSION=0.1.5REPO=\"gke-release\"mkdir ha-operatorgsutil cp gs://$REPO/ha-controller/$VERSION/ha-controller-helm.tar.gz ./ha-operatortar xvf ./ha-operator/ha-controller-helm.tar.gz --directory=ha-operator/helm upgrade -i ha-operator ./ha-operator/helm-chart \\--namespace ha-operator \\--create-namespace \\--set image.repository=\"$REPO\" \\--set image.tag=\"$VERSION\" \\--set useWorkloadSeparation=false\n```The output is similar to the following:```\nRelease \"ha-operator\" does not exist. Installing it now.\nNAME: ha-operator\nLAST DEPLOYED: Thu Nov 30 15:34:18 2023\nNAMESPACE: ha-operator\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```\n- Add the Qdrant database Helm Chart repository before you can deploy it on your GKE cluster:```\nhelm repo add qdrant https://qdrant.github.io/qdrant-helm\n```\n- Create namespace `qdrant` for the database:```\nkubectl create ns qdrant\n```\n- Apply the manifest to create a regional persistent SSD disk `StorageClass` :```\nkubectl apply -n qdrant -f manifests/01-regional-pd/regional-pd.yaml\n```The `regional-pd.yaml` manifest describes the persistent SSD disk `StorageClass` : [  databases/qdrant/manifests/01-regional-pd/regional-pd.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/01-regional-pd/regional-pd.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/01-regional-pd/regional-pd.yaml) ```\napiVersion: storage.k8s.io/v1kind: StorageClassallowVolumeExpansion: truemetadata:\u00a0 name: ha-regionalparameters:\u00a0 replication-type: regional-pd\u00a0 type: pd-ssd\u00a0 availability-class: regional-hard-failoverprovisioner: pd.csi.storage.gke.ioreclaimPolicy: RetainvolumeBindingMode: WaitForFirstConsumer\n```\n- Deploy a Kubernetes configmap with a `metrics` sidecar configuration and a Qdrant cluster by using Helm:```\nkubectl apply -n qdrant -f manifests/03-prometheus-metrics/metrics-cm.yamlhelm install qdrant-database qdrant/qdrant -n qdrant \\-f manifests/02-values-file/values.yaml\n```The `metrics-cm.yaml` manifest describes the `metrics` sidecar `ConfigMap` : [  databases/qdrant/manifests/03-prometheus-metrics/metrics-cm.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/03-prometheus-metrics/metrics-cm.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/03-prometheus-metrics/metrics-cm.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: nginx-confdata:\u00a0 default.conf.template: |\u00a0 \u00a0 server {\u00a0 \u00a0 \u00a0 listen 80;\u00a0 \u00a0 \u00a0 location / {\u00a0 \u00a0 \u00a0 \u00a0 proxy_pass http://localhost:6333/metrics;\u00a0 \u00a0 \u00a0 \u00a0 proxy_http_version 1.1;\u00a0 \u00a0 \u00a0 \u00a0 proxy_set_header Host $http_host;\u00a0 \u00a0 \u00a0 \u00a0 proxy_set_header api-key ${QDRANT_APIKEY};\u00a0 \u00a0 \u00a0 \u00a0 proxy_set_header X-Forwarded-For $remote_addr;\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\n```The `values.yaml` manifest describes the Qdrant cluster configuration : [  databases/qdrant/manifests/02-values-file/values.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/02-values-file/values.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/02-values-file/values.yaml) ```\nreplicaCount: 3config:\u00a0 cluster:\u00a0 \u00a0 enabled: true\u00a0 storage:\u00a0 \u00a0 optimizers:\u00a0 \u00a0 \u00a0 deleted_threshold: 0.5\u00a0 \u00a0 \u00a0 vacuum_min_vector_number: 1500\u00a0 \u00a0 \u00a0 default_segment_number: 2\u00a0 \u00a0 \u00a0 max_segment_size_kb: null\u00a0 \u00a0 \u00a0 memmap_threshold_kb: null\u00a0 \u00a0 \u00a0 indexing_threshold_kb: 25000\u00a0 \u00a0 \u00a0 flush_interval_sec: 5\u00a0 \u00a0 \u00a0 max_optimization_threads: 1livenessProbe:\u00a0 enabled: true\u00a0 initialDelaySeconds: 60resources:\u00a0 limits:\u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 memory: 4Gi\u00a0 requests:\u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 memory: 4Gitolerations:\u00a0 - key: \"app.stateful/component\"\u00a0 \u00a0 operator: \"Equal\"\u00a0 \u00a0 value: \"qdrant\"\u00a0 \u00a0 effect: NoScheduleaffinity:\u00a0 nodeAffinity:\u00a0 \u00a0 preferredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 - weight: 1\u00a0 \u00a0 \u00a0 preference:\u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 - key: \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"qdrant\"topologySpreadConstraints:\u00a0 - maxSkew: 1\u00a0 \u00a0 topologyKey: \"topology.kubernetes.io/zone\"\u00a0 \u00a0 whenUnsatisfiable: ScheduleAnyway\u00a0 \u00a0 labelSelector:\u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 app.kubernetes.io/name: qdrant\u00a0 \u00a0 \u00a0 \u00a0 app.kubernetes.io/instance: qdrantpodDisruptionBudget:\u00a0 enabled: true\u00a0 maxUnavailable: 1persistence:\u00a0 accessModes: [\"ReadWriteOnce\"]\u00a0 size: 10Gi\u00a0 storageClassName: ha-regionalapiKey: truesidecarContainers:\u00a0 - name: metrics\u00a0 \u00a0 image: nginx:1.25\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 memory: \"128Mi\"\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"250m\"\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 memory: \"128Mi\"\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 ports:\u00a0 \u00a0 - containerPort: 80\u00a0 \u00a0 env:\u00a0 \u00a0 - name: QDRANT_APIKEY \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: qdrant-database-apikey \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: api-key\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: nginx-conf\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /etc/nginx/templates/default.conf.template\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subPath: default.conf.template\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: trueadditionalVolumes:\u00a0 - name: nginx-conf\u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 name: nginx-conf\u00a0 \u00a0 \u00a0 items:\u00a0 \u00a0 \u00a0 \u00a0 - key: default.conf.template\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: default.conf.template \n```This configuration enables the cluster mode, allowing you to setup a highly available and distributed Qdrant cluster.\n- Check the deployment status:```\nhelm ls -n qdrant\n```The output is similar to the following, if the `qdrant` database is successfully deployed:```\nNAME NAMESPACE  REVISION  UPDATED         STATUS   CHART   APP VERSION\nqdrant-database qdrant   1    2024-02-06 20:21:15.737307567 +0000 UTC deployed  qdrant-0.7.6 v1.7.4\n```\n- Wait for GKE to start the required workloads:```\nkubectl wait pods -l app.kubernetes.io/instance=qdrant-database --for condition=Ready --timeout=300s -n qdrant\n```This command might take a few minutes to complete successfully.\n- Once GKE starts the workloads, verify that GKE has created the Qdrant workloads:```\nkubectl get pod,svc,statefulset,pdb,secret -n qdrant\n```\n- Start the `HighAvailabilityApplication` (HAA) resource for Qdrant:```\nkubectl apply -n qdrant -f manifests/01-regional-pd/ha-app.yaml\n```The `ha-app.yaml` manifest describes the `HighAvailabilityApplication` resource: [  databases/qdrant/manifests/01-regional-pd/ha-app.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/01-regional-pd/ha-app.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/01-regional-pd/ha-app.yaml) ```\nkind: HighAvailabilityApplicationapiVersion: ha.gke.io/v1metadata:\u00a0 name: qdrant-database\u00a0 namespace: qdrantspec:\u00a0 resourceSelection:\u00a0 \u00a0 resourceKind: StatefulSet\u00a0 policy:\u00a0 \u00a0 storageSettings:\u00a0 \u00a0 \u00a0 requireRegionalStorage: true\u00a0 \u00a0 failoverSettings:\u00a0 \u00a0 \u00a0 forceDeleteStrategy: AfterNodeUnreachable\u00a0 \u00a0 \u00a0 afterNodeUnreachable:\u00a0 \u00a0 \u00a0 \u00a0 afterNodeUnreachableSeconds: 20 # 60 seconds total\n```The following GKE resources are created for the Qdrant cluster:- The Qdrant`StatefulSet`that controls three Pod replicas.\n- `A PodDisruptionBudget`, ensuring a maximum of one unavailable replica.\n- The`qdrant-database`Service, exposing the Qdrant port for inbound connections and replication between nodes.\n- The`qdrant-database-headless`Service, providing the list of running Qdrant Pods.\n- The`qdrant-database-apikey`Secret, facilitating secure database connection.\n- Stateful HA operator Pod and`HighlyAvailableApplication`resource, actively monitoring the Qdrant application. The`HighlyAvailableApplication`resource defines failover rules to apply against Qdrant.\n- To check if the failover rules are applied, describe the resource and confirm `Status: Message: Application is protected` .```\nkubectl describe highavailabilityapplication qdrant-ha-app -n qdrant\n```The output is similar to the following:```\nStatus:\nConditions:\n Last Transition Time: 2023-11-30T09:54:52Z\n Message:    Application is protected\n Observed Generation: 1\n Reason:    ApplicationProtected\n Status:    True\n Type:     Protected\n```\n## Upload the dataset and run search queriesQdrant organizes vectors and payloads in collections. Vector embedding is a technique that represents words or entities as numerical vectors while maintaining their semantic relationships. This is important for similarity searches as it enables finding similarities based on meaning rather than exact matches, making tasks like search and recommendation systems more effective and nuanced.\nThis section shows you how to [upload Vectors](https://qdrant.tech/articles/fastembed/) into a new Qdrant [Collection](https://qdrant.tech/documentation/concepts/collections/) and run simple search queries using the official [Qdrant client](https://github.com/qdrant/qdrant-client) .\nIn this example, you use a dataset from a CSV file that contains a list of books in different genres. Qdrant will serve as a search engine, and the Pod you create will serve as a client querying the Qdrant database.- Create the `books-dataset` and `demo-app` ConfigMaps and run the client Pod to interact with your Qdrant cluster:```\nkubectl create -n qdrant configmap books-dataset --from-file=manifests/04-qdrant-client/dataset.csvkubectl create -n qdrant configmap demo-app --from-file=manifests/04-qdrant-client/app.pykubectl apply -n qdrant -f manifests/04-qdrant-client/client-pod.yaml\n```- The Secret named`qdrant-apikey`created earlier is mounted to the client Pod as an environment variable named`APIKEY`.\n- The`books-dataset`ConfigMap contains a`csv`file with book data for the Qdrant collection\n- The`demo-app`ConfigMap contains Python code to create the Qdrant collection from`books-dataset`.\nThe `client-pod.yaml` manifest describes the `qdrant-client` Pod: [  databases/qdrant/manifests/04-qdrant-client/client-pod.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/client-pod.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/client-pod.yaml) ```\napiVersion: v1kind: Podmetadata:\u00a0 name: qdrant-client\u00a0 annotations:\u00a0 \u00a0 cluster-autoscaler.kubernetes.io/safe-to-evict: \"true\"spec:\u00a0 restartPolicy: Never\u00a0 containers:\u00a0 - name: qdrant-client\u00a0 \u00a0 image: python:3.11-slim-bookworm\u00a0 \u00a0 command: [\"sleep\", \"3600\"]\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 memory: \"3500Mi\"\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 memory: \"3500Mi\"\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 env:\u00a0 \u00a0 - name: APIKEY\u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: qdrant-database-apikey\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: api-key\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 - name: books-dataset\u00a0 \u00a0 \u00a0 mountPath: /usr/local/dataset\u00a0 \u00a0 - name: script-volume\u00a0 \u00a0 \u00a0 mountPath: /usr/local/script\u00a0 volumes:\u00a0 - name: books-dataset\u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 name: books-dataset\u00a0 - name: script-volume\u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 name: demo-app\n```\n- Wait for GKE to start the client Pod:```\nkubectl wait pod qdrant-client --for=condition=Ready --timeout=300s -n qdrant\n```\n- Connect to the client Pod when it's ready:```\nkubectl exec -it qdrant-client -n qdrant -- /bin/bash\n```\n- Create a virtual environment:```\npython -m venv venvsource venv/bin/activate\n```\n- Install a Qdrant client with [fastembed](https://github.com/qdrant/fastembed) support to eliminate reliance on external model deployment services:```\npip install qdrant-client[fastembed]\n```\n- Run the `app.py` Python script to upload and query the dataset:```\npython usr/local/script/app.py \"drama about people and unhappy love\"\n```The `app.py` script does the following:- Import the required Python and Qdrant libraries:\n [  databases/qdrant/manifests/04-qdrant-client/app.py ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) ```\nfrom qdrant_client import QdrantClientfrom qdrant_client.http import modelsimport osimport sysimport csv\n```- Load data from a CSV: It reads the`dataset.csv`file for inserting data into a Qdrant collection.\n [  databases/qdrant/manifests/04-qdrant-client/app.py ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) ```\nbooks = [*csv.DictReader(open('/usr/local/dataset/dataset.csv'))]\n```- Create a Qdrant collection and insert data: It establishes a connection to Qdrant, creates a new collection named`my_books`, and uploads the book data to`my_books`.\n [  databases/qdrant/manifests/04-qdrant-client/app.py ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) ```\n# Add my_books to the collection qdrant.add(collection_name=\"my_books\", documents=documents, metadata=metadata, ids=ids, parallel=2)\n```- Query the Qdrant database: It runs a search query about`drama about people and unhappy love`and displays results.\n [  databases/qdrant/manifests/04-qdrant-client/app.py ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/04-qdrant-client/app.py) ```\nresults = qdrant.query(\u00a0 \u00a0 collection_name=\"my_books\",\u00a0 \u00a0 query_text=query_string,\u00a0 \u00a0 limit=2,)for result in results:\u00a0 \u00a0 print(\"Title:\", result.metadata[\"title\"], \"\\nAuthor:\", result.metadata[\"author\"])\u00a0 \u00a0 print(\"Description:\", result.metadata[\"document\"], \"Published:\", result.metadata[\"publishDate\"], \"\\nScore:\", result.score)\u00a0 \u00a0 print(\"-----\")\n```This query performs a semantic search against your `my_books` collection in Qdrant, retrieving a maximum of two results with highest match score relevant to your query text.It prints each result separated by a line of dashes, in the following format :- `Title`: Title of the book\n- `Author`: Author of the book\n- `Description`: As stored in your document's`description`metadata field\n- `Published`: Book publication date\n- `Score`: Qdrant's relevancy score\nThe output is similar to the following:```\nTitle: Romeo and Juliet\nAuthor: William Shakespeare, Paul Werstine (Editor), Barbara A. Mowat (Editor), Paavo Emil Cajander (Translator)\nDescription: In Romeo and Juliet, Shakespeare creates a violent world, in which two young people fall in love. It is not simply that their families disapprove; the Montagues and the Capulets are engaged in a blood feud.In this death-filled setting, the movement from love at first sight to the lovers' final union in death seems almost inevitable. And yet, this play set in an extraordinary world has become the quintessential story of young love. In part because of its exquisite language, it is easy to respond as if it were about all young lovers. Published: 01/01/04\nScore: 0.8935013\n----Title: The Unbearable Lightness of Being\nAuthor: Milan Kundera, Michael Henry Heim (Translator)\nDescription: In The Unbearable Lightness of Being, Milan Kundera tells the story of a young woman in love with a man torn between his love for her and his incorrigible womanizing and one of his mistresses and her humbly faithful lover. This magnificent novel juxtaposes geographically distant places, brilliant and playful reflections, and a variety of styles, to take its place as perhaps the major achievement of one of the world's truly great writers. Published: 10/27/09\nScore: 0.8931863\n----\n```\n- Exit the Python script:```\nexit()\n```\n- Exit the client:```\nexit\n```\n## View Prometheus metrics for your clusterThe GKE cluster is configured with [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) , which enables collection of metrics in the Prometheus format. This service provides a fully managed solution for monitoring and alerting, allowing for collection, storage, and analysis of metrics from the cluster and its applications.\nThe following diagram shows how Prometheus collects metrics for your cluster:The GKE private cluster in the diagram contains the following components:- Qdrant Pods that expose metrics on the path`/`and port`80`. These metrics are provided by the sidecar container named`metrics`.\n- Prometheus-based collectors that process the metrics from the Qdrant Pods.\n- A PodMonitoring resource that sends the metrics to Cloud Monitoring.\nTo export and view the metrics, follow these steps:- Create the [PodMonitoring](/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring) resource to scrape metrics by `labelSelector` :```\nkubectl apply -n qdrant -f manifests/03-prometheus-metrics/pod-monitoring.yaml\n```The `pod-monitoring.yaml` manifest describes the `PodMonitoring` resource: [  databases/qdrant/manifests/03-prometheus-metrics/pod-monitoring.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/03-prometheus-metrics/pod-monitoring.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/qdrant/manifests/03-prometheus-metrics/pod-monitoring.yaml) ```\napiVersion: monitoring.googleapis.com/v1kind: PodMonitoringmetadata:\u00a0 name: qdrantspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: qdrant\u00a0 \u00a0 \u00a0 app.kubernetes.io/instance: qdrant-database\u00a0 endpoints:\u00a0 - port: 80\u00a0 \u00a0 interval: 30s\u00a0 \u00a0 path: / \n```\n- [Create a Cloud Monitoring dashboard](/sdk/gcloud/reference/monitoring/dashboards/create) with the configurations defined in `dashboard.json` :```\ngcloud --project \"${PROJECT_ID}\" monitoring dashboards create --config-from-file monitoring/dashboard.json\n```\n- After the command runs successfully, go to the Cloud Monitoring [Dashboards](/monitoring/dashboards) : [Go to Dashboards overview](https://console.cloud.google.com/monitoring/dashboards) \n- From the list of dashboards, open the `Qdrant Overview` dashboard. It might take 1-2 minutes to collect and display metrics.The dashboard shows a count of key metrics:- Collections\n- Embedded vectors\n- Pending operations\n- Running nodes## Back up your cluster configurationThe [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) feature lets you to schedule regular backups of your entire GKE cluster configuration, including the deployed workloads and their data.\nIn this tutorial, you configure a backup plan for your GKE cluster to perform backups of all workloads, including Secrets and Volumes, every day at 3 AM. To ensure efficient storage management, backups older than three days would be automatically deleted.\nTo configure [Backup plans](/kubernetes-engine/docs/add-on/backup-for-gke/how-to/backup-plan#gcloud) , follow these steps:- Enable the Backup for GKE feature for your cluster:```\ngcloud container clusters update ${KUBERNETES_CLUSTER_PREFIX}-cluster \\--project=${PROJECT_ID} \\--region=${REGION} \\--update-addons=BackupRestore=ENABLED\n```\n- Create a backup plan with a daily schedule for all namespaces within the cluster:```\ngcloud beta container backup-restore backup-plans create ${KUBERNETES_CLUSTER_PREFIX}-cluster-backup \\--project=${PROJECT_ID} \\--location=${REGION} \\--cluster=\"projects/${PROJECT_ID}/locations/${REGION}/clusters/${KUBERNETES_CLUSTER_PREFIX}-cluster\" \\--all-namespaces \\--include-secrets \\--include-volume-data \\--cron-schedule=\"0 3 * * *\" \\--backup-retain-days=3\n```The command uses the relevant environment variables at runtime.The cluster name's format is relative to your project and region as follows:```\nprojects/PROJECT_ID/locations/REGION/clusters/CLUSTER_NAME\n```When prompted, type `y.` The output is similar to the following:```\nCreate request issued for: [qdrant-cluster-backup]\nWaiting for operation [projects/PROJECT_ID/locations/us-central1/operations/operation-1706528750815-610142ffdc9ac-71be4a05-f61c99fc] to complete...\u2839\n```This operation might take a few minutes to complete successfully. After the execution is complete, the output is similar to the following:```\nCreated backup plan [qdrant-cluster-backup].\n```\n- You can see your newly created backup plan `qdrant-cluster-backup` listed on the Backup for GKE console. [Go to Backup for GKE](https://console.cloud.google.com/kubernetes/backups/backupPlans) \nIf you want to restore the saved backup configurations, see [Restore a backup](/kubernetes-engine/docs/add-on/backup-for-gke/how-to/restore) .## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to avoid billing is to delete the project you created for this tutorial.\n **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\nDelete a Google Cloud project:\n```\ngcloud projects delete PROJECT_ID\n```\nIf you deleted the project, your clean up is complete. If you didn't delete the project, proceed to delete the individual resources.\n### Delete individual resources\n- Set environment variables.```\nexport PROJECT_ID=${PROJECT_ID}export KUBERNETES_CLUSTER_PREFIX=qdrantexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform \u00a0-chdir=terraform/FOLDER destroy \\-var project_id=${PROJECT_ID} \\-var region=${REGION} \\-var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` , depending on the [type of GKE cluster you created](#create-cluster) .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,region)\")\n```\n- Delete the disks:```\nfor i in $disk_list; do\u00a0disk_name=$(echo $i| cut -d'|' -f1)\u00a0disk_region=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0echo \"Deleting $disk_name\"\u00a0gcloud compute disks delete $disk_name --region $disk_region --quietdone\n```\n- Delete the GitHub repository:```\nrm -r ~/kubernetes-engine-samples/\n```\n## What's next\n- Explore [Qdrant open source](https://github.com/qdrant/qdrant) software.\n- Try out the [Qdrant operator](https://github.com/ganochenkodg/qdrant-operator) that offers [API keys management](https://github.com/ganochenkodg/qdrant-operator/blob/main/docs/authentication.md) , TLS support with [certificate management](https://github.com/ganochenkodg/qdrant-operator/blob/main/docs/tls.md) , and [backup scheduling](https://github.com/ganochenkodg/qdrant-operator/blob/main/docs/backup-restore.md) .\n- Learn about the [best practices for deploying databases on GKE](/kubernetes-engine/docs/concepts/database-options) .\n- Discover solutions for running [data-intensive workloads with GKE](/kubernetes-engine/docs/integrations/data) .", "guide": "Google Kubernetes Engine (GKE)"}