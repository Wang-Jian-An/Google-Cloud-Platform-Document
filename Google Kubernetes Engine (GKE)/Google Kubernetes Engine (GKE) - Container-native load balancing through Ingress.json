{"title": "Google Kubernetes Engine (GKE) - Container-native load balancing through Ingress", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing", "abstract": "# Google Kubernetes Engine (GKE) - Container-native load balancing through Ingress\nThis page explains how to use container-native load balancing in Google Kubernetes Engine (GKE). Container-native load balancing allows load balancers to target Kubernetes Pods directly and to evenly distribute traffic to Pods.\nFor more information on the benefits, requirements, and limitations of container-native load balancing, see [Container-native load balancing](/kubernetes-engine/docs/concepts/container-native-load-balancing) .\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Use container-native load balancing\nThe following sections walk you through a container-native load balancing configuration on GKE. Note that for GKE clusters running version 1.17 or later and [under certain conditions](/kubernetes-engine/docs/concepts/ingress#container-native_load_balancing) , container-native load balancing is default and does not require an explicit `cloud.google.com/neg: '{\"ingress\": true}'` Service annotation.\n### Create a VPC-native cluster\nTo use container-native load balancing, your GKE cluster must have [alias IPs](/kubernetes-engine/docs/how-to/alias-ips#procedures) enabled.\nFor example, the following command creates a GKE cluster, `neg-demo-cluster` , with an auto-provisioned subnetwork:\n- For Autopilot mode, alias IP addresses are enabled by default:```\ngcloud container clusters create-auto neg-demo-cluster \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace `` with the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- For Standard mode, enable alias IP addresses when you create the cluster:```\ngcloud container clusters create neg-demo-cluster \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --create-subnetwork=\"\" \\\u00a0 \u00a0 --network=default \\\u00a0 \u00a0 --zone=us-central1-a\n```\n### Create a Deployment\nThe following sample [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) , `neg-demo-app` , runs a single instance of a containerized HTTP server. We recommend you use workloads that use [Pod readiness](/kubernetes-engine/docs/concepts/container-native-load-balancing#pod_readiness) feedback.\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 labels:\u00a0 \u00a0 run: neg-demo-app # Label for the Deployment\u00a0 name: neg-demo-app # Name of Deploymentspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 run: neg-demo-app\u00a0 template: # Pod template\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 run: neg-demo-app # Labels Pods from this Deployment\u00a0 \u00a0 spec: # Pod specification; each Pod created by this Deployment has this specification\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - image: registry.k8s.io/serve_hostname:v1.4 # Application to run in Deployment's Pods\u00a0 \u00a0 \u00a0 \u00a0 name: hostname # Container name\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 9376\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \n``````\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 labels:\u00a0 \u00a0 run: neg-demo-app # Label for the Deployment\u00a0 name: neg-demo-app # Name of Deploymentspec:\u00a0 minReadySeconds: 60 # Number of seconds to wait after a Pod is created and its status is Ready\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 run: neg-demo-app\u00a0 template: # Pod template\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 run: neg-demo-app # Labels Pods from this Deployment\u00a0 \u00a0 spec: # Pod specification; each Pod created by this Deployment has this specification\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - image: registry.k8s.io/serve_hostname:v1.4 # Application to run in Deployment's Pods\u00a0 \u00a0 \u00a0 \u00a0 name: hostname # Container name\u00a0 \u00a0 \u00a0 # Note: The following line is necessary only on clusters running GKE v1.11 and lower.\u00a0 \u00a0 \u00a0 # For details, see https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#align_rollouts\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 9376\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 terminationGracePeriodSeconds: 60 # Number of seconds to wait for connections to terminate before shutting down Pods\u00a0 \n```\nIn this Deployment, each container runs an HTTP server. The HTTP server returns the hostname of the application server (the name of the Pod on which the server runs) as a response.\nSave this manifest as `neg-demo-app.yaml` , then create the Deployment:\n```\nkubectl apply -f neg-demo-app.yaml\n```\n### Create a Service for a container-native load balancer\nAfter you have created a Deployment, you need to group its Pods into a [Service](https://kubernetes.io/docs/concepts/services-networking/service/) .\nThe following sample Service, `neg-demo-svc` , targets the sample Deployment that you created in the previous section:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: neg-demo-svc # Name of Service\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"ingress\": true}' # Creates a NEG after an Ingress is createdspec: # Service's specification\u00a0 type: ClusterIP\u00a0 selector:\u00a0 \u00a0 run: neg-demo-app # Selects Pods labelled run: neg-demo-app\u00a0 ports:\u00a0 - name: http\u00a0 \u00a0 port: 80 # Service's port\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 9376\n```\n**Note:** The Service is exposed on port 80, and it redirects traffic to port 9376, the targetPort. The targetPort corresponds to a containerPort in a serving Pod. It's also possible to use a Service of type NodePort with container-native load balancing. That is, GKE will still create NEG(s) as long as the `cloud.google.com/neg` annotation is present. A Service of type ClusterIP is recommended unless you explicitly need the nodePort provided by a NodePort Service. LoadBalancer Services are not supported as Ingress backends.\nThe Service's annotation, `cloud.google.com/neg: '{\"ingress\": true}'` , enables container-native load balancing. However, the load balancer is not created until you [create an Ingress](#create_ingress) for the Service.\nSave this manifest as `neg-demo-svc.yaml` , then create the Service:\n```\nkubectl apply -f neg-demo-svc.yaml\n```\n### Create an Ingress for the Service\nThe following sample [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) , `neg-demo-ing` , targets the Service that you created:\n```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: neg-demo-ingspec:\u00a0 defaultBackend:\u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 name: neg-demo-svc # Name of the Service targeted by the Ingress\u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 number: 80 # Should match the port used by the Service\n```\nSave this manifest as `neg-demo-ing.yaml` , then create the Ingress:\n```\nkubectl apply -f neg-demo-ing.yaml\n```\nUpon creating the Ingress, an Application Load Balancer is created in the project, and NEGs are created in each zone in which the cluster runs. The endpoints in the NEG and the endpoints of the Service are kept in sync.\n## Verify the Ingress\nAfter you have deployed a workload, grouped its Pods into a Service, and created an Ingress for the Service, you should verify that the Ingress has provisioned the container-native load balancer successfully.\nRetrieve the status of the Ingress:\n```\nkubectl describe ingress neg-demo-ing\n```\nThe output includes `ADD` and `CREATE` events:\n```\nEvents:\nType  Reason Age    From      Message\n----  ------ ----    ----      ------Normal ADD  16m    loadbalancer-controller default/neg-demo-ing\nNormal Service 4s     loadbalancer-controller default backend set to neg-demo-svc:32524\nNormal CREATE 2s     loadbalancer-controller ip: 192.0.2.0\n```\n## Test the load balancer\nThe following sections explain how you can test the functionality of a container-native load balancer.\n### Visit Ingress IP address\nWait several minutes for the Application Load Balancer to be configured.\nYou can verify that the container-native load balancer is functioning by visiting the Ingress' IP address.\nTo get the Ingress IP address, run the following command:\n```\nkubectl get ingress neg-demo-ing\n```\nIn the command output, the Ingress' IP address is displayed in the `ADDRESS` column. Visit the IP address in a web browser.\n### Check backend service health status\nYou can also get the health status of the load balancer's [backend service](/load-balancing/docs/backend-service) .\n- Get a list of the backend services running in your project:```\ngcloud compute backend-services list\n```Record the name of the backend service that includes the name of the Service, such as `neg-demo-svc` .\n- Get the health status of the backend service:```\ngcloud compute backend-services get-health BACKEND_SERVICE_NAME --global\n```Replace `` with the name of the backend service.\n### Test the Ingress\nAnother way you can test that the load balancer functions as expected is by scaling the sample Deployment, sending test requests to the Ingress, and verifying that the correct number of replicas respond.\n- Scale the `neg-demo-app` Deployment from one instance to two instances:```\nkubectl scale deployment neg-demo-app --replicas 2\n```This command might take several minutes to complete.\n- Verify that the rollout is complete:```\nkubectl get deployment neg-demo-app\n```The output should include two available replicas:```\nNAME   DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\nneg-demo-app 2   2   2   2   26m\n```\n- Get the Ingress IP address:```\nkubectl describe ingress neg-demo-ing\n```If this command returns a 404 error, wait a few minutes for the load balancer to start, then try again.\n- Count the number of distinct responses from the load balancer:```\nfor i in `seq 1 100`; do \\\u00a0 curl --connect-timeout 1 -s IP_ADDRESS && echo; \\done \u00a0| sort | uniq -c\n```Replace `` with the Ingress IP address.The output is similar to the following:```\n44 neg-demo-app-7f7dfd7bc6-dcn95\n56 neg-demo-app-7f7dfd7bc6-jrmzf\n```In this output, the number of distinct responses is the same as the number of replicas, which indicates that all backend Pods are serving traffic.## Clean up\nAfter completing the tasks on this page, follow these steps to remove the resources to prevent unwanted charges incurring on your account:\n### Delete the cluster\n```\ngcloud container clusters delete neg-demo-cluster\n```- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Select **neg-demo-cluster** and click **Delete** .\n- When prompted to confirm, click **Delete** .## Troubleshooting\nUse the techniques below to verify your networking configuration. The following sections explain how to resolve specific issues related to container-native load balancing.\n- See the [load balancing documentation](/load-balancing/docs/negs/setting-up-zonal-negs#listing-negs) for how to list your network endpoint groups.\n- You can find the name and zones of the NEG that corresponds to a service in the `neg-status` annotation of the service. Get the Service specification with:```\nkubectl get svc SVC_NAME -o yaml\n```The `metadata:annotations:cloud.google.com/neg-status` annotation lists the name of service's corresponding NEG and the zones of the NEG.\n- You can check the health of the backend service that corresponds to a NEG with the following command:```\ngcloud compute backend-services --project PROJECT_NAME \\\u00a0 \u00a0 get-health BACKEND_SERVICE_NAME --global\n```The backend service has the same name as its NEG.\n- To print a service's event logs:```\nkubectl describe svc SERVICE_NAME\n```The service's name string includes the name and namespace of the corresponding GKE Service.\n### Cannot create a cluster with alias IPs### Traffic does not reach endpoints### Stalled rollout\n## Known issues\nContainer-native load balancing on GKE has the following known issues:\n### Incomplete garbage collection\nGKE garbage collects container-native load balancers every two minutes. If a cluster is deleted before load balancers are fully removed, you need to manually delete the load balancer's NEGs.\nView the NEGs in your project by running the following command:\n```\ngcloud compute network-endpoint-groups list\n```\nIn the command output, look for the relevant NEGs.\nTo delete a NEG, run the following command, replacing `` with the name of the NEG:\n```\ngcloud compute network-endpoint-groups delete NEG_NAME\n```\n### Align workload rollouts with endpoint propagation\n**Note:** This issue does not occur in clusters that use Pod readiness feedback to manage workload rollouts. See [Pod readiness](/kubernetes-engine/docs/concepts/container-native-load-balancing#pod_readiness) for more information.\nWhen you deploy a workload to your cluster, or when you update an existing workload, the container-native load balancer can take longer to propagate new endpoints than it takes to finish the workload rollout. The sample Deployment that you deploy in this guide uses two fields to align its rollout with the propagation of endpoints: `terminationGracePeriodSeconds` and `minReadySeconds` .\n[terminationGracePeriodSeconds](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) allows the Pod to shut down gracefully by waiting for connections to terminate after a Pod is scheduled for deletion.\n[minReadySeconds](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#min-ready-seconds) adds a latency period after a Pod is created. You specify a minimum number of seconds for which a new Pod should be in [Ready status](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions) , without any of its containers crashing, for the Pod to be considered available.\nYou should configure your workloads' `minReadySeconds` and `terminationGracePeriodSeconds` values to be 60 seconds or higher to ensure that the service is not disrupted due to workload rollouts.\n`terminationGracePeriodSeconds` is available in all Pod specifications, and `minReadySeconds` is available for Deployments and DaemonSets.\nTo learn more about fine-tuning rollouts, refer to [RollingUpdateStrategy](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy) .\n### initialDelaySeconds in Pod readinessProbe not respected\nYou might expect the `initialDelaySeconds` configuration in the Pod's [readinessProbe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes) to be respected by the container-native load balancer; however, `readinessProbe` is implemented by kubelet, and the `initialDelaySeconds` configuration controls the kubelet health check, not the container-native load balancer. Container-native load balancing has its own load balancing health check.\n## What's next\n- Learn more about [NEGs](/load-balancing/docs/negs) .\n- Learn more about [VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) .", "guide": "Google Kubernetes Engine (GKE)"}