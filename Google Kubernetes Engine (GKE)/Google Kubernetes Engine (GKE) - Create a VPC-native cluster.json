{"title": "Google Kubernetes Engine (GKE) - Create a VPC-native cluster", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips", "abstract": "# Google Kubernetes Engine (GKE) - Create a VPC-native cluster\nThis page explains how to configure VPC-native [clusters](/kubernetes-engine/docs/concepts/cluster-architecture) in Google Kubernetes Engine (GKE).\nTo learn more about the benefits and requirements of VPC-native clusters, see the overview for [VPC-native clusters](/kubernetes-engine/docs/concepts/alias-ips) .\n**Note:** For GKE Autopilot clusters, VPC-native networks are enabled by default and cannot be overridden.\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Limitations\n- You cannot convert a VPC-native cluster into a routes-based cluster, and you cannot convert a routes-based cluster into a VPC-native cluster.\n- VPC-native clusters require VPC networks. [Legacy networks](/vpc/docs/legacy) are not supported.\n- As with any GKE cluster, [Service](https://kubernetes.io/docs/concepts/services-networking/service/) (ClusterIP) addresses are only available from within the cluster. If you need to access a Kubernetes Service from VM instances outside of the cluster, but within the cluster's VPC network and region, create an [internal passthrough Network Load Balancer](/load-balancing/docs/internal) .\n- If you use all of the Pod IP addresses in a subnet, you cannot replace the subnet's secondary IP address range without putting the cluster into an unstable state. However, you can create additional Pod IP address ranges using [discontiguous multi-Pod CIDR](/kubernetes-engine/docs/how-to/multi-pod-cidr) .## Create a cluster in an existing subnet\nThe following instructions demonstrate how to create a VPC-native GKE cluster in an existing subnet with your choice of [secondary range assignment method](/kubernetes-engine/docs/concepts/alias-ips#range_management) .\n- To use a [secondary range assignment method](/kubernetes-engine/docs/concepts/alias-ips#range_management) of :```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --subnetwork=SUBNET_NAME \\\u00a0 \u00a0 --cluster-ipv4-cidr=POD_IP_RANGE \\\u00a0 \u00a0 --services-ipv4-cidr=SERVICES_IP_RANGE\n```\n- To use a [secondary range assignment method](/kubernetes-engine/docs/concepts/alias-ips#range_management) of :```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --subnetwork=SUBNET_NAME \\\u00a0 \u00a0 --cluster-secondary-range-name=SECONDARY_RANGE_PODS \\\u00a0 \u00a0 --services-secondary-range-name=SECONDARY_RANGE_SERVICES\n```\nReplace the following:- ``: the name of the GKE cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of an existing subnet. The subnet's primary IP address range is used for nodes. The subnet must exist in the same region as the one used by the cluster. If omitted, GKE attempts to use a subnet in the`default`VPC network in the cluster's region.\n- If the secondary range assignment method is:- ``: an IP address range in CIDR notation, such as`10.0.0.0/14`, or the size of a CIDR block's subnet mask, such as`/14`. This is used to create the subnet's secondary IP address range for Pods. If you omit the`--cluster-ipv4-cidr`option, GKE chooses a`/14`range (2addresses) automatically. The automatically chosen range is randomly selected from`10.0.0.0/8`(a range of 2addresses) and won't include IP address ranges allocated to VMs, existing [routes](/vpc/docs/routes) , or ranges allocated to other clusters. The automatically chosen range might conflict with [reserved IPaddresses](/compute/docs/ip-addresses/reserve-static-external-ip-address) , [dynamic routes](/vpc/docs/routes#dynamic_routes) , or routes within VPCs that peer with this cluster. If you use these any of these, you should specify`--cluster-ipv4-cidr`to prevent conflicts.\n- ``: an IP address range in CIDR notation (for example,`10.4.0.0/19`) or the size of a CIDR block's subnet mask (for example,`/19`). This is used to create the subnet's secondary IP address range for Services.\n- If the secondary range assignment method is:- ``: the name of an existing secondary IP address range in the specified``. GKE uses the entire subnet secondary IP address range for the cluster's Pods.\n- ``: the name of an existing secondary IP address range in the specified- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click add_box **Create** then in the Standard or Autopilot section, click **Configure** .\n- From the navigation pane, under **Cluster** , click **Networking** .\n- In the **Network** drop-down list, select a VPC.\n- In the **Node subnet** drop-down list, select a subnet for the cluster.\n- Ensure the **Enable VPC-native traffic routing (uses alias IP)** checkbox is selected.\n- Select the **Automatically create secondary ranges** checkbox if you want the secondary range assignment method to be managed by GKE. Clear this checkbox if you have already created secondary ranges for the chosen subnet and would like the secondary range assignment method to be user-managed.\n- In the **Pod address range** field, enter a pod range, such as `10.0.0.0/14` .\n- In the **Service address range** field, enter a service range, such as `10.4.0.0/19` .\n- Configure your cluster.\n- Click **Create** .\nYou can create a VPC-native cluster with Terraform using a [Terraform module](https://github.com/terraform-google-modules/terraform-google-kubernetes-engine) .\nFor example, you can add the following block to your Terraform configuration:\n```\nmodule \"gke\" {\u00a0 source \u00a0= \"terraform-google-modules/kubernetes-engine/google\"\u00a0 version = \"~> 12.0\"\u00a0 project_id \u00a0 \u00a0 \u00a0 \u00a0= \"PROJECT_ID\"\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"CLUSTER_NAME\"\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"COMPUTE_LOCATION\"\u00a0 network \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = \"NETWORK_NAME\"\u00a0 subnetwork \u00a0 \u00a0 \u00a0 \u00a0= \"SUBNET_NAME\"\u00a0 ip_range_pods \u00a0 \u00a0 = \"SECONDARY_RANGE_PODS\"\u00a0 ip_range_services = \"SECONDARY_RANGE_SERVICES\"}\n```\nReplace the following:- ``: your project ID.\n- ``: the name of the GKE cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster. For Terraform, the Compute Engine region.\n- ``: the name of an existing network.\n- ``: the name of an existing subnet. The subnet's primary IP address range is used for nodes. The subnet must exist in the same region as the one used by the cluster.\n- ``: the name of an existing secondary IP address range in the specified\n- ``: the name of an existing secondary IP address range in the specified\nWhen you create a VPC-native cluster, you define an [IPAllocationPolicy](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.IPAllocationPolicy) object. You can reference existing subnet secondary IP address ranges or you can specify CIDR blocks. Reference existing subnet secondary IP address ranges to create a cluster whose secondary range assignment method is user-managed. Provide CIDR blocks if you want the range assignment method to be managed by GKE.\n```\n{\u00a0 \"name\": CLUSTER_NAME,\u00a0 \"description\": DESCRIPTION,\u00a0 ...\u00a0 \"ipAllocationPolicy\": {\u00a0 \u00a0 \"useIpAliases\": true,\u00a0 \u00a0 \"clusterIpv4CidrBlock\" \u00a0 \u00a0 \u00a0: string,\u00a0 \u00a0 \"servicesIpv4CidrBlock\" \u00a0 \u00a0 : string,\u00a0 \u00a0 \"clusterSecondaryRangeName\" : string,\u00a0 \u00a0 \"servicesSecondaryRangeName\": string,\u00a0 },\u00a0 ...}\n```\nThis command includes the following values:- `\"clusterIpv4CidrBlock\"`: the CIDR range for Pods. This determines the size of the secondary range for Pods, and can be in CIDR notation, such as`10.0.0.0/14`. An empty space with the given size is chosen from the available space in your VPC. If left blank, a valid range is found and created with a default size.\n- `\"servicesIpv4CidrBlock\"`: the CIDR range for Services. See description of`\"clusterIpv4CidrBlock\"`.\n- `\"clusterSecondaryRangeName\"`: the name of the secondary range for Pods. The secondary range must already exist and belong to the subnetwork associated with the cluster.\n- `\"serviceSecondaryRangeName\"`: the name of the secondary range for Services. The secondary range must already exist and belong to the subnetwork associated with the cluster.\n### Create a cluster and select the control plane IP address range\nBy default, clusters with [Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) use the primary subnet range to provision the internal IP address assigned to the control plane endpoint. You can override this default setting by selecting a different subnet range during the cluster creation time only. The following sections show you how to create a cluster with Private Service Connect and override the subnet range.\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --private-endpoint-subnetwork=SUBNET_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```\nAdd the `--enable-private-nodes` flag to create the Private Service Connect cluster as .\nReplace the following:- ``: the name of the GKE cluster.\n- ``: the name of an existing subnet.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\nGKE creates a [cluster with Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) .In GKE version 1.29 and later, you can create a cluster with Private Service Connect:\n```\ngcloud container clusters create CLUSTER_NAME --enable-ip-alias \\\u00a0 \u00a0 --enable-private-nodes \u00a0\\\u00a0 \u00a0 --private-endpoint-subnetwork=SUBNET_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:- ``: the name of the GKE cluster.\n- ``: the name of an existing subnet. If you don't provide a value for the`private-endpoint-subnetwork`flag, but you use the`master-ipv4-cidr`, GKE creates a new subnet that uses the values that you defined in`master-ipv4-cidr`. GKE uses the new subnet to provision the internal IP address for the control plane.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\nTo assign a subnet to the control plane of a new cluster, you must [add a subnet](/vpc/docs/using-vpc#add-subnets) first. Then complete the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- In the Standard or Autopilot section, click **Configure** .\n- For the **Name** , enter your cluster name.\n- For Standard clusters, from the navigation pane, under **Cluster** , click **Networking** .\n- In the **IPv4 network access** section, do the following:- To create a GKE cluster as public, select **Public cluster** .\n- To create a GKE cluster as private, select **Private cluster** .\nIn both cases, you can later [change the cluster isolation mode](/kubernetes-engine/docs/how-to/change-cluster-isolation#why_change_cluster_isolation) when editing the cluster configuration.\n- In the **Advanced networking options** section, select the **Override control plane's default private endpoint subnet** checkbox.\n- In the **Private endpoint subnet** list, select your created subnet.\n- Click **Done** . Add additional authorized networks as needed.## Create a cluster and subnet simultaneously\nThe following directions demonstrate how to create a VPC-native GKE cluster and subnet at the same time. The [secondary range assignment method](/kubernetes-engine/docs/concepts/alias-ips#range_management) is managed by GKE when you perform these two steps with one command.\n**Note:** If using [Shared VPC](/vpc/docs/shared-vpc) , you cannot simultaneously create the cluster and subnet. Instead, a Network Admin in the Shared VPC host project must create the subnet first. Then you can [create the cluster in an existing subnet](#creating_cluster) with a secondary range assignment method of user-managed.\nTo create a VPC-native cluster and subnet simultaneously:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --create-subnetwork name=SUBNET_NAME,range=NODE_IP_RANGE \\\u00a0 \u00a0 --cluster-ipv4-cidr=POD_IP_RANGE \\\u00a0 \u00a0 --services-ipv4-cidr=SERVICES_IP_RANGE\n```\nReplace the following:- ``: the name of the GKE cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of the subnet to create. The subnet's region is the same region as the cluster (or the region containing the zonal cluster). Use an empty string (`name=\"\"`) if you want GKE to generate a name for you.\n- ``: an IP address range in CIDR notation, such as`10.5.0.0/20`, or the size of a CIDR block's subnet mask, such as`/20`. This is used to create the subnet's primary IP address range for nodes. If omitted, GKE chooses an available IP range in the VPC with a size of`/20`.\n- ``: an IP address range in CIDR notation, such as`10.0.0.0/14`, or the size of a CIDR block's subnet mask, such as`/14`. This is used to create the subnet's secondary IP address range for Pods. If omitted, GKE uses a randomly chosen`/14`range containing 2addresses. The automatically chosen range is randomly selected from`10.0.0.0/8`(a range of 2addresses) and does not include IP address ranges allocated to VMs, existing [routes](/vpc/docs/routes) , or ranges allocated to other clusters. The automatically chosen range might conflict with [reserved IP addresses](/compute/docs/ip-addresses/reserve-static-external-ip-address) , [dynamic routes](/vpc/docs/routes#dynamic_routes) , or routes within VPCs that peer with this cluster. If you use these any of these, you should specify`--cluster-ipv4-cidr`to prevent conflicts.\n- ``: an IP address range in CIDR notation, such as`10.4.0.0/19`, or the size of a CIDR block's subnet mask, such as`/19`. This is used to create the subnet's secondary IP address range for Services. If omitted, GKE uses`/20`, the default Services IP address range size.\nYou cannot create a cluster and subnet simultaneously using the Google Cloud console. Instead, first [create a subnet](/vpc/docs/create-modify-vpc-networks#add-subnets) then [create the cluster in an existing subnet](#creating_cluster) .\nTo create a VPC-native cluster, define an [IPAllocationPolicy](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.IPAllocationPolicy) object in your cluster resource:\n```\n{\u00a0 \"name\": CLUSTER_NAME,\u00a0 \"description\": DESCRIPTION,\u00a0 ...\u00a0 \"ipAllocationPolicy\": {\u00a0 \u00a0 \"useIpAliases\": true,\u00a0 \u00a0 \"createSubnetwork\": true,\u00a0 \u00a0 \"subnetworkName\": SUBNET_NAME\u00a0 },\u00a0 ...}\n```\nThe `createSubnetwork` field automatically creates and provisions a subnetwork for the cluster. The `subnetworkName` field is optional; if left empty, a name is automatically chosen for the subnetwork.\n**Note:** GKE tries to clean up the created subnetwork when the cluster is deleted. But if the subnetwork is being used by other resources, GKE does not delete the subnetwork, and you must manage the life of the subnetwork yourself.\n## Use non-RFC 1918 IP address ranges\n**Note:** This feature is not supported with Windows Server node pools.\nGKE clusters can use IP address ranges outside of the RFC 1918 ranges for nodes, Pods, and Services. See [valid ranges](/vpc/docs/subnets#valid-ranges) in the VPC network documentation for a list of non-RFC 1918 private ranges that can be used as internal IP addresses for subnet ranges.\nNon-RFC 1918 IP address ranges are compatible with both [privateclusters](/kubernetes-engine/docs/concepts/private-cluster-concept) and non-private clusters.\nNon-RFC 1918 private ranges are subnet ranges \u2014 you can use them exclusively or in conjunction with RFC 1918 subnet ranges. Nodes, Pods, and Services continue to use subnet ranges as described in [IP ranges for VPC-native clusters](/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing) . If you use non-RFC 1918 ranges, keep the following in mind:\n- Subnet ranges, even those using non-RFC 1918 ranges, must be [assigned](/kubernetes-engine/docs/concepts/alias-ips#range_management) manually or by GKE the cluster's nodes are created. You cannot switch to or cease using non-RFC 1918 subnet ranges unless you replace the cluster.\n- [Internal passthrough Network Load Balancers](/kubernetes-engine/docs/how-to/internal-load-balancing) only use IP addresses from the subnet's primary IP address range. To create an internal passthrough Network Load Balancer with a non-RFC 1918 address, your subnet's primary IP address range must be non-RFC 1918.\nDestinations outside your cluster might have difficulties receiving traffic from private, non-RFC 1918 ranges. For example, RFC 1112 (class E) private ranges are typically used as multicast addresses. If a destination outside of your cluster cannot process packets whose sources are private IP addresses of the RFC 1918 range, you can do the following:\n- Use an RFC 1918 range for the subnet's primary IP address range. This way, nodes in the cluster use RFC 1918 addresses.\n- Ensure that your cluster is running the [IP masqueradeagent](/kubernetes-engine/docs/how-to/ip-masquerade-agent) and that the destinations arein the`nonMasqueradeCIDRs`list. This way, packets sent from Pods have their sources changed (SNAT) to node addresses, which are RFC 1918.## Enable privately used public IP address ranges\n**Note:** This feature is not supported with Windows Server node pools.\nGKE clusters can privately use certain public IP address ranges as internal, subnet IP address ranges. You can privately use any public IP address except for [certain restricted ranges](/vpc/docs/subnets#restricted-ranges) as described the VPC network documentation.\nYour cluster be a VPC-native cluster in order to use privately used public IP address ranges. Routes-based clusters are supported.\nPrivately used public ranges are subnet ranges. You can use them exclusively or in conjunction with other subnet ranges that use private addresses. Nodes, Pods, and Services continue to use subnet ranges as described in [IP ranges for VPC-native clusters](/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing) . Keep the following in mind when re-using public IP addresses privately:\n- When you use a external IP address range as a subnet range, your cluster can no longer communicate with systems on the Internet that use that public range. The range becomes an internal IP address range in the cluster's VPC network.\n- Subnet ranges, even those that privately use public IP address ranges, must be [assigned](/kubernetes-engine/docs/concepts/alias-ips#range_management) manually or by GKE the cluster's nodes are created. You cannot switch to or cease using privately used public IP addresses unless you replace the cluster.\nGKE by default implements SNAT on the nodes to public IP destinations. If you have configured the Pod CIDR to use external IP addresses, the SNAT rules apply to Pod-to-Pod traffic. To avoid this you have 2 options:\n- Create your cluster with the [--disable-default-snat](/sdk/gcloud/reference/beta/container/clusters/create#--disable-default-snat) flag. For more details about this flag, refer to [IP masquerading in GKE](/kubernetes-engine/docs/how-to/ip-masquerade-agent#how_ipmasq_works) .\n- Configure the [configMap ip-masq-agent](/kubernetes-engine/docs/how-to/ip-masquerade-agent) including in the`nonMasqueradeCIDRs`list at least the Pod CIDR, the Service CIDR, and the nodes subnet.## Use an IPv4/IPv6 dual-stack network to create a dual-stack cluster\nYou can create a cluster with IPv4/IPv6 dual-stack networking on a new or existing dual-stack subnet.\nThis section shows you how to complete the following tasks:\n- Create a dual-stack subnet (available in Autopilot clusters version 1.25 or later, and Standard clusters version 1.24 or later).\n- Update an existing subnet to a dual-stack subnet (available in Autopilot clusters version 1.25 or later, and Standard clusters version 1.24 or later).\n- Create a cluster with dual-stack networking (available in Autopilot clusters version 1.25 or later, and Standard clusters version 1.24 or later). GKE Autopilot clusters default to a dual-stack cluster when you use a dual-stack subnet. After cluster creation, you can update the Autopilot cluster to be IPv4-only.\n- Create a dual-stack cluster and a dual-stack subnet simultaneously (available in Autopilot clusters version 1.25 or later, and Standard clusters version 1.24 or later).\nTo learn more about the benefits and requirements of GKE clusters with dual-stack networking overview, see the [VPC-native cluster documentation](/kubernetes-engine/docs/concepts/alias-ips#dual_stack_network) .\n**Note:** Before setting up dual-stack clusters, we recommend that you review the [restrictions and limitations of dual-stack networking](/kubernetes-engine/docs/concepts/alias-ips#dual_stack_limitations) .\n### Create a dual-stack subnet\nTo create a dual-stack subnet, run the following command:\n```\ngcloud compute networks subnets create SUBNET_NAME \\\u00a0 \u00a0 --stack-type=ipv4-ipv6 \\\u00a0 \u00a0 --ipv6-access-type=ACCESS_TYPE \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --range=PRIMARY_RANGE \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:\n- ``: the name of the subnet that you choose.\n- ``: the routability to the public internet. Use`INTERNAL`for private clusters and`EXTERNAL`for public clusters. If`--ipv6-access-type`is not specified, the default access type is`EXTERNAL`.\n- ``: the name of the network that will contain the new subnet. This network must meet the following conditions:- It must be a custom mode VPC network. For more information, see how to [switch a VPC network from auto mode to custom mode](/vpc/docs/create-modify-vpc-networks#switch-network-mode) .\n- If you replace the``with`INTERNAL`, the network must use [Unique Local IPv6 Unicast Addresses (ULA)](https://datatracker.ietf.org/doc/html/rfc4193) .\n- ``: the primary IPv4 IP address range for the new subnet, in CIDR notation. For more information, see [Subnet ranges](/vpc/docs/vpc#manually_created_subnet_ip_ranges) .\n- ``: the [compute region](/compute/docs/regions-zones#available) for the cluster.\n### Update an existing subnet to a dual-stack subnet\nTo update an existing subnet to a dual-stack subnet, run the following command. Updating a subnet does not affect any existing IPv4 clusters in the subnet.\n```\ngcloud compute networks subnets update SUBNET_NAME \\\u00a0 \u00a0 --stack-type=ipv4-ipv6 \\\u00a0 \u00a0 --ipv6-access-type=ACCESS_TYPE \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:\n- ``: the name of the subnet.\n- ``: the routability to the public internet. Use`INTERNAL`for private clusters and`EXTERNAL`for public clusters. If`--ipv6-access-type`is not specified, the default access type is`EXTERNAL`.\n- ``: the [compute region](/compute/docs/regions-zones#available) for the cluster.\n### Create a cluster with dual-stack networking\nTo create a cluster with an existing dual-stack subnet, you can use `gcloud CLI` or the Google Cloud console:\n- For Autopilot clusters, run the following command:```\n\u00a0 gcloud container clusters create-auto CLUSTER_NAME \\\u00a0 \u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 \u00a0 --subnetwork=SUBNET_NAME\n```Replace the following:- ``: the name of your new Autopilot cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of a VPC network that contains the subnet. This VPC network must be a custom mode VPC network. For more information, see how to [switch a VPC network from auto mode to custom mode](/vpc/docs/create-modify-vpc-networks#switch-network-mode) .\n- `` : the name of the dual-stack subnet. To learn more, see how to [create a dual-stack subnet](#create_dual_stack_subnet) .GKE Autopilot clusters default to a dual-stack cluster when you use a dual-stack subnet. After cluster creation, you can update the Autopilot cluster to be IPv4-only.- For Standard clusters, run the following command:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --enable-dataplane-v2 \\\u00a0 \u00a0 --stack-type=ipv4-ipv6 \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --subnetwork=SUBNET_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace the following:- ``: the name of the new cluster.\n- ``: the name of a VPC network that contains the subnet. This VPC network must be a custom mode VPC network that uses [Unique Local IPv6 Unicast Addresses (ULA)](https://datatracker.ietf.org/doc/html/rfc4193) . For more information, see how to [switch a VPC network from auto mode to custom mode](/vpc/docs/create-modify-vpc-networks#switch-network-mode) .\n- ``: the name of the subnet.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- In the Standard or Autopilot section, click **Configure** .\n- Configure your cluster as needed.\n- From the navigation pane, under **Cluster** , click **Networking** .\n- In the **Network** list, select the name of your network.\n- In the **Node subnet** list, select the name of your dual-stack subnet.\n- For Standard clusters, select the **IPv4 and IPv6 (dual stack)** radio button. This option is available only if you selected a dual-stack subnet.Autopilot clusters default to a dual-stack cluster when you use a dual-stack subnet.\n- Click **Create** .### Create a dual-stack cluster and a subnet simultaneously\nYou can create a subnet and a dual-stack cluster simultaneously. GKE creates an IPv6 subnet and assigns an external IPv6 primary range to the subnet.\n- For Autopilot clusters, run the following command:```\ngcloud container clusters create-auto CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --create-subnetwork name=SUBNET_NAME\n```Replace the following:- ``: the name of your new Autopilot cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of a VPC network that contains the subnet. This VPC network must be a custom mode VPC network that uses [Unique Local IPv6 Unicast Addresses (ULA)](https://datatracker.ietf.org/doc/html/rfc4193) . For more information, see how to [switch a VPC network from auto mode to custom mode](/vpc/docs/create-modify-vpc-networks#switch-network-mode) .\n- ``: the name of the new subnet. GKE can create the subnet based on your [organization policies](/resource-manager/docs/organization-policy/org-policy-constraints) :- If your organization policies allow dual-stack, and the network is custom mode, GKE creates a dual-stack subnet and assigns an external IPv6 primary range to the subnet .\n- If your organization policies don't allow dual-stack, or if the network is in auto mode, GKE creates a single stack (IPv4) subnet.\n- For Standard clusters, run the following command:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --stack-type=ipv4-ipv6 \\\u00a0 \u00a0 --ipv6-access-type=ACCESS_TYPE \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --create-subnetwork name=SUBNET_NAME,range=PRIMARY_RANGE \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace the following:- ``: the name of the new cluster that you choose.\n- ``: the routability to the public internet. Use`INTERNAL`for private clusters and`EXTERNAL`for public clusters. If`--ipv6-access-type`is not specified, the default access type is`EXTERNAL`.\n- ``: the name of the network that will contain the new subnet. This network must meet the following conditions:- It must be a custom mode VPC network. For more information, see how to [switch a VPC network from auto mode to custom mode](/vpc/docs/create-modify-vpc-networks#switch-network-mode) .\n- If you replace the``with`INTERNAL`, the network must use [Unique Local IPv6 Unicast Addresses (ULA)](https://datatracker.ietf.org/doc/html/rfc4193) .\n- ``: the name of the new subnet that you choose.\n- ``: the primary IPv4 address range for the new subnet, in CIDR notation. For more information, see [Subnet ranges](/vpc/docs/vpc#manually_created_subnet_ip_ranges) .\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n### Update the stack type on an existing cluster\nYou can change the stack type of an existing cluster. Before you change the stack type on an existing cluster, consider the following limitations:\n- Changing the stack type is supported in new GKE clusters running version 1.25 or later. GKE clusters that have been upgraded from versions 1.24 to versions 1.25 or 1.26 might get validation errors when enabling dual-stack network. In case of errors, contact the Google Cloud [support team](/support-hub) .\n- Changing the stack type is a disruptive operation because GKE restarts components in both the control plane and nodes.\n- GKE respects your configured maintenance windows when recreating nodes. This means that the cluster stack type won't be operational on the cluster until the next maintenance window occurs. If you prefer not to wait, you can manually upgrade the node pool by setting the `--cluster-version` flag to the same GKE version the control plane is already running. You must use the gcloud CLI if you use this workaround. For more information, see [caveats for maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#caveats) .\n- Changing the stack type does not automatically change the IP family of existing Services. The following conditions apply:- If you change a single stack to dual-stack, the existing Services remain single stack.\n- If you change a dual-stack to single stack, the existing Services with IPv6 addresses get into an error state. Delete the Service and create one with the correct`ipFamilies`. To learn more, see an [example of how to set up a Deployment](/anthos/clusters/docs/on-prem/1.8/how-to/create-service-ingress#expose_your_deployment_with_a_service) .To update an existing VPC-native cluster, you can use gcloud CLI or the Google Cloud console:\nRun the following command:\n```\n\u00a0 gcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 \u00a0 --stack-type=STACK_TYPE \\\u00a0 \u00a0 \u00a0 --location=COMPUTE_LOCATION\n```\nReplace the following:- ``: the name of the cluster you want to update.\n- ``: the stack type. Replace with one of the following values:- `ipv4`: to update a dual-stack cluster to IPv4 only cluster. GKE uses the primary IPv4 address range of the cluster's subnet.\n- `ipv4-ipv6`: to update an existing IPv4 cluster to dual-stack. You can only change a cluster to dual-stack if the underlying subnet supports dual-stack. To learn more, see [Update an existing subnet to a dual-stack subnet](/kubernetes-engine/docs/how-to/alias-ips#update_an_existing_subnet_to_a_dual-stack_subnet) .\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to the cluster you want to edit, click **Actions** , then click **Edit** .\n- In the **Networking** section, next to **Stack type** , click **Edit** .\n- In the **Edit stack type** dialog, select the checkbox for the cluster stack type you need.\n- Click **Save Changes** .\n### Create a IPv4/IPv6 dual-stack Service\nYou can create an IPv4/IPv6 dual-stack Service of type [ClusterIP](/kubernetes-engine/docs/concepts/service#services_of_type_clusterip) or [NodePort](/kubernetes-engine/docs/concepts/service#service_of_type_nodeport) . New GKE clusters running version 1.29 or later support dual-stack Services of type [LoadBalancer](/kubernetes-engine/docs/concepts/service-load-balancer) . To learn more, see [IPv4/IPv6 dual-stack LoadBalancer Service](/kubernetes-engine/docs/concepts/service-load-balancer-parameters#ipv4ipv6_dual-stack_loadbalancer_service) .\nFor each of these Service types, you can define `ipFamilies` and `ipFamilyPolicy` fields as either IPv4, IPv6, or a dual-stack. To learn more, see [IPv4/IPv6 dual-stack Services](https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services) .\n## Verify the stack type, Pod, and Service IP address ranges\nAfter you create a VPC-native cluster, you can verify its Pod and Service ranges.\nTo verify the cluster, run the following command:\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nThe output has an `ipAllocationPolicy` block. The `stackType` field describes the type of network definition. For each type, you can see the following network information:- IPv4 network information:- `clusterIpv4Cidr`is the secondary range for Pods.\n- `servicesIpv4Cidr`is the secondary range for Services.\n- IPv6 network information (if a cluster has dual-stack networking):- `ipv6AccessType`: The routability to the public internet.`INTERNAL`for private IPv6 addresses and`EXTERNAL`for public IPv6 addresses.\n- `subnetIpv6CidrBlock`: The secondary IPv6 address range for the new subnet.\n- `servicesIpv6CidrBlock`: The address range assigned for the IPv6 Services on the dual-stack cluster.\nTo verify the cluster, perform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to inspect.\nThe secondary ranges are displayed in the **Networking** section:- **Pod address range** is the secondary range for Pods\n- **Service address range** is the secondary range for Services## Troubleshooting\n[view GKE IP address utilization insights](/network-intelligence-center/docs/network-analyzer/insights/kubernetes-engine/gke-ip-utilization)\n### The default network resource is not ready### Invalid value for IPCidrRange### Not enough free IP address space for Pods### Confirm whether default SNAT is disabled\nUse the following command to check the status of default SNAT:\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nReplace `` with the name of your cluster.\nThe output is similar to the following:\n```\nnetworkConfig:\n disableDefaultSnat: true\n network: ...\n```\n### Cannot use --disable-default-snat without --enable-ip-alias\nThis error message, and `must disable default sNAT (--disable-default-snat) before using public IP address privately in the cluster` , mean that you should explicitly set the `--disable-default-snat` flag when creating the cluster since you are using public IP addresses in your private cluster.\nIf you see error messages like `cannot disable default sNAT ...` , this means the default SNAT can't be disabled in your cluster. To resolve this issue, review your cluster configuration.\n### Debugging Cloud NAT with default SNAT disabled\nIf you have a private cluster created with the `--disable-default-snat` flag and have set up Cloud NAT for internet access and you aren't seeing internet-bound traffic from your Pods, make sure that the Pod range is included in the Cloud NAT configuration.\nIf there is a problem with Pod to Pod communication, examine the iptables rules on the nodes to verify that the Pod ranges are not masqueraded by iptables rules.\n[GKE IP masquerade documentation](/kubernetes-engine/docs/how-to/ip-masquerade-agent)\nIf you have not configured an IP masquerade agent for the cluster, GKE automatically ensures that Pod to Pod communication is not masqueraded. However, if an IP masquerade agent is configured, it overrides the default IP masquerade rules. Verify that additional rules are configured in the IP masquerade agent to ignore masquerading the Pod ranges.\n### The dual-stack cluster network communication is not working as expected- Verify the firewall rule content:```\ngcloud compute firewall-rules describe FIREWALL_RULE_NAME\n```Replace `` with the name of the firewall rule.Each dual-stack cluster creates a firewall rule that allows nodes and Pods to communicate with each other. The firewall rule content is similar to the following:```\nallowed:\n- IPProtocol: esp\n- IPProtocol: ah\n- IPProtocol: sctp\n- IPProtocol: tcp\n- IPProtocol: udp\n- IPProtocol: '58'\ncreationTimestamp: '2021-08-16T22:20:14.747-07:00'\ndescription: ''\ndirection: INGRESS\ndisabled: false\nenableLogging: false\nid: '7326842601032055265'\nkind: compute#firewall\nlogConfig:\n enable: false\nname: gke-ipv6-4-3d8e9c78-ipv6-all\nnetwork: https://www.googleapis.com/compute/alpha/projects/my-project/global/networks/alphanet\npriority: 1000\nselfLink: https://www.googleapis.com/compute/alpha/projects/my-project/global/firewalls/gke-ipv6-4-3d8e9c78-ipv6-all\nselfLinkWithId: https://www.googleapis.com/compute/alpha/projects/my-project/global/firewalls/7326842601032055265\nsourceRanges:\n- 2600:1900:4120:fabf::/64\ntargetTags:\n- gke-ipv6-4-3d8e9c78-node\n```The `sourceRanges` value must be the same as the `subnetIpv6CidrBlock` . The `targetTags` value must be the same as the tags on the GKE nodes. To fix this issue, [update the firewall rule](/vpc/docs/using-firewalls#updating_firewall_rules) with the [cluster ipAllocationPolicy block](#verify) information.## What's next\n- [Read the GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- [Learn about internal load balancing](/kubernetes-engine/docs/how-to/internal-load-balancing) .\n- [Learn about configuring authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) .\n- [Learn about creating cluster network policies](/kubernetes-engine/docs/how-to/network-policy) .", "guide": "Google Kubernetes Engine (GKE)"}