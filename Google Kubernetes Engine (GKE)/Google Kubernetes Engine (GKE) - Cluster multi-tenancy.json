{"title": "Google Kubernetes Engine (GKE) - Cluster multi-tenancy", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/multitenancy-overview", "abstract": "# Google Kubernetes Engine (GKE) - Cluster multi-tenancy\nThis page explains [cluster multi-tenancy](#what_is_multi-tenancy) on Google Kubernetes Engine (GKE). This includes clusters shared by different users at a single organization, and clusters that are shared by per-customer instances of a software as a service (SaaS) application. Cluster multi-tenancy is an alternative to managing many single-tenant clusters.\nThis page also summarizes the Kubernetes and GKE features that can be used to manage multi-tenant clusters.\n", "content": "## What is multi-tenancy?\nA multi-tenant cluster is shared by multiple users and/or workloads which are referred to as \"tenants\". The operators of multi-tenant clusters must isolate tenants from each other to minimize the damage that a compromised or malicious tenant can do to the cluster and other tenants. Also, cluster resources must be fairly allocated among tenants.\nWhen you plan a multi-tenant architecture you should consider the layers of resource isolation in Kubernetes: cluster, namespace, node, Pod, and container. You should also consider the security implications of sharing different types of resources among tenants. For example, scheduling Pods from different tenants on the same node could reduce the number of machines needed in the cluster. On the other hand, you might need to prevent certain workloads from being colocated. For example, you might not allow untrusted code from outside of your organization to run on the same node as containers that process sensitive information.\nAlthough Kubernetes cannot guarantee perfectly secure isolation between tenants, it does offer features that may be sufficient for specific use cases. You can separate each tenant and their Kubernetes resources into their own [namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) . You can then use [policies](https://kubernetes.io/docs/concepts/policy/) to enforce tenant isolation. Policies are usually scoped by namespace and can be used to restrict API access, to constrain resource usage, and to restrict what containers are allowed to do.\nThe tenants of a multi-tenant cluster share:\n- [Extensions](https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/) , [controllers](https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-controller) , add-ons, and [custom resource definitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) (CRDs).\n- The cluster [control plane](https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/) . This implies that the cluster operations, security, and auditing are centralized.\nOperating a multi-tenant cluster has several advantages over operating multiple, single-tenant clusters:\n- Reduced management overhead\n- Reduced resource fragmentation\n- No need to wait for cluster creation for new tenants## Multi-tenancy use cases\nThis section describes how you could configure a cluster for various multi-tenancy use cases.\n### Enterprise multi-tenancy\nIn an enterprise environment, the tenants of a cluster are distinct teams within the organization. Typically, each tenant has a corresponding namespace. Alternative models of multi-tenancy with a tenant per cluster, or a tenant per Google Cloud project, are harder to manage. Network traffic within a namespace is unrestricted. Network traffic between namespaces must be explicitly allowed. These policies can be enforced using Kubernetes [network policy](#network_policies) .\nThe users of the cluster are divided into three different roles, depending on their privilege:\nFor information on setting up multiple multi-tenant clusters for an enterprise organization, see [Best practices for enterprise multi-tenancy](/kubernetes-engine/docs/best-practices/enterprise-multitenancy) .\n### SaaS provider multi-tenancy\nThe tenants of a SaaS provider's cluster are the per-customer instances of the application, and the SaaS's control plane. To take advantage of namespace-scoped policies, the application instances should be organized into their own namespaces, as should components of the SaaS's control plane. End users can't interact with the Kubernetes control plane directly, they use the SaaS's interface instead, which in turn interacts with the Kubernetes control plane.\nFor example, a blogging platform could run on a multi-tenant cluster. In this case, the tenants are each customer's blog instance and the platform's own control plane. The platform's control plane and each hosted blog would all run in separate namespaces. Customers would create and delete blogs, update the blogging software versions through the platform's interface with no visibility into how the cluster operates.\n## Multi-tenancy policy enforcement\nGKE and Kubernetes provide several features that can be used to manage multi-tenant clusters. The following sections give an overview of these features.\n### Access control\nGKE has two access control systems: Identity and Access Management (IAM) and role-based access control (RBAC). IAM is Google Cloud's access control system for managing authentication and authorization for Google Cloud resources. You use IAM to grant users access to GKE and Kubernetes resources. RBAC is built into Kubernetes and grants granular permissions for specific resources and operations within your clusters.\nRefer to the [Access controloverview](/kubernetes-engine/docs/concepts/access-control) for more information about these options and when to use each.\nRefer to the [RBAC how-toguide](/kubernetes-engine/docs/how-to/role-based-access-control) and the [IAM how-to guide](/kubernetes-engine/docs/how-to/iam) to learn how to use these access control systems.\n[ Enable access and view cluster resources by namespace](/kubernetes-engine/docs/how-to/restrict-resources-access-by-namespace)\n### Network policies\nCluster [networkpolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) give you control over the communication between your cluster's Pods. Policies specify which namespaces, labels, and IP address ranges a Pod can communicate with.\nSee the [network policy how-to](/kubernetes-engine/docs/how-to/network-policy) for instructions on enabling network policy enforcement on GKE.\nFollow the [network policytutorial](/kubernetes-engine/docs/tutorials/network-policy) to learn how to write network policies.\n### Resource quotas\nResource quotas manage the amount of resources used by the objects in a namespace. You can set quotas in terms of CPU and memory usage, or in terms of object counts. Resource quotas let you ensure that no tenant uses more than its assigned share of cluster resources.\nRefer to the [resourcequotas](https://kubernetes.io/docs/concepts/policy/resource-quotas) documentation for more information.\n### Policy-based Pod admission control\nTo prevent Pods that violate your security boundaries from running in your cluster, use an admission controller. Admission controllers can check Pod specifications against policies that you define, and can prevent Pods that violate those policies from running in your cluster.\nGKE supports the following types of admission control:\n- [Policy Controller](/anthos-config-management/docs/how-to/installing-policy-controller) : Declare pre-defined or custom policies and enforce them in clusters at scale using fleets. Policy Controller is an implementation of the open source [Gatekeeper open policy agent](https://github.com/open-policy-agent/gatekeeper) and is a feature of GKE Enterprise.\n- [PodSecurity admission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) : Enforce pre-defined policies that correspond to the [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) in individual clusters or in specific namespaces.\n### Pod anti-affinity\n**Warning:** Pod anti-affinity rules can be circumvented by malicious tenants. The example below should only be used with clusters with trusted tenants, or with tenants who don't have direct access to the Kubernetes control plane.\nYou can use [Podanti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) to prevent Pods from different tenants from being scheduled on the same node. Anti-affinity constraints are based on Pod [labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) . For example, the Pod specification below describes a Pod with the label `\"team\": \"billing\"` , and an anti-affinity rule that prevents the Pod from being scheduled alongside Pods without the label.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: bar\u00a0 labels:\u00a0 \u00a0 team: \"billing\"spec:\u00a0 affinity:\u00a0 \u00a0 podAntiAffinity:\u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 - topologyKey: \"kubernetes.io/hostname\"\u00a0 \u00a0 \u00a0 \u00a0 labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: \"team\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: NotIn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values: [\"billing\"]\n```\nThe drawback to this technique is that malicious users could circumvent the rule by applying the `team: billing` label to an arbitrary Pod. Pod anti-affinity alone cannot securely enforce policy on clusters with untrusted tenants.\nRefer to the [Podanti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) documentation for more information.\n### Dedicated nodes with taints and tolerations\n**Warning:** Policies enforced by node taints and tolerations can be circumvented by malicious tenants. The example below should only be used with clusters with trusted tenants, or with tenants who don't have direct access to the Kubernetes control plane.\nNode taints are another way to control workload scheduling. You can use node taints to reserve specialized nodes for use by certain tenants. For example, you can dedicate [GPU equipped nodes](/kubernetes-engine/docs/how-to/gpus) to the specific tenants whose workloads require GPUs. For Autopilot clusters, node tolerations are supported only for [workload separation](/kubernetes-engine/docs/how-to/workload-separation) . Node taints are automatically added by node auto-provisioning as needed.\nTo dedicate a [node pool](/kubernetes-engine/docs/concepts/node-pools) to a certain tenant, apply a taint with `effect: \"NoSchedule\"` to the node pool. Then only Pods with a corresponding toleration can be scheduled to nodes in the node pool.\nThe drawback to this technique is that malicious users could add a toleration to their Pods to get access to the dedicated node pool. Node taints and tolerations alone cannot securely enforce policy on clusters with untrusted tenants.\nSee the [node taints how-to page](/kubernetes-engine/docs/how-to/node-taints) to learn how to control scheduling with node taints.\n## What's next\n- Read the [Best practices for enterprise multi-tenancy](/kubernetes-engine/docs/best-practices/enterprise-multitenancy) .\n- Learn how to [Optimize resource usage in a multi-tenant GKE cluster using node auto-provisioning](/solutions/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning) .", "guide": "Google Kubernetes Engine (GKE)"}