{"title": "Google Kubernetes Engine (GKE) - Autoscaling a cluster", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler", "abstract": "# Google Kubernetes Engine (GKE) - Autoscaling a cluster\nThis page shows you how to autoscale your Standard Google Kubernetes Engine (GKE) clusters. To learn about how the cluster autoscaler works, refer to [Cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\nWith Autopilot clusters, you don't need to worry about provisioning nodes or managing node pools because node pools are provisioned through [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) , and are [automatically scaled](/kubernetes-engine/docs/concepts/cluster-autoscaler) to meet the requirements of your workloads.\n", "content": "## Using the cluster autoscaler\n**Note:** Enabling or disabling cluster autoscaling might cause the control plane to restart, which takes several minutes to complete. Once autoscaling is enabled for at least one node pool, further changes to the cluster autoscaler configuration does not cause the control plane to restart until autoscaling is disabled for the last node pool. However, autoscaling might still take up to one minute for changes to propagate after the operation completes.\nThe following sections explain how to use cluster autoscaler.\n### Creating a cluster with autoscaling\nYou can create a cluster with autoscaling enabled using the Google Cloud CLI or the Google Cloud console.\nTo create a cluster with autoscaling enabled, use the `--enable-autoscaling` flag and specify `--min-nodes` and `--max-nodes` :\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-autoscaling \\\u00a0 \u00a0 --num-nodes NUM_NODES \\\u00a0 \u00a0 --min-nodes MIN_NODES \\\u00a0 \u00a0 --max-nodes MAX_NODES \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:- ``: the name of the cluster to create.\n- ``: the number of nodes to create in each location.\n- ``: the minimum number of nodes to automatically scale for the specified node pool per zone. To specify the minimum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-min-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the maximum number of nodes to automatically scale for the specified node pool per zone. To specify the maximum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-max-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the new cluster. For zonal clusters, use`--zone=` ``.\n **Example: Creating a cluster with node autoscaling enabled and min and max nodes** \nThe following command creates a cluster with 90 nodes, or 30 nodes in each of the 3 zones present in the region. Node autoscaling is enabled and resizes the number of nodes based on [cluster load](/kubernetes-engine/docs/concepts/cluster-autoscaler#operating_criteria) . The cluster autoscaler can reduce the size of the default node pool to 15 nodes or increase the node pool to a maximum of 50 nodes per zone.\n```\ngcloud container clusters create my-cluster --enable-autoscaling \\\n --num-nodes=30 \\\n --min-nodes=15 --max-nodes=50 \\\n --region=us-central\n```\n **Example: Creating a cluster with node autoscaling enabled and total nodes** \nThe following command creates a cluster with 30 nodes, or 10 nodes in each of the 3 zones present in the region. Node autoscaling is enabled and resizes the number of nodes based on [cluster load](/kubernetes-engine/docs/concepts/cluster-autoscaler#operating_criteria) . In this example, the total size of the cluster can be between 10 and 60 nodes, regardless of spreading between zones.\n```\ngcloud container clusters create my-cluster --enable-autoscaling \\\n --num-nodes 10 \\\n --region us-central1 \\\n --total-min-nodes 10 --total-max-nodes 60\n```\nTo create a new cluster in which the default node pool has autoscaling enabled:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Configure your cluster as desired.\n- From the navigation pane, under **Node Pools** , click **default-pool** .\n- Select the **Enable autoscaling** checkbox.\n- Change the values of the **Minimum number of nodes** and **Maximum number of nodes** fields as desired.\n- Click **Create** .\n### Adding a node pool with autoscaling\nYou can create a node pool with autoscaling enabled using the gcloud CLI or the Google Cloud console.\nTo add a node pool with autoscaling to an existing cluster, use the following command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --enable-autoscaling \\\u00a0 \u00a0 --min-nodes=MIN_NODES \\\u00a0 \u00a0 --max-nodes=MAX_NODES \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:- ``: the name of the desired node pool.\n- ``: the name of the cluster in which the node pool is created.\n- ``: the minimum number of nodes to automatically scale for the specified node pool per zone. To specify the minimum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-min-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the maximum number of nodes to automatically scale for the specified node pool per zone. To specify the maximum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-max-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the new cluster. For zonal clusters, use`--zone=` ``.\n **Example: Adding a node pool with node autoscaling enabled** \nThe following command creates a node pool with node autoscaling that scales the node pool to a maximum of 5 nodes and a minimum of 1 node:\n```\ngcloud container node-pools create my-node-pool \\\n --cluster my-cluster \\\n --enable-autoscaling \\\n --min-nodes 1 --max-nodes 5 \\\n --zone us-central1-c\n```\nTo add a node pool with autoscaling to an existing cluster:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add Node Pool** .\n- Configure the node pool as desired.\n- Under **Size** , select the **Enable autoscaling** checkbox.\n- Change the values of the **Minimum number of nodes** and **Maximum numberof nodes** fields as desired.\n- Click **Create** .\n### Enabling autoscaling for an existing node pool\nYou can enable autoscaling for an existing node pool using the gcloud CLI or the Google Cloud console.\nTo enable autoscaling for an existing node pool, use the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-autoscaling \\\u00a0 \u00a0 --node-pool=POOL_NAME \\\u00a0 \u00a0 --min-nodes=MIN_NODES \\\u00a0 \u00a0 --max-nodes=MAX_NODES \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:- ``: the name of the cluster to update.\n- ``: the name of the desired node pool. If you have only one node pool, supply`default-pool`as the value.\n- ``: the minimum number of nodes to automatically scale for the specified node pool per zone. To specify the minimum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-min-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the maximum number of nodes to automatically scale for the specified node pool per zone. To specify the maximum number of nodes for the entire node pool in GKE versions 1.24 and later, use`--total-max-nodes`. The flags`--total-min-nodes`and`--total-max-nodes`are mutually exclusive with the flags`--min-nodes`and`--max-nodes`.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the new cluster. For zonal clusters, use`--zone=` ``.\nTo enable autoscaling for an existing node pool:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to modify, then click **Edit** .\n- Under **Size** , select the **Enable autoscaling** checkbox.\n- Change the values of the **Minimum number of nodes** and **Maximum numberof nodes** fields as desired.\n- Click **Save** .You verify that your cluster is using autoscaling with the Google Cloud CLI or the Google Cloud console.\nDescribe the node pools in the cluster:\n```\ngcloud container node-pools describe NODE_POOL_NAME --cluster=CLUSTER_NAME |grep autoscaling -A 1\n```\nReplace the following:- ``: the name of the new node pool that you choose.\n- ``: the name of the cluster.\nIf autoscaling is enabled, the output is similar to the following:\n```\nautoscaling:\n enabled: true\n```- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to verify.\n- Click the **Nodes** tab.\n- Under **Node Pools** , verify that node pool `Autoscalling` state.\n### Creating a node pool that prioritizes optimization of unused reservations\n**Note:** Starting in GKE version 1.27, cluster autoscaler always considers [reservations](/compute/docs/instances/reservations-overview) when making the scale-up decisions, regardless of the used location policy.\nYou can use the `--location_policy=ANY` flag when you create a node pool to instruct the cluster autoscaler to [prioritize utilization of unused reservations](/kubernetes-engine/docs/concepts/cluster-autoscaler#balancing_across_zones) :\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --location_policy=ANY\n```\nReplace the following:\n- ``: the name of the new node pool that you choose.\n- ``: the name of the cluster.\n### Disabling autoscaling for an existing node pool\nYou can disable autoscaling for an existing node pool using the gcloud CLI or the Google Cloud console.\nTo disable autoscaling for a specific node pool, use the `--no-enable-autoscaling` flag:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-autoscaling \\\u00a0 \u00a0 --node-pool=POOL_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:- ``: the name of the cluster to update.\n- ``: the name of the desired node pool.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the new cluster. For zonal clusters, use`--zone=` ``.\nThe cluster size is fixed at the cluster's current default node pool size, which can be [manually updated](/sdk/gcloud/reference/container/node-pools/update) .\nTo disable autoscaling for a specific node pool:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to modify, then click **Edit** .\n- Under **Size** , clear the **Enable autoscaling** checkbox.\n- Click **Save** .\n### Resizing a node pool\nFor clusters with autoscaling enabled, the cluster autoscaler automatically resizes node pools within the boundaries specified by either the minimum size ( `--min-nodes` ) and maximum size ( `--max-nodes` ) values or the minimum total size ( `--total-min-nodes` ) and maximum total size ( `--total-max-nodes` ). These flags are mutually exclusive. You cannot manually resize a node pool by changing these values.\nIf you want to manually resize a node pool in your cluster that has autoscaling enabled, perform the following:\n- [Disable autoscaling on the node pool](#disable_autoscaling) .\n- [Manually resize the cluster](/kubernetes-engine/docs/how-to/resizing-a-cluster) .\n- [Re-enable autoscaling and specify the minimum and maximum node pool size](#enable_autoscaling) .\n### Preventing Pods scheduling on selected nodes\nYou can use `startup` or `status` taints to prevent Pods scheduling on selected nodes, depending on the use case.\nThis feature is available in GKE in version 1.28 and later.\nUse `startup` taints when there is an operation that has to complete before any Pods can run on the node. For example, Pods shouldn't run until the drivers installation on node finishes.\nCluster autoscaler treats nodes tainted with `startup` taints as unready, but taken into account during scale up logic, assuming they will become ready shortly.\nWe recommend that you don't apply the`startup`taints to the nodes for an extended period of time. The cluster autoscalerif a substantial number of nodes are tainted with startup taints, which means that they are unready. In this case, GKE doesn't scale up the cluster because even new nodes don't become ready. Therefore, GKE treats this cluster as broken.\nStartup taints are defined as all taints with the prefix `startup-taint.cluster-autoscaler.kubernetes.io/`\nUse `status` taints when GKE shouldn't use a given node to run Pods.\nCluster autoscaler treats nodes tainted with `status` taints as ready, but ignores them during scale up logic. Even though the tainted node is ready, no Pods should run. If more resources are needed by the Pods, GKE scales up the cluster and ignores the tainted nodes.\nStatus taints are defined as all taints with the prefix `status-taint.cluster-autoscaler.kubernetes.io/`\n**Deprecated: ** Ignore taints are now deprecated and treated as startup taints.\nIgnore taints are defined as all taints with the prefix `ignore-taint.cluster-autoscaler.kubernetes.io/`\n## Troubleshooting\nCheck if the issue you are running into is caused by one of the [limitations for the cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations) . Otherwise, see the following troubleshooting information for the cluster autoscaler:\n### Cluster is not downscaling\nAfter the cluster properly scales up and then attempts to scale down, underutilized nodes remain enabled and prevent the cluster from scaling down. This error occurs for one of the following reasons:\n- Restrictions can prevent a node from being deleted by the autoscaler. GKE might prevent a node's deletion if the node contains a Pod with any of these conditions:- The Pod's affinity or anti-affinity rules prevent rescheduling.\n- In GKE version 1.21 and earlier, the Pod has local storage.\n- The Pod is not managed by a Controller such as a Deployment, StatefulSet, Job or ReplicaSet.\nTo resolve this issue, set up the cluster autoscaler scheduling and eviction rules on your Pods. For more information, see [Pod scheduling and disruption](/kubernetes-engine/docs/concepts/cluster-autoscaler#scheduling-and-disruption) .\n- System Pods are running on a node. To verify that your nodes are running `kube-system` pods, perform the following steps:- Go to the **Logs Explorer** page in the Google Cloud console. [Go to Logs Explorer](https://console.cloud.google.com/logs/query) \n- Click **Query builder** .\n- Use the following query to find all network policy log records:```\n\u00a0 - resource.labels.location=\"CLUSTER_LOCATION\"\u00a0 resource.labels.cluster_name=\"CLUSTER_NAME\"\u00a0 logName=\"projects/PROJECT_ID/logs/container.googleapis.com%2Fcluster-autoscaler-visibility\"\u00a0 jsonPayload.noDecisionStatus.noScaleDown.nodes.node.mig.nodepool=\"NODE_POOL_NAME\"\n```Replace the following:- ``: The region your cluster is in.\n- ``: The name of your cluster.\n- ``: the ID of the project in which the cluster is created.\n- `` : The name of your node pool.If there are `kube-system` pods running on your node pool, the output includes the following:```\n\"no.scale.down.node.pod.kube.system.unmovable\"\n```To resolve this issue, you have to either:- Add a`PodDisruptionBudget`for the`kube-system`Pods. For more information about manually adding a`PodDisruptionBudget`for the`kube-system`Pods, see the [Kubernetes cluster autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-ca-to-move-kube-system-pods) .\n- Use a combination of node pools taints and tolerations to separate`kube-system`pods from your application pods. For more information, see [node auto-provisioning in GKE](/kubernetes-engine/docs/how-to/node-auto-provisioning#separate_system_pod) .\n### Node pool size mismatching\nThe following issue results when you configure node pool size:\n- An existing node pool size is smaller than the minimum number of nodes you specified for the cluster.\nThe following list describes the possible common causes of this behavior:\n- You specified a new minimum number of nodes when the existing number of nodes is higher.\n- You manually scaled down the node pool or the underlying Managed Instance Group. This manual operation specified the number of nodes lesser than the minimum number of nodes.\n- You deployed preempted Spot VMs within the node pool.\n- The Pod has local storage and the GKE control plane version is lower than 1.22. In GKE clusters with control plane version 1.22 or later, Pods with local storage no longer block scaling down.\n- The Pod has the annotation `\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"false\"` .For more troubleshooting steps during scale down events, refer to [Cluster not scaling down](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility#cluster-not-scalingdown) .\n- When scaling down, cluster autoscaler respects the [Pod termination grace period](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) , up to a maximum of 10 minutes. After 10 minutes, Pods are forcefully terminated.\n- You may observe a node pool size being smaller than the minimum number of nodes you specified for the cluster. This behavior happens because the autoscaler uses the minimum number of nodes parameter only when it need to determine a scaling down. These are the list of the possible common causes of this behavior.\nTo resolve this issue, manually increase the node pool size to at least the minimum number of nodes. For more information, see [how to manually resize acluster](/kubernetes-engine/docs/how-to/resizing-a-cluster#resize) .\nFor more information about the cluster autoscaler and preventing disruptions, see the following questions in the [Kubernetes cluster autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) :\n- [How does scale-down work?](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work) \n- [Does the cluster autoscaler work with PodDisruptionBudget in scale-down?](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#does-ca-work-with-poddisruptionbudget-in-scale-down) \n- [What types of Pods can prevent the cluster autoscaler from removing a node?](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node) ## What's next\n- [Learn more about cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- [View cluster autoscaler events](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility) .", "guide": "Google Kubernetes Engine (GKE)"}