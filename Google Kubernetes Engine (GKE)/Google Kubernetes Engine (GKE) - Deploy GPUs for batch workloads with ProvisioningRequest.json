{"title": "Google Kubernetes Engine (GKE) - Deploy GPUs for batch workloads with ProvisioningRequest", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/provisioningrequest", "abstract": "# Google Kubernetes Engine (GKE) - Deploy GPUs for batch workloads with ProvisioningRequest\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nThis page shows you how to optimize GPU obtainability through the [ProvisioningRequest API](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/proposals/provisioning-request.md) . We recommend this feature for large-scale batch workloads that can run during off-peak hours with defined GPU capacity management conditions. These workloads might be deep learning model training or a simulations that needs large amounts of GPUs with atomic provisioning model.\nTo run GPU workloads in Google Kubernetes Engine (GKE) without the ProvisioningRequest API, see [Run GPUs in GKE Standard node pools](/kubernetes-engine/docs/how-to/gpus) .\n#", "content": "## When to use ProvisioningRequest\nWe recommend that you use ProvisioningRequest if your workloads meet all of the following conditions:\n- You request GPUs to run your workloads.\n- You have limited or no [reserved](/kubernetes-engine/docs/how-to/consuming-reservations) GPU capacity and you want to improve obtainability of GPU resources.\n- Your workload is time-flexible and your use case can afford to wait to get all the requested capacity, for example when GKE allocates the GPU resources outside of the busiest hours.\n- For workloads requiring multiple nodes, your workload can't start running until all GPU nodes are provisioned and ready at the same time. For example, distributed machine learning training.\n- GKE provisions all GPUs nodes at the same time.\nTo learn more details about ProvisioningRequest, see [How ProvisioningRequest works](#how-provisioningrequest-works) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have an existing [Standard cluster](/kubernetes-engine/docs/how-to/creating-a-regional-cluster) in version 1.28.3-gke.1098000 or later.\n- Ensure that you have [disabled node auto-upgrades](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) to not disrupt your workloads that use ProvisioningRequest. If your cluster is enrolled in a [release channel](/kubernetes-engine/docs/concepts/release-channels) , use [maintenance windows or exclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) to disable auto-upgrades for the duration of your workload. When using ProvisioningRequest, we recommend disabling node auto-upgrades permanently.\n- Ensure that you understand the [limitations of ProvisioningRequest](#limitations) .\n- Ensure that you maintain at least one node pool without ProvisioningRequest handling enabled for the cluster to function correctly.## Create a node pool\nCreate a node pool with ProvisioningRequest enabled using the gcloud CLI:\n```\ngcloud beta container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 \u00a0--enable-queued-provisioning \\\u00a0 \u00a0 --accelerator type=GPU_TYPE,count=AMOUNT,gpu-driver-version=DRIVER_VERSION \\\u00a0 \u00a0 --enable-autoscaling \u00a0\\\u00a0 \u00a0 --num-nodes=0 \u00a0 \\\u00a0 \u00a0 --total-max-nodes TOTAL_MAX_NODES \u00a0\\\u00a0 \u00a0 --location-policy=ANY \u00a0\\\u00a0 \u00a0 --reservation-affinity=none \u00a0\\\u00a0 \u00a0 --no-enable-autorepair\n```\nReplace the following:\n- ``: The name you choose for the node pool.\n- ``: The name of the cluster.\n- ``: The cluster's Compute Engine region, such as`us-central1`. Choose a region that has at least one zone where the requested GPUs are available.\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The [GPU type](/kubernetes-engine/docs/how-to/gpus#overview) .\n- ``: The number of GPUs to attach to nodes in the node pool.\n- ``: the NVIDIA driver version to install. Can be one of the following:- `default`: Install the default driver version for your GKE version.\n- `latest`: Install the latest available driver version for your GKE version. Available only for nodes that use Container-Optimized OS.\n- ``: the maximum number of nodes to automatically scale for the entire node pool.\nOptionally, you can use the following flags:\n- `--no-enable-autoupgrade`: Recommended. Disables node auto-upgrades. Supported only in GKE clusters not currently enrolled in a release channel. To learn more, see [Disable node auto-upgrades for an existing node pool](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) .\n- `--node-locations=```: The specific zones where GKE creates the GPU nodes. The zones must be in the same region as the cluster.\n- `--machine-type=```: The Compute Engine machine type for the nodes. Required if``is`tesla-a100`or`nvidia-a100-80gb`, which can only use an A2 machine type, or if``is`nvidia-l4`, which can only use a G2 machine type. For all other GPUs, this flag is optional.\n- `--enable-gvnic`: This flag enables [gVNIC on the GPU node pools](/kubernetes-engine/docs/how-to/using-gvnic) to increase network traffic speed.\nThis command creates a node pool with the following configuration:\n- GKE enables the queued provisioning and cluster autoscaling.\n- The node pool initially has zero nodes.\n- The`--enable-queued-provisioning`flag enables ProvisioningRequests and adds the`cloud.google.com/gke-queued`taint to the node pool.\n- The`--no-enable-autorepair`and`--no-enable-autoupgrade`flags disable automatic repair and upgrade of nodes, which could disrupt workloads running on repaired or upgraded nodes. You can only disable node auto-upgrade on clusters that are not enrolled in a release channel.## Update existing node pool and enable ProvisioningRequests\n### Prerequisites\n- Ensure that you create a node pool with the `--reservation-affinity=none` flag. This flag is essential for enabling ProvisioningRequests later, as you can't change the reservation affinity after node pool creation.\n- Ensure that you maintain at least one node pool without ProvisioningRequest handling enabled for the cluster to function correctly.\n- Ensure that the node pool is empty. You can [resize the node pool](/kubernetes-engine/docs/how-to/cluster-autoscaler#resizing_a_node_pool) so that it has zero nodes.\n- Ensure that [autoscaling](/kubernetes-engine/docs/how-to/cluster-autoscaler#enable_autoscaling) is enabled and correctly configured.\n- Ensure that [autorepairs](/kubernetes-engine/docs/how-to/node-auto-repair#disable) are disabled.\n### Enable ProvisioningRequests for existing nodepool\nYou can enable ProvisioningRequest for an existing node pool using the gcloud CLI:\n```\ngcloud beta container node-pools update NODEPOOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 \u00a0--enable-queued-provisioning\n```\nReplace the following:\n- ``: name of the chosen node pool.\n- ``: name of the cluster.\n- ``: cluster's Compute Engine region, such as`us-central1`.\nThis node pool update command results in following configuration changes:\n- The`--enable-queued-provisioning`flag enables ProvisioningRequests and adds the`cloud.google.com/gke-queued`taint to the node pool.\nOptionally, you can also update the following node pool settings:\n- [Disable node auto-upgrades](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) : We recommend that you disable node auto-upgrades as node pool upgrades are not supported when using ProvisioningRequests. To disable upgrades, ensure your GKE cluster is not enrolled in a release channel.\n- Enable [gVNIC on the GPU node pools](/kubernetes-engine/docs/how-to/using-gvnic) : Google Virtual NIC (gVNIC) increases network traffic speed for GPU nodes.## Run your batch workloads with ProvisioningRequest\nTo use ProvisioningRequest, we recommend that you use [Kueue](https://kueue.sigs.k8s.io/) . Kueue implements Job queueing, deciding when Jobs should wait and when they should start, based on quotas and a hierarchy for sharing resources fairly among teams. This simplifies the setup needed to use queued VMs.\nYou can use ProvisioningRequest Kueue when you use your own internal batch scheduling tools or platform. To configure ProvisioningRequests for Jobs without Kueue, see [ProvisioningRequests for Jobs without Kueue](#create-provisioningrequest) .\n### ProvisioningRequests for Jobs with Kueue\nThe following section shows you how to configure the ProvisioningRequests for Jobs with Kueue. This section uses the samples in the `gke-dws-examples` repo. We have published the samples in the `gke-dws-examples` repo under the Apache2 license.- In Cloud Shell, run the following command:```\ngit clone https://github.com/GoogleCloudPlatform/ai-on-gkecd ai-on-gke/gke-dws-examples\n```\n- Install Kueue in your cluster with necessary configuration to enable Provisioning Request integration:```\nkubectl apply --server-side -f ./kueue-manifests.yaml\n```\nTo learn more about Kueue installation, see [Installation](https://kueue.sigs.k8s.io/docs/installation/) .\nWith the following manifest, you create a [cluster-level queue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/) named `dws-cluster-queue` and the [LocalQueue](https://kueue.sigs.k8s.io/docs/concepts/local_queue/) namespace named `dws-local-queue` . Jobs that refer to `dws-cluster-queue` queue in this namespace use ProvisioningRequests to get the GPU resources.\n[  gke-dws-examples/dws-queues.yaml ](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-dws-examples/dws-queues.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-dws-examples/dws-queues.yaml)\n```\napiVersion: kueue.x-k8s.io/v1beta1kind: ResourceFlavormetadata:\u00a0 name: \"default-flavor\"---apiVersion: kueue.x-k8s.io/v1beta1kind: AdmissionCheckmetadata:\u00a0 name: dws-provspec:\u00a0 controllerName: kueue.x-k8s.io/provisioning-request\u00a0 parameters:\u00a0 \u00a0 apiGroup: kueue.x-k8s.io\u00a0 \u00a0 kind: ProvisioningRequestConfig\u00a0 \u00a0 name: dws-config---apiVersion: kueue.x-k8s.io/v1beta1kind: ProvisioningRequestConfigmetadata:\u00a0 name: dws-configspec:\u00a0 provisioningClassName: queued-provisioning.gke.io\u00a0 managedResources:\u00a0 - nvidia.com/gpu---apiVersion: kueue.x-k8s.io/v1beta1kind: ClusterQueuemetadata:\u00a0 name: \"dws-cluster-queue\"spec:\u00a0 namespaceSelector: {} \u00a0 resourceGroups:\u00a0 - coveredResources: [\"cpu\", \"memory\", \"nvidia.com/gpu\"]\u00a0 \u00a0 flavors:\u00a0 \u00a0 - name: \"default-flavor\"\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 - name: \"cpu\"\u00a0 \u00a0 \u00a0 \u00a0 nominalQuota: 10000 \u00a0# Infinite quota.\u00a0 \u00a0 \u00a0 - name: \"memory\"\u00a0 \u00a0 \u00a0 \u00a0 nominalQuota: 10000Gi # Infinite quota.\u00a0 \u00a0 \u00a0 - name: \"nvidia.com/gpu\"\u00a0 \u00a0 \u00a0 \u00a0 nominalQuota: 10000 \u00a0# Infinite quota.\u00a0 admissionChecks:\u00a0 - dws-prov---apiVersion: kueue.x-k8s.io/v1beta1kind: LocalQueuemetadata:\u00a0 namespace: \"default\"\u00a0 name: \"dws-local-queue\"spec:\u00a0 clusterQueue: \"dws-cluster-queue\"--```\n**Note:** This cluster's queue has high quota limits and only the ProvisioningRequests integration feature is enabled. To learn more about Kueue APIs and how to set up limits, see [Kueue concepts](https://kueue.sigs.k8s.io/docs/concepts/) .\nDeploy the LocalQueue:\n```\nkubectl create -f ./dws-queues.yaml\n```\nThe output is similar to the following:\n```\nresourceflavor.kueue.x-k8s.io/default-flavor created\nadmissioncheck.kueue.x-k8s.io/dws-prov created\nprovisioningrequestconfig.kueue.x-k8s.io/dws-config created\nclusterqueue.kueue.x-k8s.io/dws-cluster-queue created\nlocalqueue.kueue.x-k8s.io/dws-local-queue created\n```\nIf you want to run Jobs that use ProvisioningRequests in other namespaces, you can create additional LocalQueues using the preceding template.\nIn the following manifest, the sample Job uses ProvisioningRequest:\n[  gke-dws-examples/job.yaml ](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-dws-examples/job.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-dws-examples/job.yaml)\n```\napiVersion: batch/v1kind: Jobmetadata:\u00a0 name: sample-job\u00a0 namespace: default\u00a0 labels:\u00a0 \u00a0 kueue.x-k8s.io/queue-name: dws-local-queuespec:\u00a0 parallelism: 1\u00a0 completions: 1\u00a0 suspend: true\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-nodepool: dws-nodepool\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"nvidia.com/gpu\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: dummy-job\u00a0 \u00a0 \u00a0 \u00a0 image: gcr.io/k8s-staging-perf-tests/sleep:v0.0.3\u00a0 \u00a0 \u00a0 \u00a0 args: [\"120s\"]\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"100m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"100Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"100m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"100Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 restartPolicy: Never\n```\nThis manifest includes the following fields that are relevant for the ProvisioningRequest configuration:\n- The`kueue.x-k8s.io/queue-name: dws-local-queue`label tells GKE that Kueue is responsible for orchestrating that Job. This label also defines the queue where the Job is queued.\n- The flag`suspend: true`tells GKE to create the Job resource but to not schedule the Pods yet. Kueue changes that flag to`false`when the nodes are ready for the Job execution.\n- Run your Job:```\nkubectl create -f ./job.yaml\n```The output is similar to the following:```\njob.batch/sample-job created\n```\n- Check the status of your Job:```\nkubectl describe job sample-job\n```The output is similar to the following:```\nEvents:\n Type Reason   Age From      Message\n ---- ------   ---- ----      ------ Normal Suspended   5m17s job-controller    Job suspended\n Normal CreatedWorkload 5m17s batch/job-kueue-controller Created Workload: default/job-sample-job-7f173\n Normal Started   3m27s batch/job-kueue-controller Admitted by clusterQueue dws-cluster-queue\n Normal SuccessfulCreate 3m27s job-controller    Created pod: sample-job-9qsfd\n Normal Resumed   3m27s job-controller    Job resumed\n Normal Completed   12s job-controller    Job completed\n```\nThe ProvisioningRequest with Kueue integration also supports other workload types available in the open source ecosystem, like the following:\n- RayJob\n- JobSet\n- Kubeflow MPIJob, TFJob, PyTorchJob.\n- Kubernetes Pods that are frequently used by workflow orchestrators\n- Flux mini cluster\nTo learn more about this support, see [Kueue's batch user](https://kueue.sigs.k8s.io/docs/tasks/#batch-user) .\n### ProvisioningRequests for Jobs without Kueue\nCreate a request through the ProvisioningRequest API for each Job. ProvisioningRequest doesn't start the Pods, it only provisions the nodes.\n- Create the following `provisioning-request.yaml` manifest:```\napiVersion: v1kind: PodTemplatemetadata:\u00a0 name: POD_TEMPLATE_NAME\u00a0 namespace: NAMESPACE_NAMEtemplate:\u00a0 spec:\u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-nodepool: NODEPOOL_NAME\u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 \u00a0 - key: \"nvidia.com/gpu\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 - name: pi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: perl\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sh\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"700m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"700m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 restartPolicy: Never---apiVersion: autoscaling.x-k8s.io/v1beta1kind: ProvisioningRequestmetadata:\u00a0 name: PROVISIONING_REQUEST_NAME\u00a0 namespace: NAMESPACE_NAMEspec:\u00a0 provisioningClassName: queued-provisioning.gke.io\u00a0 podSets:\u00a0 - count: COUNT\u00a0 \u00a0 podTemplateRef:\u00a0 \u00a0 \u00a0 name: POD_TEMPLATE_NAME\n```Replace the following:- ``: The name of your Kubernetes namespace. The namespace must be the same as the namespace of the Pods.\n- ``: Standard Kubernetes name. You refer to this name in the Pod annotation.\n- ``: Number of Pods requested. The nodes are scheduled atomically in one zone.\n- ``: A Kubernetes' standard name. GKE references to this value in the Provisioning Request PodSet.\n- ``: The name you choose for the node pool.\n- Apply the manifest:```\nkubectl apply -f provisioning-request.yaml\n```In the [JobSet](https://github.com/kubernetes-sigs/jobset) spec, link the Pods to the ProvisioningRequest using the following annotations:\n```\napiVersion: batch/v1kind: Jobspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 annotations:\u00a0 \u00a0 \u00a0 \u00a0 cluster-autoscaler.kubernetes.io/consume-provisioning-request: PROVISIONING_REQUEST_NAME\u00a0 \u00a0 \u00a0 \u00a0 cluster-autoscaler.kubernetes.io/provisioning-class-name: \"queued-provisioning.gke.io\"\n```\n**Note:** This section uses [Kubernetes Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/) or [JobSet](https://github.com/kubernetes-sigs/jobset) to configure the Pods. However, you can also use any other frameworks like Kubeflow, Ray, or custom controllers.\nThe Pod annotation key `cluster-autoscaler.kubernetes.io/consume-provisioning-request` defines which ProvisioningRequest to consume. GKE uses the `consume-provisioning-request` and `provisioning-class-name` annotations to do the following:\n- To schedule the Pods only in the nodes provisioned by ProvisioningRequest.\n- To avoid double counting of resource requests between Pods and ProvisioningRequests in the cluster autoscaler.\n- To inject`safe-to-evict: false`annotation, to prevent the cluster autoscaler from moving Pods between nodes and interrupting batch computations. You can change this behavior by specifying`safe-to-evict: true`in the Pod annotations.## Observe the status of ProvisioningRequest\nThe status of a ProvisioningRequest defines if a Pod can be scheduled or not. You can use [Kubernetes watches](https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) to observe changes efficiently or other tooling you already use for tracking statuses of Kubernetes objects. The following table describes the possible status of a ProvisioningRequest and each possible outcome:\n| ProvisioningRequest status | Description                  | Possible outcome                                      |\n|:-----------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Pending      | The request was not seen and processed yet.          | After processing, the request transitions to Accepted or Failed state.                         |\n| Accepted=true    | The request is accepted and is waiting for resources to be available.   | The request should transition to Provisioned state, if resources were found and nodes were provisioned or to Failed state if that was not possible.     |\n| Provisioned=true    | The nodes are ready.                | You have 10 minutes to Start the Pods to consume provisioned resources. After this time, the cluster autoscaler considers the nodes as not needed and removes them. |\n| Failed=true     | The nodes cannot be provisioned due to errors. Failed=true is a terminal state. | Troubleshoot the condition based on the information in the Reason and Message fields of the condition. Create and retry a new ProvisioningRequest.     |\nWhen the ProvisioningRequest reaches the `Provisioned=true` status, you can [run your Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/#running-an-example-job) to start the Pods. This avoids proliferation of unschedulable Pods for pending or failed requests, which can impact [kube-scheduler](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/) and cluster autoscaler performance.\n**Note:** If unschedulable Pods aren't a concern, the alternative is to create Pods in parallel with ProvisioningRequest.\n## Cancel ProvisioningRequest request\nTo cancel the request before it's provisioned, you can delete the ProvisioningRequest:\n```\nkubectl delete provreq PROVISIONING_REQUEST_NAME -n NAMESPACE\n```\nIn most cases, deleting ProvisioningRequest stops nodes from being created. However, depending on timing, for example if nodes were already being provisioned, the nodes might still end up created. In these cases, the cluster autoscaler removes the nodes after 10 minutes if no Pods are created.\n## How ProvisioningRequest works\nWith the ProvisioningRequest API, the following steps happen:\n- You tell GKE that your workload can wait, for an indeterminate amount of time, until all the required nodes are ready to use at once.\n- The cluster autoscaler accepts your request and calculates the number of necessary nodes, treating them as a single unit.\n- The request waits until all needed resources are available in a single zone.\n- The cluster autoscaler provisions the necessary nodes when available, all at once.\n- All pods of the workload are able to run together on newly provisioned nodes.\n- The provisioned nodes are limited to seven days of runtime. After this time, the nodes and the Pods running on them are [preempted](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/) . If the Pods finish sooner and the nodes aren't utilized, the cluster autoscaler removes them according to the [autoscaling profile](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles) .\n- The nodes aren't reused between ProvisioningRequests. Each ProvisioningRequest will order creation of new nodes with the fresh seven day runtime.\n- GKE measures runtime on a node level. The time available for running  Pods might be slightly smaller due to delays during startup. Pod retries share this runtime, which means that there is less time available for Pods after retry. GKE counts the runtime for each  ProvisioningRequest separately. [\u21a9](#fnref1) ## Quota\nThe number of Provisioning Requests that are in `Accepted` state is limited by a dedicated quota, configured per project, independently per each region. To check the name of the quota limit and current usage, follow these steps:\n- Go to the **Quotas** page in the Google Cloud console: [Go to Quotas](https://console.cloud.google.com/iam-admin/quotas) \n- In the filter_list **Filter** box, select the **Metric** property, enter `active_resize_requests` , and press Enter.\nThe default value is 100. To increase the quota follow steps listed in [Request a higher quota limit guide](/docs/quota/view-manage#requesting_higher_quota) .\n## Limitations\n- [Inter-pod anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) is not supported. Cluster autoscaler does not take inter-pod anti-affinity rules into account during node provisioning which may lead to unschedulable workload. This may happen when nodes for two or more ProvisioningRequest objects were provisioned in the same node pool.\n- Changing the max run duration of the nodes isn't supported. The ProvisioningRequest API doesn't allow users to change this duration. If the Job completes earlier, the cluster autoscaler removes the idle nodes automatically. The latency of the node removal dependens on the [autoscaling profile](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles) you use.\n- For example, node upgrades require restarting Pods on newly created nodes. To solve this limitation, disable the node auto-upgrade with the`--no-enable-autoupgrade`flag. In order to use the`--no-enable-autoupgrade`flag, don't enroll the cluster to any release channel.\n- Only GPU VMs are supported.\n- Reservations aren't supported with ProvisioningRequest nodes. You have to specify`--reservation-affinity=none`when creating the node pool. ProvisioningRequest requires and supports only the 'ANY' [location policy](/kubernetes-engine/docs/concepts/cluster-autoscaler#location_policy) for cluster autoscaling.\n- A single ProvisioningRequest can create up to 1000 VMs, which is the maximum number of nodes per zone for a single node pool.\n- GKE uses the Compute Engine`ACTIVE_RESIZE_REQUESTS`quota to control the number of ProvisioningRequest pending in a queue. By default, this quota has a 100 limit on a Google Cloud project level. If you attempt to create a ProvisioningRequest above this quota, the new request fails.\n- Workloads configured as a unit are sensitive to evictions. Automatic repair or upgrade of a node provisioned using ProvisioningRequest API preempts all workloads running on that node and makes them unschedulable. Therefore, node repair and node upgrade aren't supported in nodes that use the ProvisioningRequest API. Disable the node auto-repair with the`--no-enable-autorepair`flag and node auto-upgrade with the`--no-enable-autoupgrade`flag. Disabling node auto-upgrade is only possible on clusters that are not currently enrolled in a [release channel](/kubernetes-engine/docs/concepts/release-channels) .\n- You might see additional short-lived VMs listed in the Google Cloud console. This behavior is intended because Compute Engine might create and promptly remove VMs until the capacity to provision all of the required machines is available.## What's next\n- Learn more about [GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) .\n- Learn how to [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .", "guide": "Google Kubernetes Engine (GKE)"}