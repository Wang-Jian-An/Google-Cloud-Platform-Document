{"title": "Google Kubernetes Engine (GKE) - Control communication between Pods and Services using network policies", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy", "abstract": "# Google Kubernetes Engine (GKE) - Control communication between Pods and Services using network policies\nThis page explains how to control communication between your cluster's Pods and Services using GKE's .\nYou can also control Pods' egress traffic to any endpoint or Service outside of the cluster using fully qualified domain name (FQDN) network policies. For more information, see [Control communication between Pods and Services using FQDNs](/kubernetes-engine/docs/how-to/fqdn-network-policies) .\n", "content": "## About GKE network policy enforcement\nNetwork policy enforcement lets you create Kubernetes [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) in your cluster. Network policies create Pod-level firewall rules that determine which Pods and Services can access one another inside your cluster.\nDefining network policy helps you enable things like [defense in depth](https://en.wikipedia.org/wiki/Defense_in_depth_(computing)) when your cluster is serving a multi-level application. For example, you can create a network policy to ensure that a compromised front-end service in your application cannot communicate directly with a billing or accounting service several levels down.\nNetwork policy can also make it easier for your application to host data from multiple users simultaneously. For example, you can provide secure multi-tenancy by defining a tenant-per-namespace model. In such a model, network policy rules can ensure that Pods and Services in a given namespace cannot access other Pods or Services in a different namespace.\n**Note:** For network policy enforcement to function correctly, GKE deploys Pods to your nodes that have elevated [RBAC permissions](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-the-request-verb) , such as the ability to patch all deployments and update the status of nodes. These permissions are required for GKE to enforce your network policies based on your configuration.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Requirements and limitations\n- You must [allow egress to the metadata server](#network-policy-and-workload-identity) .\n- If you specify an`endPort`field in a Network Policy on a cluster that has GKE Dataplane V2 enabled, it might not take effect starting in GKE version 1.22. For more information, see [Network Policy port ranges don't take effect](/kubernetes-engine/docs/how-to/dataplane-v2#network-policy-dataplane) . For Autopilot clusters, GKE Dataplane V2 is always enabled.- You must allow egress to the metadata server if you use network policy with [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) .\n- Enabling network policy enforcement increases the memory footprint of the`kube-system`process by approximately 128 MB, and requires approximately 300 millicores of CPU. This means that if you enable network policies for an existing cluster, you might need to [increase the cluster's size](/kubernetes-engine/docs/how-to/resizing-a-cluster#resize) to continue running your scheduled workloads.\n- Enabling network policy enforcement requires that your nodes be re-created. If your cluster has an active [maintenance window](/kubernetes-engine/docs/how-to/maintenance-window) , your nodes are not automatically re-created until the next maintenance window. If you prefer, you can [manually upgrade your cluster](/kubernetes-engine/docs/how-to/upgrading-a-container-cluster) at any time.\n- Theminimum cluster size to run network policy enforcement is three`e2-medium`instances.\n- Network policy is not supported for clusters whose nodes are`f1-micro`or`g1-small`instances, as the resource requirements are too high.\nFor more information about node machine types and allocatable resources, see [Standard cluster architecture - Nodes](/kubernetes-engine/docs/concepts/cluster-architecture#nodes) .\n**Caution:** By default, enabling network policy enforcement for your cluster also enables enforcement for the cluster's nodes and might cause a disruption for manually deployed Pods. For more information, see [Manually deployed Pods unscheduled](#unscheduled-pods) .\n## Enable network policy enforcement\nNetwork policy enforcement is enabled by default for Autopilot clusters, so you can skip to [Create a network policy](#creating_a_network_policy) .\nYou can enable network policy enforcement in Standard by using the gcloud CLI, the Google Cloud console, or the GKE API.\nNetwork policy enforcement is built into [GKE Dataplane V2](/kubernetes-engine/docs/concepts/dataplane-v2) . You do not need to enable network policy enforcement in clusters that use GKE Dataplane V2.\n**Note:** If you enable or disable network policy enforcement for an existing cluster, GKE automatically re-creates all that cluster's node pool configurations before the cluster can run the network policy process. For more information, see [caveats and limitations](#limitations_and_requirements) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- To enable network policy enforcement , run the following command:```\ngcloud container clusters create CLUSTER_NAME --enable-network-policy\n```Replace `` with the name of the new cluster.To enable network policy enforcement , perform the following tasks:- Run the following command to enable the add-on:```\ngcloud container clusters update CLUSTER_NAME --update-addons=NetworkPolicy=ENABLED\n```Replace `` with the name of the cluster.\n- Run the following command to enable network policy enforcement on your cluster, which in turn re-creates your cluster's node pools with network policy enforcement enabled:```\ngcloud container clusters update CLUSTER_NAME --enable-network-policy\n```\nTo enable network policy enforcement :- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- In the **Create cluster** dialog, for GKE Standard, click **Configure** .\n- Configure your cluster as chosen.\n- From the navigation pane, under **Cluster** , click **Networking** .\n- Select the **Enable network policy** checkbox.\n- Click **Create** .\nTo enable network policy enforcement :- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Networking** , in the **Network policy** field, click **Edit network policy** .\n- Select the **Enable network policy for master** checkbox and click **Save Changes** .\n- Wait for your changes to apply, and then click **Edit network policy** again.\n- Select the **Enable network policy for nodes** checkbox.\n- Click **Save Changes** .\nTo enable network policy enforcement, perform the following:- Specify the `networkPolicy` object inside the `cluster` object that you provide to [projects.zones.clusters.create](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/create) or [projects.zones.clusters.update](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/update) .\n- The `networkPolicy` object requires an enum that specifies which network policy provider to use, and a boolean value that specifies whether to enable network policy. If you enable network policy but do not set the provider, the `create` and `update` commands return an error.## Disable network policy enforcement in a Standard cluster\nYou can disable network policy enforcement by using the gcloud CLI, the Google Cloud console, or the GKE API. You cannot disable network policy enforcement in Autopilot clusters or clusters that use GKE Dataplane V2.\n**Warning:** Disabling network policy enforcement is a disruptive operation. All cluster nodes re-created according to the GKE [node upgrade process](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- To disable network policy enforcement, perform the following tasks:- Disable network policy enforcement on your cluster:\n```\ngcloud container clusters update CLUSTER_NAME --no-enable-network-policy\n```Replace `` with the name of the cluster.After you run this command, GKE re-creates your cluster node pools with network policy enforcement disabled.\n- Verify that all your nodes were re-created:```\nkubectl get nodes -l projectcalico.org/ds-ready=true\n```If the operation is successful, the output is similar to the following:```\nNo resources found\n```If the output is similar to the following, then you must wait for GKE to finish updating the node pools:```\nNAME            STATUS      ROLES AGE  VERSION\ngke-calico-cluster2-default-pool-bd997d68-pgqn Ready,SchedulingDisabled <none> 15m  v1.22.10-gke.600\ngke-calico-cluster2-np2-c4331149-2mmz   Ready      <none> 6m58s v1.22.10-gke.600\n```When you disable network policy enforcement, GKE might not update the nodes immediately if your cluster has a configured maintenance window or exclusion. For more information, see [Cluster slow to update](#delay) .\n- After all of the nodes are re-created, disable the add-on:```\ngcloud container clusters update CLUSTER_NAME --update-addons=NetworkPolicy=DISABLED\n```\nTo disable network policy enforcement for an existing cluster, perform the following:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Networking** , in the **Network policy** field, click **Edit network policy** .\n- Clear the **Enable network policy for nodes** checkbox and click **Save Changes** .\n- Wait for your changes to apply, and then click **Edit network policy** again.\n- Clear the **Enable network policy for master** checkbox.\n- Click **Save Changes** .\nTo disable network policy enforcement for an existing cluster, do the following:- Update your cluster to use `networkPolicy.enabled: false` using the [setNetworkPolicy API](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/setNetworkPolicy) .\n- Verify that all your nodes were re-created using the gcloud CLI:```\nkubectl get nodes -l projectcalico.org/ds-ready=true\n```If the operation is successful, the output is similar to the following:```\nNo resources found\n```If the output is similar to the following, then you must wait for GKE to finish updating the node pools:```\nNAME            STATUS      ROLES AGE  VERSION\ngke-calico-cluster2-default-pool-bd997d68-pgqn Ready,SchedulingDisabled <none> 15m  v1.22.10-gke.600\ngke-calico-cluster2-np2-c4331149-2mmz   Ready      <none> 6m58s v1.22.10-gke.600\n```When you disable network policy enforcement, GKE might not update the nodes immediately if your cluster has a configured maintenance window or exclusion. For more information, see [Cluster slow to update](#delay) .\n- Update your cluster to use `update.desiredAddonsConfig.NetworkPolicyConfig.disabled: true` using the [updateCluster API](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/update) .## Create a network policy\nYou can create a network policy using the Kubernetes Network Policy API.\nFor further details on creating a network policy, see the following topics in the [Kubernetes documentation](https://kubernetes.io/docs) :\n- [Network Policy Overview](https://kubernetes.io/docs/concepts/services-networking/network-policies/) \n- [Declare Network Policy](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/) ## Network policy and workload identity federation for GKE\nIf you use network policy with workload identity federation for GKE, you must allow egress to the following IP addresses so your Pods can communicate with the [GKE metadata server](/kubernetes-engine/docs/concepts/workload-identity#metadata_server) .\n- For clusters running GKE version 1.21.0-gke.1000 and later, allow egress to`169.254.169.252/32`on port`988`.\n- For clusters running GKE versions earlier than 1.21.0-gke.1000, allow egress to`127.0.0.1/32`on port`988`.\n- For clusters running GKE Dataplane V2, allow egress to`169.254.169.254/32`on port`80`.\nIf you don't allow egress to these IP addresses and ports, you might experience disruptions during auto-upgrades.\n## Migrating from Calico to GKE Dataplane V2\nIf you migrate your network policies from [Calico](https://github.com/projectcalico/calico) to GKE Dataplane V2, consider the following limitations:\n- You cannot use a Pod or Service IP address in the `ipBlock.cidr` field of a `NetworkPolicy` manifest. You must reference workloads using labels. For example, the following configuration is invalid:```\n- ipBlock:\n cidr: 10.8.0.6/32\n```\n- You cannot specify an empty `ports.port` field in a `NetworkPolicy` manifest. If you specify a protocol, you must also specify a port. For example, the following configuration is invalid:```\ningress:\n- ports:\n - protocol: TCP\n```## Working with Application Load Balancers\nWhen an Ingress is applied to a Service to build an Application Load Balancer, you must configure the network policy applied to Pods behind that Service to allow the appropriate [Application Load Balancer health check IP ranges](/load-balancing/docs/health-checks) . If you are using an internal Application Load Balancer, you must also configure the network policy to allow the [proxy-only subnet](/load-balancing/docs/l7-internal/troubleshooting-l7-ilb#source-address) .\nIf you are not using container-native load balancing with network endpoint groups, node ports for a Service might forward connections to Pods on other nodes unless they are prevented from doing so by setting [externalTrafficPolicy to Local](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/) in the Service definition. If `externalTrafficPolicy` is not set to `Local` , the network policy must also allow connections from other node IPs in the cluster.\n## Inclusion of Pod IP ranges in ipBlock rules\nTo control traffic for specific Pods, always select Pods by their namespace or Pod labels by using `namespaceSelector` and `podSelector` fields in your NetworkPolicy ingress or egress rules. Don't use the `ipBlock.cidr` field to intentionally select Pod IP address ranges, which are ephemeral in nature. The Kubernetes project doesn't explicitly define the behavior of the `ipBlock.cidr` field when it includes Pod IP address ranges. Specifying broad CIDR ranges in this field, like `0.0.0.0/0` (which include the Pod IP address ranges) might have unexpected results in different implementations of NetworkPolicy.\n[migrate](#calico_migrate)\n### ipBlock behavior in GKE Dataplane V2\nWith the GKE Dataplane V2 implementation of NetworkPolicy, Pod traffic is never covered by an `ipBlock` rule. Therefore, even if you define a broad rule such as `cidr: '0.0.0.0/0'` , it will not include Pod traffic. This is useful as it lets you to, for example, allow Pods in a namespace to receive traffic from the internet, without also allowing traffic from Pods. To also include Pod traffic, select Pods explicitly using an additional Pod or namespace selector in the ingress or egress rule definitions of the NetworkPolicy.\n### ipBlock behavior in Calico\nFor the Calico implementation of NetworkPolicy, the `ipBlock` rules **do** cover Pod traffic. With this implementation, to configure a broad CIDR range without allowing Pod traffic, explicitly exclude the cluster's Pod CIDR range, like in the following example:\n```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n name: allow-non-pod-traffic\nspec:\n ingress:\n - from:\n - ipBlock:\n  cidr: '0.0.0.0/0'\n  except: ['POD_IP_RANGE']\n```\nIn this example, `` is your cluster's Pod IPv4 address range, for example `10.95.0.0/17` . If you have multiple IP ranges, these can be included individually in the array, for example `['10.95.0.0/17', '10.108.128.0/17']` .\n## Troubleshooting\n### Pods can't communicate with control plane on clusters that use Private Service Connect\nPods on GKE clusters that use [Private Service Connect](/vpc/docs/private-service-connect) might experience a communication issue with the control plane if the Pod's egress to the control plane's internal IP address is restricted in egress network policies.\nTo mitigate this issue:\n- Find if your cluster uses Private Service Connect. For more information, see [Public clusters withPrivate Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) . On clusters that use Private Service Connect, each control plane is assigned to an internal IP address from the cluster node subnet.\n- Configure your cluster's egress policy to allow traffic to the control plane's internal IP address.To find the control plane's internal IP address:\nTo look for `privateEndpoint` , run the following command:\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nReplace `` with the name of the cluster.\nThis command retrieves the `privateEndpoint` of the specified cluster.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- From the navigation pane, under **Clusters** , click the cluster whose internal IP address you want to find.\n- Under **Cluster basics** , navigate to `Internal endpoint` , where the internal IP address is listed.\nOnce you are able to locate the `privateEndpoint` or `Internal endpoint` , configure your cluster's egress policy to allow traffic to the control plane's internal IP address. For more information, see [Create a networkpolicy](/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy) .\n### Cluster slow to update\nWhen you enable or disable network policy enforcement on an existing cluster, GKE might not update the nodes immediately if the cluster has a configured [maintenance window or exclusion](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) .\n`--cluster-version`\n[caveats for maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#caveats)\n### Manually deployed Pods unscheduled\nWhen you enable network policy enforcement on the control plane of existing cluster, GKE unschedules any ip-masquerade-agent or calico node Pods that you manually deployed.\nGKE does not reschedule these Pods until network policy enforcement is enabled on the cluster nodes and the nodes are recreated.\nIf you have configured a [maintenance window or exclusion](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) , this might cause an extended disruption.\nTo minimize the duration of this disruption, you can manually assign the following labels to the cluster nodes:\n- `node.kubernetes.io/masq-agent-ds-ready=true`\n- `projectcalico.org/ds-ready=true`\n### Network policy not taking effect\nIf a NetworkPolicy is not taking effect, you can troubleshoot using the following steps:\n- Confirm that network policy enforcement is enabled. The command that you use depends on if your cluster has GKE Dataplane V2 enabled.If your cluster has GKE Dataplane V2 enabled, run the following command:```\nkubectl -n kube-system get pods -l k8s-app=cilium\n```If the output is empty, network policy enforcement is not enabled.If your cluster does not have GKE Dataplane V2 enabled, run the following command:```\nkubectl get nodes -l projectcalico.org/ds-ready=true\n```If the output is empty, network policy enforcement is not enabled.\n- Check the Pod labels:```\nkubectl describe pod POD_NAME\n```Replace `` with the name of the Pod.The output is similar to the following:```\nLabels:  app=store\n    pod-template-hash=64d9d4f554\n    version=v1\n```\n- Confirm that the labels on the policy match the labels on the Pod:```\nkubectl describe networkpolicy\n```The output is similar to the following:```\nPodSelector: app=store\n```In this output, the `app=store` labels match the `app=store` labels from the previous step.\n- Check if there are any network policies selecting your workloads:```\nkubectl get networkpolicy\n```If the output is empty, no NetworkPolicy was created in the namespace and nothing is selecting your workloads. If the output is not empty, check if the policy selects your workloads:```\nkubectl describe networkpolicy\n```The output is similar to the following:```\n...\nPodSelector:  app=nginx\nAllowing ingress traffic:\n To Port: <any> (traffic allowed to all ports)\n From:\n  PodSelector: app=store\nNot affecting egress traffic\nPolicy Types: Ingress\n```## Known issues\n### StatefulSet pod termination with Calico\nGKE clusters with [Calico](https://github.com/projectcalico/calico) network policy enabled might experience an issue where a StatefulSet pod drops existing connections when the pod is deleted. After a pod enters the `Terminating` state, the `terminationGracePeriodSeconds` configuration in the pod spec is not honored and causes disruptions for other applications that have an existing connection with the StatefulSet pod. For more information about this issue, see [Calico issue #4710](https://github.com/projectcalico/calico/issues/4710) .\nThis issue affects the following GKE versions:\n- 1.18\n- 1.19 to 1.19.16-gke.99\n- 1.20 to 1.20.11-gke.1299\n- 1.21 to 1.21.4-gke.1499\nTo mitigate this issue, upgrade your GKE control plane to one of the following versions:\n- 1.19.16-gke.100 or later\n- 1.20.11-gke.1300 or later\n- 1.21.4-gke.1500 or later\n### Pod stuck in containerCreating state\nThere can be scenario where GKE clusters with [Calico](https://github.com/projectcalico/calico) network policy enabled might experience an issue where Pods get stuck in `containerCreating` state.\nUnder the Pod **Events** tab, you see a message similar to the following:\n```\nplugin type=\"calico\" failed (add): ipAddrs is not compatible with\nconfigured IPAM: host-local\n```\nTo mitigate this issue, use host-local ipam for Calico instead of calico-ipam in GKE clusters.\n## What's next\n- [Follow the Network Policies tutorial](/kubernetes-engine/docs/tutorials/network-policy) .\n- [Read the Kubernetes documentation about network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) .\n- [Implement common approaches to restrict traffic using network policies](https://github.com/GoogleCloudPlatform/anthos-security-blueprints/tree/master/restricting-traffic) .\n- [Using network policy logging](/kubernetes-engine/docs/how-to/network-policy-logging) .\n- [Use security insights to explore other ways to harden your infrastructure](/anthos/docs/concepts/security-monitoring) .", "guide": "Google Kubernetes Engine (GKE)"}