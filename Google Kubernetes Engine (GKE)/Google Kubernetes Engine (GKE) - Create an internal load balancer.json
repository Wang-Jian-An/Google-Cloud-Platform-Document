{"title": "Google Kubernetes Engine (GKE) - Create an internal load balancer", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing", "abstract": "# Google Kubernetes Engine (GKE) - Create an internal load balancer\nThis page explains how to create an [internal passthrough Network Load Balancer](/compute/docs/load-balancing/internal) or internal load balancer on Google Kubernetes Engine (GKE). To create an external passthrough Network Load Balancer, learn how to [Create a Service of type LoadBalancer](/kubernetes-engine/docs/how-to/exposing-apps#creating_a_service_of_type_loadbalancer) .\n- [LoadBalancer Service](/kubernetes-engine/docs/concepts/service-load-balancer) .\n- [LoadBalancer Service parameters](/kubernetes-engine/docs/concepts/service-load-balancer-parameters) .\n- [Backend service-based external passthrough Network Load Balancer](/load-balancing/docs/network/networklb-backend-service) .", "content": "## Using internal passthrough Network Load Balancer subsetting\nmake your cluster's Services accessible to clients within your cluster's VPC network and to clients in networks connected to your cluster's VPC network. Clients do not have to be located within your cluster. For example, an internal LoadBalancer Service can be accessible to [virtual machine (VM) instances](/compute/docs/instances/create-start-instance) located in the cluster's VPC network.\n## Using GKE subsetting\nimproves the scalability of internal LoadBalancer Services because it uses `GCE_VM_IP` network endpoint groups (NEGs) as backends instead of instance groups. When GKE subsetting is enabled, GKE creates one NEG per [compute zone](/compute/docs/regions-zones#available) per internal LoadBalancer Service. The member endpoints in the NEG are the IP addresses of nodes that have at least one of the Service's serving Pods. For more information about GKE subsetting, see [Node grouping](/kubernetes-engine/docs/concepts/service-load-balancer#endpoint-grouping) .\n### Requirements and limitations\nGKE subsetting has the following requirements and limitations:\n- You can enable GKE subsetting in new and existing Standard clusters in GKE versions 1.18.19-gke.1400 and later. GKE subsetting cannot be disabled once it has been enabled.\n- GKE subsetting is always enabled in Autopilot clusters.\n- GKE subsetting requires that the`HttpLoadBalancing`add-on is enabled. This add-on is enabled by default. In Autopilot clusters, you cannot disable this required add-on.\n- [Quotas for Network Endpoint Groups](/load-balancing/docs/quotas#backends) apply. Google Cloud creates one`GCE_VM_IP`NEG per internal LoadBalancer Service per zone.\n- Quotas for forwarding rules, backend services, and health checks apply. For more information, see [Quotas and limits](/load-balancing/docs/quotas) .\n- GKE subsetting cannot be used with the annotation to share a backend service among multiple load balancers,`alpha.cloud.google.com/load-balancer-backend-share`.\n- You must have Google Cloud CLI version 345.0.0 or later.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Enable GKE subsetting in a new Standard cluster\nYou can create a Standard cluster with GKE subsetting enabled using the Google Cloud CLI or the Google Cloud console. A cluster created with GKE subsetting enabled uses GKE subsetting.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Configure your cluster as desired.\n- From the navigation pane, under **Cluster** , click **Networking** .\n- Select the **Enable subsetting for L4 internal load balancers** checkbox.\n- Click **Create** .\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --cluster-version=VERSION \\\u00a0 \u00a0 --enable-l4-ilb-subsetting \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```\nReplace the following:- ``: the name of the new cluster.\n- ``: the GKE version, which must be 1.18.19-gke.1400 or later. You can also use the`--release-channel`option to select a release channel. The release channel must have a default version 1.18.19-gke.1400 or later.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n### Enable GKE subsetting in an existing Standard cluster\nYou can enable GKE subsetting for an existing Standard cluster using the gcloud CLI or the Google Cloud console. You cannot disable GKE subsetting after you have enabled it.\n- In the Google Cloud console, go to the **Google Kubernetes Engine** page. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Networking** , next to the **Subsetting for L4 Internal Load Balancers** field, click **Enable subsettingfor L4 internal load balancers** .\n- Select the **Enable subsetting for L4 internal load balancers** checkbox.\n- Click **Save Changes** .\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-l4-ilb-subsetting\n```\nReplace the following:- ``: the name of the cluster.\nEnabling GKE subsetting does not disrupt existing internal LoadBalancer Services. If you want to migrate existing internal LoadBalancer Services to use backend services with `GCE_VM_IP` NEGs as backends, you must deploy a replacement Service manifest. For more details, see [Node grouping](/kubernetes-engine/docs/concepts/service-load-balancer#endpoint-grouping) in the LoadBalancer Service concepts documention.\n### Deploy a workload\nThe following manifest describes a Deployment that runs a sample web application container image.\n- Save the manifest as `ilb-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: ilb-deploymentspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: ilb-deployment\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: ilb-deployment\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\n```\n- Apply the manifest to your cluster:```\nkubectl apply -f ilb-deployment.yaml\n```\n### Create an internal LoadBalancer Service\nThe following example creates an internal LoadBalancer Service using TCP port `8080` . GKE deploys an internal passthrough Network Load Balancer whose forwarding rule uses port `8080` :\n- Save the manifest as `ilb-svc.yaml` :```\napiVersion: v1kind: Servicemetadata:\u00a0 name: ilb-svc\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/load-balancer-type: \"Internal\"spec:\u00a0 type: LoadBalancer\u00a0 externalTrafficPolicy: Cluster\u00a0 selector:\u00a0 \u00a0 app: ilb-deployment\u00a0 ports:\u00a0 - name: tcp-port\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8080\u00a0 \u00a0 targetPort: 8080\n```Your manifest must contain the following:- A`name`for the internal LoadBalancer Service, in this case`ilb-svc`.\n- An annotation that specifies that you require an internal LoadBalancer Service. For GKE versions 1.17 and later, use the annotation`networking.gke.io/load-balancer-type: \"Internal\"`as shown in the example manifest. For earlier versions, use`cloud.google.com/load-balancer-type: \"Internal\"`instead.\n- The`type: LoadBalancer`.\n- A`spec: selector`field to specify the Pods the Service should target, for example,`app: hello`.\n- Port information:- The`port`represents the destination port on which the forwarding rule of the internal passthrough Network Load Balancer receives packets.\n- The`targetPort`must match a`containerPort`defined on each serving Pod.\n- The`port`and`targetPort`values don't need to be the same. Nodes always perform destination NAT, changing the destination load balancer forwarding rule IP address and`port`to a destination Pod IP address and`targetPort`. For more details, see [Destination Network AddressTranslation on nodes](/kubernetes-engine/docs/concepts/service-load-balancer#pp-step2) in the LoadBalancer Service concepts documentation.Your manifest can contain the following:- `spec.ipFamilyPolicy`and`ipFamilies`to define how GKE allocates IP addresses to the Service. GKE supports either single-stack (IPv4 only or IPv6 only), or dual-stack IP LoadBalancer Services. A dual-stack LoadBalancer Service is implemented with two separate internal passthrough Network Load Balancer forwarding rules: one for IPv4 traffic and one for IPv6 traffic. The GKE dual-stack LoadBalancer Service is available in version 1.29 or later. To learn more, see [IPv4/IPv6 dual-stack Services](/kubernetes-engine/docs/concepts/service-load-balancer-parameters#service_parameters) .\nFor more information see, [LoadBalancer Service parameters](/kubernetes-engine/docs/concepts/service-load-balancer-parameters#service_parameters) \n- Apply the manifest to your cluster:```\nkubectl apply -f ilb-svc.yaml\n```\n- Get detailed information about the Service:```\nkubectl get service ilb-svc --output yaml\n```The output is similar to the following:```\napiVersion: v1kind: Servicemetadata:\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"ingress\":true}'\u00a0 \u00a0 cloud.google.com/neg-status: '{\"network_endpoint_groups\":{\"0\":\"k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r\"},\"zones\":[\"ZONE_NAME\",\"ZONE_NAME\",\"ZONE_NAME\"]}'\u00a0 \u00a0 kubectl.kubernetes.io/last-applied-configuration: |\u00a0 \u00a0 \u00a0 {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{\"networking.gke.io/load-balancer-type\":\"Internal\"},\"name\":\"ilb-svc\",\"namespace\":\"default\"},\"spec\":{\"externalTrafficPolicy\":\"Cluster\",\"ports\":[{\"name\":\"tcp-port\",\"port\":8080,\"protocol\":\"TCP\",\"targetPort\":8080}],\"selector\":{\"app\":\"ilb-deployment\"},\"type\":\"LoadBalancer\"}}\u00a0 \u00a0 networking.gke.io/load-balancer-type: Internal\u00a0 \u00a0 service.kubernetes.io/backend-service: k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r\u00a0 \u00a0 service.kubernetes.io/firewall-rule: k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r\u00a0 \u00a0 service.kubernetes.io/firewall-rule-for-hc: k8s2-pn2h9n5f-l4-shared-hc-fw\u00a0 \u00a0 service.kubernetes.io/healthcheck: k8s2-pn2h9n5f-l4-shared-hc\u00a0 \u00a0 service.kubernetes.io/tcp-forwarding-rule: k8s2-tcp-pn2h9n5f-default-ilb-svc-3bei4n1r\u00a0 creationTimestamp: \"2022-07-22T17:26:04Z\"\u00a0 finalizers:\u00a0 - gke.networking.io/l4-ilb-v2\u00a0 - service.kubernetes.io/load-balancer-cleanup\u00a0 name: ilb-svc\u00a0 namespace: default\u00a0 resourceVersion: \"51666\"\u00a0 uid: d7a1a865-7972-44e1-aa9e-db5be23d6567spec:\u00a0 allocateLoadBalancerNodePorts: true\u00a0 clusterIP: 10.88.2.141\u00a0 clusterIPs:\u00a0 - 10.88.2.141\u00a0 externalTrafficPolicy: Cluster\u00a0 internalTrafficPolicy: Cluster\u00a0 ipFamilies:\u00a0 - IPv4\u00a0 ipFamilyPolicy: SingleStack\u00a0 ports:\u00a0 - name: tcp-port\u00a0 \u00a0 nodePort: 30521\u00a0 \u00a0 port: 8080\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 8080\u00a0 selector:\u00a0 \u00a0 app: ilb-deployment\u00a0 sessionAffinity: None\u00a0 type: LoadBalancerstatus:\u00a0 loadBalancer:\u00a0 \u00a0 ingress:\u00a0 \u00a0 - ip: 10.128.15.245\n```The output has the following attributes:- The IP address of the internal passthrough Network Load Balancer's forwarding rule is included in`status.loadBalancer.ingress`. This IP address is different from the value of`clusterIP`. In this example, the load balancer's forwarding rule IP address is`10.128.15.245`.\n- Any Pod that has the label`app: ilb-deployment`is a serving Pod for this Service. These are the Pods that receive packets routed by the internal passthrough Network Load Balancer.\n- Clients call the Service by using this`loadBalancer`IP address and the TCP destination port specified in the`port`field of the Service manifest. For complete details about how packets are routed once received by a node, see [Packet processing](/kubernetes-engine/docs/concepts/service-load-balancer#node-packet-processing) .\n- GKE assigned a`nodePort`to the Service; in this example, port`30521`is assigned. The`nodePort`is not relevant to the internal passthrough Network Load Balancer.\n- Inspect the Service network endpoint group:```\nkubectl get svc ilb-svc -o=jsonpath=\"{.metadata.annotations.cloud\\.google\\.com/neg-status}\"\n```The output is similar to the following:```\n{\"network_endpoint_groups\":{\"0\":\"k8s2-knlc4c77-default-ilb-svc-ua5ugas0\"},\"zones\":[\"ZONE_NAME\"]}\n```The response indicates that GKE has created a network endpoint group named `k8s2-knlc4c77-default-ilb-svc-ua5ugas0` . This annotation is present in services of type `LoadBalancer` that use GKE subsetting and is not present in Services that do not use GKE subsetting.\n### Verify internal passthrough Network Load Balancer components\nThe internal passthrough Network Load Balancer's forwarding rule IP address is `10.128.15.245` in the example included in the [Create an internal LoadBalancer Service](#create) section. You can see this forwarding rule is included in the list of forwarding rules in the cluster's project by using the Google Cloud CLI:\n```\ngcloud compute forwarding-rules list --filter=\"loadBalancingScheme=INTERNAL\"\n```\nThe output includes the relevant internal passthrough Network Load Balancer forwarding rule, its IP address, and the backend service referenced by the forwarding rule ( `k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r` in this example).\n```\nNAME       ... IP_ADDRESS ... TARGET\n...\nk8s2-tcp-pn2h9n5f-default-ilb-svc-3bei4n1r 10.128.15.245 ZONE_NAME/backendServices/k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r\n```\nYou can describe the load balancer's backend service by using the Google Cloud CLI:\n```\ngcloud compute backend-services describe k8s2-tcp-pn2h9n5f-default-ilb-svc-3bei4n1r --region=COMPUTE_REGION\n```\nReplace `` with the [compute region](/compute/docs/regions-zones#available) of the backend service.\nThe output includes the backend `GCE_VM_IP` NEG or NEGs for the Service ( `k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r` in this example):\n```\nbackends:\n- balancingMode: CONNECTION\n group: .../ZONE_NAME/networkEndpointGroups/k8s2-pn2h9n5f-default-ilb-svc-3bei4n1r\n...\nkind: compute#backendService\nloadBalancingScheme: INTERNAL\nname: aae3e263abe0911e9b32a42010a80008\n...\n```\nTo determine the list of nodes in a subset for a service, use the following command:\n```\ngcloud compute network-endpoint-groups list-network-endpoints NEG_NAME \\\u00a0 \u00a0 --zone=COMPUTE_ZONE\n```\nReplace the following:\n- ``: the name of the network endpoint group created by the GKE controller.\n- ``: the [compute zone](/compute/docs/regions-zones#available) of the network endpoint group to operate on.\nTo determine the list of healthy nodes for an internal passthrough Network Load Balancer, use the following command:\n```\ngcloud compute backend-services get-health SERVICE_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:\n- ``: the name of the backend service. This value is the same as the name of the network endpoint group created by the GKE controller.\n- ``: the [compute region](/compute/docs/regions-zones#available) of the backend service to operate on.\n### Test connectivity to the internal passthrough Network Load Balancer\nSSH into a VM instance in the same VPC network and in the same region as the cluster, then run the following command:\n```\ncurl LOAD_BALANCER_IP\n```\nReplace `` with the load balancer's forwarding rule IP address.\nThe response shows the output of `ilb-deployment` :\n```\nHello, world!\nVersion: 1.0.0\nHostname: ilb-deployment-77b45987f7-pw54n\n```\nThe internal passthrough Network Load Balancer is only accessible within the same VPC network ( [or a connected network](https://cloud.google.com/load-balancing/docs/internal/internal-tcp-udp-lb-and-other-networks) ). By default, the load balancer's forwarding rule has global access disabled, so client VMs, Cloud VPN tunnels, or Cloud Interconnect attachments (VLANs) must be located in the same region as the internal passthrough Network Load Balancer. To support clients in all regions, you can enable global access on the load balancer's forwarding rule by including the [global access](/kubernetes-engine/docs/concepts/service-load-balancer-parameters#global_access_internal_lb) annotation in the Service manifest.\n### Delete the internal LoadBalancer Service and load balancer resources\nYou can delete the Deployment and Service using `kubectl delete` or the Google Cloud console.\nTo delete the Deployment, run the following command:\n```\nkubectl delete deployment ilb-deployment\n```To delete the Service, run the following command:\n```\nkubectl delete service ilb-svc\n```To delete the Deployment, perform the following steps:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Select the Deployment you want to delete, then click **Delete** .\n- When prompted to confirm, select the **Delete Horizontal Pod Autoscaler associated with selected Deployment** checkbox, then click **Delete** .\nTo delete the Service, perform the following steps:- Go to the **Services & Ingress** page in the Google Cloud console. [Go to Services & Ingress](https://console.cloud.google.com/kubernetes/discovery) \n- Select the Service you want to delete, then click **Delete** .\n- When prompted to confirm, click **Delete** .\n### Shared IP\nThe internal passthrough Network Load Balancer allows the [sharing of a Virtual IP address amongst multiple forwarding rules](/load-balancing/docs/internal/multiple-forwarding-rules-same-ip) . This is useful for expanding the number of simultaneous ports on the same IP or for accepting UDP and TCP traffic on the same IP. It allows up to a maximum of 50 exposed ports per IP address. Shared IPs are supported natively on GKE clusters with internal LoadBalancer Services. When deploying, the Service's `loadBalancerIP` field is used to indicate which IP should be shared across Services.\nA shared IP for multiple load balancers has the following limitations and capabilities:\n- Each Service (or forwarding rule) can have a maximum of five ports.\n- A maximum of ten Services (forwarding rules) can share an IP address. This results in a maximum of 50 ports per shared IP.\n- Each forwarding rule that shares the same IP address must use a unique combination of protocols and ports. Therefore, every internal LoadBalancer Service must use a unique set of protocols and ports.\n- A combination of TCP-only and UDP-only Services is supported on the same shared IP, however you cannot expose both TCP and UDP ports in the same Service.To enable an internal LoadBalancer Services to share a common IP, follow these steps:\n- Create a static internal IP with `--purpose SHARED_LOADBALANCER_VIP` . An IP address must be created with this purpose to enable its ability to be shared. If you create the static internal IP address in a Shared VPC, you must create the IP address in the same service project as the instance that will use the IP address, even though the value of the IP address will come from the range of available IPs in a selected shared subnet of the Shared VPC network. Refer to [reserving a static internal IP](/vpc/docs/provisioning-shared-vpc#reserve_internal_ip) on the page for more information.\n- Deploy up to ten internal LoadBalancer Services using this static IP in the `loadBalancerIP` field. The internal passthrough Network Load Balancers are reconciled by the GKE service controller and deploy using the same frontend IP.\nThe following example demonstrates how this is done to support multiple TCP and UDP ports against the same internal load balancer IP.\n- Create a static IP in the same region as your GKE cluster. The subnet must be the same subnet that the load balancer uses, which by default is the same subnet that is used by the GKE cluster node IPs.If your cluster and the VPC network are in the same project:```\ngcloud compute addresses create IP_ADDR_NAME \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --subnet=SUBNET \\\u00a0 \u00a0 --addresses=IP_ADDRESS \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --purpose=SHARED_LOADBALANCER_VIP\n```If your cluster is in a Shared VPC service project but uses a Shared VPC network in a host project:```\ngcloud compute addresses create IP_ADDR_NAME \\\u00a0 \u00a0 --project=SERVICE_PROJECT_ID \\\u00a0 \u00a0 --subnet=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/SUBNET \\\u00a0 \u00a0 --addresses=IP_ADDRESS \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --purpose=SHARED_LOADBALANCER_VIP\n```Replace the following:- ``: a name for the IP address object.\n- ``: the ID of the service project.\n- ``: the ID of your project (single project).\n- ``: the ID of the Shared VPC host project.\n- ``: the [compute region](/compute/docs/regions-zones#available) containing the shared subnet.\n- ``: an unused internal IP address from the selected subnet's primary IP address range. If you omit specifying an IP address, Google Cloud selects an unused internal IP address from the selected subnet's primary IP address range. To determine an automatically selected address, you'll need to run [gcloud compute addresses describe](/sdk/gcloud/reference/compute/addresses/describe) .\n- ``: the name of the shared subnet.\n- Save the following TCP Service configuration to a file named `tcp-service.yaml` and then deploy to your cluster. Replace `` with the IP address you chose in the previous step.```\napiVersion: v1kind: Servicemetadata:\u00a0 name: tcp-service\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/load-balancer-type: \"Internal\"spec:\u00a0 type: LoadBalancer\u00a0 loadBalancerIP: IP_ADDRESS\u00a0 selector:\u00a0 \u00a0 app: myapp\u00a0 ports:\u00a0 - name: 8001-to-8001\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8001\u00a0 \u00a0 targetPort: 8001\u00a0 - name: 8002-to-8002\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8002\u00a0 \u00a0 targetPort: 8002\u00a0 - name: 8003-to-8003\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8003\u00a0 \u00a0 targetPort: 8003\u00a0 - name: 8004-to-8004\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8004\u00a0 \u00a0 targetPort: 8004\u00a0 - name: 8005-to-8005\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 port: 8005\u00a0 \u00a0 targetPort: 8005\n```\n- Apply this Service definition against your cluster:```\nkubectl apply -f tcp-service.yaml\n```\n- Save the following UDP Service configuration to a file named `udp-service.yaml` and then deploy it. It also uses the `` that you specified in the previous step.```\napiVersion: v1kind: Servicemetadata:\u00a0 name: udp-service\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/load-balancer-type: \"Internal\"spec:\u00a0 type: LoadBalancer\u00a0 loadBalancerIP: IP_ADDRESS\u00a0 selector:\u00a0 \u00a0 app: my-udp-app\u00a0 ports:\u00a0 - name: 9001-to-9001\u00a0 \u00a0 protocol: UDP\u00a0 \u00a0 port: 9001\u00a0 \u00a0 targetPort: 9001\u00a0 - name: 9002-to-9002\u00a0 \u00a0 protocol: UDP\u00a0 \u00a0 port: 9002\u00a0 \u00a0 targetPort: 9002\n```\n- Apply this file against your cluster:```\nkubectl apply -f udp-service.yaml\n```\n- Validate that the VIP is shared amongst load balancer forwarding rules by listing them out and filtering for the static IP. This shows that there is a UDP and a TCP forwarding rule both listening across seven different ports on the shared `` , which in this example is `10.128.2.98` .```\ngcloud compute forwarding-rules list | grep 10.128.2.98\nab4d8205d655f4353a5cff5b224a0dde       us-west1 10.128.2.98  UDP   us-west1/backendServices/ab4d8205d655f4353a5cff5b224a0dde\nacd6eeaa00a35419c9530caeb6540435       us-west1 10.128.2.98  TCP   us-west1/backendServices/acd6eeaa00a35419c9530caeb6540435\n```## Restrictions and limits\n### Restrictions for internal passthrough Network Load Balancers\n- For clusters running Kubernetes version 1.7.4 and later, you can use internal load balancers with [custom-mode subnets](/compute/docs/vpc#subnet-ranges) in addition to auto-mode subnets.\n- Clusters running Kubernetes version 1.7.X and later support using a reserved IP address for the internal passthrough Network Load Balancer if you create the reserved IP address with the [--purpose](/sdk/gcloud/reference/compute/addresses/create#--purpose) flag set to`SHARED_LOADBALANCER_VIP`. Refer to [Enabling Shared IP](#enabling_shared_ip) for step-by-step directions. GKE only preserves the IP address of an internal passthrough Network Load Balancer if the Service references an internal IP address with that purpose. Otherwise, GKE might change the load balancer's IP address (`spec.loadBalancerIP`) if the Service is updated (for example, if ports are changed).\n- Even if the load balancer's IP address changes (see previous point), the`spec.clusterIP`remains constant.\n### Restrictions for internal UDP load balancers\n- Internal UDP load balancers don't support using`sessionAffinity: ClientIP`.## Known issues\n### Connection timeout every 10 minutes\nInternal LoadBalancer Services created with Subsetting might observe traffic disruptions roughly every 10 minutes. This bug has been fixed in versions:\n- 1.18.19-gke.1700 and later\n- 1.19.10-gke.1000 and later\n- 1.20.6-gke.1000 and later\n### Error creating load balancer in Standard tier\nWhen you create an internal passthrough Network Load Balancer in a project with the [project default network tier](/network-tiers/docs/set-network-tier#setting_the_tier_for_all_resources_in_a_project) set to Standard, the following error message appears:\n```\nError syncing load balancer: failed to ensure load balancer: googleapi: Error 400: STANDARD network tier (the project's default network tier) is not supported: Network tier other than PREMIUM is not supported for loadBalancingScheme=INTERNAL., badRequest\n```\nTo resolve this issue in GKE versions earlier than 1.23.3-gke.900, configure the project default network tier to Premium.\nThis issue is resolved in GKE versions 1.23.3-gke.900 and later when [GKE subsetting](#subsetting) is enabled.\nThe GKE controller creates internal passthrough Network Load Balancers in the Premium network tier even if the project default network tier is set to Standard.\n## What's next\n- [Read the GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- [Learn more about Compute Engine load balancers](/compute/docs/load-balancing) .\n- [Learn how to create a VPC-native cluster](/kubernetes-engine/docs/how-to/alias-ips) .\n- [Learn about IP masquerade agent](/kubernetes-engine/docs/how-to/ip-masquerade-agent) .\n- [Learn about configuring authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) .", "guide": "Google Kubernetes Engine (GKE)"}