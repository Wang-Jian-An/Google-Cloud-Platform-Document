{"title": "Google Kubernetes Engine (GKE) - GKE Ingress for Application Load Balancers", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/ingress", "abstract": "# Google Kubernetes Engine (GKE) - GKE Ingress for Application Load Balancers\nThis page provides a general overview of what Ingress for external Application Load Balancers is and how it works. Google Kubernetes Engine (GKE) provides a built-in and managed Ingress controller called GKE Ingress. This controller implements Ingress resources as Google Cloud load balancers for HTTP(S) workloads in GKE.\n", "content": "## Overview\nIn GKE, an [Ingress object](https://kubernetes.io/docs/concepts/services-networking/ingress/) defines rules for routing HTTP(S) traffic to applications running in a cluster. An Ingress object is associated with one or more [Service objects](/kubernetes-engine/docs/concepts/service) , each of which is associated with a set of Pods. To learn more about how Ingress exposes applications using Services, see [Service networking overview](/kubernetes-engine/docs/concepts/service-networking) .\nWhen you create an Ingress object, the [GKE Ingress controller](https://github.com/kubernetes/ingress-gce) creates a [Google Cloud HTTP(S) Load Balancer](/load-balancing/docs/https) and configures it according to the information in the Ingress and its associated Services.\nTo use Ingress, you must have the HTTP load balancing add-on enabled. GKE clusters have HTTP load balancing enabled by default; you must not disable it.\n## Ingress for external and internal traffic\nGKE Ingress resources come in two types:\n- [Ingress for external Application Load Balancers](/kubernetes-engine/docs/concepts/ingress-xlb) deploys the [classic Application Load Balancer](/load-balancing/docs/https) . This internet-facing load balancer is deployed globally across Google's edge network as a managed and scalable pool of load balancing resources. Learn how to [set up and use Ingress for external Application Load Balancers](/kubernetes-engine/docs/how-to/load-balance-ingress) .\n- [Ingress for internal Application Load Balancers](/kubernetes-engine/docs/concepts/ingress-ilb) deploys the [internal Application Load Balancer](/load-balancing/docs/l7-internal) . These internal Application Load Balancers are powered by Envoy proxy systems outside of your GKE cluster, but within your VPC network. Learn how to [set up and use Ingress for internal Application Load Balancers](/kubernetes-engine/docs/how-to/internal-load-balance-ingress) .\n**Important:** Whenever GKE creates an external Application Load Balancer or an internal Application Load Balancer through an Ingress object, you should avoid changing the load balancer's configuration using methods outside of GKE. Your customizations to settings for the load balancer's objects \u2013 forwarding rules, target proxies, URL maps, backend services, and health checks \u2013 are **overwritten** when new resources changes are applied, during periodic syncs, or during cluster upgrades. If you need to manage an external Application Load Balancer or an internal Application Load Balancer outside of GKE, use [container native load balancing for standalone NEGs](/kubernetes-engine/docs/how-to/standalone-neg) instead.\n## GKE Ingress controller behavior\nFor clusters running GKE versions 1.18 and later, whether or not the GKE Ingress controller processes an Ingress depends on the value of the `kubernetes.io/ingress.class` annotation:\n| kubernetes.io/ingress.class value    | ingressClassName value   | GKE Ingress controller behavior                       |\n|:-----------------------------------------------|:----------------------------------|:------------------------------------------------------------------------------------------------------------------------|\n| Not set          | Not set       | Process the Ingress manifest and create an external Application Load Balancer.           |\n| Not set          | Any value       | Takes no action. The Ingress manifest could be processed by a third-party Ingress controller if one has been deployed. |\n| gce           | Any value. This field is ignored. | Process the Ingress manifest and create an external Application Load Balancer.           |\n| gce-internal         | Any value. This field is ignored. | Process the Ingress manifest and create an internal Application Load Balancer.           |\n| Set to a value other than gce or gce-internal | Any value       | Takes no action. The Ingress manifest could be processed by a third-party Ingress controller if one has been deployed. |\nFor clusters running older GKE versions, the GKE controller processes any Ingress that does not have the annotation `kubernetes.io/ingress.class` , or has the annotation with the value `gce` or `gce-internal` .\n### kubernetes.io/ingress.class annotation deprecation\nAlthough the `kubernetes.io/ingress.class` annotation is [deprecated in Kubernetes](https://kubernetes.io/docs/concepts/services-networking/ingress/#deprecated-annotation) , GKE continues to use this annotation.\nYou cannot use the `ingressClassName` field to specify a GKE Ingress. You must use the `kubernetes.io/ingress.class` annotation.\n## Features of external Application Load Balancers\nAn external Application Load Balancer, configured by Ingress, includes the following features:\n- Flexible configuration for Services. An Ingress defines how traffic reaches your Services and how the traffic is routed to your application. In addition, an Ingress can provide a single IP address for multiple Services in your cluster.\n- Integration with Google Cloud network services\n- Support for multiple TLS certificates. An Ingress can specify the use of multiple TLS certificates for request termination.\nFor a comprehensive list, see [Ingress configuration](/kubernetes-engine/docs/how-to/ingress-configuration) .\n## Container-native load balancing\n**Note:** This feature is not supported with Windows Server node pools.\n[Container-native load balancing](/kubernetes-engine/docs/how-to/container-native-load-balancing) is the practice of load balancing directly to Pod endpoints in GKE using [Network Endpoint Groups (NEGs)](/load-balancing/docs/negs) .\nWhen using Instance Groups, Compute Engine load balancers send traffic to VM IPs as backends. When running containers on VMs, in which containers share the same host interface, this introduces some limitations:\n- It incurs two hops of load balancing - one hop from the load balancer to the VM`NodePort`and another hop through kube-proxy routing to the Pod IP (which may reside on a different VM).\n- Additional hops add latency and make the traffic path more complex.\n- The Compute Engine load balancer has no direct visibility to Pods resulting in suboptimal traffic balancing.\n- Environmental events like VM or Pod loss are more likely to cause intermittent traffic loss due to the double traffic hop.\nWith NEGs, traffic is load balanced from the load balancer directly to the Pod IP as opposed to traversing the VM IP and kube-proxy networking. In addition, [Pod readiness gates](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate) are implemented to determine the health of Pods from the perspective of the load balancer and not just the Kubernetes in-cluster health probes. This improves overall traffic stability by making the load balancer infrastructure aware of lifecycle events such as Pod startup, Pod loss, or VM loss. These capabilities resolve the above limitations and result in more performant and stable networking.\nContainer-native load balancing is enabled by default for Services when all of the following conditions are true:\n- For Services created in GKE clusters 1.17.6-gke.7 and up\n- Using VPC-native clusters\n- Not using a Shared VPC\n- Not using GKE Network Policy\nIn these conditions, Services will be annotated automatically with `cloud.google.com/neg: '{\"ingress\": true}'` indicating that a NEG should be created to mirror the Pod IPs within the Service. The NEG is what allows Compute Engine load balancers to communicate directly with Pods. Note that existing Services created prior to GKE 1.17.6-gke.7+ will not be automatically annotated by the Service controller.\nFor GKE 1.17.6-gke.7+ clusters where NEG annotation is automatic, it is possible to disable NEGs and force the Compute Engine external load balancer to use an instance group as its backends if necessary. This can be done by explicitly annotating Services with `cloud.google.com/neg: '{\"ingress\": false}'` . It is not possible to disable NEGs with Ingress for internal Application Load Balancers.\nFor clusters where NEGs are not the default, it is still **stronglyrecommended** to use container-native load balancing, but it must be enabled explicitly on a per-Service basis. The annotation should be applied to Services in the following manner:\n```\nkind: Service...\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"ingress\": true}'...\n```\n**Caution:** When you add this annotation, a new BackendService is created for the existing Service. This can result in a temporary outage for your Service.\n## Shared VPC\nIngress and MultiClusterIngress resources are supported in [Shared VPC](/vpc/docs/shared-vpc) , but they require additional preparation.\nThe Ingress controller runs on the GKE control plane and makes API calls to Google Cloud using the GKE service account of the cluster's project. By default, when a cluster that is located in a Shared VPC service project uses a Shared VPC network, the Ingress controller cannot use the service project's GKE service account to create and update ingress allow firewall rules in the host project.\nYou can grant the service project's GKE service account permissions to create and [manage VPC firewall rules](/kubernetes-engine/docs/how-to/cluster-shared-vpc#managing_firewall_resources) in the host project. By granting these permissions, GKE creates ingress allow firewall rules for the following:\n- Google Front End (GFE) proxies and health check systems used by external Application Load Balancers for external Ingress. For more information, see the [External Application Load Balancer overview](/load-balancing/docs/https#firewall-rules) .\n- Health check systems for internal Application Load Balancers used by internal Ingress. **Note:** Whether or not you're using Shared VPC, if your cluster uses an internal Ingress, you must manually [create ingress allow firewallrules](/kubernetes-engine/docs/concepts/firewall-rules#ingress-fws) to permit traffic from the [proxy-only subnet](/load-balancing/docs/proxy-only-subnets) in the cluster's VPC network and region. Manual creation of these rules is needed because the GKE Ingress controller cannot create ingress allow firewall rules to permit traffic for an internal Application Load Balancer's proxy-only subnet. For more information, see [Required networking environment in Ingress for internal Application Load Balancers](/kubernetes-engine/docs/concepts/ingress-ilb#required_networking_environment) .\n### Manually provision firewall rules from the host project\nIf your security policies only allow firewall management from the host project, then you can provision these firewall rules manually. When deploying Ingress in a Shared VPC, the Ingress resource event provides the specific firewall rule you need to add necessary to provide access.\nTo manually provision a rule:\n- View the Ingress resource event:```\nkubectl describe ingress INGRESS_NAME\n```Replace with the name of your Ingress.You should see output similar to the following example:```\nEvents:\nType Reason Age     From      Message\n---- ------ ----     ----      ------Normal Sync 9m34s (x237 over 38h) loadbalancer-controller Firewall change required by security admin: `gcloud compute firewall-rules update k8s-fw-l7--6048d433d4280f11 --description \"GCE L7 firewall rule\" --allow tcp:30000-32767,tcp:8080 --source-ranges 130.211.0.0/22,35.191.0.0/16 --target-tags gke-l7-ilb-test-b3a7e0e5-node --project <project>`\n```The suggested required firewall rule appears in the `Message` column.\n- Copy and apply the suggested firewall rules from the host project. Applying the rule provides access to your Pods from the load balancer and Google Cloud health checkers. **Note:** Different types of Ingress require different rules. GKE might show a firewall rule warning even if you have your own custom ingress firewall rules to allow the traffic. You can ignore the warning in that case.\n**Note:** For Multi Cluster Ingress, this manual provision step is the only supported way to install firewall rules in a host project. Changing the service account permissions for automatic provisioning is not supported.\n### Providing the GKE Ingress controller permission to manage host project firewall rules\nIf you want a GKE cluster in a service project to create and manage the firewall resources in your host project, the service project's GKE service account must be granted the appropriate IAM permissions using one of the following strategies:\n- Grant the service project's GKE service account the [Compute Security Admin role](/compute/docs/access/iam#compute.securityAdmin) to the host project. The following example demonstrates this strategy.\n- For a finer grained approach, [create a custom IAM role](/iam/docs/creating-custom-roles#creating_a_custom_role) that includes only the following permissions: `compute.networks.updatePolicy` , `compute.firewalls.list` , `compute.firewalls.get` , `compute.firewalls.create` , `compute.firewalls.update` , and `compute.firewalls.delete` . Grant the service project's GKE service account that custom role to the host project.\nIf you have clusters in more than one service project, you must choose one of the strategies and repeat it for each service project's GKE service account.\n```\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\u00a0 --member=serviceAccount:service-SERVICE_PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com \\\u00a0 --role=roles/compute.securityAdmin\n```\nReplace the following:\n- ``: the [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) of the [Shared VPC host project](/vpc/docs/shared-vpc#shared_vpc_host_project_and_service_project_associations) .\n- ``: the [project number](/resource-manager/docs/creating-managing-projects#identifying_projects) of the [service project](/vpc/docs/shared-vpc#shared_vpc_host_project_and_service_project_associations) that contains your cluster.## Multiple backend services\nEach external Application Load Balancer or internal Application Load Balancer uses a single URL map, which references one or more backend services. One backend service corresponds to each Service referenced by the Ingress.\nFor example, you can configure the load balancer to route requests to different backend services depending on the URL path. Requests sent to your-store.example could be routed to a backend service that displays full-price items, and requests sent to your-store.example/discounted could be routed to a backend service that displays discounted items.\nYou can also configure the load balancer to route requests according to the hostname. Requests sent to your-store.example could go to one backend service, and requests sent to your-experimental-store.example could go to another backend service.\nIn a GKE cluster, you create and configure an HTTP(S) load balancer by creating a Kubernetes Ingress object. An Ingress object must be associated with one or more Service objects, each of which is associated with a set of Pods.\nHere is a manifest for an Ingress called `my-ingress` :\n```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: my-ingressspec:\u00a0 rules:\u00a0 - http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - path: /*\u00a0 \u00a0 \u00a0 \u00a0 pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-products\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 60000\u00a0 \u00a0 \u00a0 - path: /discounted\u00a0 \u00a0 \u00a0 \u00a0 pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-discounted-products\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 80\n```\n**Note:** To use Ingress, you must have the external Application Load Balancer add-on enabled. GKE clusters have external Application Load Balancers enabled by default; you must not disable it.\nWhen you create the Ingress, the GKE ingress controller creates and configures an external Application Load Balancer or an internal Application Load Balancer according to the information in the Ingress and the associated Services. Also, the load balancer is given a stable IP address that you can associate with a domain name.\nIn the preceding example, assume you have associated the load balancer's IP address with the domain name your-store.example. When a client sends a request to your-store.example, the request is routed to a Kubernetes Service named `my-products` on port 60000. And when a client sends a request to your-store.example/discounted, the request is routed to a Kubernetes Service named `my-discounted-products` on port 80.\nThe only supported wildcard character for the `path` field of an Ingress is the `*` character. The `*` character must follow a forward slash ( `/` ) and must be the last character in the pattern. For example, `/*` , `/foo/*` , and `/foo/bar/*` are valid patterns, but `*` , `/foo/bar*` , and `/foo/*/bar` are not.\nA more specific pattern takes precedence over a less specific pattern. If you have both `/foo/*` and `/foo/bar/*` , then `/foo/bar/bat` is taken to match `/foo/bar/*` .\nFor more information about path limitations and pattern matching, see the [URL Maps documentation](/load-balancing/docs/url-map) .\nThe manifest for the `my-products` Service might look like this:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-productsspec:\u00a0 type: NodePort\u00a0 selector:\u00a0 \u00a0 app: products\u00a0 \u00a0 department: sales\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 60000\u00a0 \u00a0 targetPort: 50000\n```\nIn the Service manifest, you must use `type: NodePort` unless you're using [container native load balancing](/kubernetes-engine/docs/concepts/container-native-load-balancing) . If using container native load balancing, use the `type: ClusterIP` .\nIn the Service manifest, the `selector` field says any Pod that has both the `app: products` label and the `department: sales` label is a member of this Service.\nWhen a request comes to the Service on port 60000, it is routed to one of the member Pods on TCP port 50000.\nEach member Pod must have a container listening on TCP port 50000.\nThe manifest for the `my-discounted-products` Service might look like this:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-discounted-productsspec:\u00a0 type: NodePort\u00a0 selector:\u00a0 \u00a0 app: discounted-products\u00a0 \u00a0 department: sales\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 80\u00a0 \u00a0 targetPort: 8080\n```\nIn the Service manifest, the `selector` field says any Pod that has both the `app: discounted-products` label and the `department: sales` label is a member of this Service.\nWhen a request comes to the Service on port 80, it is routed to one of the member Pods on TCP port 8080.\nEach member Pod must have a container listening on TCP port 8080.\n## Default backend\nYou can specify a default backend for your Ingress by providing a `spec.defaultBackend` field in your Ingress manifest. This will handle any requests that don't match the paths defined in the `rules` field. For example, in the following Ingress, any requests that don't match `/discounted` are sent to a service named `my-products` on port 60001.\n```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: my-ingressspec:\u00a0 defaultBackend:\u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 name: my-products\u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 number: 60001\u00a0 rules:\u00a0 - http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - path: /discounted\u00a0 \u00a0 \u00a0 \u00a0 pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-discounted-products\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 80\n```\nIf you don't specify a default backend, GKE provides a default backend that returns 404. This is created as a `default-http-backend` NodePort service on the cluster in the `kube-system` namespace.\nThe 404 HTTP response is similar to the following:\n```\nresponse 404 (backend NotFound), service rules for the path non-existent\n```\nTo set up GKE Ingress with a customer default backend, see the [GKE Ingress with custom default backend](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/single-cluster/ingress-custom-default-backend) .\n## Ingress to Compute Engine resource mappings\nThe GKE Ingress controller deploys and manages Compute Engine load balancer resources based on the Ingress resources that are deployed in the cluster. The mapping of Compute Engine resources depends on the structure of the Ingress resource.\nThe following manifest describes an Ingress:\n```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: my-ingressspec:\u00a0 rules:\u00a0 - http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - path: /*\u00a0 \u00a0 \u00a0 \u00a0 pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-products\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 60000\u00a0 \u00a0 \u00a0 - path: /discounted\u00a0 \u00a0 \u00a0 \u00a0 pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: my-discounted-products\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 80\n```\nThis Ingress manifest instructs GKE to create the following Compute Engine resources:\n- A forwarding rule and IP address.\n- [Compute Engine firewall rules](/kubernetes-engine/docs/concepts/firewall-rules) that permit traffic for load balancer health checks and application traffic from Google Front Ends or Envoy proxies.\n- A target HTTP proxy and a target HTTPS proxy, if you configured TLS.\n- A URL map which with a single host rule referencing a single path matcher. The path matcher has two path rules, one for`/*`and another for`/discounted`. Each path rule maps to a unique backend service.\n- NEGs which hold a list of Pod IP addresses from each Service as endpoints. These are created as a result of the`my-discounted-products`and`my-products`Services.## Options for providing SSL certificates\nYou can provide SSL certificates to an HTTP(S) load balancer using the following methods:## Health checks\nWhen you expose one or more Services through an Ingress using the default Ingress controller, GKE creates a [classic Application Load Balancer](/kubernetes-engine/docs/concepts/ingress-xlb) or an [internal Application Load Balancer](/kubernetes-engine/docs/concepts/ingress-ilb) . Both of these load balancers support multiple [backendservices](/load-balancing/docs/backend-service) on a single [URLmap](/load-balancing/docs/url-map-concepts) . Each of the backend services corresponds to a Kubernetes Service, and each backend service must reference a [Google Cloud health check](/load-balancing/docs/health-check-concepts) . This health check is from a Kubernetes liveness or readiness probe because the health check is implemented outside of the cluster.\n**Note:** Load balancer health checks are specified . While it's possible to use the same health check for all backend services of the load balancer, the health check reference isn't specified for the whole load balancer (at the Ingress object itself).\nGKE uses the following procedure to create a health check for each backend service corresponding to a Kubernetes Service:\n- If the Service references [a BackendConfig CRD](#direct_hc) with `healthCheck` information, GKE uses that to create the health check. Both the GKE Enterprise Ingress controller and the GKE Ingress controller support creating health checks this way.\n- If the Service does reference a `BackendConfig` CRD:- GKE can infer some or all of the parameters for a health check if the Serving Pods use a Pod template with a container whose readiness probe has attributes that can be interpreted as health check parameters. See [Parameters from a readiness probe](#interpreted_hc) for implementation details and [Default and inferred parameters](#def_inf_hc) for a list of attributes that can be used to create health check parameters. Only the GKE Ingress controller supports inferring parameters from a readiness probe.\n- If the Pod template for the Service's serving Pods does **not** have a container with a readiness probe whose attributes can be interpreted as health check parameters, [the default values](#def_inf_hc) are used to create the health check. Both the GKE Enterprise Ingress controller and the GKE Ingress controller can create a health check using only the default values.**Note:** `containerPort` field must be defined under pod spec and `targetPort` field must be defined under service spec for health check to infer parameters defined under readiness probe. If they are not defined, the health check for the backend service defaults to \"/\" path even if parameters are defined under readiness probe.\n### Default and inferred parameters\nThe following parameters are used when you do **not** specify health check parameters for the corresponding Service using [a BackendConfigCRD](#direct_hc) .\n| Health check parameter | Default value                 | Inferable value                                                                                                                                                                                       |\n|:-------------------------|:--------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Protocol     | HTTP                   | if present in the Service annotation cloud.google.com/app-protocols                                                                                                                                                                          |\n| Request path    | /                    | if present in the serving Pod's spec: containers[].readinessProbe.httpGet.path                                                                                                                                                                       |\n| Request Host header  | Host: backend-ip-address              | if present in the serving Pod's spec: containers[].readinessProbe.httpGet.httpHeaders                                                                                                                                                                     |\n| Expected response  | HTTP 200 (OK)                 | HTTP 200 (OK) cannot be changed                                                                                                                                                                                   |\n| Check interval   | for zonal NEGs: 15 seconds for instance groups: 60 seconds      | if present in the serving Pod's spec: for zonal NEGs: containers[].readinessProbe.periodSeconds for instance groups: containers[].readinessProbe.periodSeconds + 60 seconds                                                                                                                                                |\n| Check timeout   | 5 seconds                  | if present in the serving Pod's spec: containers[].readinessProbe.timeoutSeconds                                                                                                                                                                       |\n| Healthy threshold  | 1                    | 1 cannot be changed                                                                                                                                                                                      |\n| Unhealthy threshold  | for zonal NEGs: 2 for instance groups: 10          | same as default: for zonal NEGs: 2 for instance groups: 10                                                                                                                                                                            |\n| Port specification  | for zonal NEGs: the Service's port for instance groups: the Service's nodePort | The health check probes are sent to the port number specified by the spec.containers[].readinessProbe.httpGet.port, as long as all of the following are also true: The readiness probe's port number must match the serving Pod's containers[].spec.ports.containerPort The serving Pod's containerPort matches the Service's targetPort The Ingress service backend port specification references a valid port from spec.ports[] of the Service. This can be done in one of two ways: spec.rules[].http.paths[].backend.service.port.name in the Ingress matches spec.ports[].name defined in the corresponding Service spec.rules[].http.paths[].backend.service.port.number in the Ingress matches spec.ports[].port defined in the corresponding Service |\n| Destination IP address | for zonal NEGs: the Pod's IP address for instance groups: the node's IP address | same as default: for zonal NEGs: the Pod's IP address for instance groups: the node's IP address                                                                                                                                                                  |\n**Note:** A valid Kubernetes readiness probe supports setting multiple HTTP headers in [readinessProbe.httpGet](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#httpgetaction-v1-core) . If `readinessProbe.httpGet.httpHeaders` specifies more than just the `Host` header, the load balancer's health check parameters are set to default values instead of values inferred from the readiness probe. This limitation exists because [health checks](/load-balancing/docs/health-check-concepts#headers) only support setting the `Host` header.\n### Parameters from a readiness probe\nWhen GKE creates the health check for the Service's backend service, GKE can copy certain parameters from readiness probe used by that Service's serving Pods. This option is supported by the GKE Ingress controller.\nSupported readiness probe attributes that can be interpreted as health check parameters are listed along with the default values in [Default and inferredparameters](#def_inf_hc) . Default values are used for any attributes not specified in the readiness probe or if you don't specify a readiness probe at all.\nIf serving Pods for your Service contain containers, or if you're using the GKE Enterprise Ingress controller, you should use a `BackendConfig` CRD to define health check parameters. For more information, see [When to use aBackendConfig CRD instead](#direct_hc_instead) .\n**Warning:** When a backend service's health check parameters are inferred from a serving Pod's readiness probe, GKE does **not** keep the readiness probe and health check synchronized: - - If you change the readiness probe for the serving Pods of a Service  referenced by an Ingress after GKE has created the  external Application Load Balancer or the internal Application Load Balancer for that Ingress, the changes you make  to the readiness probe will **not** be copied to the health check for the  corresponding backend service on the load balancer.- - Conversely, if you make changes to the health check used by a backend  service of a load balancer created on behalf of an Ingress,  GKE will **not** update any readiness probe, nor will it  revert any changes you make to the health check.\nInstead of relying on parameters from Pod readiness probes, you should explicitly define health check parameters for a backend service by creating a [BackendConfig CRD](#direct_hc) for the Service in these situations:\n- **If you're using GKE Enterprise:** The GKE Enterprise Ingress controller does **not** support obtaining health check parameters from the readiness probes of serving Pods. It can only create health checks using [implicitparameters](#def_inf_hc) or as defined in a [BackendConfig CRD](#direct_hc) .\n- **If you have more than one container in the serving Pods:** GKE does not have a way to select the readiness probe of a from which to infer health check parameters. Because each container can have its own readiness probe, and because a readiness probe isn't a required parameter for a container, you should define the health check for the corresponding backend service by referencing a [BackendConfigCRD](#direct_hc) on the corresponding Service.\n- **If you need control over the port used for the load balancer's healthchecks:** GKE only uses the readiness probe's `containers[].readinessProbe.httpGet.port` for the backend service's health check when that port matches the service port for the Service referenced in the Ingress `spec.rules[].http.paths[].backend.servicePort` .\n### Parameters from a BackendConfig CRD\nYou can specify the backend service's health check parameters [using thehealthCheck parameter of a BackendConfigCRD](/kubernetes-engine/docs/how-to/ingress-configuration#direct_health) referenced by the corresponding Service. This gives you more flexibility and control over health checks for a classic Application Load Balancer or an internal Application Load Balancer created by an Ingress. See [Ingressconfiguration](/kubernetes-engine/docs/how-to/ingress-configuration) for GKE version compatibility.\nThis example `BackendConfig` CRD defines the health check protocol (type), a request path, a port, and a check interval in its `spec.healthCheck` attribute:\n```\napiVersion: cloud.google.com/v1\nkind: BackendConfig\nmetadata:\n name: http-hc-config\nspec:\n healthCheck:\n checkIntervalSec: 15\n port: 15020\n type: HTTPS\n requestPath: /healthz\n```\nTo configure all the fields available when configuring a `BackendConfig` health check, use the [custom health check configuration](/kubernetes-engine/docs/how-to/ingress-configuration#direct_health) example.\nTo set up GKE Ingress with a custom HTTP health check, see [GKE Ingress with custom HTTP health check](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/single-cluster/ingress-custom-http-health-check) .\n## Using multiple TLS certificates\nSuppose you want an HTTP(S) load balancer to serve content from two hostnames: your-store.example and your-experimental-store.example. Also, you want the load balancer to use one certificate for your-store.example and a different certificate for your-experimental-store.example.\nYou can do this by specifying multiple certificates in an Ingress manifest. The load balancer chooses a certificate if the Common Name (CN) in the certificate matches the hostname used in the request. For detailed information on how to configure multiple certificates, see [Using multiple SSL certificates in HTTPS Load Balancing with Ingress](/kubernetes-engine/docs/how-to/ingress-multi-ssl) .\n## Kubernetes Service compared to Google Cloud backend service\nA Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) and a [Google Cloud backend service](/load-balancing/docs/backend-service) are different things. There is a strong relationship between the two, but the relationship is not necessarily one to one. The GKE ingress controller creates a Google Cloud backend service for each ( `service.name` , `service.port` ) pair in an Ingress manifest. So it is possible for one Kubernetes Service object to be related to several Google Cloud backend services.\n## Limitations\n- In clusters using versions earlier than 1.16, the total length of the namespace and name of an Ingress must not exceed 40 characters. Failure to follow this guideline may cause the GKE Ingress controller to act abnormally. For more information, see this [Github issue about long names](https://github.com/kubernetes/ingress-gce/issues/537) .\n- In clusters using NEGs, ingress reconciliation time may be affected by the number of ingresses. For example, a cluster with 20 ingresses, each containing 20 distinct NEG backends, may result in a latency of more than 30 minutes for an ingress change to be reconciled. This especially impacts regional clusters due to the increased number of NEGs needed.\n- [Quotas for URL maps](/load-balancing/docs/quotas#url_maps) apply.\n- [Quotas for Compute Engine resources](/compute/docs/resource-quotas) apply.\n- If you're not using NEGs with the [GKE ingress controller](https://github.com/kubernetes/ingress-gce) then GKE clusters have a limit of 1000 nodes. When services are deployed with NEGs, there is no GKE node limit. Any non-NEG Services exposed through Ingress do not function correctly on clusters above 1000 nodes.\n- For the GKE Ingress controller to use your `readinessProbes` as health checks, the Pods for an Ingress must exist at the time of Ingress creation. If your replicas are scaled to 0, the default health check applies. For more information, see this [Github issue comment about health checks](https://github.com/kubernetes/ingress-gce/issues/241#issuecomment-384749607) .\n- Changes to a Pod's `readinessProbe` do not affect the Ingress after it is created.\n- An external Application Load Balancer terminates TLS in locations that are distributed globally, to minimize latency between clients and the load balancer. If you require geographic control over where TLS is terminated, you should use a [custom ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) exposed through a GKE Service of type `LoadBalancer` instead, and terminate TLS on backends that are located in regions appropriate to your needs.\n- Combining multiple Ingress resources into a single Google Cloud load balancer is not supported.\n- You must turn off mutual TLS on your application because it is [not supported for external Application Load Balancers](/load-balancing/docs/https#limitations) .\n### External Ingress and routes-based clusters\nIf you use routes-based clusters with external Ingress, the GKE Ingress controller cannot use container-native load balancing using `GCE_VM_IP_PORT` network endpoint groups (NEGs). Instead, the Ingress controller uses unmanaged instance group backends that include all nodes in all node pools. If these unmanaged instance groups are also used by `LoadBalancer` Services, it can cause issues related to the [Single load-balanced instance group limitation](/kubernetes-engine/docs/concepts/service-load-balancer#limit-lb-ig) .\nSome older external Ingress objects created in VPC-native clusters might use instance group backends on the backend services of each external Application Load Balancer they create. This is not relevant to internal Ingress because internal Ingress resources always use `GCE_VM_IP_PORT` NEGs and require VPC-native clusters.\nTo learn how to troubleshoot 502 errors with external Ingress, see [External Ingress produces HTTP 502 errors](/kubernetes-engine/docs/troubleshooting/troubleshoot-load-balancing#ingress-502s) .\n## Implementation details\n- The Ingress controller performs periodic checks of service account permissions by fetching a test resource from your Google Cloud project. You will see this as a`GET`of the (non-existent) global`BackendService`with the name`k8s-ingress-svc-acct-permission-check-probe`. As this resource should not normally exist, the`GET`request will return \"not found\". This is expected; the controller is checking that the API call is not rejected due to authorization issues. If you create a BackendService with the same name, then the`GET`will succeed instead of returning \"not found\".## Templates for the Ingress configuration\n- In [GKE Networking Recipes](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main) , you can find templates provided by GKE on many common Ingress usage under the [Ingress](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/single-cluster) section.## What's next\n- [Learn about GKE Networking Recipes](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main) \n- [Learn more about load balancing in Google Cloud](/load-balancing/docs/https) .\n- [Read an overview of networking in GKE](/kubernetes-engine/docs/concepts/network-overview) .\n- [Learn how to configure Ingress for internal Application Load Balancers](/kubernetes-engine/docs/how-to/internal-load-balance-ingress) .\n- [Learn how to configure Ingress for external Application Load Balancers](/kubernetes-engine/docs/how-to/load-balance-ingress) .", "guide": "Google Kubernetes Engine (GKE)"}