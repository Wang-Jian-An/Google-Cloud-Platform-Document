{"title": "Google Kubernetes Engine (GKE) - Creating a private cluster", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters", "abstract": "# Google Kubernetes Engine (GKE) - Creating a private cluster\nThis page explains how to create a private Google Kubernetes Engine (GKE) cluster, which is a type of VPC-native cluster. In a private cluster, nodes only have [internal IP addresses](/vpc/docs/ip-addresses) , which means that nodes and Pods are isolated from the internet by default.\nInternal IP addresses for nodes come from the primary IP address range of the subnet you choose for the cluster. Pod IP addresses and Service IP addresses come from two subnet secondary IP address ranges of that same subnet. For more information, see [IP ranges for VPC-native clusters](/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing) .\nGKE versions 1.14.2 and later support any internal IP address ranges, including private ranges (RFC 1918 and other private ranges) and privately used public IP address ranges. See the VPC documentation for a list of [valid internal IP address ranges](/vpc/docs/subnets#valid-ranges) .\nTo learn more about how private clusters work, see [Private clusters](/kubernetes-engine/docs/concepts/private-cluster-concept) .\n", "content": "## Before you begin\nMake yourself familiar with the [requirements, restrictions, and limitations](#req_res_lim) before moving to the next step.\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure you have the correct permission to create clusters. At minimum, you should be a [Kubernetes Engine Cluster Admin](/iam/docs/understanding-roles#kubernetes-engine-roles) .\n- Ensure you have a route to the Default Internet Gateway.## Creating a private cluster with no client access to the public endpoint\nIn this section, you create the following resources:\n- A private cluster named`private-cluster-0`that has private nodes, and that has no client access to the public endpoint.\n- A network named`my-net-0`.\n- A subnet named`my-subnet-0`.\n**Important:** Even if you disable access to the public endpoint, GKE can use the control plane's public endpoint for cluster management purposes, such as [scheduled maintenance](/kubernetes-engine/docs/scheduled-maintenance) and [automatic upgrades](/kubernetes-engine/versioning-and-upgrades#automatic_cp_upgrades) .\n### Create a network and subnet\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click **Create VPC network** .\n- For **Name** , enter `my-net-0` .\n- For **Subnet creation mode** , select **Custom** .\n- In the **New subnet** section, for **Name** , enter `my-subnet-0` .\n- In the **Region** list, select the region that you want.\n- For **IP address range** , enter `10.2.204.0/22` .\n- Set **Private Google Access** to **On** .\n- Click **Done** .\n- Click **Create** .\n### Create a private cluster\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click add_box **Create** then in the Standard or Autopilot section, click **Configure** .\n- For the **Name** , specify `private-cluster-0` .\n- In the navigation pane, click **Networking** .\n- In the **Network** list, select **my-net-0** .\n- In the **Node subnet** list, select **my-subnet-0** .\n- Select the **Private cluster** radio button.\n- Clear the **Access control plane using its external IP address** checkbox.\n- (Optional for Autopilot): Set **Control plane IP range** to `172.16.0.32/28` .\n- Click **Create** .\n- For Autopilot clusters, run the following command:```\ngcloud container clusters create-auto private-cluster-0 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-0 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --enable-private-endpoint\n```\n- For Standard clusters, run the following command:```\ngcloud container clusters create private-cluster-0 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-0 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --enable-private-endpoint \\\u00a0 \u00a0 --master-ipv4-cidr 172.16.0.32/28\n```\nwhere:- `--create-subnetwork name=my-subnet-0`causes GKE to automatically create a subnet named`my-subnet-0`.\n- `--enable-master-authorized-networks`specifies that access to the public endpoint is restricted to IP address ranges that you authorize.\n- `--enable-ip-alias`makes the cluster VPC-native (not required for Autopilot).\n- `--enable-private-nodes`indicates that the cluster's nodes don't have external IP addresses.\n- `--enable-private-endpoint`indicates that the cluster is managed using the internal IP address of the control plane API endpoint.\n- `--master-ipv4-cidr 172.16.0.32/28`specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The [use of non RFC 1918 internal IP addresses](/kubernetes-engine/docs/concepts/alias-ips#internal_ip_addresses) is supported.\nTo create a cluster without a publicly-reachable control plane, specify the `enablePrivateEndpoint: true` field in the `privateClusterConfig` resource.\n**Note:** In an Autopilot cluster, the `--master-ipv4-cidr` parameter is assigned a subnet within the 172.16.0.0/16 range by default. This range allows for various subnet sizes.\nAt this point, these are the only IP addresses that have access to the control plane:\n- The primary range of`my-subnet-0`.\n- The secondary range used for Pods.\nFor example, suppose you created a VM in the primary range of `my-subnet-0` . Then on that VM, you could [configure kubectl to use the internal IP address](/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#internal_ip) of the control plane.\nIf you want to access the control plane from outside `my-subnet-0` , you must authorize at least one address range to have access to the private endpoint.\nSuppose you have a VM that is in the default network, in the same region as your cluster, but not in `my-subnet-0` .\nFor example:\n- `my-subnet-0`:`10.0.0.0/22`\n- Pod secondary range:`10.52.0.0/14`\n- VM address:`10.128.0.3`\nYou could authorize the VM to access the control plane by using this command:\n```\ngcloud container clusters update private-cluster-0 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --master-authorized-networks 10.128.0.3/32\n```\n## Creating a private cluster with limited access to the public endpoint\nWhen creating a private cluster using this configuration, you can choose to use an automatically generated subnet, or a custom subnet.\n### Using an automatically generated subnet\nIn this section, you create a private cluster named `private-cluster-1` where GKE automatically generates a subnet for your cluster nodes. The subnet has Private Google Access enabled. In the subnet, GKE automatically creates two secondary ranges: one for Pods and one for Services.\nYou can use the Google Cloud CLI or the GKE API.\n- For Autopilot clusters, run the following command:```\ngcloud container clusters create-auto private-cluster-1 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-1 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --enable-private-nodes\n```\n- For Standard clusters, run the following command:```\ngcloud container clusters create private-cluster-1 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-1 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --master-ipv4-cidr 172.16.0.0/28\n```\nwhere:- `--create-subnetwork name=my-subnet-1`causes GKE to automatically create a subnet named`my-subnet-1`.\n- `--enable-master-authorized-networks`specifies that access to the public endpoint is restricted to IP address ranges that you authorize.\n- `--enable-ip-alias`makes the cluster VPC-native (not required for Autopilot).\n- `--enable-private-nodes`indicates that the cluster's nodes don't have external IP addresses.\n- `--master-ipv4-cidr 172.16.0.0/28`specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The [use of non RFC 1918 internal IP addresses](/kubernetes-engine/docs/concepts/alias-ips#internal_ip_addresses) is supported.\nSpecify the `privateClusterConfig` field in the `Cluster` API resource:\n```\n{\u00a0 \"name\": \"private-cluster-1\",\u00a0 ...\u00a0 \"ipAllocationPolicy\": {\u00a0 \u00a0 \"createSubnetwork\": true,\u00a0 },\u00a0 ...\u00a0 \u00a0 \"privateClusterConfig\" {\u00a0 \u00a0 \u00a0 \"enablePrivateNodes\": boolean # Creates nodes with internal IP addresses only\u00a0 \u00a0 \u00a0 \"enablePrivateEndpoint\": boolean # false creates a cluster control plane with a publicly-reachable endpoint\u00a0 \u00a0 \u00a0 \"masterIpv4CidrBlock\": string # CIDR block for the cluster control plane\u00a0 \u00a0 \u00a0 \"privateEndpoint\": string # Output only\u00a0 \u00a0 \u00a0 \"publicEndpoint\": string # Output only\u00a0 }}\n```\nAt this point, these are the only IP addresses that have access to the cluster control plane:\n- The primary range of`my-subnet-1`.\n- The secondary range used for Pods.\nSuppose you have a group of machines, outside of your VPC network, that have addresses in the range `203.0.113.0/29` . You could authorize those machines to access the public endpoint by entering this command:\n```\ngcloud container clusters update private-cluster-1 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --master-authorized-networks 203.0.113.0/29\n```\nNow these are the only IP addresses that have access to the control plane:\n- The primary range of`my-subnet-1`.\n- The secondary range used for Pods.\n- Address ranges that you have authorized, for example,`203.0.113.0/29`.\n### Using a custom subnet\nIn this section, you create the following resources:\n- A private cluster named`private-cluster-2`.\n- A network named`my-net-2`.\n- A subnet named`my-subnet-2`, with primary range`192.168.0.0/20`, for your cluster nodes. Your subnet has the following secondary address ranges:- `my-pods`for the Pod IP addresses.\n- `my-services`for the Service IP addresses.\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click **Create VPC network** .\n- For **Name** , enter `my-net-2` .\n- For **Subnet creation mode** , select **Custom** .\n- In the **New subnet** section, for **Name** , enter `my-subnet-2` .\n- In the **Region** list, select the region that you want.\n- For **IP address range** , enter `192.168.0.0/20` .\n- Click **Create secondary IP range** . For **Subnet range name** , enter `my-services` , and for **Secondary IP range** , enter `10.0.32.0/20` .\n- Click **Add IP range** . For **Subnet range name** , enter `my-pods` , and for **Secondary IP range** , enter `10.4.0.0/14` .\n- Set **Private Google Access** to **On** .\n- Click **Done** .\n- Click **Create** .\nCreate a private cluster that uses your subnet:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click add_box **Create** then in the Standard or Autopilot section, click **Configure** .\n- For the **Name** , enter `private-cluster-2` .\n- From the navigation pane, click **Networking** .\n- Select the **Private cluster** radio button.\n- To create a control plane that is accessible from authorized external IP ranges, keep the **Access control plane using its external IP address** checkbox selected.\n- (Optional for Autopilot) Set **Control plane IP range** to `172.16.0.16/28` .\n- In the **Network** list, select **my-net-2** .\n- In the **Node subnet** list, select **my-subnet-2** .\n- Clear the **Automatically create secondary ranges** checkbox.\n- In the **Pod secondary CIDR range** list, select **my-pods** .\n- In the **Services secondary CIDR range** list, select **my-services** .\n- Select the **Enable control plane authorized networks** checkbox.\n- Click **Create** .\nFirst, create a network for your cluster. The following command creates a network, `my-net-2` :\n```\ngcloud compute networks create my-net-2 \\\u00a0 \u00a0 --subnet-mode custom\n```Next, create a subnet, `my-subnet-2` , in the `my-net-2` network, with secondary ranges `my-pods` for Pods and `my-services` for Services:\n```\ngcloud compute networks subnets create my-subnet-2 \\\u00a0 \u00a0 --network my-net-2 \\\u00a0 \u00a0 --range 192.168.0.0/20 \\\u00a0 \u00a0 --secondary-range my-pods=10.4.0.0/14,my-services=10.0.32.0/20 \\\u00a0 \u00a0 --enable-private-ip-google-access\n```Now, create a private cluster, `private-cluster-2` , using the network, subnet, and secondary ranges you created.- For Autopilot clusters, run the following command:```\ngcloud container clusters create-auto private-cluster-2 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --network my-net-2 \\\u00a0 \u00a0 --subnetwork my-subnet-2 \\\u00a0 \u00a0 --cluster-secondary-range-name my-pods \\\u00a0 \u00a0 --services-secondary-range-name my-services \\\u00a0 \u00a0 --enable-private-nodes\n```\n- For Standard clusters, run the following command:```\ngcloud container clusters create private-cluster-2 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --network my-net-2 \\\u00a0 \u00a0 --subnetwork my-subnet-2 \\\u00a0 \u00a0 --cluster-secondary-range-name my-pods \\\u00a0 \u00a0 --services-secondary-range-name my-services \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --master-ipv4-cidr 172.16.0.16/28 \\\u00a0 \u00a0 --no-enable-basic-auth \\\u00a0 \u00a0 --no-issue-client-certificate\n```At this point, these are the only IP addresses that have access to the control plane:\n- The primary range of`my-subnet-2`.\n- The secondary range`my-pods`.\nSuppose you have a group of machines, outside of `my-net-2` , that have addresses in the range `203.0.113.0/29` . You could authorize those machines to access the public endpoint by entering this command:\n```\ngcloud container clusters update private-cluster-2 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --master-authorized-networks 203.0.113.0/29\n```\nAt this point, these are the only IP addresses that have access to the control plane:\n- The primary range of`my-subnet-2`.\n- The secondary range`my-pods`.\n- Address ranges that you have authorized, for example,`203.0.113.0/29`.\n### Using Cloud Shell to access a private cluster\n**Important:** If you have enabled a private endpoint, you cannot access your GKE control plane with Cloud Shell.\nThe private cluster you created in the [Using an automatically generated subnet](#auto_subnet) section, `private-cluster-1` , has a public endpoint and has authorized networks enabled. If you want to use [Cloud Shell](/shell/docs/quickstart) to access the cluster, you must add the external IP address of your Cloud Shell to the cluster's list of authorized networks.\nTo do this:\n- In your Cloud Shell command-line window, use `dig` to find the external IP address of your Cloud Shell:```\ndig +short myip.opendns.com @resolver1.opendns.com\n```\n- Add the external address of your Cloud Shell to your cluster's list of authorized networks:```\ngcloud container clusters update private-cluster-1 \\\u00a0 \u00a0 --enable-master-authorized-networks \\\u00a0 \u00a0 --master-authorized-networks EXISTING_AUTH_NETS,SHELL_IP/32\n```Replace the following:- `` : the IP addresses of your existing list of authorized networks. You can find your authorized networks in the console or by running the following command:```\ngcloud container clusters describe private-cluster-1 --format \"flattened(masterAuthorizedNetworksConfig.cidrBlocks[])\"\n```\n- `` : the external IP address of your Cloud Shell.\n- Get credentials, so that you can use `kubectl` to access the cluster:```\ngcloud container clusters get-credentials private-cluster-1 \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --internal-ip\n```Replace `` with your project ID.\n- Use `kubectl` , in Cloud Shell, to access your private cluster:```\nkubectl get nodes\n```The output is similar to the following:```\nNAME            STATUS ROLES AGE VERSION\ngke-private-cluster-1-default-pool-7d914212-18jv Ready <none> 104m v1.21.5-gke.1302\ngke-private-cluster-1-default-pool-7d914212-3d9p Ready <none> 104m v1.21.5-gke.1302\ngke-private-cluster-1-default-pool-7d914212-wgqf Ready <none> 104m v1.21.5-gke.1302\n```## Creating a private cluster with unrestricted access to the public endpoint\nIn this section, you create a private cluster where any IP address can access the control plane.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click add_box **Create** then in the Standard or Autopilot section, click **Configure** .\n- For the **Name** , enter `private-cluster-3` .\n- In the navigation pane, click **Networking** .\n- Select the **Private cluster** option.\n- Keep the **Access control plane using its external IP address** checkbox selected.\n- (Optional for Autopilot) Set **Control plane IP range** to `172.16.0.32/28` .\n- Leave **Network** and **Node subnet** set to `default` . This causes GKE to generate a subnet for your cluster.\n- Clear the **Enable control plane authorized networks** checkbox.\n- Click **Create** .\n- For Autopilot clusters, run the following command:```\ngcloud container clusters create-auto private-cluster-3 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-3 \\\u00a0 \u00a0 --no-enable-master-authorized-networks \\\u00a0 \u00a0 --enable-private-nodes\n```\n- For Standard clusters, run the following command:```\ngcloud container clusters create private-cluster-3 \\\u00a0 \u00a0 --create-subnetwork name=my-subnet-3 \\\u00a0 \u00a0 --no-enable-master-authorized-networks \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --master-ipv4-cidr 172.16.0.32/28\n```\nwhere:- `--create-subnetwork name=my-subnet-3`causes GKE to automatically create a subnet named`my-subnet-3`.\n- `--no-enable-master-authorized-networks`disables authorized networks for the cluster.\n- `--enable-ip-alias`makes the cluster VPC-native (not required for Autopilot).\n- `--enable-private-nodes`indicates that the cluster's nodes don't have external IP addresses.\n- `--master-ipv4-cidr 172.16.0.32/28`specifies an internal IP address range for the control plane (optional for Autopilot). This setting is permanent for this cluster and must be unique within the VPC. The [use of non RFC 1918 internal IP addresses](/kubernetes-engine/docs/concepts/alias-ips#internal_ip_addresses) is supported.\n## Other private cluster configurations\nIn addition to the preceding configurations, you can run private clusters with the following configurations.\n### Granting private nodes outbound internet access\nTo provide outbound internet access for your private nodes, such as to pull images from an external registry, use [Cloud NAT](/nat/docs/overview) to create and configure a Cloud Router. Cloud NAT lets private clusters establish outbound connections over the internet to send and receive packets.\nThe Cloud Router allows all your nodes in the region to use Cloud NAT for all primary and [alias IP ranges](/vpc/docs/alias-ip) . It also automatically allocates the external IP addresses for the NAT gateway.\nFor instructions to create and configure a Cloud Router, refer to [Create a Cloud NAT configuration using Cloud Router](/nat/docs/gke-example#create-nat) in the Cloud NAT documentation.\n**Note:** To provide outbound internet access for your Pods and Services, configure Cloud NAT to [Specify subnet ranges for NAT](/nat/docs/set-up-manage-network-address-translation#specify_subnet_ranges_for_nat)\n### Creating a private cluster in a Shared VPC network\nTo learn how to create a private cluster in a Shared VPC network, see [Creating a private cluster in a Shared VPC](/kubernetes-engine/docs/how-to/cluster-shared-vpc#create_private_cluster) .\n### Deploying a Windows Server container application to a private cluster\nTo learn how to deploy a Windows Server container application to a private cluster, refer to the [Windows node pool documentation](/kubernetes-engine/docs/how-to/deploying-windows-app#deploying_a_windows_server_application_to_a_private_cluster) .\n### Accessing the control plane's private endpoint globally\nThe control plane's private endpoint is implemented by an internal passthrough Network Load Balancer in the control plane's VPC network. Clients that are internal or are connected through Cloud VPN tunnels and Cloud Interconnect VLAN attachments can access internal passthrough Network Load Balancers.\nBy default, these clients must be [located in the same region](/load-balancing/docs/internal#client-access) as the load balancer.\nWhen you enable control plane global access, the internal passthrough Network Load Balancer is globally accessible: Client VMs and on-premises systems can connect to the control plane's private endpoint, subject to the authorized networks configuration, from any region.\nFor more information about the internal passthrough Network Load Balancers and global access, see [Internalload balancers and connectednetworks](/load-balancing/docs/internal/internal-tcp-udp-lb-and-other-networks) .\n**Note:** Egress charges apply for traffic between regions in the Google Cloud network, so accessing the control plane's private endpoint from another region incurs [standard cross-regional egress charges](/vpc/network-pricing#general) .\nBy default, global access is not enabled for the control plane's private endpoint when you create a private cluster. To enable control plane global access, use the following tools based on your cluster mode:\n- For Standard clusters, you can use`Google Cloud CLI`or the Google Cloud console.\n- For Autopilot clusters, you can use the`google_container_cluster`Terraform resource.\nTo create a new private cluster with control plane global access enabled, perform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click add_box **Create** then in the Standard or Autopilot section, click **Configure** .\n- Enter a **Name** .\n- In the navigation pane, click **Networking** .\n- Select **Private cluster** .\n- Select the **Enable Control plane global access** checkbox.\n- Configure other fields as you want.\n- Click **Create** .\nTo enable control plane global access for an existing private cluster, perform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to the cluster you want to edit, click **Actions** , then click **Edit** .\n- In the **Networking** section, next to **Control plane globalaccess** , click **Edit** .\n- In the **Edit control plane global access** dialog, select the **Enable Control plane global access** checkbox.\n- Click **Save Changes** .\nAdd the `--enable-master-global-access` flag to create a private cluster with global access to the control plane's private endpoint enabled:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --enable-master-global-access\n```\nYou can also enable global access to the control plane's private endpoint for an existing private cluster:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-master-global-access\n```\n **Note:** To disable control plane private endpoint global access, use the `--no-enable-master-global-access` flag.\nYou can verify that global access to the control plane's private endpoint is enabled by running the following command and looking at its output.\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nThe output includes a `privateClusterConfig` section where you can see the status of `masterGlobalAccessConfig` .\n```\nprivateClusterConfig:\n enablePrivateNodes: true\n masterIpv4CidrBlock: 172.16.1.0/28\n peeringName: gke-1921aee31283146cdde5-9bae-9cf1-peer\n privateEndpoint: 172.16.1.2\n publicEndpoint: 34.68.128.12\n masterGlobalAccessConfig:\n enabled: true\n```\n### Accessing the control plane's private endpoint from other networks\nWhen you create a [GKE private cluster](/kubernetes-engine/docs/how-to/private-clusters) and disable the control plane's public endpoint, you must administer the cluster with tools like `kubectl` using its control plane's private endpoint. You can access the cluster's control plane's private endpoint from another network, including the following:\n- An on-premises network that's connected to the cluster's VPC network using Cloud VPN tunnels or Cloud Interconnect VLAN attachments\n- Another VPC network that's connected to the cluster's VPC network using Cloud VPN tunnels\nThe following diagram shows a routing path between an on-premises network and GKE control plane nodes:\nTo allow systems in another network to connect to a cluster's control plane private endpoint, complete the following requirements:\n- Identify and record relevant network information for the cluster and its control plane's private endpoint.```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0--location=COMPUTE_LOCATION \\\u00a0 \u00a0--format=\"yaml(network, privateClusterConfig)\"\n```Replace the following:- ``: the name for the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) of the cluster\nFrom the output of the command, identify and record the following information to use in the next steps:- `network`: The name or URI for the cluster's VPC network.\n- `privateEndpoint`: The IPv4 address of the control plane's private endpoint or the enclosing IPv4 CIDR range (`masterIpv4CidrBlock`).\n- `peeringName`: The name of the VPC Network Peering connection used to connect the cluster's VPC network to the control plane's VPC network.\nThe output is similar to the following:```\nnetwork: cluster-networkprivateClusterConfig:\u00a0 enablePrivateNodes: true\u00a0 masterGlobalAccessConfig:\u00a0 \u00a0 enabled: true\u00a0 masterIpv4CidrBlock: 172.16.1.0/28\u00a0 peeringName: gke-1921aee31283146cdde5-9bae-9cf1-peer\u00a0 privateEndpoint: 172.16.1.2\u00a0 publicEndpoint: 34.68.128.12\n```\n- Consider [enabling control plane private endpoint globalaccess](#cp-global-access-enable) to allow packets to enter from any region in the cluster's VPC network. Enabling control plane private endpoint global access lets you connect to the private endpoint using Cloud VPN tunnels or Cloud Interconnect VLAN attachments located in any region, not just the cluster's region.\n- Create a route for the `privateEndpoint` IP address or the `masterIpv4CidrBlock` IP address range in the other network. Because the control plane's private endpoint IP address always fits within the `masterIpv4CidrBlock` IPv4 address range, creating a route for either the `privateEndpoint` IP address or its enclosing range provides a path for packets from the other network to the control plane's private endpoint if:- **The other network connects to the cluster's VPCnetwork using Cloud Interconnect VLAN attachments or Cloud VPNtunnels that use dynamic (BGP) routes** : Use a Cloud Router custom route advertisement. For more information, see [Advertising CustomIPRanges](/network-connectivity/docs/router/docs/how-to/advertising-custom-ip) in the Cloud Router documentation.\n- **The other network connects to the cluster's VPCnetwork using Classic VPN tunnels that do not use dynamicroutes** : You must configure a static route in the other network.\n **Important:** If [control plane global access](#cp-global-access) is disabled, the Cloud VPN tunnels or Cloud Interconnect VLAN attachments that connect the cluster's VPC network to the other network must be located in the same region as the cluster.\n- Configure the cluster's VPC network to export its custom routes in the peering relationship to the control plane's VPC network. Google Cloud always configures the control plane's VPC network to import custom routes from the cluster's VPC network. This step provides a path for packets from the control plane's private endpoint back to the other network.To enable custom route export from your cluster's VPC network, use the following command:```\ngcloud compute networks peerings update PEERING_NAME \\\u00a0 \u00a0 --network=CLUSTER_VPC_NETWORK \\\u00a0 \u00a0 --export-custom-routes\n```Replace the following:- ``: the name for the peering that connects the cluster's VPC network to the control plane VPC network\n- ``: the name or URI of the cluster's VPC network\nFor more details about how to update route exchange for existing VPC Network Peering connections, see [Update the peeringconnection](/vpc/docs/using-vpc-peering#update-peer-connection) .Custom routes in the cluster's VPC network include routes whose destinations are IP address ranges in other networks, for example, an on-premises network. To ensure that these routes become effective as custom routes in the control plane's VPC network, see [Supported destinations from the other network](#to_on_prem) .The address ranges that the other network sends to Cloud Routers in the cluster's VPC network must adhere to the following conditions:\n- While your cluster's VPC might accept a default route ( `0.0.0.0/0` ), the control plane's VPC network always rejects default routes because it already has a local default route. If the other network sends a default route to your VPC network, the other network must also send the specific destinations of systems that need to connect to the control plane's private endpoint. For more details, see [Routing order](/vpc/docs/routes#routeselection) .\n- If the control plane's VPC network accepts routes that effectively replace a default route, those routes break connectivity to Google Cloud APIs and services, interrupting the cluster control plane. As a representative example, the other network must not advertise routes with destinations `0.0.0.0/1` and `128.0.0.0/1` . Refer to the previous point for an alternative.\nMonitor the [Cloud Router limits](/router/quotas#limits) , especially the maximum number of unique destinations for learned routes.\n## Verify that nodes don't have external IP addresses\nAfter you create a private cluster, verify that the cluster's nodes don't have external IP addresses.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the list of clusters, click the cluster name.\n- For Autopilot clusters, in the **Cluster basics** section, check the **External endpoint** field. The value is **Disabled** .\nFor Standard clusters, do the following:- On the **Clusters** page, click the **Nodes** tab.\n- Under **Node Pools** , click the node pool name.\n- On the **Node pool details** page, under **Instance groups** , click the name of your instance group. For example, gke-private-cluster-0-default-pool-5c5add1f-grp`.\n- In the list of instances, verify that your instances do not have external IP addresses.\nRun the following command:\n```\nkubectl get nodes --output wide\n```\nThe output's `EXTERNAL-IP` column is empty:\n```\nSTATUS ... VERSION  EXTERNAL-IP OS-IMAGE ...\nReady  v.8.7-gke.1     Container-Optimized OS\nReady  v1.8.7-gke.1    Container-Optimized OS\nReady  v1.8.7-gke.1    Container-Optimized OS\n```\n## Viewing the cluster's subnet and secondary address ranges\nAfter you create a private cluster, you can view the subnet and secondary address ranges that you or GKE provisioned for the cluster.\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click the name of the subnet. For example, `gke-private-cluster-0-subnet-163e3c97` .\n- Under **IP address range** , you can see the primary address range of your subnet. This is the range used for nodes.\n- Under **Secondary IP ranges** , you can see the IP address range for Pods and the range for Services.\nTo list the subnets in your cluster's network, run the following command:\n```\ngcloud compute networks subnets list \\\u00a0 \u00a0 --network NETWORK_NAME\n```\nReplace `` with the private cluster's network. If you created the cluster with an automatically-created subnet, use `default` .\nIn the command output, find the name of the cluster's subnet.Get information about the automatically created subnet:\n```\ngcloud compute networks subnets describe SUBNET_NAME\n```\nReplace `` with the name of the subnet.\nThe output shows the primary address range for nodes (the first `ipCidrRange` field) and the secondary ranges for Pods and Services (under `secondaryIpRanges` ):\n```\n...\nipCidrRange: 10.0.0.0/22\nkind: compute#subnetwork\nname: gke-private-cluster-1-subnet-163e3c97\n...\nprivateIpGoogleAccess: true\n...\nsecondaryIpRanges:\n- ipCidrRange: 10.40.0.0/14\n rangeName: gke-private-cluster-1-pods-163e3c97\n- ipCidrRange: 10.0.16.0/20\n rangeName: gke-private-cluster-1-services-163e3c97\n...\n```\n## Viewing a private cluster's endpoints\nYou can view a private cluster's endpoints using the gcloud CLI or the Google Cloud console.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the cluster name.\n- In the **Details** tab, under **Cluster basics** , look for the **Endpoint** field.\nRun the following command:\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nThe output shows both the private and public endpoints:\n```\n...\nprivateClusterConfig:\nenablePrivateEndpoint: true\nenablePrivateNodes: true\nmasterIpv4CidrBlock: 172.16.0.32/28\nprivateEndpoint: 172.16.0.34\npublicEndpoint: 35.239.154.67\n```\n## Pulling container images from an image registry\nIn a private cluster, the container runtime can pull container images from [Artifact Registry](/artifact-registry/docs) ; it cannot pull images from any other container image registry on the internet. This is because the nodes in a private cluster don't have external IP addresses, so by default they cannot communicate with services outside of the Google Cloud network.\nThe nodes in a private cluster can communicate with Google Cloud services, like Artifact Registry, if they are on a subnet that has [Private Google Access](/vpc/docs/private-google-access) enabled.\nThe following commands create a Deployment that pulls a sample image from an Artifact Registry repository:\n```\nkubectl run hello-deployment --image=us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\n```\n**Note:** While the [Docker Hub mirror](/container-registry/docs/using-dockerhub-mirroring) `mirror.gcr.io` is accessible from a private cluster, it shouldn't be exclusively relied upon. The mirror is only a cache, so images are periodically removed, and a private cluster is not able to fall back to Docker Hub.\n## Adding firewall rules for specific use cases\nThis section explains how to add a firewall rule to a private cluster. By default, firewall rules restrict your cluster control plane to only initiate TCP connections to your nodes and Pods on ports `443` (HTTPS) and `10250` (kubelet). For some Kubernetes features, you might need to add firewall rules to allow access on additional ports.\n**Note:** The allowed ports ( `443` and `10250` ) refer to the ports exposed by your nodes and Pods, the ports exposed by any Kubernetes services. For example, if the cluster control plane attempts to access a service on port `443` , but the service is by a pod using port `9443` , this will be blocked by the firewall unless you add a firewall rule to explicitly allow ingress to port `9443` .\nKubernetes features that require additional firewall rules include:\n- [Admission webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) \n- [Aggregated API servers](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/) \n- [Webhook conversion](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion) \n- [Dynamic audit configuration](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#service-reference) \n- Generally, any API that has a ServiceReference field requires additional firewall rules.\nAdding a firewall rule allows traffic from the cluster control plane to all of the following:\n- The specified port of each node (hostPort).\n- The specified port of each Pod running on these nodes.\n- The specified port of each Service running on these nodes.\nTo learn about firewall rules, refer to [Firewall rules](/load-balancing/docs/https#firewall_rules) in the Cloud Load Balancing documentation.\nTo add a firewall rule in a private cluster, you need to record the cluster control plane's CIDR block and the target used. After you have recorded this you can create the rule.\n### Step 1. View control plane's CIDR block\nYou need the cluster control plane's CIDR block to add a firewall rule.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the cluster name.\nIn the **Details** tab, under **Networking** , take note of the value in the **Control plane address range** field.Run the following command:\n```\ngcloud container clusters describe CLUSTER_NAME\n```\nReplace `` with the name of your private cluster.\nIn the command output, take note of the value in the **masterIpv4CidrBlock** field.### Step 2. View existing firewall rules\nYou need to specify the [target](/vpc/docs/firewalls#rule_assignment) (in this case, the destination nodes) that the cluster's existing firewall rules use.\n- Go to the **Firewall policies** page in the Google Cloud console. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- For **Filter table** for **VPC firewall rules** , enter `gke-` `` .\nIn the results, take note of the value in the **Targets** field.Run the following command:\n```\ngcloud compute firewall-rules list \\\u00a0 \u00a0 --filter 'name~^gke-CLUSTER_NAME' \\\u00a0 \u00a0 --format 'table(\u00a0 \u00a0 \u00a0 \u00a0 name,\u00a0 \u00a0 \u00a0 \u00a0 network,\u00a0 \u00a0 \u00a0 \u00a0 direction,\u00a0 \u00a0 \u00a0 \u00a0 sourceRanges.list():label=SRC_RANGES,\u00a0 \u00a0 \u00a0 \u00a0 allowed[].map().firewall_rule().list():label=ALLOW,\u00a0 \u00a0 \u00a0 \u00a0 targetTags.list():label=TARGET_TAGS\u00a0 \u00a0 )'\n```\nIn the command output, take note of the value in the **Targets** field.\n **Note** : To view firewall rules for a Shared VPC, add the`--project` ``flag to the command.For more information on Shared VPC, see [Setting up clusters with Shared VPC](/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules) .### Step 3. Add a firewall rule\n- Go to the **Firewall policies** page in the Google Cloud console. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- Click **Create Firewall Rule** .\n- For **Name** , enter the name for the firewall rule.\n- In the **Network** list, select the relevant network.\n- In **Direction of traffic** , click **Ingress** .\n- In **Action on match** , click **Allow** .\n- In the **Targets** list, select **Specified target tags** .\n- For **Target tags** , enter the target value that you noted previously.\n- In the **Source filter** list, select **IPv4 ranges** .\n- For **Source IPv4 ranges** , enter the cluster control plane's CIDR block.\n- In **Protocols and ports** , click **Specified protocols and ports** , select the checkbox for the relevant protocol ( **tcp** or **udp** ), and enter the port number in the protocol field.\n- Click **Create** .\nRun the following command:\n```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME \\\u00a0 \u00a0 --action ALLOW \\\u00a0 \u00a0 --direction INGRESS \\\u00a0 \u00a0 --source-ranges CONTROL_PLANE_RANGE \\\u00a0 \u00a0 --rules PROTOCOL:PORT \\\u00a0 \u00a0 --target-tags TARGET\n```\nReplace the following:- ``: the name you choose for the firewall rule.\n- ``: the cluster control plane's IP address range (`masterIpv4CidrBlock`) that you collected previously.\n- `` `:` ``: the port and its protocol,`tcp`or`udp`.\n- ``: the target (`Targets`) value that you collected previously.\n **Note** : To add a firewall rule for a Shared VPC, add the following flags to the command:```\n--project HOST_PROJECT_ID\n--network NETWORK_ID\n```For more information on Shared VPC, see [Setting up clusters with Shared VPC](/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules) .\n## Protecting a private cluster with VPC Service Controls\nTo further secure your GKE private clusters, you can protect them using VPC Service Controls.\nVPC Service Controls provides additional security for your GKE private clusters to help mitigate the risk of data exfiltration. Using VPC Service Controls, you can add projects to service perimeters that protect resources and services from requests that originate outside the perimeter.\nTo learn more about service perimeters, see [Service perimeter details and configuration](/vpc-service-controls/docs/service-perimeters) .\nIf you use Artifact Registry with your GKE private cluster in a VPC Service Controls service perimeter, you must [configure routing to the restricted virtual IP](/vpc-service-controls/docs/set-up-gke) to prevent exfiltration of data.\n## VPC peering reuse\nAny private clusters you create after January 15, 2020 [reuse VPC Network Peering connections](/kubernetes-engine/docs/concepts/private-cluster-concept#network_peering_reuse) .\nAny private clusters you created prior to January 15, 2020 use a unique VPC Network Peering connection. Each VPC network can peer with up to 25 other VPC networks which means for these clusters there is a limit of at most 25 private clusters per network (assuming peerings are not being used for other purposes).\nThis feature is not backported to previous releases. To enable VPC Network Peering reuse on older private clusters, you can delete a cluster and recreate it. Upgrading a cluster does not cause it to reuse an existing VPC Network Peering connection.\nEach location can support a maximum of 75 private clusters if the clusters have VPC Network Peering reuse enabled. Zones and regions are treated as separate locations.\nFor example, you can create up to 75 private zonal clusters in `us-east1-a` and another 75 private regional clusters in `us-east1` . This also applies if you are using private clusters in a Shared VPC network. The [maximum number of connections to a single VPC network is 25](/vpc/docs/quota#vpc-peering) , which means you can only create private clusters using 25 unique locations.\nVPC Network Peering reuse only applies to clusters in the same location, for example regional clusters in the same region or zonal clusters in the same zone. At maximum, you can have four VPC Network Peerings per region if you create both regional clusters and zonal clusters in all of the zones of that region.\nYou can check if your private cluster reuses VPC Network Peering connections using the gcloud CLI or the Google Cloud console.\nCheck the **VPC peering** row on the **Cluster details** page. If your cluster is reusing VPC peering connections, the output begins with `gke-n` . For example, `gke-n34a117b968dee3b2221-93c6-40af-peer` .```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --format=\"value(privateClusterConfig.peeringName)\"\n```\nIf your cluster is reusing VPC peering connections, the output begins with `gke-n` . For example, `gke-n34a117b968dee3b2221-93c6-40af-peer` .\n**Note:** To configure more than 75 private clusters per location, contact the Google Cloud support team. This is only applicable if your clusters have VPC Network Peering reuse enabled.\n## Cleaning up\n**Note:** If you delete a VPC Network Peering connection while there are resources attached on the GKE cluster, the cluster goes in a repair state resulting in permanent loss of the attached resources. You can't recover the deleted resources. For information on workaround, see [Troubleshooting: VPC Network Peering connection on privatecluster is accidentallydeleted](/kubernetes-engine/docs/how-to/private-clusters#vpc_peering_network) .\nAfter completing the tasks on this page, follow these steps to remove the resources to prevent unwanted charges incurring on your account:\n### Delete the clusters\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Select each cluster.\n- Click **Delete** .\n```\ngcloud container clusters delete -q private-cluster-0 private-cluster-1 private-cluster-2 private-cluster-3\n```### Delete the network\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the list of networks, click `my-net-0` .\n- On the **VPC network details** page, click **Delete VPC Network** .\n- In the **Delete a network** dialog, click **Delete** .\n```\ngcloud compute networks delete my-net-0\n```\n## Requirements, restrictions, and limitations\nPrivate clusters have the following :\n- Private clusters must be [VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) . VPC-native clusters don't support [legacy networks](/vpc/docs/legacy) .\nPrivate clusters have the following :\n- You cannot convert an existing, non-private cluster to a private cluster.\n- When you use`172.17.0.0/16`for your control plane IP range, you cannot use this range for nodes, Pod, or Services IP addresses.\n- Deleting the VPC peering between the cluster control plane and the cluster nodes, deleting the firewall rules that allow ingress traffic from the cluster control plane to nodes on port 10250, or deleting the default route to the default internet gateway, causes a private cluster to stop functioning. If you delete the default route, you must ensure traffic to necessary Google Cloud services is routed. For more information, see [custom routing](/vpc/docs/configure-private-google-access#config-routing-custom) .\n- When custom route export is enabled for the VPC, creating routes that overlap with [Google Cloud IP ranges](/vpc/docs/configure-private-google-access#ip-addr-defaults) might break your cluster.\n- You can add up to 50 authorized networks (allowed CIDR blocks) in a project. For more information, refer to [Add an authorized network to an existing cluster](/kubernetes-engine/docs/how-to/authorized-networks#add) .\nPrivate clusters have the following :\n- The size of the RFC 1918 block for the cluster control plane must be`/28`.\n- While GKE can detect overlap with the control plane address block, it cannot detect overlap within a Shared VPC network.\n- All nodes in a private cluster are created without a public IP; they have limited access to Google Cloud APIs and services. To provide outbound internet access for your private nodes, you can use [Cloud NAT](/nat/docs/overview#NATwithGKE) .\n- [Private Google Access](/vpc/docs/private-google-access) is enabled automatically when you create a private cluster unless you are using Shared VPC. You must not disable Private Google Access unless you are using NAT to access the internet.\n- Any private clusters you created prior to January 15, 2020 have a limit of at most 25 private clusters per network (assuming peerings are not being used for other purposes). See [VPC peering reuse](/kubernetes-engine/docs/how-to/private-clusters#vpc_peering_reuse) for more information.\n- Every private cluster requires a peering route between VPCs, but only one peering operation can happen at a time. If you attempt to create multiple private clusters at the same time, cluster creation may time out. To avoid this, create new private clusters serially so that the VPC peering routes already exist for each subsequent private cluster. Attempting to create a single private cluster may also time out if there are operations running on your VPC.\n- If you [expand the primary IP range of a subnet](/vpc/docs/create-modify-vpc-networks#expand-subnet) to accommodate additional nodes, then you must [add the expanded subnet's primary IP address range to the list of authorized networks](/kubernetes-engine/docs/how-to/authorized-networks#add) for your cluster. If you don't, ingress-allow firewall rules relevant to the control plane aren't updated, and new nodes created in the expanded IP address space won't be able to register with the control plane. This can lead to an outage where new nodes are continuously deleted and replaced. Such an outage can happen when performing node pool upgrades or when nodes are automatically replaced due to liveness probe failures.\n- Don't create firewall rules or [hierarchical firewall policy rules](/vpc/docs/firewall-policies#hierarchical_firewall_policy_rule_details) that have a [higher priority](/vpc/docs/firewalls#priority_order_for_firewall_rules) than the [automatically created firewall rules](/kubernetes-engine/docs/concepts/firewall-rules#cluster-fws) .## Troubleshooting\nThe following sections explain how to resolve common issues related to private clusters.\n### VPC Network Peering connection on private cluster is accidentally deleted### Cluster overlaps with active peer### Can't reach control plane of a private cluster\nIncrease the likelihood that your cluster control plane is reachable by implementing any of the cluster endpoint access configuration. For more information, see [access to cluster endpoints](/kubernetes-engine/docs/concepts/private-cluster-concept#overview) .### Can't create cluster due to overlapping IPv4 CIDR block### Can't create cluster due to services range already in use by another cluster### Can't create subnet### Can't pull image from public Docker Hub### API request that triggers admission webhook timing out### Can't create cluster due to health check failing### kubelet Failed to create pod sandbox### Private cluster nodes created but not joining the cluster\nOften when using custom routing and third-party network appliances on the VPC your private cluster is using, the default route ( `0.0.0.0/0` ) is redirected to the appliance instead of the default internet gateway. In addition to the control plane connectivity, you need to ensure that the following destinations are reachable:\n- *.googleapis.com\n- *.gcr.io\n- gcr.io\nConfigure [Private Google Access](/vpc/docs/configure-private-google-access) for all three domains. This best practice allows the new nodes to startup and join the cluster while keeping the internet bound traffic restricted.\n### Workloads on private GKE clusters unable to access internet\nPods in private GKE clusters cannot access the internet. For example, after running the `apt update` command from the Pod `exec shell` , it reports an error similar to the following:\n```\n0% [Connecting to deb.debian.org (199.232.98.132)] [Connecting to security.debian.org (151.101.130.132)]\n```\nIf subnet secondary IP address range used for Pods in the cluster is not configured on Cloud NAT gateway, the Pods cannot connect to the internet as they don't have an external IP address configured for Cloud NAT gateway.\nEnsure you configure the Cloud NAT gateway to apply at least the following subnet IP address ranges for the subnet that your cluster uses:\n- Subnet primary IP address range (used by nodes)\n- Subnet secondary IP address range used for Pods in the cluster\n- Subnet secondary IP address range used for Services in the cluster\nTo learn more, see [how to add secondary subnet IP range used for Pods](/nat/docs/set-up-manage-network-address-translation#specify_subnet_ranges_for_nat)\n## What's next\n- [Read the GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- [Learn how to create VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) .\n- [Learn more about VPC Network Peering](/vpc/docs/vpc-peering) .\n- [Follow the tutorial about accessing private GKE clusters with Cloud Build private pools](/architecture/accessing-private-gke-clusters-with-cloud-build-private-pools) .", "guide": "Google Kubernetes Engine (GKE)"}