{"title": "Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with Triton and TensorRT-LLM", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tensortllm", "abstract": "# Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with Triton and TensorRT-LLM\nThis tutorial shows you how to serve a [Gemma](https://ai.google.dev/gemma/docs/) large language model (LLM) using graphical processing units (GPUs) on Google Kubernetes Engine (GKE) with the NVIDIA [Triton](https://developer.nvidia.com/triton-inference-server) and [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/) serving stack. In this tutorial, you download the 2B and 7B parameter instruction tuned Gemma models and deploy them on a GKE [Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-autopilot) or [Standard](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-standard) cluster using a container that runs Triton and TensorRT-LLM.\nThis guide is a good starting point if you need the granular control, scalability, resilience, portability, and cost-effectiveness of managed Kubernetes when deploying and serving your AI/ML workloads. If you need a unified managed AI platform to rapidly build and serve ML models cost effectively, we recommend that you try our [Vertex AI](/vertex-ai) deployment solution.", "content": "## BackgroundBy serving Gemma using GPUs on GKE with Triton and TensorRT-LLM, you can implement a robust, production-ready inference serving solution with all the benefits of managed [Kubernetes](https://kubernetes.io/) , including efficient scalability and higher availability. This section describes the key technologies used in this guide.\n### Gemma [Gemma](https://ai.google.dev/gemma/docs/) is a set of openly available, lightweight, generative artificial intelligence (AI) models released under an open license. These AI models are available to run in your applications, hardware, mobile devices, or hosted services. You can use the Gemma models for text generation, however you can also tune these models for specialized tasks.\nTo learn more, see the [Gemma documentation](https://ai.google.dev/gemma/docs) .\n### GPUsGPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\nBefore you use GPUs in GKE, we recommend that you complete the following learning path:- Learn about [current GPU version availability](/compute/docs/gpus) \n- Learn about [GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) \n### TensorRT-LLMNVIDIA TensorRT-LLM (TRT-LLM) is a toolkit with a Python API for assembling optimized solutions to define LLMs and build TensorRT engines that perform inference efficiently on NVIDIA GPUs. TensorRT-LLM includes features such as:- Optimized transformer implementation with layer fusions, activation caching, memory buffer reuse, and [PagedAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention) \n- In-flight or continuous batching to improve the overall serving throughput\n- Tensor parallelism and pipeline parallelism for distributed serving on multiple GPUs\n- Quantization (FP16, FP8, INT8)\nTo learn more, refer to the [TensorRT-LLM documentation](https://nvidia.github.io/TensorRT-LLM/) .\n### TritonNVIDIA Triton Inference Server is a open source inference server for AI/ML applications. Triton supports high-performance inference on both NVIDIA GPUs and CPUs with optimized backends, including TensorRT and TensorRT-LLM. Triton includes features such as:- Multi-GPU, multi-node inference\n- Concurrent multiple model execution\n- Model ensembling or chaining\n- Static, dynamic, and continuous or in-flight batching of prediction requests\nTo learn more, refer to the [Triton documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) .## ObjectivesThis guide is intended for Generative AI customers who use [PyTorch](https://pytorch.org/) , new or existing users of GKE, ML Engineers, MLOps (DevOps) engineers, or platform administrators who are interested in using Kubernetes container orchestration capabilities for serving LLMs on H100, A100, and L4 GPU hardware.\nBy the end of this guide, you should be able to perform the following steps:- Prepare your environment with a GKE cluster in Autopilot mode.\n- Deploy a container with Triton and TritonRT-LLM to your cluster.\n- Use Triton and TensorRT-LLM to serve the Gemma 2B or 7B model through curl.\n## Before you begin- Make sure that you have the following role or roles on the project:      roles/container.admin, roles/iam.serviceAccountAdmin\n- Create a [Kaggle](https://www.kaggle.com/) account, if you don't already have one.\n- [Ensure your project has sufficientquota](/compute/resource-usage#gpu_quota) for GPUs in GKE.\n## Prepare your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) and [gcloud CLI](/sdk/gcloud) .\nTo set up your environment with Cloud Shell, follow these steps:- In the Google Cloud console, launch a Cloud Shell session by clicking **Activate Cloud Shell** in the [Google Cloud console](http://console.cloud.google.com) . This launches a session in the bottom pane of Google Cloud console.\n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=REGIONexport CLUSTER_NAME=triton\n```Replace the following values:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : A region that supports the accelerator type you want to use, for example,`us-central1`for L4 GPU.## Get access to the modelTo get access to the Gemma models, you must sign in to the Kaggle platform, and get a Kaggle API token.\n### Sign the license consent agreementYou must sign the consent agreement to use Gemma. Follow these instructions:- Access the [model consent page](https://www.kaggle.com/models/google/gemma) on Kaggle.com.\n- Login to Kaggle if you haven't done so already.\n- Click **Request Access** .\n- In the **Choose Account for Consent** section, select **Verify via Kaggle Account** to use your Kaggle account for consent.\n- Accept the model **Terms and Conditions** .\n### Generate an access tokenTo access the model through Kaggle, you need a Kaggle API token. Follow these steps to generate a new token if you don't have one already:- In your browser, go to **Kaggle settings** .\n- Under the API section, click **Create New Token** .\nA file named `kaggle.json` file is downloaded.\n### Upload the access token to Cloud ShellIn Cloud Shell, upload the Kaggle API token to your Google Cloud project:- In Cloud Shell, clickmore_vert **More** > **Upload** .\n- Select File and click **Choose Files** .\n- Open the`kaggle.json`file.\n- Click **Upload** .\n## Create and configure Google Cloud resourcesFollow these instructions to create the required resources.\n **Note:** You may need to create a capacity reservation for usage of some accelerators. To learn how to reserve and consume reserved resources, see [Consuming reserved zonal resources](/kubernetes-engine/docs/how-to/consuming-reservations) .\n### Create a GKE cluster and node poolYou can serve Gemma on GPUs in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience. To choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\nIn Cloud Shell, run the following command:\n```\ngcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid \\\u00a0 --cluster-version=1.28\n```\nGKE creates an Autopilot cluster with CPU and GPU nodes as requested by the deployed workloads.- In Cloud Shell, run the following command to create a Standard cluster:```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 --location=${REGION}-a \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --machine-type=e2-standard-4 \\\u00a0 \u00a0 --num-nodes=1\n```The cluster creation might take several minutes.\n- Run the following command to create a [node pool](/kubernetes-engine/docs/concepts/node-pools) for your cluster:```\ngcloud container node-pools create gpupool \\\u00a0 \u00a0 --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 --location=${REGION}-a \\\u00a0 \u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 \u00a0 --machine-type=g2-standard-12 \\\u00a0 \u00a0 --num-nodes=1\n```GKE creates a single node pool containing one L4 GPU node.### Create Kubernetes Secret for Kaggle credentialsIn this tutorial, you use a Kubernetes Secret for the Kaggle credentials.\nIn Cloud Shell, do the following:- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}\n```\n- Create a Secret to store the Kaggle credentials:```\nkubectl create secret generic kaggle-secret \\\u00a0 \u00a0 --from-file=kaggle.json \\\u00a0 \u00a0 --dry-run=client -o yaml | kubectl apply -f ```\n## Create a PersistentVolume resource to store checkpointsIn this section, you create a PersistentVolume backed by a persistent disk to store the model checkpoints.- Create the following `trtllm_checkpoint_pv.yaml` manifest: [  ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml) ```\napiVersion: v1kind: PersistentVolumeClaimmetadata:\u00a0 name: model-dataspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 100G\n```\n- Apply the manifest:```\nkubectl apply -f trtllm_checkpoint_pv.yaml\n```\n## Download the TensorRT-LLM engine files for GemmaIn this section, you run a [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) to download the TensorRT-LLM engine files and store the files in the PersistentVolume you created earlier. The Job also prepares configuration files for deploying the model on the Triton server in the next step. This process can take a few minutes.\nThe TensorRT-LLM engine is built from the Gemma 2B-it (instruction tuned) PyTorch checkpoint of Gemma using `bfloat16` activation, input sequence length=2048, and output sequence length=1024 targeted L4 GPUs. You can deploy the model on a single L4 GPU.- Create the following `job-download-gemma-2b.yaml` manifest: [  ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 ACTIVATION_DTYPE=bfloat16 && \\\u00a0 \u00a0 TOKENIZER_DIR=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/tokenizer.model && \\\u00a0 \u00a0 ENGINE_PATH=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/ && \\\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository && \\\u00a0 \u00a0 mkdir -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 mkdir -p ${ENGINE_PATH} && \\\u00a0 \u00a0 mkdir -p ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 rm -f /data/${MODEL_NAME}_${VARIATION_NAME}/*.tar.gz && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f | xargs -I '{}' mv '{}' ${ENGINE_PATH} && \\\u00a0 \u00a0 # copying configuration files\u00a0 \u00a0 echo -e \"\\nCreating configuration files\" && \\\u00a0 \u00a0 cp -r /tensorrtllm_backend/all_models/inflight_batcher_llm/* ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 # updating configuration files\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,preprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,postprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/ensemble/config.pbtxt triton_max_batch_size:64 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_batching,max_queue_delay_microseconds:600,batch_scheduler_policy:guaranteed_no_evict,enable_trt_overlap:False && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to ${ENGINE_PATH}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-gemma-2b\u00a0 labels:\u00a0 \u00a0 app: data-loader-gemma-2bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-gemma-2b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/tensorrtllm/2b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: WORLD_SIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\n```\n- Apply the manifest:```\nkubectl apply -f job-download-gemma-2b.yaml\n```\n- View the logs for the Job:```\nkubectl logs -f job/data-loader-gemma-2b\n```The output from the logs is similar to the following:```\n...\nCreating configuration files\n+ echo -e '\\n02-16-2024 04:07:45 Completed building TensortRT-LLM engine at /data/trt_engine/gemma/2b/bfloat16/1-gpu/'\n+ echo -e '\\nCreating configuration files'\n...\n```\n- Wait for the Job to complete:```\nkubectl wait --for=condition=complete --timeout=900s job/data-loader-gemma-2b\n```The output is similar to the following:```\njob.batch/data-loader-gemma-2b condition met\n```\n- Verify the Job completed successfully (this may take a few minutes):```\nkubectl get job/data-loader-gemma-2b\n```The output is similar to the following:```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-gemma-2b 1/1   \n##s  #m\n##s\n```\nThe TensorRT-LLM engine is built from the Gemma 7B-it (instruction tuned) PyTorch checkpoint of Gemma using `bfloat16` activation, input sequence length=1024, and output sequence length=512 targeted L4 GPUs. You can deploy the model on a single L4 GPU.- Create the following `job-download-gemma-7b.yaml` manifest: [  ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 ACTIVATION_DTYPE=bfloat16 && \\\u00a0 \u00a0 TOKENIZER_DIR=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/tokenizer.model && \\\u00a0 \u00a0 ENGINE_PATH=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/ && \\\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository && \\\u00a0 \u00a0 mkdir -p ${ENGINE_PATH} && \\\u00a0 \u00a0 mkdir -p ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 rm -f /data/${MODEL_NAME}_${VARIATION_NAME}/*.tar.gz && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f | xargs -I '{}' mv '{}' ${ENGINE_PATH} && \\\u00a0 \u00a0 # copying configuration files\u00a0 \u00a0 echo -e \"\\nCreating configuration files\" && \\\u00a0 \u00a0 cp -r /tensorrtllm_backend/all_models/inflight_batcher_llm/* ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 # updating configuration files\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,preprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,postprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/ensemble/config.pbtxt triton_max_batch_size:64 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_batching,max_queue_delay_microseconds:600,batch_scheduler_policy:guaranteed_no_evict,enable_trt_overlap:False && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to ${ENGINE_PATH}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-gemma-7b\u00a0 labels:\u00a0 \u00a0 app: data-loader-gemma-7bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-gemma-7b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/tensorrtllm/7b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: WORLD_SIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\n```\n- Apply the manifest:```\nkubectl apply -f job-download-gemma-7b.yaml\n```\n- View the logs for the Job:```\nkubectl logs -f job/data-loader-gemma-7b\n```The output from the logs is similar to the following:```\n...\nCreating configuration files\n+ echo -e '\\n02-16-2024 04:07:45 Completed building TensortRT-LLM engine at /data/trt_engine/gemma/7b/bfloat16/1-gpu/'\n+ echo -e '\\nCreating configuration files'\n...\n```\n- Wait for the Job to complete:```\nkubectl wait --for=condition=complete --timeout=900s job/data-loader-gemma-7b\n```The output is similar to the following:```\njob.batch/data-loader-gemma-7b condition met\n```\n- Verify the Job completed successfully (this may take a few minutes):```\nkubectl get job/data-loader-gemma-7b\n```The output is similar to the following:```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-gemma-7b 1/1   \n##s  #m\n##s\n```\nMake sure the Job is completed successfully before proceeding to the next section.## Deploy TritonIn this section, you deploy a container that uses Triton with the TensorRT-LLM backend to serve the Gemma model you want to use.- Create the following `deploy-triton-server.yaml` manifest: [  ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: launch-tritonserverdata:\u00a0 entrypoint.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 # Launch Triton Inference server\u00a0 \u00a0 WORLD_SIZE=1\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository\u00a0 \u00a0 python3 /tensorrtllm_backend/scripts/launch_triton_server.py \\\u00a0 \u00a0 \u00a0 --world_size ${WORLD_SIZE} \\\u00a0 \u00a0 \u00a0 --model_repo ${TRITON_MODEL_REPO}\u00a0 \u00a0 tail -f /dev/null---apiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: triton-gemma-deployment\u00a0 labels:\u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 version: v1 spec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server \u00a0 \u00a0 \u00a0 version: v1\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: triton\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 \u00a0 \u00a0 version: v1\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server \u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 imagePullPolicy: IfNotPresent\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/entrypoint.sh\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: http\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8001\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: grpc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8002\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: metrics\u00a0 \u00a0 \u00a0 \u00a0 livenessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failureThreshold: 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 600\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /v2/health/live\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: http\u00a0 \u00a0 \u00a0 \u00a0 readinessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failureThreshold: 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 600\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /v2/health/ready\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: http\u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 runAsUser: 1000\u00a0 \u00a0 \u00a0 \u00a0 fsGroup: 1000\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: launch-tritonserver\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"---apiVersion: v1kind: Servicemetadata:\u00a0 name: triton-server\u00a0 labels:\u00a0 \u00a0 app: gemma-server spec:\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - port: 8000\u00a0 \u00a0 \u00a0 targetPort: http\u00a0 \u00a0 \u00a0 name: http-inference-server\u00a0 \u00a0 - port: 8001\u00a0 \u00a0 \u00a0 targetPort: grpc\u00a0 \u00a0 \u00a0 name: grpc-inference-server\u00a0 \u00a0 - port: 8002\u00a0 \u00a0 \u00a0 targetPort: metrics\u00a0 \u00a0 \u00a0 name: http-metrics\u00a0 selector:\u00a0 \u00a0 app: gemma-server\n```\n- Apply the manifest:```\nkubectl apply -f deploy-triton-server.yaml\n```\n- Wait for the deployment to be available:```\nkubectl wait --for=condition=Available --timeout=900s deployment/triton-gemma-deployment\n```\n- View the logs from manifest:```\nkubectl logs -f -l app=gemma-server\n```The deployment resource launches the Triton server and loads the model data. This process can take a few minutes (up to 20 minutes or longer). The output is similar to the following:```\nI0216 03:24:57.387420 29 server.cc:676]\n+------------------+---------+--------+\n| Model   | Version | Status |\n+------------------+---------+--------+\n| ensemble   | 1  | READY |\n| postprocessing | 1  | READY |\n| preprocessing | 1  | READY |\n| tensorrt_llm  | 1  | READY |\n| tensorrt_llm_bls | 1  | READY |\n+------------------+---------+--------+\n....\n....\n....\nI0216 03:24:57.425104 29 grpc_server.cc:2519] Started GRPCInferenceService at 0.0.0.0:8001\nI0216 03:24:57.425418 29 http_server.cc:4623] Started HTTPService at 0.0.0.0:8000\nI0216 03:24:57.466646 29 http_server.cc:315] Started Metrics Service at 0.0.0.0:8002\n```\n## Serve the modelIn this section, you interact with the model.\n### Set up port forwardingRun the following command to set up port forwarding to the model:\n```\nkubectl port-forward service/triton-server 8000:8000\n```\nThe output is similar to the following:\n```\nForwarding from 127.0.0.1:8000 -> 8000\nForwarding from [::1]:8000 -> 8000\nHandling connection for 8000\n```\n### Interact with the model using curlThis section shows how you can perform a basic smoke test to verify your deployed instruction tuned model. For simplicity, this section describes the testing approach only using the 2B instruction tuned model.\nIn a new terminal session, use `curl` to chat with your model:\n```\nUSER_PROMPT=\"I'm new to coding. If you could only recommend one programming language to start with, what would it be and why?\"curl -X POST localhost:8000/v2/models/ensemble/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"text_input\": \"<start_of_turn>user\\n${USER_PROMPT}<end_of_turn>\\n\",\u00a0 \u00a0 \"temperature\": 0.9,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\nThe following output shows an example of the model response:\n```\n{\n \"context_logits\": 0,\n \"cum_log_probs\": 0,\n \"generation_logits\": 0,\n \"model_name\": \"ensemble\",\n \"model_version\": \"1\",\n \"output_log_probs\": [0.0,0.0,...],\n \"sequence_end\": false,\n \"sequence_id\": 0,\n \"sequence_start\": false,\n \"text_output\":\"Python.\\n\\nPython is an excellent choice for beginners due to its simplicity, readability, and extensive documentation. Its syntax is close to natural language, making it easier for beginners to understand and write code. Python also has a vast collection of libraries and tools that make it versatile for various projects. Additionally, Python's dynamic nature allows for easier learning and experimentation, making it a perfect choice for newcomers to get started.Here are some specific reasons why Python is a good choice for beginners:\\n\\n- Simple and Easy to Read: Python's syntax is designed to be close to natural language, making it easier for\"\n}\n```\n **Success:** You've successfully served Gemma using GPUs on GKE with Triton and TensorRT-LLM.## Troubleshoot issues\n- If you get the message`Empty reply from server`, it's possible the container has not finished downloading the model data. [Check the Pod's logs](#deploy-triton) again for the`Connected`message which indicates that the model is ready to serve.\n- If you see`Connection refused`, verify that your [port forwarding is active](#setup-port-forwarding) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the deployed resourcesTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:\n```\ngcloud container clusters delete ${CLUSTER_NAME} \\\u00a0 --region=${REGION}\n```## What's next\n- Learn more about [GPUs inGKE](/kubernetes-engine/docs/concepts/gpus) .\n- Learn how to [deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .\n- Learn how to [deploy GPU workloads in Standard](/kubernetes-engine/docs/how-to/gpus) .\n- Explore the TensorRT-LLM [GitHub repository](https://github.com/NVIDIA/TensorRT-LLM) and [documentation](https://nvidia.github.io/TensorRT-LLM/) .\n- Explore the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) .\n- Discover how to run optimized AI/ML workloads with [GKEplatform orchestrationcapabilities](/kubernetes-engine/docs/integrations/ai-infra) .", "guide": "Google Kubernetes Engine (GKE)"}