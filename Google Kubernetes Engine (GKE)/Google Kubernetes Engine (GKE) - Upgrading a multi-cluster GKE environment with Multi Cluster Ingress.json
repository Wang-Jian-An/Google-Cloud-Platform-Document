{"title": "Google Kubernetes Engine (GKE) - Upgrading a multi-cluster GKE environment with Multi Cluster Ingress", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/upgrading-multi-cluster-gke-environment-multi-cluster-ingress", "abstract": "# Google Kubernetes Engine (GKE) - Upgrading a multi-cluster GKE environment with Multi Cluster Ingress\nLast reviewed 2022-12-15 UTC\nThis tutorial shows how to upgrade a multi-cluster Google Kubernetes Engine (GKE) environment using [Multi Cluster Ingress](/kubernetes-engine/docs/concepts/multi-cluster-ingress) . This tutorial is a continuation of the [multi-cluster GKEupgrades using Multi Cluster Ingressdocument](/kubernetes-engine/docs/concepts/multi-cluster-gke-upgrades-multi-cluster-ingress) that explains the process, architecture, and terms in more detail. We recommend that you read the concept document before this tutorial.\nFor a detailed comparison between Multi Cluster Ingress (MCI), Multi-cluster Gateway (MCG), and load balancer with Standalone Network Endpoint Groups (LB and Standalone NEGs), see [Choose your multi-cluster load balancing API forGKE](/kubernetes-engine/docs/concepts/choose-mc-lb-api) .\nThis document is intended for Google Cloud administrators who are responsible for maintaining fleets for GKE clusters.\nWe recommend auto-upgrading your GKE clusters. Auto-upgrade is a fully managed way of getting your clusters (control plane and nodes) automatically updated on a release schedule that is determined by Google Cloud. This requires no intervention from the operator. However, if you want more control over how and when clusters are upgraded, this tutorial walks through a method of upgrading multiple clusters where your apps run on all clusters. It then uses Multi Cluster Ingress to drain one cluster at a time before upgrading.", "content": "## ArchitectureThis tutorial uses the following architecture. There are a total of three clusters: two clusters ( `blue` and `green` ) act as identical clusters with the same app deployed and one cluster ( `ingress-config` ) acts as the control plane cluster that configures Multi Cluster Ingress. In this tutorial, you deploy a sample app to two app clusters ( `blue` and `green` clusters).## Objectives\n- Create three GKE clusters and register them as a fleet.\n- Configure one GKE cluster (`ingress-config`) as the central configuration cluster.\n- Deploy a sample app to the other GKE clusters.\n- Configure Multi Cluster Ingress to send client traffic to the app that is running on both app clusters.\n- Set up a load generator to the app and configure monitoring.\n- Remove (drain) one app cluster from the multi-cluster Ingress and upgrade the drained cluster.\n- Spill traffic back to the upgraded cluster using Multi Cluster Ingress.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Networking](/vpc/network-pricing#vpc-pricing) \n- [Cloud Load Balancing](/vpc/network-pricing#lb) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- This tutorial requires you to [Set up Multi Cluster Ingress](/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup#requirements_for) so that the following is set up:- Two or more clusters with the same apps, such as namespaces, Deployments, and Services, running on all clusters.\n- Autoupgrade is turned off for all clusters.\n- Clusters are [VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) that use alias IP address ranges\n- Have HTTP load balancing enabled (enabled by default).\n- `gcloud --version`must be 369 or higher. GKE cluster registration steps depend on this version or higher.\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set your default project:```\nexport PROJECT=$(gcloud info --format='value(config.project)')gcloud config set project ${PROJECT}\n```\n- Enable the GKE, Hub, and `multiclusteringress` APIs:```\ngcloud services enable container.googleapis.com \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0gkehub.googleapis.com \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0multiclusteringress.googleapis.com \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0multiclusterservicediscovery.googleapis.com\n```\n## Set up the environment\n- In Cloud Shell, clone the repository to get the files for this tutorial:```\ncd ${HOME}git clone https://github.com/GoogleCloudPlatform/gke-multicluster-upgrades.git\n```\n- Create a `WORKDIR` directory:```\ncd gke-multicluster-upgradesexport WORKDIR=`pwd`\n```\n## Create and register GKE clusters to HubIn this section, you create three GKE clusters and register them to GKE Enterprise Hub.\n### Create GKE clusters\n- In Cloud Shell, create three GKE clusters:```\ngcloud container clusters create ingress-config --zone us-west1-a \\--release-channel=None --no-enable-autoupgrade --num-nodes=4 \\--enable-ip-alias --workload-pool=${PROJECT}.svc.id.goog --quiet --asyncgcloud container clusters create blue --zone us-west1-b --num-nodes=3 \\--release-channel=None --no-enable-autoupgrade --enable-ip-alias \\--workload-pool=${PROJECT}.svc.id.goog --quiet --asyncgcloud container clusters create green --zone us-west1-c --num-nodes=3 \\--release-channel=None --no-enable-autoupgrade --enable-ip-alias \\--workload-pool=${PROJECT}.svc.id.goog --quiet\n```For the purpose of this tutorial, you create the clusters in a single region, in three different zones: `us-west1-a` , `us-west1-b` , and `us-west1-c` . For more information about regions and zones, see [Geography and regions](/docs/geography-and-regions) .\n- Wait a few minutes until all clusters are successfully created. Ensure that the clusters are running:```\ngcloud container clusters list\n```The output is similar to the following:```\nNAME: ingress-config\nLOCATION: us-west1-a\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.233.186.135\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 4\nSTATUS: RUNNING\nNAME: blue\nLOCATION: us-west1-b\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 34.82.35.222\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 3\nSTATUS: RUNNING\nNAME: green\nLOCATION: us-west1-c\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.185.204.26\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 3\nSTATUS: RUNNING\n```\n- Create a [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) file and connect to all clusters to generate entries in the `kubeconfig` file:```\ntouch gke-upgrade-kubeconfigexport KUBECONFIG=gke-upgrade-kubeconfiggcloud container clusters get-credentials ingress-config \\\u00a0 \u00a0 --zone us-west1-a --project ${PROJECT}gcloud container clusters get-credentials blue --zone us-west1-b \\\u00a0 \u00a0 --project ${PROJECT}gcloud container clusters get-credentials green --zone us-west1-c \\\u00a0 \u00a0 --project ${PROJECT}\n```You use the [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) file to create authentication to clusters by creating a user and context for each cluster. After you create the `kubeconfig` file, you can quickly switch context between clusters.\n- Verify that you have three clusters in the `kubeconfig` file:```\nkubectl config view -ojson | jq -r '.clusters[].name'\n```The output is the following:```\ngke_gke-multicluster-upgrades_us-west1-a_ingress-config\ngke_gke-multicluster-upgrades_us-west1-b_blue\ngke_gke-multicluster-upgrades_us-west1-c_green\n```\n- Get the context for the three clusters for later use:```\nexport INGRESS_CONFIG_CLUSTER=$(kubectl config view -ojson | jq \\\u00a0 \u00a0 -r '.clusters[].name' | grep ingress-config)export BLUE_CLUSTER=$(kubectl config view -ojson | jq \\\u00a0 \u00a0 -r '.clusters[].name' | grep blue)export GREEN_CLUSTER=$(kubectl config view -ojson | jq \\\u00a0 \u00a0 -r '.clusters[].name' | grep green)echo -e \"${INGRESS_CONFIG_CLUSTER}\\n${BLUE_CLUSTER}\\n${GREEN_CLUSTER}\"\n```The output is the following:```\ngke_gke-multicluster-upgrades_us-west1-a_ingress-config\ngke_gke-multicluster-upgrades_us-west1-b_blue\ngke_gke-multicluster-upgrades_us-west1-c_green\n```\n### Register GKE clusters to a fleetRegistering your clusters to a fleet lets you operate your Kubernetes clusters across hybrid environments. Clusters registered to fleets can use advanced GKE features such as Multi Cluster Ingress. To register a GKE cluster to a fleet, you can use a Google Cloud service account directly, or use the recommended [Workload Identity](/kubernetes-engine/docs/concepts/workload-identity) approach, which allows a Kubernetes service account in your GKE cluster to act as an Identity and Access Management service account.- Register the three clusters as a fleet:```\ngcloud container fleet memberships register ingress-config \\\u00a0 \u00a0 --gke-cluster=us-west1-a/ingress-config \\\u00a0 \u00a0 --enable-workload-identitygcloud container fleet memberships register blue \\\u00a0 \u00a0 --gke-cluster=us-west1-b/blue \\\u00a0 \u00a0 --enable-workload-identitygcloud container fleet memberships register green \\\u00a0 \u00a0 --gke-cluster=us-west1-c/green \\\u00a0 \u00a0 --enable-workload-identity\n```\n- Verify that the clusters are registered:```\ngcloud container fleet memberships list\n```The output is similar to the following:```\nNAME: blue\nEXTERNAL_ID: 401b4f08-8246-4f97-a6d8-cf1b78c2a91d\nNAME: green\nEXTERNAL_ID: 8041c36a-9d42-40c8-a67f-54fcfd84956e\nNAME: ingress-config\nEXTERNAL_ID: 65ac48fe-5043-42db-8b1e-944754a0d725\n```\n- Configure the `ingress-config` cluster as the configuration cluster for Multi Cluster Ingress by enabling the `multiclusteringress` feature through the Hub:```\ngcloud container fleet ingress enable --config-membership=ingress-config\n```The preceding command adds the `MulticlusterIngress` and `MulticlusterService` [CRDs (Custom Resource Definitions)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) to the `ingress-config` cluster. This command takes a few minutes to complete. Wait before proceeding to the next step.\n- Verify that the `ingress-cluster` cluster was successfully configured for Multi Cluster Ingress:```\nwatch gcloud container fleet ingress describe\n```Wait until the output is similar to the following:```\ncreateTime: '2022-07-05T10:21:40.383536315Z'\nmembershipStates:\n projects/662189189487/locations/global/memberships/blue:\n state:\n  code: OK\n  updateTime: '2022-07-08T10:59:44.230329189Z'\n projects/662189189487/locations/global/memberships/green:\n state:\n  code: OK\n  updateTime: '2022-07-08T10:59:44.230329950Z'\n projects/662189189487/locations/global/memberships/ingress-config:\n state:\n  code: OK\n  updateTime: '2022-07-08T10:59:44.230328520Z'\nname: projects/gke-multicluster-upgrades/locations/global/features/multiclusteringress\nresourceState:\n state: ACTIVE\nspec:\n multiclusteringress:\n configMembership: projects/gke-multicluster-upgrades/locations/global/memberships/ingress-config\nstate:\n state:\n code: OK\n description: Ready to use\n updateTime: '2022-07-08T10:57:33.303543609Z'\nupdateTime: '2022-07-08T10:59:45.247576318Z'\n``` **Note:** This feature is not ready to use until your configuration cluster has an `OK` status. If you don't see `code: OK` after a few minutes, go to the [troubleshooting section](/kubernetes-engine/docs/how-to/troubleshooting-and-ops) .To exit the `watch` command, press **Control+C** .\n## Deploy a sample application to the blue and green clusters\n- In Cloud Shell, deploy the sample `whereami` app to the `blue` and `green` clusters:```\nkubectl --context ${BLUE_CLUSTER} apply -f ${WORKDIR}/application-manifestskubectl --context ${GREEN_CLUSTER} apply -f ${WORKDIR}/application-manifests\n```\n- Wait a few minutes and ensure that all Pods in the `blue` and `green` clusters have a `Running` status:```\nkubectl --context ${BLUE_CLUSTER} get podskubectl --context ${GREEN_CLUSTER} get pods\n```The output is similar to the following:```\nNAME         READY STATUS RESTARTS AGE\nwhereami-deployment-756c7dc74c-zsmr6 1/1  Running 0   74s\nNAME         READY STATUS RESTARTS AGE\nwhereami-deployment-756c7dc74c-sndz7 1/1  Running 0   68s.\n```\n## Configure multi-cluster IngressIn this section, you create a multi-cluster Ingress that sends traffic to the application running on both `blue` and `green` clusters. You use [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) to create a load balancer that uses the `whereami` app in both the `blue` and `green` clusters as backends. To create the load balancer, you need two resources: a `MultiClusterIngress` and one or more `MultiClusterServices` . `MultiClusterIngress` and `MultiClusterService` objects are multi-cluster analogs for the existing Kubernetes Ingress and Service resources used in the single cluster context.- In Cloud Shell, deploy the `MulticlusterIngress` resource to the `ingress-config` cluster:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} apply -f ${WORKDIR}/multicluster-manifests/mci.yaml\n```The output is the following:```\nmulticlusteringress.networking.gke.io/whereami-mci created\n```\n- Deploy the `MulticlusterService` resource to the `ingress-config` cluster:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} apply -f ${WORKDIR}/multicluster-manifests/mcs-blue-green.yaml\n```The output is the following:```\nmulticlusterservice.networking.gke.io/whereami-mcs created\n```\n- To compare the two resources, do the following:- Inspect the `MulticlusterIngress` resource:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} get multiclusteringress -o yaml\n```The output contains the following:```\nspec:\n template:\n spec:\n  backend:\n  serviceName: whereami-mcs\n  servicePort: 8080\n```The `MulticlusterIngress` resource is similar to the Kubernetes Ingress resource except that the `serviceName` specification points to a `MulticlusterService` resource.\n- Inspect the `MulticlusterService` resource:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} get multiclusterservice -o yaml\n```The output contains the following:```\nspec:\n clusters:\n - link: us-west1-b/blue\n - link: us-west1-c/green\n template:\n spec:\n  ports:\n  - name: web\n  port: 8080\n  protocol: TCP\n  targetPort: 8080\n  selector:\n  app: whereami\n```The `MulticlusterService` resource is similar to a Kubernetes Service resource, except it has a `clusters` specification. The `clusters` value is the list of registered clusters where the `MulticlusterService` resource is created.\n- Verify that the `MulticlusterIngress` resource created a load balancer with a backend service pointing to the `MulticlusterService` resource:```\nwatch kubectl --context ${INGRESS_CONFIG_CLUSTER} \\\u00a0 \u00a0 \u00a0 get multiclusteringress -o jsonpath=\"{.items[].status.VIP}\"\n```This may take up to 10 minutes. Wait until the output is similar to the following:```\n34.107.246.9\n```To exit the `watch` command, press `Control+C` .\n- In Cloud Shell, get the Cloud Load Balancing VIP:```\nexport GCLB_VIP=$(kubectl --context ${INGRESS_CONFIG_CLUSTER} \\\u00a0 \u00a0 \u00a0 \u00a0get multiclusteringress -o json | jq -r '.items[].status.VIP') \\\u00a0 \u00a0 \u00a0 \u00a0&& echo ${GCLB_VIP}\n```The output is similar to the following:```\n34.107.246.9\n```\n- Use `curl` to access the load balancer and the deployed application:```\ncurl ${GCLB_VIP}\n```The output is similar to the following:```\n{\n \"cluster_name\": \"green\",\n \"host_header\": \"34.107.246.9\",\n \"pod_name\": \"whereami-deployment-756c7dc74c-sndz7\",\n \"pod_name_emoji\": \"\ud83d\ude07\",\n \"project_id\": \"gke-multicluster-upgrades\",\n \"timestamp\": \"2022-07-08T14:26:07\",\n \"zone\": \"us-west1-c\"\n}\n```\n- Run the `curl` command repeatedly. Notice that the requests are being load balanced between the `whereami` application that is deployed to two clusters, `blue` and `green` .\n## Set up the load generatorIn this section, you set up a `loadgenerator` Service that generates client traffic to the Cloud Load Balancing VIP. First, traffic is sent to the `blue` and `green` clusters because the `MulticlusterService` resource is set up to send traffic to both clusters. Later, you configure the `MulticlusterService` resource to send traffic to a single cluster.- Configure the `loadgenerator` manifest to send client traffic to the Cloud Load Balancing:```\nTEMPLATE=loadgen-manifests/loadgenerator.yaml.templ && envsubst < ${TEMPLATE} > ${TEMPLATE%.*}\n```\n- Deploy the `loadgenerator` in the `ingress-config` cluster:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} apply -f ${WORKDIR}/loadgen-manifests\n```\n- Verify that the `loadgenerator` Pods in the `ingress-config` cluster all have a status of `Running` :```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} get pods\n```The output is similar to the following:```\nNAME        READY STATUS RESTARTS AGE\nloadgenerator-5498cbcb86-hqscp 1/1  Running 0   53s\nloadgenerator-5498cbcb86-m2z2z 1/1  Running 0   53s\nloadgenerator-5498cbcb86-p56qb 1/1  Running 0   53s\n```If any of the Pods don't have a status of `Running` , wait a few minutes and then run the command again.\n## Monitor trafficIn this section, you monitor the traffic to the `whereami` app using the Google Cloud console.\nIn the previous section, you set up a `loadgenerator` deployment that simulates client traffic by accessing the `whereami` app through the Cloud Load Balancing VIP. You can monitor these metrics through the Google Cloud console. You set up monitoring first so that you can monitor as you drain clusters for upgrades (described in the next section).- Create a dashboard to show the traffic reaching Multi Cluster Ingress:```\nexport DASH_ID=$(gcloud monitoring dashboards create \\\u00a0 \u00a0 --config-from-file=dashboards/cloud-ops-dashboard.json \\\u00a0 \u00a0 --format=json | jq \u00a0-r \".name\" | awk -F '/' '{print $4}')\n```The output is similar to the following:```\nCreated [721b6c83-8f9b-409b-a009-9fdf3afb82f8]\n```\n- Metrics from Cloud Load Balancing are available in the Google Cloud console. Generate the URL:```\necho \"https://console.cloud.google.com/monitoring/dashboards/builder/${DASH_ID}/?project=${PROJECT}&timeDomain=1h\"\n```The output is similar to the following:```\nhttps://console.cloud.google.com/monitoring/dashboards/builder/721b6c83-8f9b-409b-a009-9fdf3afb82f8/?project=gke-multicluster-upgrades&timeDomain=1h\"\n```\n- In a browser, go to the URL generated by the preceding command.The traffic to the sample application is going from the load generator to both `blue` and `green` clusters (noted by the two zones the clusters are in). The timeline metrics chart shows traffic going to both backends.The `k8s1-` mouseover values indicate that the network endpoint group (NEGs) for the two frontend `MulticlusterServices` are running in the `blue` and `green` clusters. **Note:** The chart colors may not correspond to the `blue` and `green` clusters. It may take several minutes for traffic to become visible in the chart. \n## Drain and upgrade the blue clusterIn this section, you drain the `blue` cluster. Draining a cluster means that you remove it from the load balancing pool. After you drain the `blue` cluster, all client traffic destined for the application goes to the `green` cluster. You can monitor this process as described in the previous section. After the cluster is drained, you can upgrade the drained cluster. After the upgrade, you can put it back in the load balancing pool. You repeat these steps to upgrade the other cluster (not shown in this tutorial).\nTo drain the `blue` cluster, you update the `MulticlusterService` resource in the `ingress-cluster` cluster and remove the `blue` cluster from the `clusters` specification.\n### Drain the blue cluster\n- In Cloud Shell, update the `MulticlusterService` resource in the `ingress-config` cluster:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 apply -f ${WORKDIR}/multicluster-manifests/mcs-green.yaml\n```\n- Verify that you only have the `green` cluster in the `clusters` specification:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} get multiclusterservice \\\u00a0 \u00a0 \u00a0 \u00a0 -o json | jq '.items[].spec.clusters'\n```The output is the following:```\n[ {\n \"link\": \"us-west1-c/green\"\n }\n]\n```Only the `green` cluster is listed in the `clusters` specification so only the `green` cluster is in the load balancing pool.\n- You can see metrics from the Cloud Load Balancing metrics in the Google Cloud console. Generate the URL:```\necho \"https://console.cloud.google.com/monitoring/dashboards/builder/${DASH_ID}/?project=${PROJECT}&timeDomain=1h\"\n```\n- In a browser, go to the URL generated from the previous command.The chart shows that only the `green` cluster is receiving traffic. \n### Upgrade the blue clusterNow that the `blue` cluster is no longer receiving any client traffic, you can upgrade the cluster (control plane and nodes).- In Cloud Shell, get the current version of the clusters:```\ngcloud container clusters list\n```The output is similar to the following:```\nNAME: ingress-config\nLOCATION: us-west1-a\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.233.186.135\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 4\nSTATUS: RUNNING\nNAME: blue\nLOCATION: us-west1-b\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 34.82.35.222\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 3\nSTATUS: RUNNING\nNAME: green\nLOCATION: us-west1-c\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.185.204.26\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 3\nSTATUS: RUNNING\n```Your cluster versions might be different depending on when you complete this tutorial.\n- Get the list of available `MasterVersions` versions in the zone:```\ngcloud container get-server-config --zone us-west1-b --format=json | jq \\'.validMasterVersions[0:20]'\n```The output is similar to the following:```\n[ \"1.24.1-gke.1400\",\n \"1.23.7-gke.1400\",\n \"1.23.6-gke.2200\",\n \"1.23.6-gke.1700\",\n \"1.23.6-gke.1501\",\n \"1.23.6-gke.1500\",\n \"1.23.5-gke.2400\",\n \"1.23.5-gke.1503\",\n \"1.23.5-gke.1501\",\n \"1.22.10-gke.600\",\n \"1.22.9-gke.2000\",\n \"1.22.9-gke.1500\",\n \"1.22.9-gke.1300\",\n \"1.22.8-gke.2200\",\n \"1.22.8-gke.202\",\n \"1.22.8-gke.201\",\n \"1.22.8-gke.200\",\n \"1.21.13-gke.900\",\n \"1.21.12-gke.2200\",\n \"1.21.12-gke.1700\"\n]\n```\n- Get a list of available `NodeVersions` versions in the zone:```\ngcloud container get-server-config --zone us-west1-b --format=json | jq \\'.validNodeVersions[0:20]'\n```The output is similar to the following:```\n[ \"1.24.1-gke.1400\",\n \"1.23.7-gke.1400\",\n \"1.23.6-gke.2200\",\n \"1.23.6-gke.1700\",\n \"1.23.6-gke.1501\",\n \"1.23.6-gke.1500\",\n \"1.23.5-gke.2400\",\n \"1.23.5-gke.1503\",\n \"1.23.5-gke.1501\",\n \"1.22.10-gke.600\",\n \"1.22.9-gke.2000\",\n \"1.22.9-gke.1500\",\n \"1.22.9-gke.1300\",\n \"1.22.8-gke.2200\",\n \"1.22.8-gke.202\",\n \"1.22.8-gke.201\",\n \"1.22.8-gke.200\",\n \"1.22.7-gke.1500\",\n \"1.22.7-gke.1300\",\n \"1.22.7-gke.900\"\n]\n```\n- Set an environment variable for a `MasterVersion` and `NodeVersion` version that is in the `MasterVersions` and `NodeVersions` lists, and is higher than the current version for the `blue` cluster, for example:```\nexport UPGRADE_VERSION=\"1.22.10-gke.600\"\n```This tutorial uses the `1.22.10-gke.600` version. Your cluster versions might be different depending on the versions that are available when you complete this tutorial. For more information about upgrading, see [upgrading clusters and node pools](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading_the_cluster) . **Note:** It is recommended that you don't skip minor cluster versions to avoid potential control plane or node incompatibility issues. For more information, see [Can I skip multiple GKE versions during a clusterupgrade?](/kubernetes-engine/versioning#skip-versions) .\n- Upgrade the `control plane` node for the `blue` cluster:```\ngcloud container clusters upgrade blue \\\u00a0 \u00a0 --zone us-west1-b --master --cluster-version ${UPGRADE_VERSION}\n```To confirm the upgrade, press `Y` .This process takes a few minutes to complete. Wait until the upgrade is complete before proceeding.After the update is complete, the output is the following:```\nUpdated\n[https://container.googleapis.com/v1/projects/gke-multicluster-upgrades/zones/us-west1-b/clusters/blue].\n```\n- Upgrade the nodes in the `blue` cluster:```\ngcloud container clusters upgrade blue \\\u00a0 \u00a0 --zone=us-west1-b --node-pool=default-pool \\\u00a0 \u00a0 --cluster-version ${UPGRADE_VERSION}\n```To confirm the update, press `Y` .This process takes a few minutes to complete. Wait until the node upgrade is complete before proceeding.After the upgrade is complete, the output is the following:```\nUpgrading blue... Done with 3 out of 3 nodes (100.0%): 3 succeeded...done.\nUpdated [https://container.googleapis.com/v1/projects/gke-multicluster-upgrades/zones/us-west1-b/clusters/blue].\n```\n- Verify that the `blue` cluster is upgraded:```\ngcloud container clusters list\n```The output is similar to the following:```\nNAME: ingress-config\nLOCATION: us-west1-a\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.233.186.135\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 4\nSTATUS: RUNNING\nNAME: blue\nLOCATION: us-west1-b\nMASTER_VERSION: 1.22.10-gke.600\nMASTER_IP: 34.82.35.222\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.10-gke.600\nNUM_NODES: 3\nSTATUS: RUNNING\nNAME: green\nLOCATION: us-west1-c\nMASTER_VERSION: 1.22.8-gke.202\nMASTER_IP: 35.185.204.26\nMACHINE_TYPE: e2-medium\nNODE_VERSION: 1.22.8-gke.202\nNUM_NODES: 3\nSTATUS: RUNNING\n```\n## Add the blue cluster back to the load balancing poolIn this section, you add the `blue` cluster back into the load balancing pool.- In Cloud Shell, verify that the application deployment is running on the `blue` cluster before you add it back to the load balancing pool:```\nkubectl --context ${BLUE_CLUSTER} get pods\n```The output is similar to the following:```\nNAME         READY STATUS RESTARTS AGE\nwhereami-deployment-756c7dc74c-xdnb6 1/1  Running 0   17m\n```\n- Update the `MutliclusterService` resource to add the `blue` cluster back to the load balancing pool:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} apply \\\u00a0 \u00a0 \u00a0 \u00a0 -f ${WORKDIR}/multicluster-manifests/mcs-blue-green.yaml\n```\n- Verify that you have both `blue` and `green` clusters in the clusters specification:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} get multiclusterservice \\\u00a0 \u00a0 \u00a0 \u00a0 -o json | jq '.items[].spec.clusters'\n```The output is the following:```\n[ {\n \"link\": \"us-west1-b/blue\"\n },\n {\n \"link\": \"us-west1-c/green\"\n }\n]\n```The `blue` and `green` clusters are now in the `clusters` specification.\n- Metrics from the Cloud Load Balancing metrics are available in the Google Cloud console. Generate the URL:```\necho \"https://console.cloud.google.com/monitoring/dashboards/builder/${DASH_ID}/?project=${PROJECT}&timeDomain=1h\"\n```\n- In a browser, go to the URL generated by the previous command.The chart shows that both blue and green clusters are receiving traffic from the load generator using the load balancer. **Note:** It may take several minutes for traffic to rebalance. Congratulations. You successfully upgraded a GKE cluster in a multi-cluster architecture using Multi Cluster Ingress.\n- To upgrade the `green` cluster, repeat the process to [drain and upgrade the blue cluster](#draining_and_upgrading_the_blue_cluster) , replacing `blue` with `green` throughout.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the tutorial. Alternatively, you can delete the individual resources.\n### Delete the clusters\n- In Cloud Shell, unregister and delete the `blue` and `green` clusters:```\ngcloud container fleet memberships unregister blue --gke-cluster=us-west1-b/bluegcloud container clusters delete blue --zone us-west1-b --quietgcloud container fleet memberships unregister green --gke-cluster=us-west1-c/greengcloud container clusters delete green --zone us-west1-c --quiet\n```\n- Delete the `MuticlusterIngress` resource from the `ingress-config` cluster:```\nkubectl --context ${INGRESS_CONFIG_CLUSTER} delete -f ${WORKDIR}/multicluster-manifests/mci.yaml\n```This command deletes the Cloud Load Balancing resources from the project.\n- Unregister and delete the `ingress-config` cluster:```\ngcloud container fleet memberships unregister ingress-config --gke-cluster=us-west1-a/ingress-configgcloud container clusters delete ingress-config --zone us-west1-a --quiet\n```\n- Verify all clusters are deleted:```\ngcloud container clusters list\n```The output is the following:```\n*&lt;null&gt;*\n```\n- Reset the `kubeconfig` file:```\nunset KUBECONFIG\n```\n- Remove the `WORKDIR` folder:```\ncd ${HOME}rm -rf ${WORKDIR}\n```\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about [Multi Cluster Ingress](/kubernetes-engine/docs/concepts/multi-cluster-ingress) .\n- Learn how to [deploy Multi Cluster Ingress across clusters](/kubernetes-engine/docs/how-to/multi-cluster-ingress) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}