{"title": "Google Kubernetes Engine (GKE) - Running multi-instance GPUs", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gpus-multi", "abstract": "# Google Kubernetes Engine (GKE) - Running multi-instance GPUs\nThis page provides instructions on how to partition an NVIDIA A100 or H100 graphics processing unit (GPU) to share a single GPU across multiple containers on Google Kubernetes Engine (GKE).\nThis page assumes that you are familiar with Kubernetes concepts such as [Pods](https://kubernetes.io/docs/concepts/workloads/pods/) , [nodes](https://kubernetes.io/docs/concepts/architecture/nodes/) , [deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) , and [namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) and are familiar with GKE concepts such as [node pools](/kubernetes-engine/docs/concepts/node-pools) , [autoscaling](/kubernetes-engine/docs/concepts/cluster-autoscaler) , and [auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\n", "content": "## Introduction\nKubernetes allocates one full GPU per container even if the container only needs a fraction of the GPU for its workload, which might lead to wasted resources and cost overrun, especially if you are using the latest generation of powerful GPUs. To improve GPU utilization, multi-instance GPUs allow you to partition a single supported GPU in up to seven slices. Each slice can be allocated to one container on the node independently, for a maximum of seven containers per GPU. Multi-instance GPUs provide hardware isolation between the workloads, and consistent and predictable QoS for all containers running on the GPU.\nFor [CUDA](https://developer.nvidia.com/cuda-zone) \u00ae applications, multi-instance GPUs are largely transparent. Each GPU partition appears as a regular GPU resource, and the programming model remains unchanged.\nFor more information on multi-instance GPUs, refer to the [NVIDIA multi-instance GPU user guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html) .\n### Supported GPUs\nThe following GPU types support multi-instance GPUs:\n- NVIDIA A100 (40GB)\n- NVIDIA A100 (80GB)\n- NVIDIA H100 (80GB) **Note:** If you split NVIDIA H100 GPUs with multi-instance GPUs, you can't use [GPUDirect-TCPX](/kubernetes-engine/docs/how-to/gpu-bandwidth-gpudirect-tcpx) to maximize GPU bandwidth.\n### Multi-instance GPU partitions\nThe A100 GPU and H100 GPU consist of seven compute units and eight memory units, which can be partitioned into GPU instances of varying sizes. The GPU partition sizes use the following syntax: `[compute]g.[memory]gb` . For example, a GPU partition size of `1g.5gb` refers to a GPU instance with one compute unit (1/7th of streaming multiprocessors on the GPU), and one memory unit (5\u00a0GB). The partition size for the GPUs can be specified when you create a cluster. See the [Create a cluster with multi-instance GPUs enabled](#create-cluster) section for an example.\nThe [partitioning table](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#partitioning) in the NVIDIA multi-instance GPU user guide lists all the different GPU partition sizes, along with the amount of compute and memory resources available on each GPU partition. The table also shows the number of GPU instances for each partition size that can be created on the GPU.\nThe following table lists the partition sizes that GKE supports:\n| Partition size        | GPU instances        |\n|:--------------------------------------------|:--------------------------------------------|\n| GPU: NVIDIA A100 (40GB) (nvidia-tesla-a100) | GPU: NVIDIA A100 (40GB) (nvidia-tesla-a100) |\n| 1g.5gb          | 7           |\n| 2g.10gb          | 3           |\n| 3g.20gb          | 2           |\n| 7g.40gb          | 1           |\n| GPU: NVIDIA A100 (80GB) (nvidia-a100-80gb) | GPU: NVIDIA A100 (80GB) (nvidia-a100-80gb) |\n| 1g.10gb          | 7           |\n| 2g.20gb          | 3           |\n| 3g.40gb          | 2           |\n| 7g.80gb          | 1           |\n| GPU: NVIDIA H100 (80GB) (nvidia-h100-80gb) | GPU: NVIDIA H100 (80GB) (nvidia-h100-80gb) |\n| 1g.10gb          | 7           |\n| 1g.20gb          | 4           |\n| 2g.20gb          | 3           |\n| 3g.40gb          | 2           |\n| 7g.80gb          | 1           |\nEach GPU on each node within a node pool is partitioned the same way. For example, consider a node pool with two nodes, four GPUs on each node, and a partition size of `1g.5gb` . GKE creates seven partitions of size `1g.5gb` on each GPU. Since there are four GPUs on each node, there will be 28 `1g.5gb` GPU partitions available on each node. Since there are two nodes in the node pool, a total of 56 `1g.5gb` GPU partitions are available in the entire node pool.\nTo create a GKE cluster with more than one type of GPU partition, you must create multiple node pools. For example, if you want nodes with `1g.5gb` and `3g.20gb` GPU partitions in a cluster, you must create two node pools: one with the GPU partition size set to `1g.5gb` , and the other with `3g.20gb` .\nEach node is labeled with the size of GPU partitions that are available on the node. This labeling allows workloads to target nodes with the needed GPU partition size. For example, on a node with `1g.5gb` GPU instances, the node is labeled as:\n```\ncloud.google.com/gke-gpu-partition-size=1g.5gb\n```\n### How it works\nTo use multi-instance GPUs, you perform the following tasks:\n- [Create a cluster with multi-instance GPUs enabled](#create-cluster) .\n- [Manually install drivers](#install-driver) .\n- [Verify how many GPU resources are on the node](#verify-gpu) .\n- [Deploy containers on the node](#deploy-container) .## Pricing\nMulti-instance GPUs are exclusive to A100 GPUs and H100 GPUs and are subject to the corresponding GPU pricing in addition to any other products used to run your workloads. You can only attach whole GPUs to nodes in your cluster for partitioning. For GPU pricing information, refer to the [GPUs pricing](/compute/gpus-pricing) page.\n## Limitations\n- Using multi-instance GPU partitions with GKE is not recommended for untrusted workloads.\n- Auto-scaling and auto-provisioning GPU partitions is fully supported on GKE version 1.20.7-gke.400 or later. In earlier versions only node pools with at least one node can be auto-scaled based on demand for specific GPU partition sizes from workloads.\n- GPU utilization metrics (for example`duty_cycle`) are not available for GPU instances.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Multi-instance GPUs are supported on GKE version 1.19.7-gke.2503 or later.\n- You must have sufficient NVIDIA A100 GPU quota. See [Requesting an increase in quota](/compute/quotas#requesting_additional_quota) .## Create a cluster with multi-instance GPUs enabled\nWhen you create a cluster with multi-instance GPUs, you must specify `gpuPartitionSize` along with `acceleratorType` and `acceleratorCount` . The `acceleratorType` must be `nvidia-tesla-a100` , `nvidia-a100-80gb` , or `nvidia-h100-80gb` .\nThe following example shows how to create a GKE cluster with one node, and seven GPU partitions of size `1g.5gb` on the node. The other steps in this page use a GPU partition size of `1g.5gb` , which creates seven partitions on each GPU. You can also use any of the supported GPU partition sizes mentioned earlier.\n- To create a cluster with multi-instance GPUs enabled using the Google Cloud CLI, run the following command:```\ngcloud container clusters create CLUSTER_NAME \u00a0\\\u00a0 \u00a0 --project=PROJECT_ID \u00a0\\\u00a0 \u00a0 --zone ZONE \u00a0\\\u00a0 \u00a0 --cluster-version=CLUSTER_VERSION \u00a0\\\u00a0 \u00a0 --accelerator type=nvidia-tesla-a100,count=1,gpu-partition-size=1g.5gb,gpu-driver-version=DRIVER_VERSION \u00a0\\\u00a0 \u00a0 --machine-type=a2-highgpu-1g \u00a0\\\u00a0 \u00a0 --num-nodes=1\n```Replace the following:- ``: the name of your new cluster.\n- ``: the ID of your Google Cloud project.\n- ``: the [compute zone](/compute/docs/regions-zones#available) for the cluster control plane.\n- ``: the version must be`1.19.7-gke.2503`or later.\n- ``: the NVIDIA driver version to install. Can be one of the following:- `default`: Install the default driver version for your GKE version.\n- `latest`: Install the latest available driver version for your GKE version. Available only for nodes that use Container-Optimized OS.\n- `disabled`: Skip automatic driver installation. You **must** [manually install a driver](#install-driver) after you create the cluster. If you omit`gpu-driver-version`, this is the default option.\n- Configure `kubectl` to connect to the newly created cluster:```\ngcloud container clusters get-credentials CLUSTER_NAME\n```## Install drivers\nIf you chose to disable automatic driver installation when creating the cluster, or if you're running a GKE version earlier than 1.27.2-gke.1200, you must [manually install a compatible NVIDIA driver](/kubernetes-engine/docs/how-to/gpus#installing_drivers) after creation completes. Multi-instance GPUs require an NVIDIA driver version 450.80.02 or later.\nAfter the driver is installed, multi-instance GPU mode will be enabled. If you automatically installed drivers, your nodes will reboot when the GPU device plugin starts to create GPU partitions. If you manually installed drivers, your nodes reboot when driver installation completes. The reboot might take a few minutes to complete.\n## Verify how many GPU resources are on the node\nRun the following command to verify that the capacity and allocatable count of `nvidia.com/gpu` resources is 7:\n```\nkubectl describe nodes\n```\nHere's the output from the command:\n```\n...\nCapacity:\n ...\n nvidia.com/gpu:    7\nAllocatable:\n ...\n nvidia.com/gpu:    7\n```\n## Deploy containers on the node\nYou can deploy up to one container per multi-instance GPU device on the node. In this example, with a partition size of `1g.5gb` , there are seven multi-instance GPU partitions available on the node. As a result, you can deploy up to seven containers that request GPUs on this node.\n- Here's an example that starts the `cuda:11.0.3-base-ubi7` container and runs `nvidia-smi` to print the UUID of the GPU within the container. In this example, there are seven containers, and each container receives one GPU partition. This example also sets the node selector to target nodes with `1g.5gb` GPU partitions.```\ncat <<EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: cuda-simplespec:\u00a0 replicas: 7\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: cuda-simple\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: cuda-simple\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-gpu-partition-size: 1g.5gb\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: cuda-simple\u00a0 \u00a0 \u00a0 \u00a0 image: nvidia/cuda:11.0.3-base-ubi7\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /usr/local/nvidia/bin/nvidia-smi -L; sleep 300\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1EOF\n```\n- Verify that all seven Pods are running:```\nkubectl get pods\n```Here's the output from the command:```\nNAME       READY STATUS RESTARTS AGE\ncuda-simple-849c47f6f6-4twr2 1/1  Running 0   7s\ncuda-simple-849c47f6f6-8cjrb 1/1  Running 0   7s\ncuda-simple-849c47f6f6-cfp2s 1/1  Running 0   7s\ncuda-simple-849c47f6f6-dts6g 1/1  Running 0   7s\ncuda-simple-849c47f6f6-fk2bs 1/1  Running 0   7s\ncuda-simple-849c47f6f6-kcv52 1/1  Running 0   7s\ncuda-simple-849c47f6f6-pjljc 1/1  Running 0   7s\n```\n- View the logs to see the GPU UUID, using the name of a Pod from the previous command:```\nkubectl logs cuda-simple-849c47f6f6-4twr2\n```Here's the output from the command:```\nGPU 0: A100-SXM4-40GB (UUID: GPU-45eafa61-be49-c331-f8a2-282736687ab1)\n MIG 1g.5gb Device 0: (UUID: MIG-GPU-45eafa61-be49-c331-f8a2-282736687ab1/11/0)\n```\n- Repeat for any other logs that you want to view:```\nkubectl logs cuda-simple-849c47f6f6-8cjrb\n```Here's the output from the command:```\nGPU 0: A100-SXM4-40GB (UUID: GPU-45eafa61-be49-c331-f8a2-282736687ab1)\n MIG 1g.5gb Device 0: (UUID: MIG-GPU-45eafa61-be49-c331-f8a2-282736687ab1/7/0)\n```## What's next\n- Learn more about [GPUs](/compute/docs/gpus) .\n- Learn how to [configure time-sharing on GPUs](/kubernetes-engine/docs/how-to/timesharing-gpus) .\n- Learn more about [cluster multi-tenancy](/kubernetes-engine/docs/concepts/multitenancy-overview) .\n- Learn more about [best practices for enterprise multi-tenancy](/kubernetes-engine/docs/best-practices/enterprise-multitenancy) .", "guide": "Google Kubernetes Engine (GKE)"}