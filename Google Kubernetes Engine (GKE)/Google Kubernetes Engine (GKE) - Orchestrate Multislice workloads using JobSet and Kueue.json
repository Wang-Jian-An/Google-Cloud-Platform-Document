{"title": "Google Kubernetes Engine (GKE) - Orchestrate Multislice workloads using JobSet and Kueue", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/tpu-multislice-kueue", "abstract": "# Google Kubernetes Engine (GKE) - Orchestrate Multislice workloads using JobSet and Kueue\nThis tutorial shows you how to run a [Jax](https://github.com/google/jax) workload using TPU Multislice in Google Kubernetes Engine (GKE) and [Kueue](https://kueue.sigs.k8s.io) . Kueue implements Job queueing, deciding when Jobs should wait and when they should start, based on quotas and a hierarchy for sharing resources fairly among teams.\nThis tutorial shows how to orchestrate Multislice workloads which require TPU resources to run concurrently.\nBefore you use TPUs in GKE, we recommend that you complete the following learning path:- Learn about current TPU version availability with the [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) .\n- Learn about [TPU Multislice in GKE](/kubernetes-engine/docs/how-to/tpu-multislice) .\n", "content": "## ObjectivesThis tutorial is intended for GKE administrators who have existing GKE Standard mode clusters and want to run Multislice workloads for the first time.\nThis tutorial covers the following steps:- Prepare your environment with a GKE Standard cluster with three v5e TPU slices. Each TPU slice has a`2x4`topology and four chips per host. Therefore, 24 TPU v5e chips in total.\n- Create the Kueue resources to ensure that quotas are shared fairly between the workloads.\n- Run your Multislice workload.\n## Before you beginBefore you start, make sure you have performed the following tasks:- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- [Install JobSet](https://github.com/kubernetes-sigs/jobset/blob/main/docs/setup/install.md) v0.2.3 or later.\n- [Install Kueue](https://kueue.sigs.k8s.io/docs/installation/) v0.4.1 or later.\n## Prepare the environment\n- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the default environment variables:```\ngcloud config set project PROJECT_IDgcloud config set compute/region COMPUTE_REGION\n```Replace the following values:- : your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : the [Compute Engine region](/compute/docs/regions-zones#available) .\n### Create a GKE Standard clusterUse Cloud Shell to do the following:- Create a Standard cluster:```\ngcloud container clusters create multislice-cluster \\\u00a0 \u00a0 --region us-west4-a \\\u00a0 \u00a0 --cluster-version 1.27.4-gke.900 \\\u00a0 \u00a0 --num-nodes=3 \\\u00a0 \u00a0 --machine-type=e2-standard-4 \\\u00a0 \u00a0 --project= PROJECT_ID\n```Cluster creation might take several minutes. GKE creates the `multislice-cluster` cluster in `us-west4-a` which has capacity for the `ct5lp-hightpu-4t` machine type.\n### Create three TPU node pools\n- Create the first node pool named `nodepool1` :```\ngcloud beta container node-pools create nodepool1 \\\u00a0 \u00a0 \u00a0 --zone=us-west4-a \\\u00a0 \u00a0 \u00a0 --cluster=multislice-cluster \\\u00a0 \u00a0 \u00a0 --node-locations=us-west4-a \\\u00a0 \u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 \u00a0 --tpu-topology=2x4 \\\u00a0 \u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 \u00a0 --project=PROJECT_ID\n```\n- Create the second node pool named `nodepool2` :```\ngcloud beta container node-pools create nodepool2 \\\u00a0 \u00a0 \u00a0 --zone=us-west4-a \\\u00a0 \u00a0 \u00a0 --cluster=multislice-cluster \\\u00a0 \u00a0 \u00a0 --node-locations=us-west4-a \\\u00a0 \u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 \u00a0 --tpu-topology=2x4 \\\u00a0 \u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 \u00a0 --project=PROJECT_ID\n```\n- Create the third node pool named `nodepool3` :```\ngcloud beta container node-pools create nodepool3 \\\u00a0 \u00a0 \u00a0 --zone=us-west4-a \\\u00a0 \u00a0 \u00a0 --cluster=multislice-cluster \\\u00a0 \u00a0 \u00a0 --node-locations=us-west4-a \\\u00a0 \u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 \u00a0 --tpu-topology=2x4 \\\u00a0 \u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 \u00a0 --project=PROJECT_ID\n```\nGKE creates three node pools. Each node pool is a separate TPU slice.## Create the Kueue resources\n- Create the following `kueue.yaml` manifest:```\napiVersion: kueue.x-k8s.io/v1beta1kind: ResourceFlavormetadata:\u00a0 name: \"vlp-24\"spec:\u00a0 nodeLabels:\u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4---apiVersion: kueue.x-k8s.io/v1beta1kind: ClusterQueuemetadata:\u00a0 name: \"cluster-queue\"spec:\u00a0 namespaceSelector: {}\u00a0 queueingStrategy: BestEffortFIFO\u00a0 resourceGroups:\u00a0 - coveredResources: [\"google.com/tpu\"]\u00a0 \u00a0 flavors:\u00a0 \u00a0 - name: \"vlp-24\"\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 - name: \"google.com/tpu\"\u00a0 \u00a0 \u00a0 \u00a0 nominalQuota: 24---apiVersion: kueue.x-k8s.io/v1beta1kind: LocalQueuemetadata:\u00a0 namespace: default\u00a0 name: multislice-queuespec:\u00a0 clusterQueue: cluster-queue\n```\n- Apply the `kueue.yaml` manifest:```\nkubectl apply -f kueue.yaml\n```GKE creates the following Kueue resources:\n- [ResourceFlavor](https://kueue.sigs.k8s.io/docs/concepts/resource_flavor/) : An abstraction of the resources in a cluster. In this example, three TPU slices with`2x4`topology and four chips per host, therefore 24 TPU chips.\n- [ClusterQueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/) : A global queue managing workloads and cluster resources.\n- [LocalQueue](https://kueue.sigs.k8s.io/docs/concepts/local_queue/) : Groups closely related workloads that are typically run by a single tenant (user). Each LocalQueue points to a ClusterQueue from which resources are allocated to run its workloads. A [Kueue Workload](https://kueue.sigs.k8s.io/docs/concepts/workload/) is an abstraction representing a batch workload, in this case, each workload is a JobSet.\n## Define your Multislice workloads with JobSetsIn this section, you create three JobSets. These JobSets run a Jax workload which outputs the global number of TPU chips in the slice, then sleeps for 60 seconds to simulate some model training time, then exits.- Create the following `jobsets-multislice.yaml` manifest:```\napiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: multislice-1slice \u00a0\u00a0 labels:\u00a0 \u00a0 kueue.x-k8s.io/queue-name: multislice-queue \u00a0\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: slice\u00a0 \u00a0 \u00a0 replicas: 1\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: jax-tpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: python:3.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -c 'import jax; print(\"Global device count:\", jax.device_count())'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4---apiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: multislice-2slice\u00a0 labels:\u00a0 \u00a0 kueue.x-k8s.io/queue-name: multislice-queue\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: slice\u00a0 \u00a0 \u00a0 replicas: 2\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: jax-tpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: python:3.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -c 'import jax; print(\"Global device count:\", jax.device_count())'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sleep 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4---apiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: multislice-3slice\u00a0 labels:\u00a0 \u00a0 kueue.x-k8s.io/queue-name: multislice-queue\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: slice\u00a0 \u00a0 \u00a0 replicas: 3\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: jax-tpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: python:3.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sleep 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\n```\n- Apply the `jobsets-multislice.yaml` manifest:```\nkubectl apply -f jobsets-multislice.yaml\n```\nGKE creates the Jobs with the following resource requests:- The`multislice-1slice`JobSet creates one Job that requires one TPU slice in total.\n- The`multislice-2slice`JobSet creates two Jobs that require two TPU slices in total.\n- The`multislice-3slice`JobSet creates three Jobs that require three TPU slice in total.\nBecause the cluster only has three TPU slices, not all JobSets can run at once. When Kueue enqueues all three of `multislice-3slice` JobSets, its Jobs run alone to completion. The `multislice-1slice` and `multislice-2slice` wait and run together afterwards.\n### Verify Kueue admitted the workloads\n- Check the enqueued workloads in Kueue:```\nkubectl get workloads\n```The output is similar to the following:```\nNAME        QUEUE    ADMITTED BY  AGE\njobset-multislice-1slice-2530a multislice-queue     3s\njobset-multislice-2slice-ffb02 multislice-queue     4s\njobset-multislice-3slice-8c695 multislice-queue cluster-queue 10s\n```\nKueue enqueues one or more workloads, depending on the TPU resources they require.## Monitor the workloads\n- Monitor which pods are running:```\nkubectl get pods\n```The output is similar to the following:```\nNAME        READY STATUS  RESTARTS AGE\nmultislice-1slice-slice-0-0-pf2ll 1/1  Running  0   1s\nmultislice-1slice-slice-0-1-55g62 1/1  Running  0   1s\nmultislice-2slice-slice-0-0-f4hf7 1/1  Running  0   3s\nmultislice-2slice-slice-0-1-c8kv7 1/1  Running  0   3s\nmultislice-2slice-slice-1-0-7h46t 1/1  Running  0   3s\nmultislice-2slice-slice-1-1-lj9hb 1/1  Running  0   3s\nmultislice-3slice-slice-0-0-wzq9t 0/1  Completed 0   2m31s\nmultislice-3slice-slice-0-1-zf4dp 0/1  Completed 0   2m30s\nmultislice-3slice-slice-1-0-hbfn5 0/1  Completed 0   2m31s\nmultislice-3slice-slice-1-1-45fgl 0/1  Completed 0   2m30s\nmultislice-3slice-slice-2-0-wjbp4 0/1  Completed 0   2m30s\nmultislice-3slice-slice-2-1-lwnvs 0/1  Completed 0   2m30s\n```See that GKE scheduled, created, and ran the Pods for `multislice-3slice` first. Then, GKE ran the Pods from `multislice-1slice` and `multislice-2slice` JobSets.\n **Success:** You've successfully run a three Multislice Jax workloads.## Enable Kueue workload priorities and preemptionOptionally, you can assign Kueue workloads priorities which determine the order in which enqueued workloads are admitted by Kueue.\n **Note:** Depending on the configuration of the ClusterQueue's preemption policy, some preemption scenarios might occur. For example, GKE can preempt a running workload to run a higher priority workload if there are not sufficient resources to run both workloads concurrently. GKE stops and re-enqueues a preempted workload to be continue processing when possible.- Update your `ClusterQueue` to have a preemption policy:```\napiVersion: kueue.x-k8s.io/v1beta1kind: ResourceFlavormetadata:\u00a0 name: \"vlp-24\"spec:\u00a0 nodeLabels:\u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4---apiVersion: kueue.x-k8s.io/v1beta1kind: ClusterQueuemetadata:\u00a0 name: \"cluster-queue\"spec:\u00a0 namespaceSelector: {}\u00a0 resourceGroups:\u00a0 - coveredResources: [\"google.com/tpu\"]\u00a0 \u00a0 flavors:\u00a0 \u00a0 - name: \"vlp-24\"\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 - name: \"google.com/tpu\"\u00a0 \u00a0 \u00a0 \u00a0 nominalQuota: 24\u00a0preemption:\u00a0 \u00a0 reclaimWithinCohort: Any\u00a0 \u00a0 withinClusterQueue: LowerPriority---apiVersion: kueue.x-k8s.io/v1beta1kind: LocalQueuemetadata:\u00a0 namespace: default\u00a0 name: multislice-queuespec:\u00a0 clusterQueue: cluster-queue\n```\n- Create a `PriorityClass` for each distinct priority level you want to assign to workloads:```\napiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata:\u00a0 name: low-priorityvalue: 100globalDefault: falsedescription: \"This low priority class should be used for some Pods only.\"\n```\n- Assign the `priorityClassName` to your JobSet:```\napiVersion: jobset.x-k8s.io/v1alpha2kind: JobSetmetadata:\u00a0 name: low-priority\u00a0 labels:\u00a0 \u00a0 kueue.x-k8s.io/queue-name: multislice-queue\u00a0 annotations:\u00a0 \u00a0 alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepoolspec:\u00a0 failurePolicy:\u00a0 \u00a0 maxRestarts: 4\u00a0 replicatedJobs:\u00a0 \u00a0 - name: slice\u00a0 \u00a0 \u00a0 replicas: 1\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parallelism: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 completions: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backoffLimit: 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 priorityClassName: low-priority\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: jax-tpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: python:3.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sleep 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4 # Number of TPU chips per worker\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resource\n- Delete the Kueue quota system:```\nkubectl delete -n team-a localqueuekubectl delete -n team-b localqueuekubectl delete clusterqueuekubectl delete clusterqueuekubectl delete clusterqueuekubectl delete resourceflavorkubectl delete resourceflavorkubectl delete resourceflavor\n```\n- Delete the Kueue manifest:```\nVERSION=kueue.x-k8s.io/v1beta1kubectl delete -f \\\u00a0 https://github.com/kubernetes-sigs/kueue/releases/download/$VERSION/manifests.yaml\n```\n- Delete the cluster:```\ngcloud container clusters delete kueue-cohort --region=COMPUTE_REGION\n```## What's next\n- Learn more about [Kueue](https://kueue.sigs.k8s.io/docs/overview/) \n- Learn how to [Implement a Job queuing system with quota sharing between namespaces on GKE](/kubernetes-engine/docs/tutorials/kueue-cohort) .", "guide": "Google Kubernetes Engine (GKE)"}