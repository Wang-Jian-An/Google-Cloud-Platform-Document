{"title": "Google Kubernetes Engine (GKE) - Compute classes in Autopilot", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-compute-classes", "abstract": "# Google Kubernetes Engine (GKE) - Compute classes in Autopilot\nThis page describes the that you can use to run Google Kubernetes Engine (GKE) Autopilot workloads that have specific hardware requirements. For instructions, refer to [Run Autopilot Pods on specific compute classes](/kubernetes-engine/docs/how-to/autopilot-compute-classes) .\n", "content": "## Overview of Autopilot compute classes\nBy default, GKE Autopilot Pods run on a compute platform that is optimized for general-purpose workloads such as web serving and medium-intensity batch jobs. This general platform provides a reliable, cost-optimized hardware configuration that can handle the requirements of most workloads.\nIf you have workloads that have unique hardware requirements, such as performing machine learning or AI tasks, running real-time high traffic databases, or needing specific CPU platforms and architecture, Autopilot offers . These compute classes are a curated subset of the Compute Engine [machine series](/compute/docs/machine-types#machine_type_comparison) , and offer flexibility beyond the default Autopilot compute class. For example, the `Scale-Out` compute class uses VMs that turn off simultaneous multi-threading and are optimized for scaling out.\nYou can request nodes backed by specific compute classes based on the requirements of each of your workloads. Similar to the default general-purpose compute class, Autopilot manages the sizing and resource allocation of your requested compute classes based on your running Pods. You can request compute classes at the Pod-level to optimize cost-efficiency by choosing the best fit for each Pod's needs.\n### Choose a specific CPU architecture\nIf your workloads are designed for specific CPU platforms or architectures, you can optionally select those platforms or architectures in your Pod specifications. For example, if you want your Pods to run on nodes that use the Arm architecture, you can choose `arm64` within the `Scale-Out` compute class.\n## Pricing\nGKE Autopilot Pods are priced based on the nodes where the Pods are scheduled. For pricing information for general-purpose workloads and Spot Pods on specific compute classes, and for information on any committed use discounts, refer to [Autopilot mode pricing](/kubernetes-engine/pricing#autopilot_mode) .\nSpot Pods on general-purpose or specialized compute classes don't qualify for committed use discounts.\n## When to use specific compute classes\nThe following table provides a technical overview of the compute classes that Autopilot supports and example use cases for Pods running on each platform. If you don't request a compute class, Autopilot places your Pods on the general-purpose compute platform, which is designed to run most workloads optimally.\n**Note:** The machine types that back each compute class might change over time.\n| Workload requirement                     | Compute class | Description                                                                               | Example use cases                         |\n|:--------------------------------------------------------------------------------------------------------|:----------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|\n| Workloads that don't require specific hardware               | General-purpose | Autopilot uses the general-purpose compute platform if you don't explicitly request a compute class in your Pod specification. You can't explicitly select the general-purpose platform in your specification. Backed by the E2 machine series.                      | Web servers Small to medium databases Development environments              |\n| Workloads that require GPUs                    | Accelerator  | Pods can access compute resources at any time No Pod memory or CPU limits Compatible GPU types are the following: nvidia-h100-80gb: NVIDIA H100 (80GB) (only available with Accelerator compute class) nvidia-a100-80gb: NVIDIA A100 (80GB) nvidia-tesla-a100: NVIDIA A100 (40GB) nvidia-l4: NVIDIA L4 nvidia-tesla-t4: NVIDIA T4  | GPU-centric AI/ML training and inference                   |\n| CPU or memory requests larger than the general-purpose compute class maximums or specific CPU platforms | Balanced  | Available CPUs: AMD EPYC Rome, AMD EPYC Milan, Intel Ice Lake, Intel Cascade Lake Available architecture: amd64 Larger supported resource requests than general-purpose Ability to set minimum CPU platforms for Pods, such as \"Intel Ice Lake or higher\". Backed by the N2 machine series (Intel) or the N2D machine series (AMD). | Web servers Medium to large databases Caching Streaming and media serving Hyperdisk Throughput and Extreme storage |\n| CPU-intensive workloads like AI/ML training or high performance computing (HPC)       | Performance  | Available architecture: amd64, arm64 Pods can access compute resources at any time No Pod memory or CPU limits For a list of Compute Engine machine series available with the Performance compute class, see Supported machine series.                        | CPU-centric AI/ML training and inference HPC batch workloads Hyperdisk Balanced, Throughput, and Extreme storage |\n| Single thread-per-core computing and horizontal scaling             | Scale-Out  | Available CPUs: Ampere Altra Arm or AMD EPYC Milan Compute Engine machine family: T2A (Arm), T2D (x86) Available architecture: arm64 or amd64 SMT off. One vCPU is equal to one physical core. Max 3.5GHz clock Backed by the Tau T2A machine series (Arm) or the Tau T2D machine series (x86).         | Web servers Containerized microservices Data log processing Large-scale Java apps Hyperdisk Throughput storage  |\n## How to select a compute class in Autopilot\nFor detailed instructions, refer to [Choose compute classes for Autopilot Pods](/kubernetes-engine/docs/how-to/autopilot-compute-classes) .\n`cloud.google.com/compute-class`\n[nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)\n[node affinity rule](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity)\n```\n\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 name: hello-app\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 replicas: 3\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: hello-app\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: hello-app\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/compute-class: \"COMPUTE_CLASS\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2000m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"2Gi\"\u00a0 \u00a0 \n```\nReplace `` with the name of the [compute class](/kubernetes-engine/docs/concepts/autopilot-compute-classes#when-to-use) based on your use case, such as `Scale-Out` .  If you select `Accelerator` , you must also specify a compatible GPU. For instructions,  see [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) . If you select `Performance` ,  you must also select a Compute Engine machine series in the node selector. For instructions,  see [Run CPU-intensive workloads with optimal performance](/kubernetes-engine/docs/how-to/performance-pods) .```\n\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 name: hello-app\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 replicas: 3\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: hello-app\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: hello-app\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 terminationGracePeriodSeconds: 25\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2000m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"2Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"1Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: cloud.google.com/compute-class\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"COMPUTE_CLASS\"\u00a0 \u00a0 \u00a0 \n```\nReplace `` with the name of the [compute class](/kubernetes-engine/docs/concepts/autopilot-compute-classes#when-to-use) based on your use case, such as `Scale-Out` . If you select `Accelerator` , you must also specify a compatible GPU. For instructions,   see [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) . If you select `Performance` ,   you must also select a Compute Engine machine series in the node selector. For instructions,   see [Run CPU-intensive workloads with optimal performance](/kubernetes-engine/docs/how-to/cpu-intensive-performance) .\nWhen you deploy the workload, Autopilot does the following:\n- Automatically provisions nodes backed by the specified configuration to run your Pods.\n- Automatically adds [taints](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to the new nodes to prevent other Pods from scheduling on those nodes. The taints are unique to each compute class. If you also select a CPU architecture, GKE adds a separate taint unique to that architecture.\n- Automatically adds tolerations corresponding to the applied taints to your deployed Pods, which lets GKE place those Pods on the new nodes.\nFor example, if you request the `Scale-Out` compute class for a Pod:\n- Autopilot adds a taint specific to`Scale-Out`for those nodes.\n- Autopilot adds a toleration for that taint to the`Scale-Out`Pods.\nPods that don't request `Scale-Out` won't get the toleration. As a result, GKE won't schedule those Pods on the `Scale-Out` nodes.\nIf you don't explicitly request a compute class in your workload specification, Autopilot schedules Pods on nodes that use the default general-purpose compute class. Most workloads can run with no issues on the general-purpose compute class.\n## How to request a CPU architecture\nIn some cases, your workloads might be built for a specific architecture, such as [Arm](https://www.arm.com/architecture) . Some compute classes, such as Balanced or Scale-Out, support multiple CPU architectures. You can request a specific architecture alongside your compute class request by specifying a label in your node selector or node affinity rule, such as in the following example:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: nginx-armspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx-arm\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx-arm\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/compute-class: COMPUTE_CLASS\u00a0 \u00a0 \u00a0 \u00a0 kubernetes.io/arch: ARCHITECTURE\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx-arm\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 2000m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 2Gi\n```\nReplace `` with the CPU architecture that you want, such as `arm64` or `amd64` .\nIf you don't explicitly request an architecture, Autopilot uses the default architecture of the specified compute class.\n### Arm architecture on Autopilot\nAutopilot supports requests for nodes that use the Arm CPU architecture. Arm nodes are more cost-efficient than similar x86 nodes while delivering performance improvements. For instructions to request Arm nodes, refer to [Deploy Autopilot workloads on Arm architecture](/kubernetes-engine/docs/how-to/autopilot-arm-workloads) .\nEnsure that you're using the correct images in your deployments. If your Pods use Arm images and you don't request Arm nodes, Autopilot schedules the Pods on x86 nodes and the Pods will crash. Similarly, if you accidentally use x86 images but request Arm nodes for the Pods, the Pods will crash.\n## Autopilot validations for compute class workloads\nAutopilot validates your workload manifests to ensure that the compute class and architecture requests in your node selector or node affinity rules are correctly formatted. The following rules apply:\n- No more than one compute class.\n- No unsupported compute classes.\n- The GKE version must support the compute class.\n- No more than one selected architecture.\n- The compute class must support the selected architecture.\nIf your workload manifest fails any of these validations, Autopilot rejects the workload.\n## Compute class regional availability\nThe following table describes the regions in which specific compute classes and CPU architectures are available:\n| Compute class availability | Compute class availability.1                                                          |\n|:-----------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| General-purpose    | All regions                                                               |\n| Balanced      | All regions                                                               |\n| Performance     | All regions that contain a supported machine series.                                                    |\n| Scale-Out     | Arm architecture (arm64): asia-southeast1 europe-west4 us-central1 x86 architecture (amd64): asia-east1 asia-southeast1 australia-southeast1 europe-west1 europe-west2 europe-west3 europe-west4 southamerica-east1 us-central1 us-east1 us-east4 us-west1 us-west4 |\nIf a compute class is available in a specific region, the hardware is available in at least two zones in that region.\n## Default, minimum, and maximum resource requests\nWhen choosing a compute class for your Autopilot workloads, make sure that you specify resource requests that meet the minimum and maximum requests for that compute class. For information about the default requests, as well as the minimum and maximum requests for each compute class, refer to [Resource requests and limits in GKE Autopilot](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .\n## What's next\n- [Learn how to select specific compute classes in yourAutopilotworkloads](/kubernetes-engine/docs/how-to/autopilot-compute-classes) .\n- [Read about the default, minimum, and maximum resource requests for eachplatform](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .", "guide": "Google Kubernetes Engine (GKE)"}