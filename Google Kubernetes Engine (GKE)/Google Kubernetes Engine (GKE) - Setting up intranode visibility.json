{"title": "Google Kubernetes Engine (GKE) - Setting up intranode visibility", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/intranode-visibility", "abstract": "# Google Kubernetes Engine (GKE) - Setting up intranode visibility\nThis guide shows you how to set up on a Google Kubernetes Engine (GKE) cluster.\nIntranode visibility configures networking on each node in the cluster so that traffic sent from one Pod to another Pod is processed by the cluster's Virtual Private Cloud (VPC) network, even if the Pods are on the same node.\nIntranode visibility is disabled by default on Standard clusters and enabled by default in Autopilot clusters.\n", "content": "## Architecture\nIntranode visibility ensures that packets sent between Pods are always processed by the VPC network, which ensures that firewall rules, routes, flow logs, and packet mirroring configurations apply to the packets.\nWhen a Pod sends a packet to another Pod on the same node, the packet leaves the node and is processed by the Google Cloud network. Then the packet is immediately sent back to the same node and forwarded to the destination Pod.\nIntranode visibility deploys the `netd` DaemonSet.\n## Benefits\nIntranode visibility provides the following benefits:\n- See [flow logs](/vpc/docs/using-flow-logs) for all traffic between Pods, including traffic between Pods on the same node.\n- Create [firewall rules](/vpc/docs/firewalls) that apply to all traffic among Pods, including traffic between Pods on the same node.\n- Use [Packet Mirroring](/vpc/docs/packet-mirroring) to clone traffic, including traffic between Pods on the same node, and forward it for examination.## Requirements and limitations\nIntranode visibility has the following requirements and limitations:\n- Your cluster must be on GKE version 1.15 or later.\n- Intranode visibility is not supported with Windows Server node pools.\n- If you enable intranode visibility, and use the`ip-masq-agent`configured with the`nonMasqueradeCIDRs`parameter, you must include the Pod CIDR range in`nonMasqueradeCIDRs`to avoid experiencing intranode connectivity issues.## Firewall rules\nWhen you enable intranode visibility, the VPC network processes all packets sent between Pods, including packets sent between Pods on the same node. This means VPC firewall rules and hierarchical firewall policies consistently apply to Pod-to-Pod communication, regardless of Pod location.\nIf you configure custom firewall rules for communication within the cluster, carefully evaluate your cluster's networking needs to determine the set of egress and ingress allow rules. You can use connectivity tests to ensure that legitimate traffic is not obstructed. For example, Pod-to-Pod communication is required for [network policy](/kubernetes-engine/docs/how-to/network-policy) to function.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Enable intranode visibility on a new cluster\nYou can create a cluster with intranode visibility enabled using the gcloud CLI or the Google Cloud console.\nTo create a single-node cluster that has intranode visibility enabled, use the `--enable-intra-node-visibility` flag:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --enable-intra-node-visibility\n```\nReplace the following:- ``: the name of your new cluster.\n- ``: the [compute region](/compute/docs/regions-zones#available) for the cluster.\nTo create a single-node cluster that has intranode visibility enabled, perform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Enter the **Name** for your cluster.\n- In the **Configure cluster** dialog, next to **GKEStandard** , click **Configure** .\n- Configure your cluster as needed.\n- From the navigation pane, under **Cluster** , click **Networking** .\n- Select the **Enable intranode visibility** checkbox.\n- Click **Create** .## Enable intranode visibility on an existing cluster\nYou can enable intranode visibility on an existing cluster using the gcloud CLI or the Google Cloud console.\nWhen you enable intranode visibility for an existing cluster, GKE restarts components in both the control plane and the worker nodes.\nTo enable intranode visibility on an existing cluster, use the `--enable-intra-node-visibility` flag:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-intra-node-visibility\n```\nReplace `` with the name of your cluster.\nTo enable intranode visibility on an existing cluster, perform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Networking** , click **Edit intranodevisibility** .\n- Select the **Enable intranode visibility** checkbox.\n- Click **Save Changes** .\n**Warning:** When you enable intranode visibility, GKE respects your configured [maintenance windows](/kubernetes-engine/docs/how-to/maintenance-window) when recreating nodes. This means that intranode visibility won't be operational on the cluster until the next maintenance window occurs. If you prefer not to wait, you can manually upgrade the node pool by setting the `--cluster-version` flag to the same GKE version the control plane is already running. You must use the gcloud CLI if you use this workaround. For more information, see [caveats for maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#caveats) .\n## Disable intranode visibility\nYou can disable intranode visibility on a cluster using the gcloud CLI or the Google Cloud console.\nWhen you disable intranode visibility for an existing cluster, GKE restarts components in both the control plane and the worker nodes.\nTo disable intranode visibility, use the `--no-enable-intra-node-visibility` flag:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-intra-node-visibility\n```\nReplace `` with the name of your cluster.\nTo disable intranode visibility, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Networking** , click **Edit intranode visibility** .\n- Clear the **Enable intranode visibility** checkbox.\n- Click **Save Changes** .\n**Warning:** When you disable intranode visibility, GKE respects your configured [maintenance windows](/kubernetes-engine/docs/how-to/maintenance-window) when recreating nodes. This means that intranode visibility won't be disabled until the next maintenance window occurs. If you prefer not to wait, you can manually upgrade the node pool by setting the `--cluster-version` flag to the same GKE version the control plane is already running. You must use the gcloud CLI if you use this workaround. For more information, see [caveats for maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#caveats) .\n## Exercise: Verify intranode visibility\nThis exercise shows you the steps required to enable intranode visibility and confirm that it is working for your cluster.\nIn this exercise, you perform the following steps:\n- Enable flow logs for the default subnet in the`us-central1`region.\n- Create a single-node cluster with intranode visibility enabled in the`us-central1-a`zone.\n- Create two Pods in your cluster.\n- Send an HTTP request from one Pod to another Pod.\n- View the flow log entry for the Pod-to-Pod request.\n### Enable flow logs\n- Enable flow logs for the default subnet:```\ngcloud compute networks subnets update default \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --enable-flow-logs\n```\n- Verify that the default subnet has flow logs enabled:```\ngcloud compute networks subnets describe default \\\u00a0 \u00a0 --region=us-central1\n```The output shows that flow logs are enabled, similar to the following:```\n...\nenableFlowLogs: true\n...\n```\n### Create a cluster\n- Create a single node cluster with intranode visibility enabled:```\ngcloud container clusters create flow-log-test \\\u00a0 \u00a0 --zone=us-central1-a \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --enable-intra-node-visibility\n```\n- Get the credentials for your cluster:```\ngcloud container clusters get-credentials flow-log-test \\\u00a0 \u00a0 --zone=us-central1-a\n```\n### Create two Pods\n- Create a Pod.Save the following manifest to a file named `pod-1.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: pod-1spec:\u00a0 containers:\u00a0 - name: container-1\u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\n```\n- Apply the manifest to your cluster:```\nkubectl apply -f pod-1.yaml\n```\n- Create a second Pod.Save the following manifest to a file named `pod-2.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: pod-2spec:\u00a0 containers:\u00a0 - name: container-2\u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\n```\n- Apply the manifest to your cluster:```\nkubectl apply -f pod-2.yaml\n```\n- View the Pods:```\nkubectl get pod pod-1 pod-2 --output wide\n```The output shows the IP addresses of your Pods, similar to the following:```\nNAME  READY  STATUS RESTARTS AGE  IP   ...\npod-1  1/1  Running 0   1d  10.52.0.13 ...\npod-2  1/1  Running 0   1d  10.52.0.14 ...\n```Note the IP addresses of `pod-1` and `pod-2` .\n### Send a request\n- Get a shell to the container in `pod-1` :```\nkubectl exec -it pod-1 -- sh\n```\n- In your shell, send a request to `pod-2` :```\nwget -qO- POD_2_IP_ADDRESS:8080\n```Replace `` with the IP address of `pod-2` .The output shows the response from the container running in `pod-2` .```\nHello, world!\nVersion: 2.0.0\nHostname: pod-2\n```\n- Type exit to leave the shell and return to your main command-line environment.\n### View flow log entries\nTo view a flow log entry, use the following command:\n```\ngcloud logging read \\\u00a0 \u00a0 'logName=\"projects/PROJECT_ID/logs/compute.googleapis.com%2Fvpc_flows\" AND jsonPayload.connection.src_ip=\"POD_1_IP_ADDRESS\" AND jsonPayload.connection.dest_ip=\"POD_2_IP_ADDRESS\"'\n```\nReplace the following:\n- ``: your project ID.\n- ``: the IP address of`pod-1`.\n- ``: the IP address of`pod-2`.\nThe output shows a flow log entry for a request from `pod-1` to `pod-2` . In this example, `pod-1` has IP address `10.56.0.13` , and `pod-2` has IP address `10.56.0.14` .\n```\n...\njsonPayload:\n bytes_sent: '0'\n connection:\n dest_ip: 10.56.0.14\n dest_port: 8080\n protocol: 6\n src_ip: 10.56.0.13\n src_port: 35414\n...\n```\n## Clean up\nTo avoid incurring unwanted charges on your account, perform the following steps to remove the resources you created:\n- Delete the cluster:```\ngcloud container clusters delete -q flow-log-test\n```\n- Disable flow logs for the default subnet:```\ngcloud compute networks subnets update default --no-enable-flow-logs\n```## What's next\n- Learn how to control the communication between your cluster's Pods and Services by [creating a cluster network policy](/kubernetes-engine/docs/how-to/network-policy) .\n- Learn about the benefits of [VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) .", "guide": "Google Kubernetes Engine (GKE)"}