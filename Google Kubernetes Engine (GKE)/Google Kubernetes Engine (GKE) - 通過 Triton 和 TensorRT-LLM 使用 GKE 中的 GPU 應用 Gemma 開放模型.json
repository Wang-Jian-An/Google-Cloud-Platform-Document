{"title": "Google Kubernetes Engine (GKE) - \u901a\u904e Triton \u548c TensorRT-LLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma \u958b\u653e\u6a21\u578b", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tensortllm?hl=zh-cn", "abstract": "# Google Kubernetes Engine (GKE) - \u901a\u904e Triton \u548c TensorRT-LLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma \u958b\u653e\u6a21\u578b\n\u672c\u6559\u7a0b\u4ecb\u7d39\u5982\u4f55\u901a\u904e NVIDIA [Triton](https://developer.nvidia.com/triton-inference-server) \u548c [TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/) \u670d\u52d9\u5806\u68e7\uff0c\u4f7f\u7528 Google Kubernetes Engine (GKE) \u4e2d\u7684\u5716\u5f62\u8655\u7406\u5668 (GPU) \u4f86\u61c9\u7528 [Gemma](https://ai.google.dev/gemma/docs/?hl=zh-cn) \u5927\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c07\u4e0b\u8f09 2B \u548c 7B \u53c3\u6578\u6307\u4ee4\u8abf\u512a Gemma \u6a21\u578b\uff0c\u4e26\u4f7f\u7528\u904b\u884c Triton \u548c TensorRT-LLM \u7684\u5bb9\u5668\u5c07\u5b83\u5011\u90e8\u7f72\u5230 GKE [Autopilot](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn#why-autopilot) \u6216 [Standard](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn#why-standard) \u96c6\u7fa3\u3002\n\u5982\u679c\u60a8\u5728\u90e8\u7f72\u548c\u61c9\u7528 AI/\u6a5f\u5668\u5b78\u7fd2\u5de5\u4f5c\u8ca0\u8f09\u6642\u9700\u8981\u5229\u7528\u4ee3\u7ba1\u5f0f Kubernetes \u7684\u7cbe\u7d30\u63a7\u5236\u3001\u53ef\u4f38\u7e2e\u6027\u3001\u5f48\u6027\u3001\u53ef\u79fb\u690d\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u90a3\u9ebc\u672c\u6307\u5357\u662f\u4e00\u500b\u5f88\u597d\u7684\u8d77\u9ede\u3002\u5982\u679c\u60a8\u9700\u8981\u7d71\u4e00\u7684\u4ee3\u7ba1\u5f0f AI \u5e73\u81fa\u4f86\u7d93\u6fdf\u9ad8\u6548\u5730\u5feb\u901f\u69cb\u5efa\u548c\u61c9\u7528\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c\u6211\u5011\u5efa\u8b70\u60a8\u8a66\u7528\u6211\u5011\u7684 [Vertex AI](https://cloud.google.com/vertex-ai?hl=zh-cn) \u90e8\u7f72\u89e3\u6c7a\u65b9\u6848\u3002", "content": "## \u80cc\u666f\u60a8\u53ef\u4ee5\u901a\u904e Triton \u548c TensorRT-LLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma\uff0c\u5f9e\u800c\u5be6\u73fe\u4e00\u500b\u53ef\u76f4\u63a5\u7528\u65bc\u751f\u7522\u74b0\u5883\u7684\u5f37\u5927\u63a8\u7406\u670d\u52d9\u89e3\u6c7a\u65b9\u6848\uff0c\u5177\u5099\u4ee3\u7ba1\u5f0f [Kubernetes](https://kubernetes.io/) \u7684\u6240\u6709\u512a\u52e2\uff0c\u5305\u62ec\u9ad8\u6548\u7684\u53ef\u4f38\u7e2e\u6027\u548c\u66f4\u9ad8\u7684\u53ef\u7528\u6027\u3002\u672c\u90e8\u5206\u4ecb\u7d39\u672c\u6307\u5357\u4e2d\u4f7f\u7528\u7684\u95dc\u9375\u6280\u8853\u3002\n### Gemma [Gemma](https://ai.google.dev/gemma/docs/?hl=zh-cn) \u662f\u4e00\u7d44\u516c\u958b\u63d0\u4f9b\u7684\u8f15\u91cf\u7d1a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd (AI) \u6a21\u578b\uff08\u6839\u64da\u958b\u653e\u8a31\u53ef\u767c\u4f48\uff09\u3002\u9019\u4e9b AI \u6a21\u578b\u53ef\u4ee5\u5728\u61c9\u7528\u3001\u786c\u4ef6\u3001\u79fb\u52d5\u8a2d\u5099\u6216\u8a17\u7ba1\u670d\u52d9\u4e2d\u904b\u884c\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 Gemma \u6a21\u578b\u751f\u6210\u6587\u672c\uff0c\u4f46\u4e5f\u53ef\u4ee5\u91dd\u5c0d\u5c08\u9580\u4efb\u52d9\u5c0d\u9019\u4e9b\u6a21\u578b\u9032\u884c\u8abf\u512a\u3002\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Gemma \u6587\u6a94](https://ai.google.dev/gemma/docs?hl=zh-cn) \u3002\n### GPU\u5229\u7528 GPU\uff0c\u60a8\u53ef\u4ee5\u52a0\u901f\u5728\u7bc0\u9ede\u4e0a\u904b\u884c\u7684\u7279\u5b9a\u5de5\u4f5c\u8ca0\u8f09\uff08\u4f8b\u5982\u6a5f\u5668\u5b78\u7fd2\u548c\u6578\u64da\u8655\u7406\uff09\u3002GKE \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u6a5f\u5668\u985e\u578b\u9078\u9805\u4ee5\u7528\u65bc\u7bc0\u9ede\u914d\u7f6e\uff0c\u5305\u62ec\u914d\u5099 NVIDIA H100\u3001L4 \u548c A100 GPU \u7684\u6a5f\u5668\u985e\u578b\u3002\n\u4f7f\u7528 GKE \u4e2d\u7684 GPU \u4e4b\u524d\uff0c\u6211\u5011\u5efa\u8b70\u60a8\u5b8c\u6210\u4ee5\u4e0b\u5b78\u7fd2\u8def\u7dda\uff1a- \u77ad\u89e3 [\u7576\u524d GPU \u7248\u672c\u53ef\u7528\u6027](https://cloud.google.com/compute/docs/gpus?hl=zh-cn) \n- \u77ad\u89e3 [GKE \u4e2d\u7684 GPU](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus?hl=zh-cn) \n### TensorRT-LLMNVIDIA TensorRT-LLM (TRT-LLM) \u662f\u4e00\u7a2e\u5177\u6709 Python API \u7684\u5de5\u5177\u5305\uff0c\u53ef\u5f59\u7de8\u7d93\u904e\u512a\u5316\u7684\u89e3\u6c7a\u65b9\u6848\uff0c\u4ee5\u5b9a\u7fa9 LLM \u4e26\u69cb\u5efa\u53ef\u5728 NVIDIA GPU \u4e0a\u9ad8\u6548\u57f7\u884c\u63a8\u7406\u7684 TensorRT \u5f15\u64ce\u3002TensorRT-LLM \u5305\u542b\u4ee5\u4e0b\u529f\u80fd\uff1a- \u5177\u6709\u5c64\u878d\u5408\u3001\u6fc0\u6d3b\u7de9\u5b58\u3001\u5167\u5b58\u7de9\u885d\u5340\u91cd\u8907\u4f7f\u7528\u548c [PagedAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention) \u4e14\u7d93\u904e\u512a\u5316\u7684 Transformer\uff08\u8f49\u63db\u5668\uff09\u5be6\u73fe\n- \u904b\u884c\u4e2d\u6279\u8655\u7406\u6216\u9023\u7e8c\u6279\u8655\u7406\uff0c\u53ef\u63d0\u9ad8\u6574\u9ad4\u670d\u52d9\u541e\u5410\u91cf\n- \u5f35\u91cf\u4e26\u884c\u8655\u7406\u548c\u6d41\u6c34\u7dda\u4e26\u884c\u8655\u7406\uff0c\u7528\u65bc\u5728\u591a\u500b GPU \u4e0a\u5be6\u73fe\u5206\u4f48\u5f0f\u670d\u52d9\n- \u91cf\u5316\uff08FP16\u3001FP8\u3001INT8\uff09\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [TensorRT-LLM \u6587\u6a94](https://nvidia.github.io/TensorRT-LLM/) \u3002\n### \u6d77\u885b\u4e00NVIDIA Triton \u63a8\u7406\u670d\u52d9\u5668\u662f\u9069\u7528\u65bc AI/\u6a5f\u5668\u5b78\u7fd2\u61c9\u7528\u7684\u958b\u6e90\u63a8\u7406\u670d\u52d9\u5668\u3002Triton \u901a\u904e\u7d93\u904e\u512a\u5316\u7684\u5f8c\u7aef\uff08\u5305\u62ec TensorRT \u548c TensorRT-LLM\uff09\uff0c\u652f\u6301\u5728 NVIDIA GPU \u548c CPU \u4e0a\u9032\u884c\u9ad8\u6027\u80fd\u63a8\u7406\u3002Triton \u5305\u542b\u4ee5\u4e0b\u529f\u80fd\uff1a- \u591a GPU\u3001\u591a\u7bc0\u9ede\u63a8\u7406\n- \u591a\u6a21\u578b\u4f75\u767c\u57f7\u884c\n- \u6a21\u578b\u96c6\u6210\u5b78\u7fd2\u6216\u93c8\u5f0f\n- \u9810\u6e2c\u8acb\u6c42\u7684\u975c\u614b\u3001\u52d5\u614b\u548c\u9023\u7e8c\u6216\u904b\u884c\u4e2d\u6279\u8655\u7406\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Triton \u6587\u6a94](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) \u3002## \u76ee\u6a19\u672c\u6307\u5357\u9069\u7528\u65bc\u4f7f\u7528 [PyTorch](https://pytorch.org/) \u7684\u751f\u6210\u5f0f AI \u5ba2\u6236\u3001GKE \u7684\u65b0\u7528\u6236\u6216\u73fe\u6709\u7528\u6236\u3001\u6a5f\u5668\u5b78\u7fd2\u5de5\u7a0b\u5e2b\u3001MLOps (DevOps) \u5de5\u7a0b\u5e2b\u6216\u662f\u5c0d\u4f7f\u7528 Kubernetes \u5bb9\u5668\u7de8\u6392\u529f\u80fd\u5728 H100\u3001A100 \u548c L4 GPU \u786c\u4ef6\u4e0a\u61c9\u7528 LLM \u611f\u8208\u8da3\u7684\u5e73\u81fa\u7ba1\u7406\u54e1\u3002\n\u95b1\u8b80\u5b8c\u672c\u6307\u5357\u5f8c\uff0c\u60a8\u61c9\u8a72\u80fd\u5920\u57f7\u884c\u4ee5\u4e0b\u6b65\u9a5f\uff1a- \u4f7f\u7528\u8655\u65bc Autopilot \u6a21\u5f0f\u7684 GKE \u96c6\u7fa3\u6e96\u5099\u74b0\u5883\u3002\n- \u5c07\u4f7f\u7528 Triton \u548c TritonRT-LLM \u7684\u5bb9\u5668\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa3\u3002\n- \u901a\u904e curl\uff0c\u4f7f\u7528 Triton \u548c TensorRT-LLM \u61c9\u7528 Gemma 2B \u6216 7B \u6a21\u578b\u3002\n## \u6e96\u5099\u5de5\u4f5c\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- \u78ba\u4fdd\u60a8\u64c1\u6709\u9805\u76ee\u7684\u4ee5\u4e0b\u4e00\u500b\u6216\u591a\u500b\u89d2\u8272\uff1a      roles/container.admin, roles/iam.serviceAccountAdmin\n- \u5982\u679c\u60a8\u9084\u6c92\u6709 [Kaggle](https://www.kaggle.com/) \u8cec\u865f\uff0c\u8acb\u5275\u5efa\u4e00\u500b\u3002\n- [\u78ba\u4fdd\u60a8\u7684\u9805\u76ee\u5177\u6709\u8db3\u5920\u7684\u914d\u984d](https://cloud.google.com/compute/resource-usage?hl=zh-cn#gpu_quota) \uff0c\u4ee5\u4fbf\u7528\u65bc GKE \u4e2d\u7684 GPU\u3002\n## \u6e96\u5099\u74b0\u5883\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c07\u4f7f\u7528 [Cloud Shell](https://cloud.google.com/shell?hl=zh-cn) \u4f86\u7ba1\u7406 Google Cloud \u4e0a\u8a17\u7ba1\u7684\u8cc7\u6e90\u3002Cloud Shell \u9810\u5b89\u88dd\u6709\u672c\u6559\u7a0b\u6240\u9700\u7684\u8edf\u4ef6\uff0c\u5305\u62ec [kubectl](https://kubernetes.io/docs/reference/kubectl/) \u548c [gcloud CLI](https://cloud.google.com/sdk/gcloud?hl=zh-cn) \u3002\n\u5982\u9700\u4f7f\u7528 Cloud Shell \u8a2d\u7f6e\u60a8\u7684\u74b0\u5883\uff0c\u8acb\u6309\u7167\u4ee5\u4e0b\u6b65\u9a5f\u64cd\u4f5c\uff1a- \u5728 Google Cloud \u63a7\u5236\u6aaf\u4e2d\uff0c\u9ede\u64ca [Google Cloud \u63a7\u5236\u6aaf](http://console.cloud.google.com?hl=zh-cn) \u4e2d\u7684 **\u6fc0\u6d3b Cloud Shell** \u4ee5\u5553\u52d5 Cloud Shell \u6703\u8a71\u3002\u6b64\u64cd\u4f5c\u6703\u5728 Google Cloud \u63a7\u5236\u6aaf\u7684\u5e95\u90e8\u7a97\u683c\u4e2d\u5553\u52d5\u6703\u8a71\u3002\n- \u8a2d\u7f6e\u9ed8\u8a8d\u74b0\u5883\u8b8a\u91cf\uff1a```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=REGIONexport CLUSTER_NAME=triton\n```\u66ff\u63db\u4ee5\u4e0b\u503c\uff1a- \uff1a\u60a8\u7684 Google Cloud [\u9805\u76ee ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects?hl=zh-cn#identifying_projects) \u3002\n- \uff1a\u652f\u6301\u8981\u4f7f\u7528\u7684\u52a0\u901f\u5668\u985e\u578b\u7684\u5340\u57df\uff0c\u4f8b\u5982\u9069\u7528\u65bc L4 GPU \u7684`us-central1`\u3002## \u7372\u53d6\u5c0d\u6a21\u578b\u7684\u8a2a\u554f\u6b0a\u9650\u5982\u9700\u7372\u53d6\u5c0d Gemma \u6a21\u578b\u7684\u8a2a\u554f\u6b0a\u9650\uff0c\u60a8\u5fc5\u9808\u767b\u9304 Kaggle \u5e73\u81fa\u4e26\u7372\u53d6 Kaggle API \u4ee4\u724c\u3002\n### \u7c3d\u7f72\u8a31\u53ef\u540c\u610f\u5354\u8b70\u60a8\u5fc5\u9808\u7c3d\u7f72\u540c\u610f\u5354\u8b70\u624d\u80fd\u4f7f\u7528 Gemma\u3002\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u64cd\u4f5c\uff1a- \u8a2a\u554f Kaggle.com \u4e0a\u7684 [\u6a21\u578b\u540c\u610f\u9801\u9762](https://www.kaggle.com/models/google/gemma) \u3002\n- \u5982\u679c\u60a8\u5c1a\u672a\u767b\u9304 Kaggle\uff0c\u8acb\u9032\u884c\u767b\u9304\u3002\n- \u9ede\u64ca **\u7533\u8acb\u8a2a\u554f\u6b0a\u9650** \u3002\n- \u5728 **Choose Account for Consent** \uff08\u9078\u64c7\u9032\u884c\u540c\u610f\u7684\u8cec\u865f\uff09\u90e8\u5206\u4e2d\uff0c\u9078\u64c7 **Verify via Kaggle Account** \uff08\u901a\u904e Kaggle \u8cec\u865f\u9a57\u8b49\uff09\uff0c\u4ee5\u4f7f\u7528\u60a8\u7684 Kaggle \u8cec\u865f\u9032\u884c\u540c\u610f\u3002\n- \u63a5\u53d7\u6a21\u578b **\u689d\u6b3e\u53ca\u689d\u4ef6** \u3002\n### \u751f\u6210\u4e00\u500b\u8a2a\u554f\u4ee4\u724c\u5982\u9700\u901a\u904e Kaggle \u8a2a\u554f\u6a21\u578b\uff0c\u60a8\u9700\u8981 Kaggle API \u4ee4\u724c\u3002\u5982\u679c\u60a8\u9084\u6c92\u6709\u4ee4\u724c\uff0c\u8acb\u6309\u7167\u4ee5\u4e0b\u6b65\u9a5f\u751f\u6210\u65b0\u4ee4\u724c\uff1a- \u5728\u700f\u89bd\u5668\u4e2d\uff0c\u8f49\u5230 **Kaggle \u8a2d\u7f6e** \u3002\n- \u5728 API \u90e8\u5206\u4e0b\uff0c\u9ede\u64ca **Create New Token** \uff08\u5275\u5efa\u65b0\u4ee4\u724c\uff09\u3002\n\u7cfb\u7d71\u5c07\u4e0b\u8f09\u540d\u7232 `kaggle.json` \u7684\u6587\u4ef6\u3002\n### \u5c07\u8a2a\u554f\u4ee4\u724c\u4e0a\u50b3\u5230 Cloud Shell\u5728 Cloud Shell \u4e2d\uff0c\u5c07 Kaggle API \u4ee4\u724c\u4e0a\u50b3\u5230 Google Cloud \u9805\u76ee\uff1a- \u5728 Cloud Shell \u4e2d\uff0c\u9ede\u64camore_vert **\u66f4\u591a** > **\u4e0a\u50b3** \u3002\n- \u9078\u64c7\u201c\u6587\u4ef6\u201d\uff0c\u7136\u5f8c\u9ede\u64ca **\u9078\u64c7\u6587\u4ef6** \u3002\n- \u6253\u958b`kaggle.json`\u6587\u4ef6\u3002\n- \u9ede\u64ca **\u4e0a\u50b3** \u3002\n## \u5275\u5efa\u548c\u914d\u7f6e Google Cloud \u8cc7\u6e90\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u5275\u5efa\u6240\u9700\u7684\u8cc7\u6e90\u3002\n **\u6ce8\u610f** \uff1a\u60a8\u53ef\u80fd\u9700\u8981\u5275\u5efa\u5bb9\u91cf\u9810\u7559\u624d\u80fd\u4f7f\u7528\u67d0\u4e9b\u52a0\u901f\u5668\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u9810\u7559\u548c\u4f7f\u7528\u9810\u7559\u7684\u8cc7\u6e90\uff0c\u8acb\u53c3\u95b1 [\u4f7f\u7528\u9810\u7559\u7684\u53ef\u7528\u5340\u7d1a\u8cc7\u6e90](https://cloud.google.com/kubernetes-engine/docs/how-to/consuming-reservations?hl=zh-cn) \u3002\n### \u5275\u5efa GKE \u96c6\u7fa3\u548c\u7bc0\u9ede\u6c60\u60a8\u53ef\u4ee5\u5728 GKE Autopilot \u6216 Standard \u96c6\u7fa3\u4e2d\u7684 GPU \u4e0a\u61c9\u7528 Gemma\u3002\u6211\u5011\u5efa\u8b70\u60a8\u4f7f\u7528 Autopilot \u96c6\u7fa3\u7372\u5f97\u5168\u8a17\u7ba1\u5f0f Kubernetes \u9ad4\u9a57\u3002\u5982\u9700\u9078\u64c7\u6700\u9069\u5408\u60a8\u7684\u5de5\u4f5c\u8ca0\u8f09\u7684 GKE \u64cd\u4f5c\u6a21\u5f0f\uff0c\u8acb\u53c3\u95b1 [\u9078\u64c7 GKE \u64cd\u4f5c\u6a21\u5f0f](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn) \u3002\n\u5728 Cloud Shell \u4e2d\uff0c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a\n```\ngcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid \\\u00a0 --cluster-version=1.28\n```\nGKE \u6703\u6839\u64da\u6240\u90e8\u7f72\u7684\u5de5\u4f5c\u8ca0\u8f09\u7684\u8acb\u6c42\uff0c\u5275\u5efa\u5177\u6709\u6240\u9700 CPU \u548c GPU \u7bc0\u9ede\u7684 Autopilot \u96c6\u7fa3\u3002- \u5728 Cloud Shell \u4e2d\uff0c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u5275\u5efa Standard \u96c6\u7fa3\uff1a```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 --location=${REGION}-a \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --machine-type=e2-standard-4 \\\u00a0 \u00a0 --num-nodes=1\n```\u96c6\u7fa3\u5275\u5efa\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\u3002\n- \u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4f86\u7232\u96c6\u7fa3\u5275\u5efa [\u7bc0\u9ede\u6c60](https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools?hl=zh-cn) \uff1a```\ngcloud container node-pools create gpupool \\\u00a0 \u00a0 --accelerator type=nvidia-l4,count=1,gpu-driver-version=latest \\\u00a0 \u00a0 --project=${PROJECT_ID} \\\u00a0 \u00a0 --location=${REGION}-a \\\u00a0 \u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 \u00a0 --machine-type=g2-standard-12 \\\u00a0 \u00a0 --num-nodes=1\n```GKE \u6703\u5275\u5efa\u4e00\u500b\u7bc0\u9ede\u6c60\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u500b L4 GPU \u7bc0\u9ede\u3002### \u7232 Kaggle \u6191\u64da\u5275\u5efa Kubernetes Secret\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u6703\u5c07 Kubernetes Secret \u7528\u65bc Kaggle \u6191\u64da\u3002\n\u5728 Cloud Shell \u4e2d\uff0c\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a- \u914d\u7f6e `kubectl` \u4ee5\u8207\u60a8\u7684\u96c6\u7fa3\u901a\u4fe1\uff1a```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}\n```\n- \u5275\u5efa\u4e00\u500b Secret \u4ee5\u5b58\u5132 Kaggle \u6191\u64da\uff1a```\nkubectl create secret generic kaggle-secret \\\u00a0 \u00a0 --from-file=kaggle.json \\\u00a0 \u00a0 --dry-run=client -o yaml | kubectl apply -f ```\n## \u5275\u5efa PersistentVolume \u8cc7\u6e90\u4ee5\u5b58\u5132\u6aa2\u67e5\u9ede\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u5275\u5efa\u4e00\u500b\u7531\u6c38\u4e45\u6027\u78c1\u76e4\u63d0\u4f9b\u652f\u6301\u7684 PersistentVolume\uff0c\u4ee5\u5b58\u5132\u6a21\u578b\u6aa2\u67e5\u9ede\u3002- \u5275\u5efa\u4ee5\u4e0b `trtllm_checkpoint_pv.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/trtllm_checkpoint_pv.yaml) ```\napiVersion: v1kind: PersistentVolumeClaimmetadata:\u00a0 name: model-dataspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 100G\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f trtllm_checkpoint_pv.yaml\n```\n## \u4e0b\u8f09\u9069\u7528\u65bc Gemma \u7684 TensorRT-LLM \u5f15\u64ce\u6587\u4ef6\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u904b\u884c\u4e00\u500b [\u4f5c\u696d](https://kubernetes.io/docs/concepts/workloads/controllers/job/) \u4ee5\u4e0b\u8f09 TensorRT-LLM \u5f15\u64ce\u6587\u4ef6\u4e26\u5c07\u6587\u4ef6\u5b58\u5132\u5728\u60a8\u4e4b\u524d\u5275\u5efa\u7684 PersistentVolume \u4e2d\u3002\u8a72\u4f5c\u696d\u9084\u6703\u6e96\u5099\u597d\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u4fbf\u5728\u4e0b\u4e00\u6b65\u4e2d\u5c07\u6a21\u578b\u90e8\u7f72\u5728 Triton \u670d\u52d9\u5668\u4e0a\u3002\u6b64\u904e\u7a0b\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\u3002\n\u8a72 TensorRT-LLM \u5f15\u64ce\u57fa\u65bc Gemma \u7684 Gemma 2B-it\uff08\u6307\u4ee4\u8abf\u512a\uff09PyTorch \u6aa2\u67e5\u9ede\u800c\u69cb\u5efa\uff0c\u4f7f\u7528\u4e86 `bfloat16` \u6fc0\u6d3b\uff0c\u8f38\u5165\u5e8f\u5217\u9577\u5ea6=2048\uff0c\u8f38\u51fa\u5e8f\u5217\u9577\u5ea6=1024\uff0c\u4ee5 L4 GPU \u7232\u76ee\u6a19\u3002\u60a8\u53ef\u4ee5\u5728\u55ae\u500b L4 GPU \u4e0a\u90e8\u7f72\u8a72\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `job-download-gemma-2b.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-2b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 ACTIVATION_DTYPE=bfloat16 && \\\u00a0 \u00a0 TOKENIZER_DIR=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/tokenizer.model && \\\u00a0 \u00a0 ENGINE_PATH=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/ && \\\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository && \\\u00a0 \u00a0 mkdir -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 mkdir -p ${ENGINE_PATH} && \\\u00a0 \u00a0 mkdir -p ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 rm -f /data/${MODEL_NAME}_${VARIATION_NAME}/*.tar.gz && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f | xargs -I '{}' mv '{}' ${ENGINE_PATH} && \\\u00a0 \u00a0 # copying configuration files\u00a0 \u00a0 echo -e \"\\nCreating configuration files\" && \\\u00a0 \u00a0 cp -r /tensorrtllm_backend/all_models/inflight_batcher_llm/* ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 # updating configuration files\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,preprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,postprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/ensemble/config.pbtxt triton_max_batch_size:64 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_batching,max_queue_delay_microseconds:600,batch_scheduler_policy:guaranteed_no_evict,enable_trt_overlap:False && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to ${ENGINE_PATH}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-gemma-2b\u00a0 labels:\u00a0 \u00a0 app: data-loader-gemma-2bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-gemma-2b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/tensorrtllm/2b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: WORLD_SIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f job-download-gemma-2b.yaml\n```\n- \u67e5\u770b\u4f5c\u696d\u7684\u65e5\u8a8c\uff1a```\nkubectl logs -f job/data-loader-gemma-2b\n```\u65e5\u8a8c\u7684\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\n...\nCreating configuration files\n+ echo -e '\\n02-16-2024 04:07:45 Completed building TensortRT-LLM engine at /data/trt_engine/gemma/2b/bfloat16/1-gpu/'\n+ echo -e '\\nCreating configuration files'\n...\n```\n- \u7b49\u5f85\u4f5c\u696d\u5b8c\u6210\uff1a```\nkubectl wait --for=condition=complete --timeout=900s job/data-loader-gemma-2b\n```\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\njob.batch/data-loader-gemma-2b condition met\n```\n- \u9a57\u8b49\u4f5c\u696d\u662f\u5426\u5df2\u6210\u529f\u5b8c\u6210\uff08\u9019\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\uff09\uff1a```\nkubectl get job/data-loader-gemma-2b\n```\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-gemma-2b 1/1   \n##s  #m\n##s\n```\n\u8a72 TensorRT-LLM \u5f15\u64ce\u57fa\u65bc Gemma \u7684 Gemma 7B-it\uff08\u6307\u4ee4\u8abf\u512a\uff09PyTorch \u6aa2\u67e5\u9ede\u800c\u69cb\u5efa\uff0c\u4f7f\u7528\u4e86 `bfloat16` \u6fc0\u6d3b\uff0c\u8f38\u5165\u5e8f\u5217\u9577\u5ea6=1024\uff0c\u8f38\u51fa\u5e8f\u5217\u9577\u5ea6=512\uff0c\u4ee5 L4 GPU \u7232\u76ee\u6a19\u3002\u60a8\u53ef\u4ee5\u5728\u55ae\u500b L4 GPU \u4e0a\u90e8\u7f72\u8a72\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `job-download-gemma-7b.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/job-download-gemma-7b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 ACTIVATION_DTYPE=bfloat16 && \\\u00a0 \u00a0 TOKENIZER_DIR=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/tokenizer.model && \\\u00a0 \u00a0 ENGINE_PATH=/data/trt_engine/${MODEL_NAME}/${VARIATION_NAME}/${ACTIVATION_DTYPE}/${WORLD_SIZE}-gpu/ && \\\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository && \\\u00a0 \u00a0 mkdir -p ${ENGINE_PATH} && \\\u00a0 \u00a0 mkdir -p ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 rm -f /data/${MODEL_NAME}_${VARIATION_NAME}/*.tar.gz && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f && \\\u00a0 \u00a0 find /data/${MODEL_NAME}_${VARIATION_NAME} -type f | xargs -I '{}' mv '{}' ${ENGINE_PATH} && \\\u00a0 \u00a0 # copying configuration files\u00a0 \u00a0 echo -e \"\\nCreating configuration files\" && \\\u00a0 \u00a0 cp -r /tensorrtllm_backend/all_models/inflight_batcher_llm/* ${TRITON_MODEL_REPO} && \\\u00a0 \u00a0 # updating configuration files\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,preprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:sp,triton_max_batch_size:64,postprocessing_instance_count:1 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/ensemble/config.pbtxt triton_max_batch_size:64 && \\\u00a0 \u00a0 python3 /tensorrtllm_backend/tools/fill_template.py -i ${TRITON_MODEL_REPO}/tensorrt_llm/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:inflight_batching,max_queue_delay_microseconds:600,batch_scheduler_policy:guaranteed_no_evict,enable_trt_overlap:False && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to ${ENGINE_PATH}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-gemma-7b\u00a0 labels:\u00a0 \u00a0 app: data-loader-gemma-7bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-gemma-7b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/tensorrtllm/7b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: WORLD_SIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f job-download-gemma-7b.yaml\n```\n- \u67e5\u770b\u4f5c\u696d\u7684\u65e5\u8a8c\uff1a```\nkubectl logs -f job/data-loader-gemma-7b\n```\u65e5\u8a8c\u7684\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\n...\nCreating configuration files\n+ echo -e '\\n02-16-2024 04:07:45 Completed building TensortRT-LLM engine at /data/trt_engine/gemma/7b/bfloat16/1-gpu/'\n+ echo -e '\\nCreating configuration files'\n...\n```\n- \u7b49\u5f85\u4f5c\u696d\u5b8c\u6210\uff1a```\nkubectl wait --for=condition=complete --timeout=900s job/data-loader-gemma-7b\n```\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\njob.batch/data-loader-gemma-7b condition met\n```\n- \u9a57\u8b49\u4f5c\u696d\u662f\u5426\u5df2\u6210\u529f\u5b8c\u6210\uff08\u9019\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\uff09\uff1a```\nkubectl get job/data-loader-gemma-7b\n```\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-gemma-7b 1/1   \n##s  #m\n##s\n```\n\u78ba\u4fdd\u4f5c\u696d\u5df2\u6210\u529f\u5b8c\u6210\uff0c\u7136\u5f8c\u518d\u7e7c\u7e8c\u4e0b\u4e00\u90e8\u5206\u3002## \u90e8\u7f72 Triton\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u90e8\u7f72\u4e00\u500b\u5bb9\u5668\uff0c\u5b83\u5c07 Triton \u8207 TensorRT-LLM \u5f8c\u7aef\u7d50\u5408\u4f7f\u7528\uff0c\u4ee5\u61c9\u7528\u60a8\u8981\u4f7f\u7528\u7684 Gemma \u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `deploy-triton-server.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/trtllm/deploy-triton-server.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: launch-tritonserverdata:\u00a0 entrypoint.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 # Launch Triton Inference server\u00a0 \u00a0 WORLD_SIZE=1\u00a0 \u00a0 TRITON_MODEL_REPO=/data/triton/model_repository\u00a0 \u00a0 python3 /tensorrtllm_backend/scripts/launch_triton_server.py \\\u00a0 \u00a0 \u00a0 --world_size ${WORLD_SIZE} \\\u00a0 \u00a0 \u00a0 --model_repo ${TRITON_MODEL_REPO}\u00a0 \u00a0 tail -f /dev/null---apiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: triton-gemma-deployment\u00a0 labels:\u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 version: v1spec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 version: v1\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: triton\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 \u00a0 \u00a0 version: v1\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/tritonserver:2.42.0\u00a0 \u00a0 \u00a0 \u00a0 imagePullPolicy: IfNotPresent\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/entrypoint.sh\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/data\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: data\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: http\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8001\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: grpc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8002\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: metrics\u00a0 \u00a0 \u00a0 \u00a0 livenessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failureThreshold: 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 600\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /v2/health/live\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: http\u00a0 \u00a0 \u00a0 \u00a0 readinessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failureThreshold: 60\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 600\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /v2/health/ready\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: http\u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 runAsUser: 1000\u00a0 \u00a0 \u00a0 \u00a0 fsGroup: 1000\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: launch-tritonserver\u00a0 \u00a0 \u00a0 - name: data\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: model-data\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: \"key\"\u00a0 \u00a0 \u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 \u00a0 \u00a0 effect: \"NoSchedule\"---apiVersion: v1kind: Servicemetadata:\u00a0 name: triton-server\u00a0 labels:\u00a0 \u00a0 app: gemma-serverspec:\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - port: 8000\u00a0 \u00a0 \u00a0 targetPort: http\u00a0 \u00a0 \u00a0 name: http-inference-server\u00a0 \u00a0 - port: 8001\u00a0 \u00a0 \u00a0 targetPort: grpc\u00a0 \u00a0 \u00a0 name: grpc-inference-server\u00a0 \u00a0 - port: 8002\u00a0 \u00a0 \u00a0 targetPort: metrics\u00a0 \u00a0 \u00a0 name: http-metrics\u00a0 selector:\u00a0 \u00a0 app: gemma-server\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f deploy-triton-server.yaml\n```\n- \u7b49\u5f85\u90e8\u7f72\u6210\u7232\u53ef\u7528\u72c0\u614b\uff1a```\nkubectl wait --for=condition=Available --timeout=900s deployment/triton-gemma-deployment\n```\n- \u67e5\u770b\u6e05\u55ae\u7684\u65e5\u8a8c\uff1a```\nkubectl logs -f -l app=gemma-server\n```\u90e8\u7f72\u8cc7\u6e90\u6703\u5553\u52d5 Triton \u670d\u52d9\u5668\u4e26\u52a0\u8f09\u6a21\u578b\u6578\u64da\u3002\u6b64\u904e\u7a0b\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\uff08\u6700\u591a 20 \u5206\u9418\u6216\u66f4\u9577\u6642\u9593\uff09\u3002\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a```\nI0216 03:24:57.387420 29 server.cc:676]\n+------------------+---------+--------+\n| Model   | Version | Status |\n+------------------+---------+--------+\n| ensemble   | 1  | READY |\n| postprocessing | 1  | READY |\n| preprocessing | 1  | READY |\n| tensorrt_llm  | 1  | READY |\n| tensorrt_llm_bls | 1  | READY |\n+------------------+---------+--------+\n....\n....\n....\nI0216 03:24:57.425104 29 grpc_server.cc:2519] Started GRPCInferenceService at 0.0.0.0:8001\nI0216 03:24:57.425418 29 http_server.cc:4623] Started HTTPService at 0.0.0.0:8000\nI0216 03:24:57.466646 29 http_server.cc:315] Started Metrics Service at 0.0.0.0:8002\n```\n## \u61c9\u7528\u6a21\u578b\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u8207\u6a21\u578b\u4e92\u52d5\u3002\n### \u8a2d\u7f6e\u7aef\u53e3\u8f49\u767c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u8a2d\u7f6e\u5230\u6a21\u578b\u7684\u7aef\u53e3\u8f49\u767c\uff1a\n```\nkubectl port-forward service/triton-server 8000:8000\n```\n\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a\n```\nForwarding from 127.0.0.1:8000 -> 8000\nForwarding from [::1]:8000 -> 8000\nHandling connection for 8000\n```\n### \u4f7f\u7528 curl \u8207\u6a21\u578b\u4e92\u52d5\u672c\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u57f7\u884c\u57fa\u672c\u5192\u7159\u6e2c\u8a66\u4ee5\u9a57\u8b49\u6240\u90e8\u7f72\u7684\u6307\u4ee4\u8abf\u512a\u6a21\u578b\u3002\u7232\u7c21\u55ae\u8d77\u898b\uff0c\u672c\u90e8\u5206\u50c5\u4ecb\u7d39\u4f7f\u7528 2B \u6307\u4ee4\u8abf\u512a\u6a21\u578b\u7684\u6e2c\u8a66\u65b9\u6cd5\u3002\n\u5728\u65b0\u7684\u7d42\u7aef\u6703\u8a71\u4e2d\uff0c\u4f7f\u7528 `curl` \u8207\u6a21\u578b\u804a\u5929\uff1a\n```\nUSER_PROMPT=\"I'm new to coding. If you could only recommend one programming language to start with, what would it be and why?\"curl -X POST localhost:8000/v2/models/ensemble/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"text_input\": \"<start_of_turn>user\\n${USER_PROMPT}<end_of_turn>\\n\",\u00a0 \u00a0 \"temperature\": 0.9,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\n\u4ee5\u4e0b\u8f38\u51fa\u986f\u793a\u4e86\u6a21\u578b\u97ff\u61c9\u7684\u793a\u4f8b\uff1a\n```\n{\n \"context_logits\": 0,\n \"cum_log_probs\": 0,\n \"generation_logits\": 0,\n \"model_name\": \"ensemble\",\n \"model_version\": \"1\",\n \"output_log_probs\": [0.0,0.0,...],\n \"sequence_end\": false,\n \"sequence_id\": 0,\n \"sequence_start\": false,\n \"text_output\":\"Python.\\n\\nPython is an excellent choice for beginners due to its simplicity, readability, and extensive documentation. Its syntax is close to natural language, making it easier for beginners to understand and write code. Python also has a vast collection of libraries and tools that make it versatile for various projects. Additionally, Python's dynamic nature allows for easier learning and experimentation, making it a perfect choice for newcomers to get started.Here are some specific reasons why Python is a good choice for beginners:\\n\\n- Simple and Easy to Read: Python's syntax is designed to be close to natural language, making it easier for\"\n}\n```\n **\u6210\u529f** \uff1a\u60a8\u5df2\u6210\u529f\u5730\u901a\u904e Triton \u548c TensorRT-LLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528\u4e86 Gemma\u3002## \u554f\u984c\u6392\u67e5\n- \u5982\u679c\u60a8\u6536\u5230`Empty reply from server`\u6d88\u606f\uff0c\u5247\u5bb9\u5668\u53ef\u80fd\u5c1a\u672a\u5b8c\u6210\u6a21\u578b\u6578\u64da\u4e0b\u8f09\u3002\u518d\u6b21 [\u6aa2\u67e5 Pod \u7684\u65e5\u8a8c](#deploy-triton) \u4e2d\u662f\u5426\u5305\u542b`Connected`\u6d88\u606f\uff0c\u8a72\u6d88\u606f\u8868\u660e\u6a21\u578b\u5df2\u6e96\u5099\u597d\u9032\u884c\u61c9\u7528\u3002\n- \u5982\u679c\u60a8\u770b\u5230`Connection refused`\uff0c\u8acb\u9a57\u8b49\u60a8\u7684 [\u7aef\u53e3\u8f49\u767c\u5df2\u5553\u7528](#setup-port-forwarding) \u3002\n## \u6e05\u7406\u7232\u907f\u514d\u56e0\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u8cc7\u6e90\u5c0e\u81f4\u60a8\u7684 Google Cloud \u8cec\u865f\u7522\u751f\u8cbb\u7528\uff0c\u8acb\u522a\u9664\u5305\u542b\u9019\u4e9b\u8cc7\u6e90\u7684\u9805\u76ee\uff0c\u6216\u8005\u4fdd\u7559\u9805\u76ee\u4f46\u522a\u9664\u5404\u500b\u8cc7\u6e90\u3002\n### \u522a\u9664\u5df2\u90e8\u7f72\u7684\u8cc7\u6e90\u7232\u907f\u514d\u56e0\u60a8\u5728\u672c\u6307\u5357\u4e2d\u5275\u5efa\u7684\u8cc7\u6e90\u5c0e\u81f4\u60a8\u7684 Google Cloud \u8cec\u865f\u7522\u751f\u8cbb\u7528\uff0c\u8acb\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a\n```\ngcloud container clusters delete ${CLUSTER_NAME} \\\u00a0 --region=${REGION}\n```## \u5f8c\u7e8c\u6b65\u9a5f\n- \u8a73\u7d30\u77ad\u89e3 [GKE \u4e2d\u7684 GPU](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus?hl=zh-cn) \u3002\n- \u77ad\u89e3\u5982\u4f55 [\u5728 Autopilot \u4e2d\u90e8\u7f72 GPU \u5de5\u4f5c\u8ca0\u8f09](https://cloud.google.com/kubernetes-engine/docs/how-to/autopilot-gpus?hl=zh-cn) \u3002\n- \u77ad\u89e3\u5982\u4f55 [\u5728 Standard \u4e2d\u90e8\u7f72 GPU \u5de5\u4f5c\u8ca0\u8f09](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus?hl=zh-cn) \u3002\n- \u700f\u89bd TensorRT-LLM [GitHub \u4ee3\u78bc\u5eab](https://github.com/NVIDIA/TensorRT-LLM) \u548c [\u6587\u6a94](https://nvidia.github.io/TensorRT-LLM/) \u3002\n- \u63a2\u7d22 [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden?hl=zh-cn) \u3002\n- \u77ad\u89e3\u5982\u4f55\u4f7f\u7528 [GKE \u5e73\u81fa\u7de8\u6392\u529f\u80fd](https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra?hl=zh-cn) \u904b\u884c\u7d93\u904e\u512a\u5316\u7684 AI/\u6a5f\u5668\u5b78\u7fd2\u5de5\u4f5c\u8ca0\u8f09\u3002", "guide": "Google Kubernetes Engine (GKE)"}