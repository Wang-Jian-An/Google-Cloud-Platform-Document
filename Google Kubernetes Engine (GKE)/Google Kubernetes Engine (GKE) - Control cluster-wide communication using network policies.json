{"title": "Google Kubernetes Engine (GKE) - Control cluster-wide communication using network policies", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/configure-cilium-network-policy", "abstract": "# Google Kubernetes Engine (GKE) - Control cluster-wide communication using network policies\nThis page explains how to configure cluster-wide network policies for Google Kubernetes Engine (GKE).\nNetwork policies and FQDN network policies help you define communication traffic rules between Pods. Network policies control how Pods communicate with each other within their applications and with external endpoints.\nAs a cluster administrator, you can configure Cilium cluster-wide network policies (CCNP), which overcome the network policy limitations for managing cluster-wide administrative traffic. Cilium cluster-wide network policies enforce strict network rules for all workloads across the entire cluster, across namespaces, overriding any application-specific rules.\nCilium cluster-wide network policy for GKE is a cluster-scoped CustomResourceDefinition (CRD) that specifies policies enforced by GKE. By enabling Cilium cluster-wide network policy in GKE, you can centrally manage network rules for your entire cluster. You can control basic Layer 3 (IP-level) and Layer 4 (port-level) access for traffic entering and leaving the cluster.\n", "content": "## Benefits\nWith Cilium cluster-wide network policy you can:\n- **Enforce centralized security** : With CCNP, you can define network access rules that apply to your entire network. These CCNP rules act as a top-level security layer, overriding any potentially conflicting policies at the namespace level.\n- **Protect multi-tenancy** : If your cluster hosts multiple teams or tenants, you can secure isolation within a shared cluster by implementing CCNP rules, which focus on network traffic control. You can enforce network-level separation by assigning namespaces or groups of namespaces to specific teams.\n- **Define flexible default policies** : With CCNP, you can define default network rules for the entire cluster. You can customize these rules when necessary without compromising your overall cluster security.\nTo implement CCNP, enable GKE Dataplane V2 on your cluster. Ensure that the CCNP CRD is enabled, then create policies that define network access rules for your cluster.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Requirements\nCilium cluster-wide network policies have the following requirements:\n- Google Cloud CLI version 465.0.0 or later.\n- You must have a GKE cluster running one of the following versions:- 1.28.6-gke.1095000 or later\n- 1.29.1-gke.1016000 or later\n- Your cluster must [use GKE Dataplane V2](/kubernetes-engine/docs/how-to/dataplane-v2) .\n- You must enable Cilium cluster-wide network policy CRD.\n### Limitations\nCilium cluster-wide network policies have the following limitations:\n- Layer 7 policies are not supported.\n- Node selectors are not supported.\n- The maximum number of`CiliumClusterwideNetworkPolicy`per cluster is 1000.## Enable Cilium cluster-wide network policy in a new cluster\nYou can enable Cilium cluster-wide network policy in a new cluster by using the Google Cloud CLI or the Google Kubernetes Engine API.\nTo enable Cilium cluster-wide network policy in a new cluster, create a new cluster with `--enable-cilium-clusterwide-network-policy` flag.\n **Autopilot** \n```\ngcloud container clusters create-auto CLUSTER_NAME \\\u00a0 \u00a0 --location COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-cilium-clusterwide-network-policy\n```\nReplace the following:- ``with the name of your cluster.\n- ``with the location of your cluster.\n **Standard** \n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-cilium-clusterwide-network-policy \\\u00a0 \u00a0 --enable-dataplane-v2\n```\nReplace the following: * `` with the name of your cluster. * `` with the location of your cluster.\nTo enable the Cilium cluster-wide network policy, you must specify the following options while creating a new cluster:\n [datapathProvider field](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#datapathprovider) in the [networkConfig object](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#networkconfig) .\n```\n{\u00a0 \"cluster\": {\u00a0 \u00a0 ...\u00a0 \u00a0 \"networkConfig\": {\u00a0 \u00a0 \u00a0 \"datapathProvider\": \"ADVANCED_DATAPATH\",\u00a0 \u00a0 \u00a0 \"enableCiliumClusterwideNetworkPolicy\": true\u00a0 \u00a0 }\u00a0 }}\n```\nVerify that `ciliumclusterwidenetworkpolicies.cilium.io` is present in the output of the following command:\n```\nkubectl get crds ciliumclusterwidenetworkpolicies.cilium.io\n```\nThe output should be similar to the following:\n```\nciliumclusterwidenetworkpolicies.cilium.io  2023-09-19T16:54:48Z\n```\n## Enable Cilium cluster-wide network policy in an existing cluster\nYou can enable Cilium cluster-wide network policy in an existing cluster by using the Google Cloud CLI or the Google Kubernetes Engine API.\n- Confirm the cluster has [GKE Dataplane V2 enabled](/kubernetes-engine/docs/how-to/dataplane-v2#create-cluster) .```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --location COMPUTE_LOCATION \\\u00a0 \u00a0 --format=\"value(networkConfig.datapathProvider)\" \\\n```Replace the following:- ``with the name of your cluster.\n- ``with the location of your cluster.\n- Update the cluster using the `--enable-cilium-clusterwide-network-policy` flag.```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --location COMPUTE_LOCATION \\\u00a0 \u00a0 --enable-cilium-clusterwide-network-policy\n```\n- Restart the anetd DaemonSet.```\nkubectl rollout restart ds -n kube-system anetd && \\\u00a0 \u00a0 kubectl rollout status ds -n kube-system anetd\n``` **Note:** Restarting the DaemonSet will cause temporary downtime for your cluster. Downtime can vary depending on the size and complexity of your cluster. We recommend that you plan your deployments accordingly.\nConfirm that the cluster is enabled for GKE Dataplane V2:\n```\n{\u00a0 \"update\": {\u00a0 \u00a0 \"desiredEnableCiliumClusterwideNetworkPolicy\": true\u00a0 },\u00a0 \"name\": \"cluster\"}To update an existing cluster, run the following update cluster command:{\u00a0 \"update\": {\u00a0 \u00a0 \"desiredEnableCiliumClusterwideNetworkPolicy\": true\u00a0 }\u00a0 \"name\": \"cluster\"}\n```\nVerify that `ciliumclusterwidenetworkpolicies.cilium.io` is present in the output of the following command:\n```\nkubectl get crds ciliumclusterwidenetworkpolicies.cilium.io\n```\nThe output should be similar to the following:\n```\nciliumclusterwidenetworkpolicies.cilium.io  2023-09-19T16:54:48Z\n```\n## Using Cilium cluster-wide network policy\nThis section lists examples for configuring Cilium cluster-wide network policy.\n### Example 1: Control ingress traffic to a workload\nThe following example enables all endpoints with the label `role=backend` to accept ingress connections on port 80 from endpoints with the label `role=frontend` . Endpoints with the label `role=backend` will reject all ingress connection that are not allowed by this policy.\n- Save the following manifest as `l4-rule-ingress.yaml` :```\napiVersion: \"cilium.io/v2\"kind: CiliumClusterwideNetworkPolicymetadata:\u00a0 name: \"l4-rule-ingress\"spec:\u00a0 endpointSelector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 role: backend\u00a0 ingress:\u00a0 \u00a0 - fromEndpoints:\u00a0 \u00a0 \u00a0 \u00a0 - matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role: frontend\u00a0 \u00a0 \u00a0 toPorts:\u00a0 \u00a0 \u00a0 \u00a0 - ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - port: \"80\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\n```\n- Apply the manifest:```\nkubectl apply -f l4-rule-ingress.yaml\n```\n### Example 2: Restrict egress traffic from a workload on a given port\nThe following rule limits all endpoints with the label `app=myService` to only be able to emit packets using TCP on port 80, to any Layer 3 destination:\n- Save the following manifest as `l4-rule-egress.yaml` :```\napiVersion: \"cilium.io/v2\"kind: CiliumClusterwideNetworkPolicymetadata:\u00a0 name: \"l4-rule-egress\"spec:\u00a0 endpointSelector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: myService\u00a0 egress:\u00a0 \u00a0 - toPorts:\u00a0 \u00a0 \u00a0 \u00a0 - ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - port: \"80\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\n```\n- Apply the manifest:```\nkubectl apply -f l4-rule-egress.yaml\n```\n### Example 3: Restrict egress traffic from a workload on a given port and CIDR\nThe following example limits all endpoints with the label `role=crawler` to only be able to send packets on port 80, protocols TCP, to a destination CIDR `192.10.2.0/24` .\n- Save the following manifest as `cidr-l4-rule.yaml` :```\n\u00a0apiVersion: \"cilium.io/v2\"\u00a0kind: CiliumClusterwideNetworkPolicy\u00a0metadata:\u00a0 \u00a0name: \"cidr-l4-rule\"\u00a0spec:\u00a0 \u00a0endpointSelector:\u00a0 \u00a0 \u00a0matchLabels:\u00a0 \u00a0 \u00a0 \u00a0role: crawler\u00a0 \u00a0egress:\u00a0 \u00a0 \u00a0- toCIDR:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- 192.0.2.0/24\u00a0 \u00a0 \u00a0 \u00a0toPorts:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- port: \"80\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0protocol: TCP\n```\n- Apply the manifest:```\nkubectl apply -f cidr-l4-rule.yaml\n```## Monitoring and troubleshooting network traffic\nYou can monitor and troubleshoot the network traffic affected by cilium cluster-wide network policies by [Network policy logging](/kubernetes-engine/docs/how-to/network-policy-logging) and [GKE Dataplane V2 observability](/kubernetes-engine/docs/concepts/about-dpv2-observability) .\n### Attempt to use Layer 7 policies or node selectors\n**Symptom**\nIf you are using GKE with GKE Dataplane V2 and you attempt to define CCNP policies that include Layer 7 rules (for example: HTTP filtering) and node selectors, you might see an error message similar to the following:\n**Error**\n```\nError from server (GKE Warden constraints violations): error when creating\n\"ccnp.yaml\": admission webhook\n\"warden-validating.common-webhooks.networking.gke.io\" denied the request: GKE\nWarden rejected the request because it violates one or more constraints.\nViolations details: {\"[denied by gke-cilium-network-policy-limitation]\":[\"L7\nrules are not allowed in CiliumClusterwideNetworkPolicy\"]} Requested by user:\n'user@example.com', groups: 'system:authenticated'.\n```\n**Potential cause**\nGKE has specific limitations on CCNPs. Layer 7 policies, which allow filtering based on application-level data (like HTTP headers), and node selectors are not supported within GKE's Cilium integration.\n**Resolution**\nIf you need advanced Layer 7 filtering capabilities in your GKE cluster, consider using Anthos Service Mesh. This provides more granular application-level traffic control.\n### Cilium cluster-wide network policy not enabled\n**Symptom**\nWhen you try to configure Cilium cluster-wide network policies (CCNP) in a cluster where where the feature has not been explicitly enabled, you won't be able to configure it and might see an error message similar to the following:\n**Error**\n```\nerror: resource mapping not found for name: \"l4-rule\" namespace: \"\" from\n\"ccnp.yaml\": no matches for kind \"CiliumClusterwideNetworkPolicy\" in version\n\"cilium.io/v2\" ensure CRDs are installed first\n```\n**Potential cause**\nCilium cluster-wide network policies rely on a Custom Resource Definition (CRD). The error message indicates that the CRD is missing in the cluster.\n**Resolution**\nEnable Cilium cluster-wide network policy CRD before using CCNPs.\n## What's next\n- Read [Kubernetes documentation about network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) \n- Read [CiliumClusterwideNetworkPolicy](https://docs.cilium.io/en/latest/network/kubernetes/policy/#ciliumclusterwidenetworkpolicy) \n- [Configure network policies for applications](/kubernetes-engine/docs/tutorials/network-policy)", "guide": "Google Kubernetes Engine (GKE)"}