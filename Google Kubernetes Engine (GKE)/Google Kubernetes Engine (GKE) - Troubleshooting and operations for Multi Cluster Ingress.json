{"title": "Google Kubernetes Engine (GKE) - Troubleshooting and operations for Multi Cluster Ingress", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/troubleshooting-and-ops", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshooting and operations for Multi Cluster Ingress\nThe GKE Enterprise Ingress controller manages Compute Engine resources. `MultiClusterIngress` and `MultiClusterService` resources map to different Compute Engine resources, so understanding the relationship between these resources helps you troubleshoot. For example, examine the following `MultiClusterIngress` resource:\n```\napiVersion: extensions/v1beta1kind: MultiClusterIngressmetadata:\u00a0 name: foo-ingressspec:\u00a0 rules:\u00a0 - host: store.foo.com\u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: store-foo\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 80\u00a0 - host: search.foo.com\u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: search-foo\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 80\n```\n#", "content": "## Compute Engine to Multi Cluster Ingress resource mappings\nThe table below shows the mapping of fleet resources to resources created in the Kubernetes clusters and Google Cloud:\n| Kubernetes resource | Google Cloud resource | Description               |\n|:----------------------|:------------------------|:-----------------------------------------------------------------------|\n| MultiClusterIngress | Forwarding rule   | HTTP(S) load balancer VIP.            |\n| MultiClusterIngress | Target proxy   | HTTP/S terminations settings taken from annotations and the TLS block. |\n| MultiClusterIngress | URL map     | Virtual host path mapping from the rules section.      |\n| MultiClusterService | Kubernetes Service  | Derived resource from template.          |\n| MultiClusterService | Backend service   | A backend service is created for each (Service, ServicePort) pair.  |\n| MultiClusterService | Network endpoint groups | Set of backend Pods participating in the Service.      |\n## Inspecting Compute Engine load balancer resources\nAfter creating a load balancer, the Multi Cluster Ingress status will contain the names of every Compute Engine resource that was created to construct the load balancer. For example:\n```\nName: \u00a0 \u00a0 \u00a0 \u00a0 shopping-serviceNamespace: \u00a0 \u00a0prodLabels: \u00a0 \u00a0 \u00a0 <none>Annotations: \u00a0<none>API Version: \u00a0networking.gke.io/v1beta1Kind: \u00a0 \u00a0 \u00a0 \u00a0 MultiClusterIngressMetadata:\u00a0 Creation Timestamp: \u00a02019-07-16T17:23:14Z\u00a0 Finalizers:\u00a0 \u00a0 mci.finalizer.networking.gke.ioSpec:\u00a0 Template:\u00a0 \u00a0 Spec:\u00a0 \u00a0 \u00a0 Backend:\u00a0 \u00a0 \u00a0 \u00a0 Service Name: \u00a0shopping-service\u00a0 \u00a0 \u00a0 \u00a0 Service Port: \u00a080Status:\u00a0 VIP: \u00a034.102.212.68\u00a0 CloudResources:\u00a0 \u00a0 Firewalls: \"mci-l7\"\u00a0 \u00a0 ForwardingRules: \"mci-abcdef-myforwardingrule\"\u00a0 \u00a0 TargetProxies: \"mci-abcdef-mytargetproxy\"\u00a0 \u00a0 UrlMap: \"mci-abcdef-myurlmap\"\u00a0 \u00a0 HealthChecks: \"mci-abcdef-80-myhealthcheck\"\u00a0 \u00a0 BackendServices: \"mci-abcdef-80-mybackendservice\"\u00a0 \u00a0 NetworkEndpointGroups: \"k8s1-neg1\", \"k8s1-neg2\", \"k8s1-neg3\"\n```\n## VIP not created\nIf you do not see a VIP, then an error may have occurred during its creation. To see if an error did occur, run the following command:\n```\nkubectl describe mci shopping-service\n```\nThe output may look similar to:\n```\nName:   shopping-service\nNamespace: prod\nLabels:  <none>\nAnnotations: <none>\nAPI Version: networking.gke.io/v1beta1\nKind:   MultiClusterIngress\nMetadata:\n Creation Timestamp: 2019-07-16T17:23:14Z\n Finalizers:\n mci.finalizer.networking.gke.io\nSpec:\n Template:\n Spec:\n  Backend:\n  Service Name: shopping-service\n  Service Port: 80\nStatus:\n VIP: 34.102.212.68\nEvents:\n Type  Reason Age From        Message\n ----  ------ ---- ----        ------ Warning SYNC 29s multi-cluster-ingress-controller error translating MCI prod/shopping-service: exceeded 4 retries with final error: error translating MCI prod/shopping-service: multiclusterservice prod/shopping-service does not exist\n```\nIn this example, the error was that the user did not create a `MultiClusterService` resource that was referenced by a `MultiClusterIngress` .\n## 502 response\nIf your load balancer acquired a VIP but is consistently serving a 502 response, the load balancer health checks may be failing. Health checks could fail for two reasons:\n- Application Pods are not healthy (see [Cloud console](#console-debugging) debugging for example).\n- A misconfigured firewall is blocking Google health checkers from performing health checks.\nIn the case of #1, make sure that your application is in fact serving a 200 response on the \"/\" path.\nIn the case of #2, make sure that a firewall named \"mci-default-l7\" exists in your VPC. The Ingress controller creates the firewall in your VPC to ensure Google health checkers can reach your backends. If the firewall does not exist, make sure there is no external automation that deletes this firewall upon its creation.\n## Traffic not added to or removed from cluster\nWhen adding a new Membership, traffic should reach the backends in the underlying cluster when applicable. Similarly, if a Membership is removed, no traffic should reach the backends in the underlying cluster. If you are not observing this behavior, check for errors on the `MultiClusterIngress` and `MultiClusterService` resource.\nCommon cases in which this error would occur include adding a new Membership on a GKE cluster that is not in VPC-native mode or adding a new Membership but not deploying an application in the GKE cluster.\n- Describe the `MultiClusterService` :```\nkubectl describe mcs zone-svc\n```\n- Describe the `MultiClusterIngress` :```\nkubectl describe mci zone-mci\n```## Config cluster migration\nTo understand more about the use cases for migration, see the [Config cluster design concept](/kubernetes-engine/docs/concepts/multi-cluster-ingress#config_cluster_design) .\nConfig cluster migration can be a disruptive operation if not handled correctly. Follow these guidelines when performing a config cluster migration:\n- Make sure to use the [static-ip annotation](/kubernetes-engine/docs/how-to/ingress-for-anthos#static) on your`MultiClusterIngress`resources. Failing to do so will result in disrupted traffic while migrating. Ephemeral IPs will be recreated when migrating config clusters.\n- The`MultiClusterIngress`and`MultiClusterService`resources must be deployed identically to the existing and new config cluster. Differences between them will result in the reconciliation of`MultiClusterService`and`MultiClusterIngress`resources that are different in the new config cluster.\n- Only a single config cluster is active at any time. Until the config cluster is changed, the`MultiClusterIngress`and`MultiClusterService`resources in the new config cluster will not impact load balancer resources.\nTo migrate the config cluster, run the following command:\n```\n\u00a0 gcloud container fleet ingress update \\\u00a0 \u00a0 --config-membership=projects/project_id/locations/global/memberships/new_config_cluster\n```\nVerify the command worked by ensuring there are no visible errors in the Feature state:\n```\n\u00a0 gcloud container fleet ingress describe\n```\n## Console debugging\nIn most cases, checking the exact state of the load balancer is helpful when debugging an issue. You can find the load balancer by going to [Load balancing](https://console.cloud.google.com/net-services/loadbalancing/) in the Google Cloud console.\n## Error/Warning codes\nMulti Cluster Ingress emits error and warning codes on `MultiClusterIngress` and `MultiClusterService` resources as well as the gcloud `multiclusteringress` Description field for known issues. These messages have documented error and warning codes to make it easier to understand what it means when something is not operating as expected. Each code consists of an error ID in the format `AVMBR123` where `123` is a unique number that corresponds to an error or warning and suggestions on how to solve it.\n### AVMBR101: Annotation [NAME] not recognized\nThis error displays when an annotation is specified on a `MultiClusterIngress` or `MultiClusterService` manifest that is not recognized. There are a couple reasons why the annotation might not be recognized:\n- The annotation is not supported in Multi Cluster Ingress. This may be expected if annotating resources that are not expected to be used by the GKE Enterprise Ingress controller.\n- The annotation is supported, but is misspelled and thus not recognized.\nIn both cases, please refer to documentation to understand the [supported annotations](/kubernetes-engine/docs/how-to/multi-cluster-ingress#annotations) and how they are specified.\n### AVMBR102: [RESOURCE_NAME] not found\nThis error displays when a supplementary resource is specified in a `MultiClusterIngress` but cannot be found in the Config Membership. For example, this error is thrown when a `MultiClusterIngress` refers to a `MultiClusterService` that cannot be found or a `MultiClusterService` refers to a BackendConfig that cannot be found. There are a couple reasons why a resource could not be found:\n- It is not in the proper namespace. Ensure that resources which reference each other are all in the same namespace.\n- The resource name is misspelled.\n- The resource truly does not exist with the proper namespace + name. In this case, please create it.\n### AVMBR103: [CLUSTER_SELECTOR] is invalid\nThis error displays when a cluster selector specified on a `MultiClusterService` is invalid. There are a couple reasons why this selector could be invalid:\n- The provided string contains a typo.\n- The provided string refers to a cluster membership that no longer exists in the fleet.\n### AVMBR104: Cannot find NEGs for Service Port [SERVICE_PORT]\nThis error is thrown when the NetworkEndpointGroup's (NEGs) for a given `MultiClusterService` and service port pair cannot be found. NEGs are the resources which contain the Pod endpoints in each of your backend clusters. The main reason why the NEGs might not exist is because there was an error creating or updating the Derived Services in your backend clusters. Check the Events on your `MultiClusterService` resource for more information.\n### AVMBR105: Missing GKE Enterprise license.\nThis error displays under Feature state, and indicates that the GKE Enterprise API (anthos.googleapis.com) is not enabled.\n### AVMBR106: Derived service is invalid: [REASON].\nThis error displays under the events of the `MultiClusterService` resource. One common reason for this error is that the Service resource derived from `MultiClusterService` has an invalid spec.\nFor example, this `MultiClusterService` does not have any `ServicePort` defined in its spec.\n```\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: zone-mcs\u00a0 namespace: whereamispec:\u00a0 clusters:\u00a0 - link: \"us-central1-a/gke-us\"\u00a0 - link: \"europe-west1-c/gke-eu\"\n```\n### AVMBR107: Missing GKE cluster resource link in Membership.\nThis error displays under Feature state and occurs because there is no GKE cluster underlying the Membership resource. You can verify this by running the following command:\n```\ngcloud container fleet memberships describe membership-name\n```\nand ensuring that there is no GKE cluster resource link under the endpoint field.\n### AVMBR108: GKE cluster [NAME] not found.\nThis error displays under Feature state and is thrown if the underlying GKE cluster for the Membership does not exist.\n### AVMBR109: [NAME] is not a VPC-native GKE cluster.\nThis error displays under Feature state. This error is thrown if the specified GKE cluster is a route-based cluster. The Multi Cluster Ingress controller creates a container-native load balancer using NEGs. Clusters must be VPC-native to use a container-native load balancer.\nFor more information, see [Creating a VPC-native cluster](/kubernetes-engine/docs/how-to/alias-ips) .\n### AVMBR110: [IAM_PERMISSION] permission missing for GKE cluster [NAME].\nThis error displays under Feature state. There are a couple reasons for this error:\n- The underlying GKE cluster for the Membership is located in a different project from the Membership itself.\n- The specified IAM permission was removed from the`MultiClusterIngress`service agent.\n### AVMBR111: Failed to get Config Membership: [REASON].\nThis error displays under Feature state. The main reason this error occurs is because the Config Membership was deleted while the Feature is enabled.\nYou should never need to delete the Config Membership. If you would like to change it, follow the [config cluster migration steps](#config_cluster_migration) .\n### AVMBR112: HTTPLoadBalancing Addon is disabled in GKE Cluster [NAME].\nThis error displays under Feature state and occurs when the `HTTPLoadBalancing` addon is disabled in a GKE cluster. You can update your GKE cluster to enable the `HTTPLoadBalancing` addon:\n```\ngcloud container clusters update name --update-addons=HttpLoadBalancing=ENABLED\n```\n### AVMBR113: This resource is orphaned.\nIn some cases, the usefulness of a resource depends on it being referenced by another resource. This error is thrown when a Kubernetes resource is created but is not referenced by another resource. For example, you will see this error if you create a `BackendConfig` resource that is not being referenced by a `MultiClusterService` .", "guide": "Google Kubernetes Engine (GKE)"}