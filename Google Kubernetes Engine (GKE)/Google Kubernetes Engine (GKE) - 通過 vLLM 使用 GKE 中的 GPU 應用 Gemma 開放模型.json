{"title": "Google Kubernetes Engine (GKE) - \u901a\u904e vLLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma \u958b\u653e\u6a21\u578b", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm?hl=zh-cn", "abstract": "# Google Kubernetes Engine (GKE) - \u901a\u904e vLLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma \u958b\u653e\u6a21\u578b\n\u672c\u6559\u7a0b\u4ecb\u7d39\u5982\u4f55\u901a\u904e [vLLM](https://github.com/vllm-project/vllm) \u670d\u52d9\u6846\u67b6\uff0c\u4f7f\u7528 Google Kubernetes Engine (GKE) \u4e2d\u7684\u5716\u5f62\u8655\u7406\u5668 (GPU) \u4f86\u61c9\u7528 [Gemma](https://ai.google.dev/gemma/docs/?hl=zh-cn) \u5927\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c07\u5f9e Hugging Face \u4e0b\u8f09 2B \u548c 7B \u53c3\u6578\u6307\u4ee4\u8abf\u512a\u548c\u9810\u8a13\u7df4 Gemma \u6a21\u578b\uff0c\u4e26\u4f7f\u7528\u904b\u884c vLLM \u7684\u5bb9\u5668\u5c07\u5b83\u5011\u90e8\u7f72\u5230 GKE [Autopilot](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn#why-autopilot) \u6216 [Standard](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn#why-standard) \u96c6\u7fa3\u3002\n\u5982\u679c\u60a8\u5728\u90e8\u7f72\u548c\u61c9\u7528 AI/\u6a5f\u5668\u5b78\u7fd2\u5de5\u4f5c\u8ca0\u8f09\u6642\u9700\u8981\u5229\u7528\u4ee3\u7ba1\u5f0f Kubernetes \u7684\u7cbe\u7d30\u63a7\u5236\u3001\u53ef\u4f38\u7e2e\u6027\u3001\u5f48\u6027\u3001\u53ef\u79fb\u690d\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u90a3\u9ebc\u672c\u6307\u5357\u662f\u4e00\u500b\u5f88\u597d\u7684\u8d77\u9ede\u3002\u5982\u679c\u60a8\u9700\u8981\u7d71\u4e00\u7684\u4ee3\u7ba1\u5f0f AI \u5e73\u81fa\u4f86\u7d93\u6fdf\u9ad8\u6548\u5730\u5feb\u901f\u69cb\u5efa\u548c\u61c9\u7528\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\uff0c\u6211\u5011\u5efa\u8b70\u60a8\u8a66\u7528\u6211\u5011\u7684 [Vertex AI](https://cloud.google.com/vertex-ai?hl=zh-cn) \u90e8\u7f72\u89e3\u6c7a\u65b9\u6848\u3002", "content": "## \u80cc\u666f\u60a8\u53ef\u4ee5\u901a\u904e vLLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528 Gemma\uff0c\u5f9e\u800c\u5be6\u73fe\u4e00\u500b\u53ef\u76f4\u63a5\u7528\u65bc\u751f\u7522\u74b0\u5883\u7684\u5f37\u5927\u63a8\u7406\u670d\u52d9\u89e3\u6c7a\u65b9\u6848\uff0c\u5177\u5099\u4ee3\u7ba1\u5f0f [Kubernetes](https://kubernetes.io/) \u7684\u6240\u6709\u512a\u52e2\uff0c\u5305\u62ec\u9ad8\u6548\u7684\u53ef\u4f38\u7e2e\u6027\u548c\u66f4\u9ad8\u7684\u53ef\u7528\u6027\u3002\u672c\u90e8\u5206\u4ecb\u7d39\u672c\u6307\u5357\u4e2d\u4f7f\u7528\u7684\u95dc\u9375\u6280\u8853\u3002\n### Gemma [Gemma](https://ai.google.dev/gemma/docs/?hl=zh-cn) \u662f\u4e00\u7d44\u516c\u958b\u63d0\u4f9b\u7684\u8f15\u91cf\u7d1a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd (AI) \u6a21\u578b\uff08\u6839\u64da\u958b\u653e\u8a31\u53ef\u767c\u4f48\uff09\u3002\u9019\u4e9b AI \u6a21\u578b\u53ef\u4ee5\u5728\u61c9\u7528\u3001\u786c\u4ef6\u3001\u79fb\u52d5\u8a2d\u5099\u6216\u8a17\u7ba1\u670d\u52d9\u4e2d\u904b\u884c\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 Gemma \u6a21\u578b\u751f\u6210\u6587\u672c\uff0c\u4f46\u4e5f\u53ef\u4ee5\u91dd\u5c0d\u5c08\u9580\u4efb\u52d9\u5c0d\u9019\u4e9b\u6a21\u578b\u9032\u884c\u8abf\u512a\u3002\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Gemma \u6587\u6a94](https://ai.google.dev/gemma/docs?hl=zh-cn) \u3002\n### GPU\u5229\u7528 GPU\uff0c\u60a8\u53ef\u4ee5\u52a0\u901f\u5728\u7bc0\u9ede\u4e0a\u904b\u884c\u7684\u7279\u5b9a\u5de5\u4f5c\u8ca0\u8f09\uff08\u4f8b\u5982\u6a5f\u5668\u5b78\u7fd2\u548c\u6578\u64da\u8655\u7406\uff09\u3002GKE \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u6a5f\u5668\u985e\u578b\u9078\u9805\u4ee5\u7528\u65bc\u7bc0\u9ede\u914d\u7f6e\uff0c\u5305\u62ec\u914d\u5099 NVIDIA H100\u3001L4 \u548c A100 GPU \u7684\u6a5f\u5668\u985e\u578b\u3002\n\u4f7f\u7528 GKE \u4e2d\u7684 GPU \u4e4b\u524d\uff0c\u6211\u5011\u5efa\u8b70\u60a8\u5b8c\u6210\u4ee5\u4e0b\u5b78\u7fd2\u8def\u7dda\uff1a- \u77ad\u89e3 [\u7576\u524d GPU \u7248\u672c\u53ef\u7528\u6027](https://cloud.google.com/compute/docs/gpus?hl=zh-cn) \n- \u77ad\u89e3 [GKE \u4e2d\u7684 GPU](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus?hl=zh-cn) \n### vLLMvLLM \u662f\u4e00\u500b\u7d93\u904e\u9ad8\u5ea6\u512a\u5316\u7684\u958b\u6e90 LLM \u670d\u52d9\u6846\u67b6\uff0c\u53ef\u63d0\u9ad8 GPU \u4e0a\u7684\u670d\u52d9\u541e\u5410\u91cf\uff0c\u5177\u6709\u5982\u4e0b\u529f\u80fd\uff1a- \u5177\u6709 [PagedAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention) \u4e14\u7d93\u904e\u512a\u5316\u7684 Transformer\uff08\u8f49\u63db\u5668\uff09\u5be6\u73fe\n- \u9023\u7e8c\u6279\u8655\u7406\uff0c\u53ef\u63d0\u9ad8\u6574\u9ad4\u670d\u52d9\u541e\u5410\u91cf\n- \u591a\u500b GPU \u4e0a\u7684\u5f35\u91cf\u4e26\u884c\u8655\u7406\u548c\u5206\u4f48\u5f0f\u670d\u52d9\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [vLLM \u6587\u6a94](https://docs.vllm.ai/en/latest/) \u3002## \u76ee\u6a19\u672c\u6307\u5357\u9069\u7528\u65bc\u4f7f\u7528 [PyTorch](https://pytorch.org/) \u7684\u751f\u6210\u5f0f AI \u5ba2\u6236\u3001GKE \u7684\u65b0\u7528\u6236\u6216\u73fe\u6709\u7528\u6236\u3001\u6a5f\u5668\u5b78\u7fd2\u5de5\u7a0b\u5e2b\u3001MLOps (DevOps) \u5de5\u7a0b\u5e2b\u6216\u662f\u5c0d\u4f7f\u7528 Kubernetes \u5bb9\u5668\u7de8\u6392\u529f\u80fd\u5728 H100\u3001A100 \u548c L4 GPU \u786c\u4ef6\u4e0a\u61c9\u7528 LLM \u611f\u8208\u8da3\u7684\u5e73\u81fa\u7ba1\u7406\u54e1\u3002\n\u95b1\u8b80\u5b8c\u672c\u6307\u5357\u5f8c\uff0c\u60a8\u61c9\u8a72\u80fd\u5920\u57f7\u884c\u4ee5\u4e0b\u6b65\u9a5f\uff1a- \u4f7f\u7528\u8655\u65bc Autopilot \u6216 Standard \u6a21\u5f0f\u7684 GKE \u96c6\u7fa3\u6e96\u5099\u74b0\u5883\u3002\n- \u5c07 vLLM \u5bb9\u5668\u90e8\u7f72\u5230\u60a8\u7684\u96c6\u7fa3\u3002\n- \u901a\u904e curl \u548c\u7db2\u9801\u804a\u5929\u754c\u9762\uff0c\u4f7f\u7528 vLLM \u61c9\u7528 Gemma 2B \u6216 7B \u6a21\u578b\u3002\n## \u6e96\u5099\u5de5\u4f5c\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- \u78ba\u4fdd\u60a8\u64c1\u6709\u9805\u76ee\u7684\u4ee5\u4e0b\u4e00\u500b\u6216\u591a\u500b\u89d2\u8272\uff1a      roles/container.admin, roles/iam.serviceAccountAdmin\n- \u5982\u679c\u60a8\u9084\u6c92\u6709 [Hugging Face](https://huggingface.co/) \u8cec\u865f\uff0c\u8acb\u5275\u5efa\u4e00\u500b\u3002\n- [\u78ba\u4fdd\u60a8\u7684\u9805\u76ee\u5177\u6709\u8db3\u5920\u7684\u914d\u984d](https://cloud.google.com/compute/resource-usage?hl=zh-cn#gpu_quota) \uff0c\u4ee5\u4fbf\u7528\u65bc GKE \u4e2d\u7684 GPU\u3002\n## \u7372\u53d6\u5c0d\u6a21\u578b\u7684\u8a2a\u554f\u6b0a\u9650\u5982\u9700\u7372\u53d6\u5c0d Gemma \u6a21\u578b\u7684\u8a2a\u554f\u6b0a\u9650\u4ee5\u4fbf\u90e8\u7f72\u5230 GKE\uff0c\u60a8\u5fc5\u9808\u5148\u7c3d\u7f72\u8a31\u53ef\u540c\u610f\u5354\u8b70\uff0c\u7136\u5f8c\u751f\u6210 Huggging Face \u8a2a\u554f\u4ee4\u724c\u3002\n### \u7c3d\u7f72\u8a31\u53ef\u540c\u610f\u5354\u8b70\u60a8\u5fc5\u9808\u7c3d\u7f72\u540c\u610f\u5354\u8b70\u624d\u80fd\u4f7f\u7528 Gemma\u3002\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u64cd\u4f5c\uff1a- \u8a2a\u554f Kaggle.com \u4e0a\u7684 [\u6a21\u578b\u540c\u610f\u9801\u9762](https://www.kaggle.com/models/google/gemma) \u3002\n- \u4f7f\u7528\u60a8\u7684 Hugging Face \u8cec\u865f\u9a57\u8b49\u540c\u610f\u60c5\u6cc1\u3002\n- \u63a5\u53d7\u6a21\u578b\u689d\u6b3e\u3002\n### \u751f\u6210\u4e00\u500b\u8a2a\u554f\u4ee4\u724c\u5982\u9700\u901a\u904e Hugging Face \u8a2a\u554f\u6a21\u578b\uff0c\u60a8\u9700\u8981 [Hugging Face \u4ee4\u724c](https://huggingface.co/docs/hub/security-tokens) \u3002\n\u5982\u679c\u60a8\u9084\u6c92\u6709\u4ee4\u724c\uff0c\u8acb\u6309\u7167\u4ee5\u4e0b\u6b65\u9a5f\u751f\u6210\u65b0\u4ee4\u724c\uff1a- \u9ede\u64ca **\u60a8\u7684\u500b\u4eba\u8cc7\u6599 > \u8a2d\u7f6e > \u8a2a\u554f\u4ee4\u724c** \u3002\n- \u9078\u64c7 **\u65b0\u5efa\u4ee4\u724c** (New Token)\u3002\n- \u6307\u5b9a\u60a8\u9078\u64c7\u7684\u540d\u7a31\u548c\u4e00\u500b\u81f3\u5c11\u7232 Read \u7684\u89d2\u8272\u3002\n- \u9078\u64c7 **\u751f\u6210\u4ee4\u724c** \u3002\n- \u5c07\u751f\u6210\u7684\u4ee4\u724c\u8907\u88fd\u5230\u526a\u8cbc\u677f\u3002\n## \u6e96\u5099\u74b0\u5883\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c07\u4f7f\u7528 [Cloud Shell](https://cloud.google.com/shell?hl=zh-cn) \u4f86\u7ba1\u7406 Google Cloud \u4e0a\u8a17\u7ba1\u7684\u8cc7\u6e90\u3002Cloud Shell \u9810\u5b89\u88dd\u6709\u672c\u6559\u7a0b\u6240\u9700\u7684\u8edf\u4ef6\uff0c\u5305\u62ec [kubectl](https://kubernetes.io/docs/reference/kubectl/) \u548c [gcloud CLI](https://cloud.google.com/sdk/gcloud?hl=zh-cn) \u3002\n\u5982\u9700\u4f7f\u7528 Cloud Shell \u8a2d\u7f6e\u60a8\u7684\u74b0\u5883\uff0c\u8acb\u6309\u7167\u4ee5\u4e0b\u6b65\u9a5f\u64cd\u4f5c\uff1a- \u5728 Google Cloud \u63a7\u5236\u6aaf\u4e2d\uff0c\u9ede\u64ca [Google Cloud \u63a7\u5236\u6aaf](http://console.cloud.google.com?hl=zh-cn) \u4e2d\u7684 **\u6fc0\u6d3b Cloud Shell** \u4ee5\u5553\u52d5 Cloud Shell \u6703\u8a71\u3002\u6b64\u64cd\u4f5c\u6703\u5728 Google Cloud \u63a7\u5236\u6aaf\u7684\u5e95\u90e8\u7a97\u683c\u4e2d\u5553\u52d5\u6703\u8a71\u3002\n- \u8a2d\u7f6e\u9ed8\u8a8d\u74b0\u5883\u8b8a\u91cf\uff1a```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=REGIONexport CLUSTER_NAME=vllmexport HF_TOKEN=HF_TOKEN\n```\u66ff\u63db\u4ee5\u4e0b\u503c\uff1a- \uff1a\u60a8\u7684 Google Cloud [\u9805\u76ee ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects?hl=zh-cn#identifying_projects) \u3002\n- \uff1a\u652f\u6301\u8981\u4f7f\u7528\u7684\u52a0\u901f\u5668\u985e\u578b\u7684\u5340\u57df\uff0c\u4f8b\u5982\u9069\u7528\u65bc L4 GPU \u7684`us-central1`\u3002\n- \uff1a\u60a8\u4e4b\u524d\u751f\u6210\u7684 Hugging Face \u4ee4\u724c\u3002## \u5275\u5efa\u548c\u914d\u7f6e Google Cloud \u8cc7\u6e90\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u5275\u5efa\u6240\u9700\u7684\u8cc7\u6e90\u3002\n **\u6ce8\u610f** \uff1a\u60a8\u53ef\u80fd\u9700\u8981\u5275\u5efa\u5bb9\u91cf\u9810\u7559\u624d\u80fd\u4f7f\u7528\u67d0\u4e9b\u52a0\u901f\u5668\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u9810\u7559\u548c\u4f7f\u7528\u9810\u7559\u7684\u8cc7\u6e90\uff0c\u8acb\u53c3\u95b1 [\u4f7f\u7528\u9810\u7559\u7684\u53ef\u7528\u5340\u7d1a\u8cc7\u6e90](https://cloud.google.com/kubernetes-engine/docs/how-to/consuming-reservations?hl=zh-cn) \u3002\n### \u5275\u5efa GKE \u96c6\u7fa3\u548c\u7bc0\u9ede\u6c60\u60a8\u53ef\u4ee5\u5728 GKE Autopilot \u6216 Standard \u96c6\u7fa3\u4e2d\u7684 GPU \u4e0a\u61c9\u7528 Gemma\u3002\u6211\u5011\u5efa\u8b70\u60a8\u4f7f\u7528 Autopilot \u96c6\u7fa3\u7372\u5f97\u5168\u8a17\u7ba1\u5f0f Kubernetes \u9ad4\u9a57\u3002\u5982\u9700\u9078\u64c7\u6700\u9069\u5408\u60a8\u7684\u5de5\u4f5c\u8ca0\u8f09\u7684 GKE \u64cd\u4f5c\u6a21\u5f0f\uff0c\u8acb\u53c3\u95b1 [\u9078\u64c7 GKE \u64cd\u4f5c\u6a21\u5f0f](https://cloud.google.com/kubernetes-engine/docs/concepts/choose-cluster-mode?hl=zh-cn) \u3002\n\u5728 Cloud Shell \u4e2d\uff0c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a\n```\ngcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid \\\u00a0 --cluster-version=1.28\n```\nGKE \u6703\u6839\u64da\u6240\u90e8\u7f72\u7684\u5de5\u4f5c\u8ca0\u8f09\u7684\u8acb\u6c42\uff0c\u5275\u5efa\u5177\u6709\u6240\u9700 CPU \u548c GPU \u7bc0\u9ede\u7684 Autopilot \u96c6\u7fa3\u3002- \u5728 Cloud Shell \u4e2d\uff0c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u5275\u5efa Standard \u96c6\u7fa3\uff1a```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 --release-channel=rapid \\\u00a0 --num-nodes=1\n```\u96c6\u7fa3\u5275\u5efa\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\u3002\n- \u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4f86\u7232\u96c6\u7fa3\u5275\u5efa [\u7bc0\u9ede\u6c60](https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools?hl=zh-cn) \uff1a```\ngcloud container node-pools create gpupool \\\u00a0 --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --location=${REGION} \\\u00a0 --node-locations=${REGION}-a \\\u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 --machine-type=g2-standard-24 \\\u00a0 --num-nodes=1\n```GKE \u6703\u5275\u5efa\u4e00\u500b\u7bc0\u9ede\u6c60\uff0c\u5176\u4e2d\u6bcf\u500b\u7bc0\u9ede\u6709\u5169\u500b L4 GPU\u3002### \u7232 Hugging Face \u6191\u64da\u5275\u5efa Kubernetes Secret\u5728 Cloud Shell \u4e2d\uff0c\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a- \u914d\u7f6e `kubectl` \u4ee5\u8207\u60a8\u7684\u96c6\u7fa3\u901a\u4fe1\uff1a```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}\n```\n- \u5275\u5efa\u5305\u542b Hugging Face \u4ee4\u724c\u7684 Kubernetes Secret\uff1a```\nkubectl create secret generic hf-secret \\--from-literal=hf_api_token=$HF_TOKEN \\--dry-run=client -o yaml | kubectl apply -f ```\n## \u90e8\u7f72 vLLM\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u90e8\u7f72 vLLM \u5bb9\u5668\u4ee5\u61c9\u7528\u60a8\u8981\u4f7f\u7528\u7684 Gemma \u6a21\u578b\u3002\u5982\u9700\u77ad\u89e3\u6307\u4ee4\u8abf\u512a\u548c\u9810\u8a13\u7df4\u6a21\u578b\uff0c\u4ee5\u53ca\u7232\u60a8\u7684\u61c9\u7528\u5834\u666f\u9078\u64c7\u54ea\u500b\u6a21\u578b\uff0c\u8acb\u53c3\u95b1 [\u8abf\u512a\u6a21\u578b](https://ai.google.dev/gemma/docs?hl=zh-cn#tuned-models) \u3002\n\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u90e8\u7f72 Gemma 2B \u6307\u4ee4\u8abf\u512a\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `vllm-2b-it.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f vllm-2b-it.yaml\n```\n\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u90e8\u7f72 Gemma 7B \u6307\u4ee4\u8abf\u512a\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `vllm-7b-it.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f vllm-7b-it.yaml\n```\n\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u90e8\u7f72 Gemma 2B \u9810\u8a13\u7df4\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `vllm-2b.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f vllm-2b.yaml\n```\n\u8acb\u6309\u7167\u4ee5\u4e0b\u8aaa\u660e\u90e8\u7f72 Gemma 7B \u9810\u8a13\u7df4\u6a21\u578b\u3002- \u5275\u5efa\u4ee5\u4e0b `vllm-7b.yaml` \u6e05\u55ae\uff1a [  ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f vllm-7b.yaml\n```\n\u96c6\u7fa3\u4e2d\u7684 Pod \u6703\u5f9e Hugging Face \u4e0b\u8f09\u6a21\u578b\u6b0a\u91cd\u4e26\u5553\u52d5\u670d\u52d9\u5f15\u64ce\u3002\n\u7b49\u5f85\u90e8\u7f72\u6210\u7232\u53ef\u7528\u72c0\u614b\uff1a\n```\nkubectl wait --for=condition=Available --timeout=700s deployment/vllm-gemma-deployment\n```\n\u67e5\u770b\u6b63\u5728\u904b\u884c\u7684\u90e8\u7f72\u7684\u65e5\u8a8c\uff1a\n```\nkubectl logs -f -l app=gemma-server\n```\n\u90e8\u7f72\u8cc7\u6e90\u6703\u4e0b\u8f09\u6a21\u578b\u6578\u64da\u3002\u6b64\u904e\u7a0b\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\u3002\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a\n```\nINFO 01-26 19:02:54 model_runner.py:689] Graph capturing finished in 4 secs.\nINFO:  Started server process [1]\nINFO:  Waiting for application startup.\nINFO:  Application startup complete.\nINFO:  Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```\n\u78ba\u4fdd\u6a21\u578b\u5df2\u5b8c\u5168\u4e0b\u8f09\uff0c\u7136\u5f8c\u518d\u7e7c\u7e8c\u4e0b\u4e00\u90e8\u5206\u3002## \u61c9\u7528\u6a21\u578b\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u8207\u6a21\u578b\u4e92\u52d5\u3002\n### \u8a2d\u7f6e\u7aef\u53e3\u8f49\u767c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u8a2d\u7f6e\u5230\u6a21\u578b\u7684\u7aef\u53e3\u8f49\u767c\uff1a\n```\nkubectl port-forward service/llm-service 8000:8000\n```\n\u8f38\u51fa\u985e\u4f3c\u65bc\u4ee5\u4e0b\u5167\u5bb9\uff1a\n```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n### \u4f7f\u7528 curl \u8207\u6a21\u578b\u4e92\u52d5\u672c\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u57f7\u884c\u57fa\u672c\u7684\u5192\u7159\u6e2c\u8a66\u4f86\u9a57\u8b49\u6240\u90e8\u7f72\u7684\u9810\u8a13\u7df4\u6216\u6307\u4ee4\u8abf\u512a\u6a21\u578b\u3002\u7232\u7c21\u55ae\u8d77\u898b\uff0c\u672c\u90e8\u5206\u50c5\u4ecb\u7d39\u4f7f\u7528 2B \u9810\u8a13\u7df4\u548c\u6307\u4ee4\u8abf\u512a\u6a21\u578b\u7684\u6e2c\u8a66\u65b9\u6cd5\u3002\n\u5728\u65b0\u7684\u7d42\u7aef\u6703\u8a71\u4e2d\uff0c\u4f7f\u7528 `curl` \u8207\u6a21\u578b\u804a\u5929\uff1a\n```\nUSER_PROMPT=\"Java is a\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"prompt\": \"${USER_PROMPT}\",\u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \"top_p\": 1.0,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\n\u4ee5\u4e0b\u8f38\u51fa\u986f\u793a\u4e86\u6a21\u578b\u97ff\u61c9\u7684\u793a\u4f8b\uff1a\n```\n{\"predictions\":[\"Prompt:\\nJava is a\\nOutput:\\n<strong>programming language</strong> that is primarily aimed at developers. It was originally created by creators of the Java Virtual Machine (JVM). Java is multi-paradigm, which means it supports object-oriented, procedural, and functional programming paradigms. Java is object-oriented, which means that it is designed to support classes and objects. Java is a dynamically typed language, which means that the type of variables are not determined at compile time. Java is also a multi-paradigm language, which means it supports more than one programming paradigm. Java is also a very lightweight language, which means that it is a very low level language compared to other popular\"]}\n```\n\u5728\u65b0\u7684\u7d42\u7aef\u6703\u8a71\u4e2d\uff0c\u4f7f\u7528 `curl` \u8207\u6a21\u578b\u804a\u5929\uff1a\n```\nUSER_PROMPT=\"I'm new to coding. If you could only recommend one programming language to start with, what would it be and why?\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"prompt\": \"<start_of_turn>user\\n${USER_PROMPT}<end_of_turn>\\n\",\u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \"top_p\": 1.0,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\n\u4ee5\u4e0b\u8f38\u51fa\u986f\u793a\u4e86\u6a21\u578b\u97ff\u61c9\u7684\u793a\u4f8b\uff1a\n```\n{\"predictions\":[\"Prompt:\\n<start_of_turn>user\\nI'm new to coding. If you could only recommend one programming language to start with, what would it be and why?<end_of_turn>\\nOutput:\\n**Python** is an excellent choice for beginners due to the following reasons:\\n\\n* **Clear and simple syntax:** Python boasts a simple and straightforward syntax that makes it easy to learn the fundamentals of programming.\\n* **Extensive libraries and modules:** Python comes with a vast collection of libraries and modules that address various programming tasks, including data manipulation, machine learning, and web development.\\n* **Large and supportive community:** Python has a vibrant and active community that offers resources, tutorials, and support to help you along your journey.\\n* **Cross-platform compatibility:** Python can be run on various platforms, including Windows, macOS, and\"]}\n```\n **\u6210\u529f** \uff1a\u60a8\u5df2\u6210\u529f\u5730\u901a\u904e vLLM \u4f7f\u7528 GKE \u4e2d\u7684 GPU \u61c9\u7528\u4e86 Gemma\u3002\u60a8\u73fe\u5728\u53ef\u4ee5\u8207\u6a21\u578b\u4e92\u52d5\u3002\n### \uff08\u53ef\u9078\uff09\u901a\u904e Gradio \u804a\u5929\u754c\u9762\u8207\u6a21\u578b\u4e92\u52d5\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u69cb\u5efa\u4e00\u500b\u7db2\u9801\u804a\u5929\u61c9\u7528\uff0c\u53ef\u8b93\u60a8\u8207\u6307\u4ee4\u8abf\u512a\u6a21\u578b\u4e92\u52d5\u3002\u7232\u7c21\u55ae\u8d77\u898b\uff0c\u672c\u90e8\u5206\u50c5\u4ecb\u7d39\u4f7f\u7528 2B-it \u6a21\u578b\u7684\u6e2c\u8a66\u65b9\u6cd5\u3002\n [Gradio](https://github.com/gradio-app/gradio) \u662f\u4e00\u500b Python \u5eab\uff0c\u5b83\u5177\u6709\u4e00\u500b\u53ef\u7232\u804a\u5929\u6a5f\u5668\u4eba\u5275\u5efa\u754c\u9762\u7684 `ChatInterface` \u5c01\u88dd\u5bb9\u5668\u3002\n- \u5728 Cloud Shell \u4e2d\uff0c\u5c07\u4ee5\u4e0b\u6e05\u55ae\u4fdd\u5b58\u7232 `gradio.yaml` \uff1a [  ai-ml/llm-serving-gemma/vllm/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/gradio.yaml) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service:8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"vllm\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gemma\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>user\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>model\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradiospec:\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 targetPort: 7860\u00a0 type: ClusterIP\n```\n- \u61c9\u7528\u6e05\u55ae\uff1a```\nkubectl apply -f gradio.yaml\n```\n- \u7b49\u5f85\u90e8\u7f72\u6210\u7232\u53ef\u7528\u72c0\u614b\uff1a```\nkubectl wait --for=condition=Available --timeout=300s deployment/gradio\n```\n- \u5728 Cloud Shell \u4e2d\uff0c\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a```\nkubectl port-forward service/gradio 8080:8080\n```\u9019\u6703\u5275\u5efa\u5f9e Cloud Shell \u5230 Gradio \u670d\u52d9\u7684\u7aef\u53e3\u8f49\u767c\u3002\n- \u9ede\u64ca Cloud Shell \u4efb\u52d9\u6b04\u53f3\u4e0a\u89d2\u7684 **\u7db2\u9801\u9810\u89bd** \u6309\u9215\u3002\u9ede\u64ca **\u5728\u7aef\u53e3 8080 \u4e0a\u9810\u89bd** \u3002\u700f\u89bd\u5668\u4e2d\u6703\u6253\u958b\u4e00\u500b\u65b0\u7684\u6a19\u7c64\u9801\u3002\n- \u4f7f\u7528 Gradio \u804a\u5929\u754c\u9762\u8207 Gemma \u4e92\u52d5\u3002\u6dfb\u52a0\u63d0\u793a\uff0c\u7136\u5f8c\u9ede\u64ca **\u63d0\u4ea4** \u3002\n## \u554f\u984c\u6392\u67e5\n- \u5982\u679c\u60a8\u6536\u5230`Empty reply from server`\u6d88\u606f\uff0c\u5247\u5bb9\u5668\u53ef\u80fd\u5c1a\u672a\u5b8c\u6210\u6a21\u578b\u6578\u64da\u4e0b\u8f09\u3002\u518d\u6b21 [\u6aa2\u67e5 Pod \u7684\u65e5\u8a8c](#deploy-vllm) \u4e2d\u662f\u5426\u5305\u542b`Connected`\u6d88\u606f\uff0c\u8a72\u6d88\u606f\u8868\u660e\u6a21\u578b\u5df2\u6e96\u5099\u597d\u9032\u884c\u61c9\u7528\u3002\n- \u5982\u679c\u60a8\u770b\u5230`Connection refused`\uff0c\u8acb\u9a57\u8b49\u60a8\u7684 [\u7aef\u53e3\u8f49\u767c\u5df2\u5553\u7528](#setup-port-forwarding) \u3002\n## \u6e05\u7406\u7232\u907f\u514d\u56e0\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u8cc7\u6e90\u5c0e\u81f4\u60a8\u7684 Google Cloud \u8cec\u865f\u7522\u751f\u8cbb\u7528\uff0c\u8acb\u522a\u9664\u5305\u542b\u9019\u4e9b\u8cc7\u6e90\u7684\u9805\u76ee\uff0c\u6216\u8005\u4fdd\u7559\u9805\u76ee\u4f46\u522a\u9664\u5404\u500b\u8cc7\u6e90\u3002\n### \u522a\u9664\u5df2\u90e8\u7f72\u7684\u8cc7\u6e90\u7232\u907f\u514d\u56e0\u60a8\u5728\u672c\u6307\u5357\u4e2d\u5275\u5efa\u7684\u8cc7\u6e90\u5c0e\u81f4\u60a8\u7684 Google Cloud \u8cec\u865f\u7522\u751f\u8cbb\u7528\uff0c\u8acb\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a\n```\ngcloud container clusters delete ${CLUSTER_NAME} \\\u00a0 --region=${REGION}\n```## \u5f8c\u7e8c\u6b65\u9a5f\n- \u8a73\u7d30\u77ad\u89e3 [GKE \u4e2d\u7684 GPU](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus?hl=zh-cn) \u3002\n- [\u67e5\u770b GitHub \u4e2d\u7684\u793a\u4f8b\u4ee3\u78bc](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/ai-ml/llm-serving-gemma/vllm) \uff0c\u77ad\u89e3\u5982\u4f55\u5728\u5176\u4ed6\u52a0\u901f\u5668\uff08\u5305\u62ec A100 \u548c H100 GPU\uff09\u4e0a\u5c07 Gemma \u8207 vLLM \u642d\u914d\u4f7f\u7528\u3002\n- \u77ad\u89e3\u5982\u4f55 [\u5728 Autopilot \u4e2d\u90e8\u7f72 GPU \u5de5\u4f5c\u8ca0\u8f09](https://cloud.google.com/kubernetes-engine/docs/how-to/autopilot-gpus?hl=zh-cn) \u3002\n- \u77ad\u89e3\u5982\u4f55 [\u5728 Standard \u4e2d\u90e8\u7f72 GPU \u5de5\u4f5c\u8ca0\u8f09](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus?hl=zh-cn) \u3002\n- \u700f\u89bd vLLM [GitHub \u4ee3\u78bc\u5eab](https://github.com/vllm-project/vllm) \u548c [\u6587\u6a94](https://docs.vllm.ai/en/latest/) \u3002\n- \u63a2\u7d22 [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden?hl=zh-cn) \u3002\n- \u77ad\u89e3\u5982\u4f55\u4f7f\u7528 [GKE \u5e73\u81fa\u7de8\u6392\u529f\u80fd](https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra?hl=zh-cn) \u904b\u884c\u7d93\u904e\u512a\u5316\u7684 AI/\u6a5f\u5668\u5b78\u7fd2\u5de5\u4f5c\u8ca0\u8f09\u3002", "guide": "Google Kubernetes Engine (GKE)"}