{"title": "Google Kubernetes Engine (GKE) - Deploy a stateful MySQL cluster on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/mysql", "abstract": "# Google Kubernetes Engine (GKE) - Deploy a stateful MySQL cluster on GKE\nThis document is intended for database administrators, cloud architects, and operations professionals interested in deploying a highly available MySQL topology on Google Kubernetes Engine.\nFollow this tutorial to learn how to deploy a [MySQL InnoDB Cluster](https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-innodb-cluster.html) and a [MySQL InnoDB ClusterSet](https://dev.mysql.com/doc/mysql-shell/8.0/en/innodb-clusterset.html) , in addition to [MySQL Router](https://dev.mysql.com/doc/mysql-router/8.0/en/) middleware on your GKE cluster, and how to perform upgrades.", "content": "## Objectives\nIn this tutorial, you will learn how to:\n- Create and deploy a stateful Kubernetes service.\n- Deploy a MySQL InnoDB Cluster for high availability.\n- Deploy Router middleware for database operation routing.\n- Deploy a MySQL InnoDB ClusterSet for disaster tolerance.\n- Simulate a MySQL cluster failover.\n- Perform a MySQL version upgrade.\nThe following sections describe the architecture of the solution you will build in this tutorial.In your regional GKE cluster, using a StatefulSet, you deploy a MySQL database instance with the necessary naming and configuration to create a MySQL InnoDB Cluster. To provide fault tolerance and high availability, you deploy three database instance Pods. This ensures that the majority of Pods on different zones are available at any given time for a successful primary election using a consensus protocol, and makes your MySQL InnoDB Cluster tolerant of single zonal failures.Once deployed, you designate one Pod as the primary instance to serve both read and write operations. The other two Pods are secondary read-only replicas. If the primary instance experiences an infrastructure failure, you can promote one of these two replica Pods to become the primary.\nIn a separate namespace, you deploy three MySQL Router Pods to provide connection routing for improved resilience. Instead of directly connecting to the database service, your applications connect to MySQL Router Pods. Each Router Pod is aware of the status and purpose of each MySQL InnoDB Cluster Pod, and routes application operations to the respective healthy Pod. The routing state is cached in the Router Pods and updated from the cluster metadata stored on each node of the MySQL InnoDB Cluster. In the case of an instance failure, the Router adjusts the connection routing to a live instance.You can create a MySQL InnoDB ClusterSet from an initial MySQL InnoDB Cluster. This lets you increase disaster tolerance if the primary cluster is no longer available.If the MySQL InnoDB Cluster primary instance is no longer available, you can promote a replica cluster in the ClusterSet to primary. When using MySQL Router middleware, your application does not need to track the health of the primary database instance. Routing is adjusted to send connections to the new primary after the election has occurred. However, it is your responsibility to ensure that applications connecting to your MySQL Router middleware follow best practices for resilience, so that connections are retried if an error occurs during cluster failover.## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Compute Engine](/compute/disks-image-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n### Set up your project### Set up roles\n- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `role/storage.objectViewer, role/logging.logWriter, role/artifactregistry.Admin, roles/container.clusterAdmin, role/container.serviceAgent, roles/serviceusage.serviceUsageAdmin, roles/iam.serviceAccountAdmin` ```\n$ gcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.### Set up your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with Docker and the `kubectl` and gcloud CLI.\nTo use Cloud Shell to set up your environment:- Set environment variables.```\nexport PROJECT_ID=PROJECT_IDexport CLUSTER_NAME=gkemulti-westexport REGION=COMPUTE_REGION\n```Replace the following values:- : your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : your [Compute Engine region](/compute/docs/regions-zones#available) . For this tutorial, the region is`us-west1`. Typically, you want a region that is close to you.\n- Set the default environment variables.```\n\u00a0gcloud config set project PROJECT_ID\u00a0gcloud config set compute/region COMPUTE_REGION\n```\n- Clone the code repository.```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory.```\ncd kubernetes-engine-samples/databases/gke-stateful-mysql/kubernetes\n```## Create a GKE clusterIn this section, you create a [regional GKE cluster](/kubernetes-engine/docs/concepts/regional-clusters) . Unlike a zonal cluster, a regional cluster's control plane is replicated into several zones, so an outage in a single zone doesn't make the control plane unavailable.\nTo create a GKE cluster, follow these steps:\n- In Cloud Shell, create a GKE Autopilot cluster in the `us-west1` region.```\ngcloud container clusters create-auto $CLUSTER_NAME \\\u00a0 \u00a0 --region=$REGION\n```\n- Get the GKE cluster credentials.```\ngcloud container clusters get-credentials $CLUSTER_NAME \\\u00a0 --region=$REGION\n```\n- Deploy a Service across three zones. [  databases/gke-stateful-mysql/kubernetes/prepare-for-ha.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/prepare-for-ha.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/prepare-for-ha.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: prepare-three-zone-ha\u00a0 labels:\u00a0 \u00a0 app: prepare-three-zone-haspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: prepare-three-zone-ha\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: prepare-three-zone-ha\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 # Tell Kubernetes to avoid scheduling a replica in a zone where there\u00a0 \u00a0 \u00a0 \u00a0 # is already a replica with the label \"app: prepare-three-zone-ha\"\u00a0 \u00a0 \u00a0 \u00a0 podAntiAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: app\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - prepare-three-zone-ha\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 topologyKey: \"topology.kubernetes.io/zone\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: prepare-three-zone-ha\u00a0 \u00a0 \u00a0 \u00a0 image: busybox:latest\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"/bin/sh\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"while true; do sleep 3600; done\"\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"0.5Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"0.5Gi\"\n``````\nkubectl apply -f prepare-for-ha.yaml\n```By default, Autopilot provisions resources in two zones. The Deployment defined in `prepare-for-ha.yaml` ensures that Autopilot provisions nodes across three zones in your cluster, by setting `replicas:3` , `podAntiAffinity` with `requiredDuringSchedulingIgnoredDuringExecution` , and `topologyKey: \"topology.kubernetes.io/zone\"` .\n- Check the status of the Deployment.```\nkubectl get deployment prepare-three-zone-ha --watch\n```When you see three Pods in the ready state, cancel this command with `CTRL+C` . The output is similar to the following:```\nNAME     READY UP-TO-DATE AVAILABLE AGE\nprepare-three-zone-ha 0/3  3   0   9s\nprepare-three-zone-ha 1/3  3   1   116s\nprepare-three-zone-ha 2/3  3   2   119s\nprepare-three-zone-ha 3/3  3   3   2m16s\n```\n- Run this script to validate that your Pods have been deployed across three zones.```\nbash ../scripts/inspect_pod_node.sh default\n```Each line of the output corresponds to a Pod, and the second column indicates the cloud zone. The output is similar to the following:```\ngk3-gkemulti-west1-default-pool-eb354e2d-z6mv us-west1-b prepare-three-zone-ha-7885d77d9c-8f7qb\ngk3-gkemulti-west1-nap-25b73chq-739a9d40-4csr us-west1-c prepare-three-zone-ha-7885d77d9c-98fpn\ngk3-gkemulti-west1-default-pool-160c3578-bmm2 us-west1-a prepare-three-zone-ha-7885d77d9c-phmhj\n```\n- In Cloud Shell, create a GKE Standard cluster in the `us-west1` region.```\ngcloud container clusters create $CLUSTER_NAME \\\u00a0 --region=$REGION \\\u00a0 --machine-type=\"e2-standard-2\" \\\u00a0 --disk-type=\"pd-standard\" \\\u00a0 --num-nodes=\"5\"\n```\n- Get the GKE cluster credentials.```\ngcloud container clusters get-credentials $CLUSTER_NAME \\\u00a0 --region=$REGION\n```\n## Deploy MySQL StatefulSetsIn this section, you deploy one MySQL [StatefulSet](/kubernetes-engine/docs/concepts/statefulset) . Each StatefulSet consists of three MySQL replicas.\nTo deploy the MySQL StatefulSet, follow these steps:- Create a namespace for the StatefulSet.```\nkubectl create namespace mysql1\n```\n- Create the MySQL secret. [  databases/gke-stateful-mysql/kubernetes/secret.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/secret.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/secret.yaml) ```\napiVersion: v1kind: Secretmetadata:\u00a0 name: mysql-secrettype: Opaquedata:\u00a0 password: UGFzc3dvcmQkMTIzNDU2 # Password$123456\u00a0 admin-password: UGFzc3dvcmQkMTIzNDU2 # Password$123456\n``````\nkubectl apply -n mysql1 -f secret.yaml\n```The password is deployed with each Pod, and is used by management scripts and commands for MySQL InnoDB Cluster and ClusterSet deployment in this tutorial.\n- Create the StorageClass. [  databases/gke-stateful-mysql/kubernetes/storageclass.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/storageclass.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/storageclass.yaml) ```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: fast-storageclassprovisioner: pd.csi.storage.gke.iovolumeBindingMode: WaitForFirstConsumerreclaimPolicy: RetainallowVolumeExpansion: trueparameters:\u00a0 type: pd-balanced\n``````\nkubectl apply -n mysql1 -f storageclass.yaml\n```This storage class uses the `pd-balanced` Persistent Disk type that balances performance and cost. The `volumeBindingMode` field is set to `WaitForFirstConsumer` meaning that GKE delays provisioning of a PersistentVolume until the Pod is created. This setting ensures that the disk is provisioned in the same zone where the Pod is scheduled.\n- Deploy the StatefulSet of MySQL instance Pods. [  databases/gke-stateful-mysql/kubernetes/c1-mysql.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/c1-mysql.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/c1-mysql.yaml) ```\napiVersion: apps/v1kind: StatefulSetmetadata:\u00a0 name: dbc1\u00a0 labels:\u00a0 \u00a0 app: mysqlspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: mysql\u00a0 serviceName: mysql\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: mysql\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 topologySpreadConstraints:\u00a0 \u00a0 \u00a0 - maxSkew: 1\u00a0 \u00a0 \u00a0 \u00a0 topologyKey: \"topology.kubernetes.io/zone\"\u00a0 \u00a0 \u00a0 \u00a0 whenUnsatisfiable: DoNotSchedule\u00a0 \u00a0 \u00a0 \u00a0 labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: mysql\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 podAntiAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: app\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mysql\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 topologyKey: \"kubernetes.io/hostname\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: mysql\u00a0 \u00a0 \u00a0 \u00a0 image: mysql/mysql-server:8.0.28\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /bin/bash\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 - >-\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /entrypoint.sh\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --server-id=$((20 + \u00a0$(echo $HOSTNAME | grep -o '[^-]*$') + 1))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --report-host=${HOSTNAME}.mysql.mysql1.svc.cluster.local\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --binlog-checksum=NONE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --enforce-gtid-consistency=ON\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --gtid-mode=ON\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --default-authentication-plugin=mysql_native_password\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MYSQL_ROOT_PASSWORD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: mysql-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: password\u00a0 \u00a0 \u00a0 \u00a0 - name: MYSQL_ADMIN_PASSWORD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: mysql-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: admin-password\u00a0 \u00a0 \u00a0 \u00a0 - name: MYSQL_ROOT_HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: '%'\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - name: mysql\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerPort: 3306\u00a0 \u00a0 \u00a0 \u00a0 - name: mysqlx\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerPort: 33060\u00a0 \u00a0 \u00a0 \u00a0 - name: xcom\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerPort: 33061\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"1Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"1Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"500m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"1Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"1Gi\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: mysql\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /var/lib/mysql\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subPath: mysql\u00a0 \u00a0 \u00a0 \u00a0 readinessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mysql -h127.0.0.1 -uroot -p$MYSQL_ROOT_PASSWORD -e'SELECT 1'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 30\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeoutSeconds: 1\u00a0 \u00a0 \u00a0 \u00a0 livenessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mysqladmin -uroot -p$MYSQL_ROOT_PASSWORD ping\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 30\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 10\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeoutSeconds: 5\u00a0 updateStrategy:\u00a0 \u00a0 rollingUpdate:\u00a0 \u00a0 \u00a0 partition: 0\u00a0 \u00a0 type: RollingUpdate\u00a0 volumeClaimTemplates:\u00a0 - metadata:\u00a0 \u00a0 \u00a0 name: mysql\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: mysql\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 storageClassName: fast-storageclass\u00a0 \u00a0 \u00a0 volumeMode: Filesystem\u00a0 \u00a0 \u00a0 accessModes:\u00a0 \u00a0 \u00a0 - ReadWriteOnce\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 10Gi\n``````\nkubectl apply -n mysql1 -f c1-mysql.yaml\n```This command deploys the StatefulSet consisting of three replicas. In this tutorial, the primary MySQL cluster is deployed across three zones in `us-west1` . The output is similar to the following:```\nservice/mysql created\nstatefulset.apps/dbc1 created\n```In this tutorial, the resource limits and requests are set to minimal values to save cost. When planning for a production workload, make sure to set these values appropriately for your organization's needs.\n- Verify the StatefulSet is created successfully.```\nkubectl get statefulset -n mysql1 --watch\n```It can take about 10 minutes for the StatefulSet to be ready.\n- When all three pods are in a ready state, exit the command using `Ctrl+C` . If you see `PodUnscheduleable` errors due to insufficient CPU or memory, wait a few minutes for the control plane to resize to accommodate the large workload.The output is similar to the following:```\nNAME READY AGE\ndbc1 1/3  39s\ndbc1 2/3  50s\ndbc1 3/3  73s\n```\n- To inspect the placement of your Pods on the GKE cluster nodes, run this script:```\nbash ../scripts/inspect_pod_node.sh mysql1 mysql\n```The output shows the Pod name, the GKE node name, and the zone where the node is provisioned, and looks similar to the following:```\ngke-gkemulti-west-5-default-pool-4bcaca65-jch0 us-west1-b dbc1-0\ngke-gkemulti-west-5-default-pool-1ac6e8b5-ddjx us-west1-c dbc1-1\ngke-gkemulti-west-5-default-pool-1f5baa66-bf8t us-west1-a dbc1-2\n```The columns in the output represent the hostname, cloud zone, and Pod name, respectively.The `topologySpreadConstraints` policy in the StatefulSet specification ( `c1-mysql.yaml` ) directs the scheduler to place the Pods evenly across the failure domain ( `topology.kubernetes.io/zone` ).The `podAntiAffinity` policy enforces the constraint that Pods are required to not be placed on the same GKE cluster node ( `kubernetes.io/hostname` ). For the MySQL instance Pods, this policy results in the Pods being deployed evenly across the three zones in the Google Cloud region. This placement enables high availability of the MySQL InnoDB Cluster by placing each database instance in a separate failure domain.\n## Prepare the primary MySQL InnoDB ClusterTo configure a MySQL InnoDB Cluster, follow these steps:- In the [Cloud Shell](/shell) terminal, set the group replication configurations for the MySQL instances to be added to your cluster.```\nbash ../scripts/c1-clustersetup.sh\n``` [  databases/gke-stateful-mysql/scripts/c1-clustersetup.sh ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/scripts/c1-clustersetup.sh) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/scripts/c1-clustersetup.sh) ```\nPOD_ORDINAL_START=${1:-0}POD_ORDINAL_END=${2:-2}for i in $(seq ${POD_ORDINAL_START} ${POD_ORDINAL_END}); do\u00a0 echo \"Configuring pod mysql1/dbc1-${i}\"\u00a0 cat <<' \u00a0EOF' | kubectl -n mysql1 exec -i dbc1-${i} -- bash -c 'mysql -uroot -proot --password=${MYSQL_ROOT_PASSWORD}'INSTALL PLUGIN group_replication SONAME 'group_replication.so';RESET PERSIST IF EXISTS group_replication_ip_allowlist;RESET PERSIST IF EXISTS binlog_transaction_dependency_tracking;SET @@PERSIST.group_replication_ip_allowlist = 'mysql.mysql1.svc.cluster.local';SET @@PERSIST.binlog_transaction_dependency_tracking = 'WRITESET';\u00a0 EOFdone\n```The script will remotely connect into each of the three MySQL instances to set and persist the following environment variables:- `group_replication_ip_allowlist`: allows the instance within the cluster to connect to any instance in the group.\n- `binlog_transaction_dependency_tracking='WRITESET'`: allows parallelized transactions which won't conflict.\nIn MySQL versions earlier than 8.0.22, use `group_replication_ip_whitelist` instead of `group_replication_ip_allowlist` .\n- Open a second terminal, so that you do not need to create a shell for each Pod.\n- Connect to MySQL Shell on the Pod `dbc1-0` .```\nkubectl -n mysql1 exec -it dbc1-0 -- \\\u00a0 \u00a0 /bin/bash \\\u00a0 \u00a0 -c 'mysqlsh --uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-0.mysql.mysql1.svc.cluster.local\"'\n```\n- Verify the MySQL group replication allowlist for connecting to other instances.```\n\\sql SELECT @@group_replication_ip_allowlist;\n```The output is similar to the following:```\n+----------------------------------+\n| @@group_replication_ip_allowlist |\n+----------------------------------+\n| mysql.mysql1.svc.cluster.local |\n+----------------------------------+\n```\n- Verify the `server-id` is unique on each of the instances.```\n\\sql SELECT @@server_id;\n```The output is similar to the following:```\n+-------------+\n| @@server_id |\n+-------------+\n|   21 |\n+-------------+\n```\n- Configure each instance for MySQL InnoDB Cluster usage and create an administrator account on each instance.```\n\\jsdba.configureInstance('root@dbc1-0.mysql.mysql1.svc.cluster.local', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"),clusterAdmin: 'icadmin', clusterAdminPassword: os.getenv(\"MYSQL_ADMIN_PASSWORD\")});dba.configureInstance('root@dbc1-1.mysql.mysql1.svc.cluster.local', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"),clusterAdmin: 'icadmin', clusterAdminPassword: os.getenv(\"MYSQL_ADMIN_PASSWORD\")});dba.configureInstance('root@dbc1-2.mysql.mysql1.svc.cluster.local', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"),clusterAdmin: 'icadmin', clusterAdminPassword: os.getenv(\"MYSQL_ADMIN_PASSWORD\")});\n```All instances must have the same username and password in order for the MySQL InnoDB Cluster to function properly. Each command produces output similar to the following:```\n...\nThe instance 'dbc1-2.mysql:3306' is valid to be used in an InnoDB cluster.\nCluster admin user 'icadmin'@'%' created.\nThe instance 'dbc1-2.mysql.mysql1.svc.cluster.local:3306' is already\nready to be used in an InnoDB cluster.\nSuccessfully enabled parallel appliers.\n```\n- Verify that the instance is ready to be used in a MySQL InnoDB Cluster.```\ndba.checkInstanceConfiguration()\n```The output is similar to the following:```\n...\nThe instance 'dbc1-0.mysql.mysql1.svc.cluster.local:3306' is valid to be used in an InnoDB cluster.\n{\n \"status\": \"ok\"\n}\n```Optionally, you can connect to each MySQL instance and repeat this command. For example, run this command to check the status on the `dbc1-1` instance:```\nkubectl -n mysql1 exec -it dbc1-0 -- \\\u00a0 \u00a0 /bin/bash \\\u00a0 \u00a0 -c 'mysqlsh --uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-1.mysql.mysql1.svc.cluster.local\" \\\u00a0 \u00a0 --js --execute \"dba.checkInstanceConfiguration()\"'\n```\n## Create the primary MySQL InnoDB ClusterNext, create the MySQL InnoDB Cluster using the MySQL Admin `createCluster` command. Start with the `dbc1-0` instance, which will be the primary instance for the cluster, then add two additional replicas to the cluster.\nTo initialize the MySQL InnoDB Cluster, follow these steps:- Create the MySQL InnoDB Cluster.```\nvar cluster=dba.createCluster('mycluster');\n```Running the `createCluster` command triggers these operations:- Deploy the metadata schema.\n- Verify that the configuration is correct for Group Replication.\n- Register it as the seed instance of the new cluster.\n- Create necessary internal accounts, such as the replication user account.\n- Start Group Replication.\nThis command initializes a MySQL InnoDB Cluster with the host `dbc1-0` as the primary. The cluster reference is stored in the cluster variable.The output looks similar to the following:```\nA new InnoDB cluster will be created on instance 'dbc1-0.mysql:3306'.\nValidating instance configuration at dbc1-0.mysql:3306...\nThis instance reports its own address as dbc1-0.mysql.mysql1.svc.cluster.local:3306\nInstance configuration is suitable.\nNOTE: Group Replication will communicate with other instances using\n'dbc1-0.mysql:33061'. Use the localAddress\noption to override.\nCreating InnoDB cluster 'mycluster' on\n'dbc1-0.mysql.mysql1.svc.cluster.local:3306'...\nAdding Seed Instance...\nCluster successfully created. Use Cluster.addInstance() to add MySQL\ninstances.\nAt least 3 instances are needed for the cluster to be able to withstand\nup to one server failure.\n```\n- Add the second instance to the cluster.```\ncluster.addInstance('icadmin@dbc1-1.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"), recoveryMethod: 'clone'});\n```\n- Add the remaining instance to the cluster.```\ncluster.addInstance('icadmin@dbc1-2.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"), recoveryMethod: 'clone'});\n```The output is similar to the following:```\n...\nThe instance 'dbc1-2.mysql:3306' was successfully added to the cluster.\n```\n- Verify the cluster's status.```\ncluster.status()\n```This command shows the status of the cluster. The topology consists of three hosts, one primary and two secondary instances. Optionally, you can call `cluster.status({extended:1})` .The output is similar to the following:```\n{\n \"clusterName\": \"mysql1\",\n \"defaultReplicaSet\": {\n  \"name\": \"default\",\n  \"primary\": \"dbc1-0.mysql:3306\",\n  \"ssl\": \"REQUIRED\",\n  \"status\": \"OK\",\n  \"statusText\": \"Cluster is ONLINE and can tolerate up to ONE failure.\",\n  \"topology\": {\n   \"dbc1-0.mysql:3306\": {\n    \"address\": \"dbc1-0.mysql:3306\",\n    \"memberRole\": \"PRIMARY\",\n    \"mode\": \"R/W\",\n    \"readReplicas\": {},\n    \"replicationLag\": null,\n    \"role\": \"HA\",\n    \"status\": \"ONLINE\",\n    \"version\": \"8.0.28\"\n   },\n   \"dbc1-1.mysql:3306\": {\n    \"address\": \"dbc1-1.mysql:3306\",\n    \"memberRole\": \"SECONDARY\",\n    \"mode\": \"R/O\",\n    \"readReplicas\": {},\n    \"replicationLag\": null,\n    \"role\": \"HA\",\n    \"status\": \"ONLINE\",\n    \"version\": \"8.0.28\"\n   },\n   \"dbc1-2.mysql:3306\": {\n    \"address\": \"dbc1-2.mysql:3306\",\n    \"memberRole\": \"SECONDARY\",\n    \"mode\": \"R/O\",\n    \"readReplicas\": {},\n    \"replicationLag\": null,\n    \"role\": \"HA\",\n    \"status\": \"ONLINE\",\n    \"version\": \"8.0.28\"\n   }\n  },\n  \"topologyMode\": \"Single-Primary\"\n },\n \"groupInformationSourceMember\": \"dbc1-0.mysql:3306\"\n}\n```Optionally, you can call `cluster.status({extended:1})` to obtain additional status details.\n### Create a sample databaseTo create a sample database, follow these steps:- Create a database and load data into the database.```\n\\sqlcreate database loanapplication;use loanapplicationCREATE TABLE loan (loan_id INT unsigned AUTO_INCREMENT PRIMARY KEY, firstname VARCHAR(30) NOT NULL, lastname VARCHAR(30) NOT NULL , status VARCHAR(30) );\n```\n- Insert sample data into the database. To insert data, you must be connected to the primary instance of the cluster.```\nINSERT INTO loan (firstname, lastname, status) VALUES ( 'Fred','Flintstone','pending');INSERT INTO loan (firstname, lastname, status) VALUES ( 'Betty','Rubble','approved');\n```\n- Verify that the table contains the three rows inserted in the previous step.```\nSELECT * FROM loan;\n```The output is similar to the following:```\n+---------+-----------+------------+----------+\n| loan_id | firstname | lastname | status |\n+---------+-----------+------------+----------+\n|  1 | Fred  | Flintstone | pending |\n|  2 | Betty  | Rubble  | approved |\n+---------+-----------+------------+----------+\n2 rows in set (0.0010 sec)\n```\n## Create a MySQL InnoDB ClusterSetYou can create a MySQL InnoDB ClusterSet to manage replication from your primary cluster to replica clusters, using a dedicated ClusterSet replication channel.\nA MySQL InnoDB ClusterSet provides disaster tolerance for MySQL InnoDB Cluster deployments by linking a primary MySQL InnoDB Cluster with one or more replicas of itself in alternate locations, such as multiple zones and multiple regions.\nIf you closed MySQL Shell, create a new shell by running this command in a new Cloud Shell terminal:\n```\n\u00a0 kubectl -n mysql1 exec -it dbc1-0 -- \\\u00a0 \u00a0 \u00a0 /bin/bash -c 'mysqlsh \\\u00a0 \u00a0 \u00a0 --uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-0.mysql.mysql1.svc.cluster.local\"'\n```\nTo create a MySQL InnoDB ClusterSet, follow these steps:- In your MySQL Shell terminal, obtain a cluster object.```\n\\jscluster=dba.getCluster()\n```The output is similar to the following:```\n<Cluster:mycluster>\n```\n- Initialize a MySQL InnoDB ClusterSet with the existing MySQL InnoDB Cluster stored in the cluster object as the primary.```\nclusterset=cluster.createClusterSet('clusterset')\n```The output is similar to the following:```\nA new ClusterSet will be created based on the Cluster 'mycluster'.\n* Validating Cluster 'mycluster' for ClusterSet compliance.\n* Creating InnoDB ClusterSet 'clusterset' on 'mycluster'...\n* Updating metadata...\nClusterSet successfully created. Use ClusterSet.createReplicaCluster() to add Replica Clusters to it.\n<ClusterSet:clusterset>\n```\n- Check the status of your MySQL InnoDB ClusterSet.```\nclusterset.status()\n```The output is similar to the following:```\n{\n \"clusters\": {\n  \"mycluster\": {\n   \"clusterRole\": \"PRIMARY\",\n   \"globalStatus\": \"OK\",\n   \"primary\": \"dbc1-0.mysql:3306\"\n  }\n },\n \"domainName\": \"clusterset\",\n \"globalPrimaryInstance\": \"dbc1-0.mysql:3306\",\n \"primaryCluster\": \"mycluster\",\n \"status\": \"HEALTHY\",\n \"statusText\": \"All Clusters available.\"\n}\n```Optionally, you can call `clusterset.status({extended:1})` to obtain additional status details, including information about the cluster.\n- Exit MySQL Shell.```\n\\q\n```\n## Deploy a MySQL RouterYou can deploy a MySQL Router to direct client application traffic to the proper clusters. Routing is based on the connection port of the application issuing a database operation:- are routed to the primary Cluster instance in the primary ClusterSet.\n- can be routed to any instance in the primary Cluster.\nWhen you start a MySQL Router, it is bootstrapped against the MySQL InnoDB ClusterSet deployment. The MySQL Router instances connected with the MySQL InnoDB ClusterSet are aware of any controlled switchovers or emergency failovers and direct traffic to the new primary cluster.\nTo deploy a MySQL Router, follow these steps:- In the Cloud Shell terminal, deploy the MySQL Router.```\nkubectl apply -n mysql1 -f c1-router.yaml\n```The output is similar to the following:```\nconfigmap/mysql-router-config created\nservice/mysql-router created\ndeployment.apps/mysql-router created\n```\n- Check the readiness of the MySQL Router deployment.```\nkubectl -n mysql1 get deployment mysql-router --watch\n```When all three Pods are ready, the output is similar to the following:```\nNAME   READY UP-TO-DATE AVAILABLE AGE\nmysql-router 3/3  3   0   3m36s\n```If you see a `PodUnschedulable` error in the console, wait a minute or two while GKE provisions more nodes. Refresh, and you should see `3/3 OK` .\n- Start MySQL Shell on any member of the existing cluster.```\nkubectl -n mysql1 exec -it dbc1-0 -- \\\u00a0 \u00a0 /bin/bash -c 'mysqlsh --uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-0.mysql\"'\n```This command connects to the `dbc1-0` Pod, then starts a shell connected to the `dbc1-0` MySQL instance.\n- Verify the router configuration.```\nclusterset=dba.getClusterSet()clusterset.listRouters()\n```The output is similar to the following:```\n{\n \"domainName\": \"clusterset\",\n \"routers\": {\n \"mysql-router-7cd8585fbc-74pkm::\": {\n  \"hostname\": \"mysql-router-7cd8585fbc-74pkm\",\n  \"lastCheckIn\": \"2022-09-22 23:26:26\",\n  \"roPort\": 6447,\n  \"roXPort\": 6449,\n  \"rwPort\": 6446,\n  \"rwXPort\": 6448,\n  \"targetCluster\": null,\n  \"version\": \"8.0.27\"\n },\n \"mysql-router-7cd8585fbc-824d4::\": {\n  ...\n },\n \"mysql-router-7cd8585fbc-v2qxz::\": {\n  ...\n }\n }\n}\n```\n- Exit MySQL Shell.```\n\\q\n```\n- Run this script to inspect the placement of the MySQL Router Pods.```\nbash ../scripts/inspect_pod_node.sh mysql1 | sort\n```The script shows the node and Cloud Zone placement of the all of the Pods in the `mysql1` namespace, where the output is similar to the following:```\ngke-gkemulti-west-5-default-pool-1ac6e8b5-0h9v us-west1-c mysql-router-6654f985f5-df97q\ngke-gkemulti-west-5-default-pool-1ac6e8b5-ddjx us-west1-c dbc1-1\ngke-gkemulti-west-5-default-pool-1f5baa66-bf8t us-west1-a dbc1-2\ngke-gkemulti-west-5-default-pool-1f5baa66-kt03 us-west1-a mysql-router-6654f985f5-qlfj9\ngke-gkemulti-west-5-default-pool-4bcaca65-2l6s us-west1-b mysql-router-6654f985f5-5967d\ngke-gkemulti-west-5-default-pool-4bcaca65-jch0 us-west1-b dbc1-0\n```You can observe that the MySQL Router Pods are distributed equally across the zones; that is, not placed on the same node as a MySQL Pod, or on the same node as another MySQL Router Pod.\n## Manage GKE and MySQL InnoDB Cluster upgradesUpdates for both MySQL and Kubernetes are released on a regular schedule. Follow operational best practices to update your software environment regularly. By default, GKE manages cluster and node pool upgrades for you. Kubernetes and GKE also provide additional features to facilitate MySQL software upgrades.\n### Plan for GKE upgradesYou can take proactive steps and set configurations to mitigate risk and facilitate a smoother cluster upgrade when you are running stateful services, including:- **Standard clusters** : Follow [GKE best practices for upgrading clusters](/kubernetes-engine/docs/best-practices/upgrading-clusters) . Choose an appropriate [upgrade strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) to ensure the upgrades happen during the period of the maintenance window:- Choose [surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) if cost optimization is important and if your workloads can tolerate a graceful shutdown in less than 60 minutes.\n- Choose [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) if your workloads are less tolerant of disruptions, and a temporary cost increase due to higher resource usage is acceptable.\nTo learn more, see [Upgrade a cluster running a stateful workload](/kubernetes-engine/docs/tutorials/upgrading-stateful-workload#configure_a_node_pool_upgrade_strategy) . Autopilot clusters are [automatically upgraded](/kubernetes-engine/docs/concepts/cluster-upgrades-autopilot) , based on the release channel you selected.\n- Use [maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) to ensure upgrades happen when you intend them. Before the maintenance window, ensure your database backups are successful.\n- Before allowing traffic to the upgraded MySQL nodes, use Readiness Probes and Liveness Probes to ensure they are ready for traffic.\n- Create Probes that assess whether replication is in sync before accepting traffic. This can be done through custom scripts, depending on the complexity and scale of your database.\nWhen a MySQL InnoDB Cluster is running on GKE, there must be a sufficient number of instances running at any time to meet the quorum requirement.\nIn this tutorial, given a MySQL cluster of three instances, two instances must be available to form a quorum. A `PodDisruptionBudget` policy allows you to limit the number of Pods that can be terminated at any given time. This is useful for both steady state operations of your stateful services and for cluster upgrades.\nTo ensure that a limited number of Pods are concurrently disrupted, you set the PDB for your workload to `maxUnavailable: 1` . This ensures that at any point in the service operation, no more than one Pod is not running.\n **Note:** You can also set the `minAvailable` value to ensure that a minimum number of Pods are running. However, if using `minAvailable` alone, to guarantee cluster availability, make sure that the value is increased if the size of the cluster increases. In contrast, the `maxUnavailable` value provides quorum protection for the cluster without any changes; the tradeoff is that only one instance can be disrupted for upgrade at a time.\nThe following `PodDisruptionBudget` policy manifest sets the maximum unavailable Pods to one for your MySQL application.\n [  databases/gke-stateful-mysql/kubernetes/mysql-pdb-maxunavailable.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/mysql-pdb-maxunavailable.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/gke-stateful-mysql/kubernetes/mysql-pdb-maxunavailable.yaml) \n```\napiVersion: policy/v1kind: PodDisruptionBudgetmetadata:\u00a0 name: mysql-pdbspec:\u00a0 maxUnavailable: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: mysql\n```\nTo apply the PDB policy to your cluster, follow these steps:- Apply the PDB policy using `kubectl` .```\nkubectl apply -n mysql1 -f mysql-pdb-maxunavailable.yaml\n```\n- View the status of the PDB.```\nkubectl get poddisruptionbudgets -n mysql1 mysql-pdb -o yaml\n```In the `status` section of the output, see the `currentHealthy` and `desiredHealthy` Pods counts. The output is similar to the following:```\nstatus:\n...\n currentHealthy: 3\n desiredHealthy: 2\n disruptionsAllowed: 1\n expectedPods: 3\n...\n```\n### Plan for MySQL binary upgradesKubernetes and GKE provide features to facilitate upgrades for the MySQL binary. However, you need to perform some operations to prepare for the upgrades.\nKeep the following considerations in mind before you begin the upgrade process:- Upgrades should first be carried out in a test environment. For production systems, you should perform further testing in a pre-production environment.\n- For some binary releases, you cannot downgrade the version once an upgrade has been performed. Take the time to understand the implications of an upgrade.\n- Replication sources can replicate to a newer version. However, copying from a newer to an older version is typically not supported.\n- Make sure you have a complete database backup before deploying the upgraded version.\n- Keep in mind the ephemeral nature of Kubernetes Pods. Any configuration state stored by the Pod that is not on the persistent volume will be lost when the Pod is redeployed.\n- For MySQL binary upgrades, use the same PDB, node pool update strategy, and Probes as described earlier.\nIn a production environment, you should follow these best practices:- Create a container image with the new version of MySQL.\n- Persist the image build instructions in a source control repository.\n- Use an automated image build and testing pipeline such as Cloud Build, and store the image binary in an image registry such as Artifact Registry.\nTo keep this tutorial simple, you will not build and persist a container image; instead, you use the public MySQL images.To perform the MySQL binary upgrade, you issue a declarative command that modifies the image version of the StatefulSet resource. GKE performs the necessary steps to stop the current Pod, deploy a new Pod with the upgraded binary, and attach the persistent disk to the new Pod.- Verify that the PDB was created.```\nkubectl get poddisruptionbudgets -n mysql1\n```\n- Get the list of stateful sets.```\nkubectl get statefulsets -n mysql1\n```\n- Get the list of running Pods using the `app` label.```\nkubectl get pods --selector=app=mysql -n mysql1\n```\n- Update the MySQL image in the stateful set.```\nkubectl \u00a0-n mysql1 \\\u00a0 \u00a0 set image statefulset/dbc1 \\\u00a0 \u00a0 mysql=mysql/mysql-server:8.0.30\n```The output is similar to the following:```\nstatefulset.apps/mysql image updated\n```\n- Check the status of the terminating Pods and new Pods.```\nkubectl get pods --selector=app=mysql -n mysql1\n```\nDuring the upgrade, you can verify the status of the rollout, the new Pods, and the existing Service.- Confirm the upgrade by running the `rollout status` command.```\nkubectl rollout status statefulset/dbc1 -n mysql1\n```The output is similar to the following:```\npartitioned roll out complete: 3 new pods have been updated...\n```\n- Confirm the image version by inspecting the stateful set.```\nkubectl get statefulsets -o wide -n mysql1\n```The output is similar to the following:```\nNAME READY AGE CONTAINERS IMAGES\ndbc1 3/3  37m mysql  mysql/mysql-server:8.0.30\n```\n- Check the status of the cluster.```\nkubectl -n mysql1 \\\u00a0 \u00a0 \u00a0exec -it dbc1-0 -- \\\u00a0 \u00a0 \u00a0 \u00a0/bin/bash \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0-c 'mysqlsh \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-1.mysql.mysql1.svc.cluster.local\" \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--js \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--execute \"print(dba.getClusterSet().status({extended:1})); print(\\\"\\\\n\\\")\"'\n```For each cluster instance, look for the status and version values in the output. The output is similar to the following:```\n...\n \"status\": \"ONLINE\",\n \"version\": \"8.0.30\"\n...\n```\n **Warning:** Some binary versions cannot be downgraded. Understand the implications and constraints before performing a binary upgrade.\nWhen you revert the deployment of an upgraded binary version, the rollout process is reversed and a new set of Pods is deployed with the previous image version.\nTo revert the deployment to the previous working version, use the `rollout undo` command:\n```\nkubectl rollout undo statefulset/dbc1 -n mysql1\n```\nThe output is similar to the following:\n```\nstatefulset.apps/dbc1 rolled back\n```## Scale your database cluster horizontallyTo scale your MySQL InnoDB Cluster horizontally, you add additional nodes to the GKE cluster node pool (only required if you are using Standard), deploy additional MySQL instances, then add each instance to the existing MySQL InnoDB Cluster.\n### Add nodes to your Standard clusterThis operation is not needed if you are using a Autopilot cluster.\nTo add nodes to your Standard cluster, follow the instructions below for Cloud Shell or the Google Cloud console. For detailed steps, see [Resize a node pool](/kubernetes-engine/docs/how-to/node-pools#resizing_a_node_pool) \nIn Cloud Shell, resize the default node pool to eight instances in each managed instance group.\n```\ngcloud container clusters resize ${CLUSTER_NAME} \\\u00a0 \u00a0 \u00a0--node-pool default-pool \\\u00a0 \u00a0 \u00a0--num-nodes=8\n```\nTo add nodes to your Standard cluster:- Open the`gkemulti-west1`Cluster page in the Google Cloud console.\n- Select **Nodes** , and click on **default pool** .\n- Scroll down to **Instances groups** .\n- For each instance group, resize the`Number of nodes`value from 5 to 8 nodes.### Add MySQL Pods to the primary clusterTo deploy additional MySQL Pods to scale your cluster horizontally, follow these steps:- In Cloud Shell, update the number of replicas in the MySQL deployment from three replicas to five replicas.```\nkubectl scale \u00a0-n mysql1 --replicas=5 -f c1-mysql.yaml\n```\n- Verify the progress of the deployment.```\nkubectl -n mysql1 get pods --selector=app=mysql -o wide\n```To determine whether the Pods are ready, use the `--watch` flag to watch the deployment. If you are using Autopilot clusters and see `Pod Unschedulable` errors, this might indicate GKE is provisioning nodes to accommodate the additional Pods.\n- Configure the group replication settings for the new MySQL instances to add to the cluster```\nbash ../scripts/c1-clustersetup.sh 3 4\n```The script submits the commands to the instances running on the Pods with ordinals 3 through 4.\n- Open MySQL Shell.```\nkubectl -n mysql1 \\\u00a0 exec -it dbc1-0 -- \\\u00a0 \u00a0 \u00a0 /bin/bash \\\u00a0 \u00a0 \u00a0 \u00a0 -c 'mysqlsh \\\u00a0 \u00a0 \u00a0 \u00a0 --uri=\"root:$MYSQL_ROOT_PASSWORD@dbc1-0.mysql\"'\n```\n- Configure the two new MySQL instances.```\ndba.configureInstance('root:$MYSQL_ROOT_PASSWORD@dbc1-3.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"),clusterAdmin: 'icadmin', clusterAdminPassword: os.getenv(\"MYSQL_ADMIN_PASSWORD\")});dba.configureInstance('root:$MYSQL_ROOT_PASSWORD@dbc1-4.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"),clusterAdmin: 'icadmin', clusterAdminPassword: os.getenv(\"MYSQL_ADMIN_PASSWORD\")});\n```The commands check if the instance is configured properly for MySQL InnoDB Cluster usage and perform the necessary configuration changes.\n- Add one of the new instances to the primary cluster.```\ncluster = dba.getCluster()cluster.addInstance('icadmin@dbc1-3.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"), recoveryMethod: 'clone'});\n```\n- Add a second new instance to the primary cluster.```\ncluster.addInstance('icadmin@dbc1-4.mysql', {password: os.getenv(\"MYSQL_ROOT_PASSWORD\"), recoveryMethod: 'clone'});\n```\n- Obtain the ClusterSet status, which also includes the Cluster status.```\nclusterset = dba.getClusterSet()clusterset.status({extended: 1})\n```The output is similar to the following:```\n\"domainName\": \"clusterset\",\n\"globalPrimaryInstance\": \"dbc1-0.mysql:3306\",\n\"metadataServer\": \"dbc1-0.mysql:3306\",\n\"primaryCluster\": \"mycluster\",\n\"status\": \"HEALTHY\",\n\"statusText\": \"All Clusters available.\"\n```\n- Exit MySQL Shell.```\n\\q\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to avoid billing is to delete the project you created for the tutorial.\n **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\nDelete a Google Cloud project:\n```\ngcloud projects delete PROJECT_ID\n```## What's next\n- Learn more about how the [Google Cloud Observability MySQL integration](/stackdriver/docs/solutions/agents/ops-agent/third-party/mysql) collects performance metrics related to InnoDB.\n- Learn more about [backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) , a service for backing up and restoring workloads in GKE.\n- Explore [Persistent Volumes](/kubernetes-engine/docs/concepts/persistent-volumes) in more detail.", "guide": "Google Kubernetes Engine (GKE)"}