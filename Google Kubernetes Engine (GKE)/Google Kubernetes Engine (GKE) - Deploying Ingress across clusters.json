{"title": "Google Kubernetes Engine (GKE) - Deploying Ingress across clusters", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-ingress", "abstract": "# Google Kubernetes Engine (GKE) - Deploying Ingress across clusters\nThis page shows you how to deploy an Ingress that serves an application across multiple GKE clusters. To learn more about Multi Cluster Ingress, see [Multi Cluster Ingress](/kubernetes-engine/docs/concepts/multi-cluster-ingress) .\nFor a detailed comparison between Multi Cluster Ingress (MCI), Multi-cluster Gateway (MCG), and load balancer with Standalone Network Endpoint Groups (LB and Standalone NEGs), see [Choose your multi-cluster load balancing API forGKE](/kubernetes-engine/docs/concepts/choose-mc-lb-api) .\n", "content": "## Deployment tutorial\nIn the following tasks, you will deploy a fictional app named `whereami` and a `MultiClusterIngress` in two clusters. The Ingress provides a shared virtual IP (VIP) address for the app deployments.\nThis page builds upon the work done in [Setting up Multi Cluster Ingress](/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup) , where you created and registered two clusters. Confirm you have two clusters that are also registered to a [fleet](/anthos/fleet-management/docs) :\n```\ngcloud container clusters list\n```\nThe output is similar to the following:\n```\nNAME LOCATION  MASTER_VERSION MASTER_IP  MACHINE_TYPE NODE_VERSION  NUM_NODES STATUS\ngke-eu europe-west1-b 1.16.8-gke.9 ***    e2-medium  1.16.8-gke.9  2   RUNNING\ngke-us us-central1-b 1.16.8-gke.9 ***    e2-medium  1.16.6-gke.13 * 2   RUNNING\n```\n### Creating the Namespace\nBecause fleets have the property of [namespace sameness](/kubernetes-engine/docs/concepts/multi-cluster-ingress#namespace_sameness) , we recommend that you coordinate Namespace creation and management across clusters so identical Namespaces are owned and managed by the same group. You can create Namespaces per team, per environment, per application, or per application component. Namespaces can be as granular as necessary, as long as a Namespace `ns1` in one cluster has the same meaning and usage as `ns1` in another cluster.\nIn this example, you create a `whereami` Namespace for each application in each cluster.\n- Create a file named `namespace.yaml` that has the following content:```\napiVersion: v1kind: Namespacemetadata:\u00a0 name: whereami\n``` **Note:** You can use `kubectl config use-context` to switch between clusters when deploying resources. Use `kubectl config get-contexts` to see which ones are available. You can use `kubectl config rename-context` to rename contexts to more human-friendly names.\n- Switch to the gke-us context:```\nkubectl config use-context gke-us\n```\n- Create the Namespace:```\nkubectl apply -f namespace.yaml\n```\n- Switch to the gke-eu context:```\nkubectl config use-context gke-eu\n```\n- Create the Namespace:```\nkubectl apply -f namespace.yaml\n```The output is similar to the following:```\nnamespace/whereami created\n```\n### Deploying the app\n- Create a file named `deploy.yaml` that has the following content:```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: whereami-deployment\u00a0 namespace: whereami\u00a0 labels:\u00a0 \u00a0 app: whereamispec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: whereami\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: whereami\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: frontend\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/whereami:v1.2.20\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\n```\n- Switch to the gke-us context:```\nkubectl config use-context gke-us\n```\n- Deploy the `whereami` app:```\nkubectl apply -f deploy.yaml\n```\n- Switch to the gke-eu context:```\nkubectl config use-context gke-eu\n```\n- Deploy the `whereami` app:```\nkubectl apply -f deploy.yaml\n```\n- Verify that the `whereami` app has successfully deployed in each cluster:```\nkubectl get deployment --namespace whereami\n```The output should be similar to the following in both clusters:```\nNAME   READY UP-TO-DATE AVAILABLE AGE\nwhereami-deployment 1/1  1   1   12m\n```\n### Deploying through the config cluster\nNow that the application is deployed across `gke-us` and `gke-eu` , you will deploy a load balancer by deploying `MultiClusterIngress` and `MultiClusterService` resources in the config cluster. These are the multi-cluster equivalents of Ingress and Service resources.\nIn the [setup guide](/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup#specifying_a_config_cluster) , you configured the `gke-us` cluster as the config cluster. The config cluster is used to deploy and configure Ingress across all clusters.\n- Set the context to the config cluster.```\nkubectl config use-context gke-us\n``` **Note:** Only one cluster can be the active config cluster at any time. You can deploy `MultiClusterIngress` and `MultiClusterService` resources to other clusters, but they won't be seen or processed by the Multi Cluster Ingress controller.- Create a file named `mcs.yaml` that has the following content:```\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: whereami-mcs\u00a0 namespace: whereamispec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 app: whereami\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - name: web\u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 \u00a0 targetPort: 8080\n```\n- Deploy the `MultiClusterService` resource that matches the `whereami` app:```\nkubectl apply -f mcs.yaml\n```\n- Verify that the `whereami-mcs` resource has successfully deployed in the config cluster:```\nkubectl get mcs -n whereami\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 AGEwhereami-mcs \u00a0 9m26s\n```This `MultiClusterService` creates a derived headless Service in every cluster that matches Pods with `app: whereami` . You can see that one exists in the `gke-us` cluster `kubectl get service -n whereami` .The output is similar to the following:```\nNAME        TYPE  CLUSTER-IP EXTERNAL-IP PORT(S)   AGE\nmci-whereami-mcs-svc-lgq966x5mxwwvvum ClusterIP None   <none>  8080/TCP   4m59s\n```\nA similar headless Service will also exist in `gke-eu` . These local Services are used to dynamically select Pod endpoints to program the global Ingress load balancer with backends.- Create a file named `mci.yaml` that has the following content:```\napiVersion: networking.gke.io/v1kind: MultiClusterIngressmetadata:\u00a0 name: whereami-ingress\u00a0 namespace: whereamispec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 serviceName: whereami-mcs\u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\n```Note that this configuration routes all traffic to the `MultiClusterService` named `whereami-mcs` that exists in the `whereami` namespace.\n- Deploy the `MultiClusterIngress` resource that references `whereami-mcs` as a backend:```\nkubectl apply -f mci.yaml\n```The output is similar to the following:```\nmulticlusteringress.networking.gke.io/whereami-ingress created\n```Note that `MultiClusterIngress` has the same schema as the Kubernetes Ingress. The Ingress resource semantics are also the same with the exception of the `backend.serviceName` field.\nThe `backend.serviceName` field in a `MultiClusterIngress` references a `MultiClusterService` in the fleet API rather than a Service in a Kubernetes cluster. This means that any of the settings for Ingress, such as TLS termination, settings can be configured in the same way.\n## Validating a successful deployment status\nGoogle Cloud Load Balancer deployment might take several minutes to deploy for new load balancers. Updating existing load balancers completes faster because new resources don't need to be deployed. The `MultiClusterIngress` resource details the underlying Compute Engine resources that have been created on behalf of the `MultiClusterIngress` .\n- Verify that deployment has succeeded:```\nkubectl describe mci whereami-ingress -n whereami\n```The output is similar to the following:```\nName:   whereami-ingress\nNamespace: whereami\nLabels:  <none>\nAnnotations: kubectl.kubernetes.io/last-applied-configuration:\n    {\"apiVersion\":\"networking.gke.io/v1\",\"kind\":\"MultiClusterIngress\",\"metadata\":{\"annotations\":{},\"name\":\"whereami-ingress\",\"namespace\":\"whe...\nAPI Version: networking.gke.io/v1\nKind:   MultiClusterIngress\nMetadata:\n Creation Timestamp: 2020-04-10T23:35:10Z\n Finalizers:\n mci.finalizer.networking.gke.io\n Generation:  2\n Resource Version: 26458887\n Self Link:   /apis/networking.gke.io/v1/namespaces/whereami/multiclusteringresses/whereami-ingress\n UID:    62bec0a4-8a08-4cd8-86b2-d60bc2bda63d\nSpec:\n Template:\n Spec:\n  Backend:\n  Service Name: whereami-mcs\n  Service Port: 8080\nStatus:\n Cloud Resources:\n Backend Services:\n  mci-8se3df-8080-whereami-whereami-mcs\n Firewalls:\n  mci-8se3df-default-l7\n Forwarding Rules:\n  mci-8se3df-fw-whereami-whereami-ingress\n Health Checks:\n  mci-8se3df-8080-whereami-whereami-mcs\n Network Endpoint Groups:\n  zones/europe-west1-b/networkEndpointGroups/k8s1-e4adffe6-whereami-mci-whereami-mcs-svc-lgq966x5m-808-88670678\n  zones/us-central1-b/networkEndpointGroups/k8s1-a6b112b6-whereami-mci-whereami-mcs-svc-lgq966x5m-808-609ab6c6\n Target Proxies:\n  mci-8se3df-whereami-whereami-ingress\n URL Map: mci-8se3df-whereami-whereami-ingress\n VIP:  34.98.102.37\nEvents:\n Type Reason Age     From        Message\n ---- ------ ----     ----        ------ Normal ADD  3m35s     multi-cluster-ingress-controller whereami/whereami-ingress\n Normal UPDATE 3m10s (x2 over 3m34s) multi-cluster-ingress-controller whereami/whereami-ingress\n```There are several fields that indicate the status of this Ingress deployment:- `Events` is the first place to look. If an error has occurred it will be listed here.\n- `Cloud Resource` lists the Compute Engine resources like forwarding rules, backend services, and firewall rules that have been created by the Multi Cluster Ingress controller. If these are not listed it means that they have not been created yet. You can inspect individual Compute Engine resources with the Console or `gcloud` command to get its status.\n- `VIP` lists an IP address when one has been allocated. Note that the load balancer may not yet be processing traffic even though the VIP exists. If you don't see a VIP after a couple minutes, or if the load balancer is not serving a 200 response within 10 minutes, see [Troubleshooting and operations](/kubernetes-engine/docs/how-to/troubleshooting-and-ops) .\nIf the output events are `Normal` , then the `MultiClusterIngress` deployment is likely successful, but the only way to determine that the full traffic path is functional is to test it.\n- Validate that the application is serving on the VIP with the `/ping` endpoint:```\ncurl INGRESS_VIP/ping\n```Replace `` with the virtual IP (VIP) address.The output is similar to the following:```\n{\"cluster_name\": \"gke-us\",\"host_header\": \"34.120.175.141\",\"pod_name\": \"whereami-deployment-954cbf78-mtlpf\",\"pod_name_emoji\": \"\ud83d\ude0e\",\"project_id\": \"my-project\",\"timestamp\": \"2021-11-29T17:01:59\",\"zone\": \"us-central1-b\"}\n```The output should indicate the region and backend of the application.\n- You can also go to the `http://` `` URL in your browser to see a graphical version of the application that shows the region that it's being served from.The cluster that the traffic is forwarded to depends on your location. The GCLB is designed to forward client traffic to the closest available backend with capacity.## Resource specs\n### MultiClusterService spec\nThe `MultiClusterService` definition consists of two pieces:\n- A `template` section that defines the Service to be created in the Kubernetes clusters. Note that while the `template` section contains fields supported in a typical Service, there are only two fields that are supported in a `MultiClusterService` : `selector` and `ports` . The other fields are ignored.\n- An optional `clusters` section that defines which clusters receive traffic and the load balancing properties for each cluster. If no `clusters` section is specified or if no clusters are listed, all clusters are used by default.\nThe following manifest describes a standard `MultiClusterService` :\n```\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: NAME\u00a0 namespace: NAMESPACEspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 app: POD_LABEL\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - name: web\u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 \u00a0 port: PORT\u00a0 \u00a0 \u00a0 \u00a0 targetPort: TARGET_PORT\n```\nReplace the following:\n- ``: the name of the`MultiClusterService`. This name is referenced by the`serviceName`field in the`MultiClusterIngress`resources.\n- ``: the Kubernetes Namespace that the`MultiClusterService`is deployed in. It must match be in the same Namespace as the`MultiClusterIngress`and the Pods across all clusters in the fleet.\n- ``: the label that determines which pods are selected as backends for this`MultiClusterService`across all clusters in the fleet.\n- ``: must match with the port referenced by the`MultiClusterIngress`that references this`MultiClusterService`.\n- ``: the port that is used to send traffic to the Pod from the GCLB. A NEG is created in each cluster with this port as its serving port.\n### MultiClusterIngress spec\nThe following `mci.yaml` describes the load balancer frontend:\n```\napiVersion: networking.gke.io/v1kind: MultiClusterIngressmetadata:\u00a0 name: NAME\u00a0 namespace: NAMESPACEspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0serviceName: DEFAULT_SERVICE\u00a0 \u00a0 \u00a0 \u00a0servicePort: PORT\u00a0 \u00a0 \u00a0 rules:\u00a0 \u00a0 \u00a0 \u00a0 - host: HOST_HEADER\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - path: PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: SERVICE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: PORT\n```\nReplace the following:\n- ``: the name of the`MultiClusterIngress`resource.\n- ``: the Kubernetes Namespace that the`MultiClusterIngress`is deployed in. It must be in the same Namespace as the`MultiClusterService`and the Pods across all clusters in the fleet.\n- ``: acts as the default backend for all traffic that does not match any host or path rules. This is a required field and a default backend must be specified in the`MultiClusterIngress`even if there are other host or path matches configured.\n- ``: any valid port number. This must match with the`port`field of the`MultiClusterService`resources.\n- ``: matches traffic by the HTTP host header field. The`host`field is optional.\n- ``: matches traffic by the path of the HTTP URL. The`path`field is optional.\n- ``: the name of a`MultiClusterService`that is deployed in the same Namespace and config cluster as this`MultiClusterIngress`.## Multi Cluster Ingress features\nThis section shows you how to configure additional Multi Cluster Ingress features.\n### Cluster selection\nBy default, Services derived from Multi Cluster Ingress are scheduled on every member cluster. However, you may want to apply ingress rules to specific clusters. Some use-cases include:\n- Applying Multi Cluster Ingress to all clusters but the config cluster for isolation of the config cluster.\n- Migrating workloads between clusters in a blue-green fashion.\n- Routing to application backends that only exist in a subset of clusters.\n- Using a single L7 VIP for host or path routing to backends that live on different clusters.\nCluster selection lets you select clusters by region or name in the `MultiClusterService` object. This controls which clusters your `MultiClusterIngress` is pointing to and where the derived Services are scheduled. Clusters within the same fleet and region shouldn't have the same name so that clusters can be referenced uniquely.\n- Open `mcs.yaml````\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: whereami-mcs\u00a0 namespace: whereamispec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 app: whereami\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - name: web\u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 \u00a0 targetPort: 8080\n```This specification creates Derived Services in all clusters, the default behavior.\n- Append the following lines in the clusters section:```\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: whereami-mcs\u00a0 namespace: whereamispec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 app: whereami\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - name: web\u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 \u00a0 targetPort: 8080\u00a0 clusters:\u00a0 - link: \"us-central1-b/gke-us\"\u00a0 - link: \"europe-west1-b/gke-eu\"\n```This example creates Derived Service resources only in gke-us and gke-eu clusters. You must select clusters to selectively apply ingress rules. If the \"clusters\" section of the `MultiClusterService` is not specified or if no clusters are listed, it is interpreted as the default \"all\" clusters.\n### HTTPS support\nThe Kubernetes [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) supports HTTPS. Before enabling HTTPS support, you must create a static IP address. This static IP allows HTTP and HTTPS to share the same IP address. For more information, see [Creating a static IP](#static) .\nOnce you have created a static IP address, you can create a Secret.\n**Note:** The public key certificate must be .PEM encoded and match the given private key.\n- Create a Secret:```\nkubectl -n whereami create secret tls SECRET_NAME --key PATH_TO_KEYFILE --cert PATH_TO_CERTFILE\n```Replace the following:- ``with the name of your Secret.\n- ``with the path to the TLS key file.\n- ``with the path to the TLS certificate file.\n- Update the `mci.yaml` file with the Secret name:```\napiVersion: networking.gke.io/v1kind: MultiClusterIngressmetadata:\u00a0 name: whereami-ingress\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/static-ip: STATIC_IP_ADDRESSspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 serviceName: whereami-mcs\u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\u00a0 \u00a0 \u00a0 tls:\u00a0 \u00a0 \u00a0 - secretName: SECRET_NAME\n```Replace the `` with the name of your Secret. The `` is the IP address or the complete URL of the address you allocated in the [Creating a staticIP](#static) section.\n- Redeploy the `MultiClusterIngress` resource:```\nkubectl apply -f mci.yaml\n```The output is similar to the following:```\nmulticlusteringress.networking.gke.io/whereami-ingress configured\n```\n### BackendConfig support\nThe following BackendConfig CRD lets you customize settings on the Compute Engine BackendService resource:\n```\napiVersion: cloud.google.com/v1kind: BackendConfigmetadata:\u00a0 name: whereami-health-check-cfg\u00a0 namespace: whereamispec:\u00a0 healthCheck:\u00a0 \u00a0 checkIntervalSec: [int]\u00a0 \u00a0 timeoutSec: [int]\u00a0 \u00a0 healthyThreshold: [int]\u00a0 \u00a0 unhealthyThreshold: [int]\u00a0 \u00a0 type: [HTTP | HTTPS | HTTP2 | TCP]\u00a0 \u00a0 port: [int]\u00a0 \u00a0 requestPath: [string]\u00a0 timeoutSec: [int]\u00a0 connectionDraining:\u00a0 \u00a0 drainingTimeoutSec: [int]\u00a0 sessionAffinity:\u00a0 \u00a0 affinityType: [CLIENT_IP | CLIENT_IP_PORT_PROTO | CLIENT_IP_PROTO | GENERATED_COOKIE | HEADER_FIELD | HTTP_COOKIE | NONE]\u00a0 \u00a0 affinityCookieTtlSec: [int]\u00a0 cdn:\u00a0 \u00a0 enabled: [bool]\u00a0 \u00a0 cachePolicy:\u00a0 \u00a0 \u00a0 includeHost: [bool]\u00a0 \u00a0 \u00a0 includeQueryString: [bool]\u00a0 \u00a0 \u00a0 includeProtocol: [bool]\u00a0 \u00a0 \u00a0 queryStringBlacklist: [string list]\u00a0 \u00a0 \u00a0 queryStringWhitelist: [string list]\u00a0 securityPolicy:\u00a0 \u00a0 name: ca-how-to-security-policy\u00a0 logging:\u00a0 \u00a0 enable: [bool]\u00a0 \u00a0 sampleRate: [float]\u00a0 iap:\u00a0 \u00a0 enabled: [bool]\u00a0 \u00a0 oauthclientCredentials:\u00a0 \u00a0 \u00a0 secretName: [string]\n```\nTo use BackendConfig, attach it on your `MultiClusterService` resource using an annotation:\n```\napiVersion: networking.gke.io/v1kind: MultiClusterServicemetadata:\u00a0 name: whereami-mcs\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/backend-config: '{\"ports\": {\"8080\":\"whereami-health-check-cfg\"}}'spec:\u00a0template:\u00a0 \u00a0spec:\u00a0 \u00a0 \u00a0selector:\u00a0 \u00a0 \u00a0 \u00a0app: whereami\u00a0 \u00a0 \u00a0ports:\u00a0 \u00a0 \u00a0- name: web\u00a0 \u00a0 \u00a0 \u00a0protocol: TCP\u00a0 \u00a0 \u00a0 \u00a0port: 8080\u00a0 \u00a0 \u00a0 \u00a0targetPort: 8080\n```\nFor more information about BackendConfig semantics, see [Associating a service port with a BackendConfig](/kubernetes-engine/docs/concepts/backendconfig#associating_a_service_port_with_a_backendconfig) .\n### gRPC support\nConfiguring gRPC applications on Multi Cluster Ingress requires very specific setup. Here are some tips to make sure your load balancer is configured properly:\n- Make sure that the traffic from the load balancer to your application is HTTP/2. Use [application protocols](#application_protocols) to configure this.\n- Make sure that your application is properly configured for SSL since this is a [requirement](/load-balancing/docs/https#protocol_to_the_backends) of HTTP/2. Note that using self-signed certs is acceptable.\n- You must turn off mTLS on your application because mTLS is [not supported](/load-balancing/docs/https#client_communications_with_the_load_balancer) for L7 external load balancers.## Resource lifecycle\n### Configuration changes\n`MultiClusterIngress` and `MultiClusterService` resources behave as standard Kubernetes objects, so changes to the objects are asynchronously reflected in the system. Any changes that result in an invalid configuration cause associated Google Cloud objects to remain unchanged and raise an error in the object event stream. Errors associated with the configuration will be reported as events.\n### Managing Kubernetes resources\nDeleting the Ingress object tears down the HTTP(S) load balancer so traffic is no longer forwarded to any defined `MultiClusterService` .\nDeleting the `MultiClusterService` removes the associated derived services in each of the clusters.\n### Managing clusters\nThe set of clusters targeted by the load balancer can be changed by adding or removing clusters from the fleet.\nFor example, to remove the `gke-eu` cluster as a backend for an ingress, run:\n```\ngcloud container fleet memberships unregister CLUSTER_NAME \\\u00a0 --gke-uri=URI\n```\nReplace the following:\n- ``: the name of your cluster.\n- ``: the URI of the GKE cluster.\nTo add a cluster in Europe, run:\n```\ngcloud container fleet memberships register europe-cluster \\\u00a0 --context=europe-cluster --enable-workload-identity\n```\nYou can find out more about cluster registration options in [Register a GKE cluster](/anthos/fleet-management/docs/register/gke) .\nNote that registering or unregistering a cluster changes its status as a backend for all Ingresses. Unregistering the `gke-eu` cluster removes it as an available backend for all Ingresses you create. The reverse is true for registering a new cluster.\n**Note:** If you want to delete a fleet member cluster, ensure you unregister the cluster before you delete it. Failure to do so could result in unexpected behavior.\n### Disabling Multi Cluster Ingress\nBefore disabling Multi Cluster Ingress you must ensure that you first delete your `MultiClusterIngress` and `MultiClusterService` resources and verify any associated networking resources are deleted.\nThen, to disable Multi Cluster Ingress, use the following command:\n```\ngcloud container fleet ingress disable\n```\nIf you don't delete `MultiClusterIngress` and `MultiClusterService` resources before disabling Multi Cluster Ingress, you might encounter an error similar to the following:\n```\nFeature has associated resources that should be cleaned up before deletion.\n```\nIf you want to force disable Multi Cluster Ingress, use the following command:\n```\ngcloud container fleet ingress disable --force\n```\n**Note:** Force disabling might result in unexpected behavior. It is not recommended to force disable for production environments.\n## Annotations\nThe following annotations are supported on `MultiClusterIngress` and `MultiClusterService` resources.\n| Annotation       | Description                      |\n|:-----------------------------------|:-------------------------------------------------------------------------------------------------|\n| networking.gke.io/frontend-config | References a FrontendConfig resource in the same Namespace as the MultiClusterIngress resource. |\n| networking.gke.io/static-ip  | Refers to the literal IP address of a global static IP.           |\n| networking.gke.io/pre-shared-certs | Refers to a global SSLCertificate resource.              |\n| Annotation      | Description                                                             |\n|:--------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| networking.gke.io/app-protocols | Use this annotation to set the protocol for communication between the load balancer and the application. Possible protocols are HTTP, HTTPS, and HTTP/2. See HTTPS between load balancer and your application and HTTP/2 for load balancing with Ingress. |\n| cloud.google.com/backend-config | Use this annotation to configure the backend service associated with a servicePort. For more information, see Ingress configuration.                              |\n### SSL Policies and HTTPS Redirects\nYou can use the FrontendConfig resource to configure SSL policies and HTTPS redirects. SSL policies allow you to specify which cipher suites and TLS versions are accepted by the load balancer. HTTPS redirects allow you to enforce the redirection from HTTP or port 80 to HTTPS or port 443. The following steps configure an SSL policy and HTTPS redirect together. Note that they can also be configured independently.\n- Create an [SSL policy](/load-balancing/docs/use-ssl-policies#creating_an_ssl_policy_with_a_google-managed_profile) that will reject requests using a version lower than TLS v1.2.```\ngcloud compute ssl-policies create tls-12-policy \\\u00a0--profile MODERN \\\u00a0--min-tls-version 1.2 \\\u00a0--project=PROJECT_ID\n```Replace `` with the project ID where your GKE clusters are running.\n- View your policy to ensure it has been created.```\ngcloud compute ssl-policies list --project=PROJECT_ID\n```The output is similar to the following:```\nNAME   PROFILE MIN_TLS_VERSION\ntls-12-policy MODERN TLS_1_2\n```\n- Create a certificate for `foo.example.com` as in [the example](/kubernetes-engine/docs/how-to/deploying-gateways#creating_and_storing_a_tls_certificate) . Once you have the `key.pem` and `cert.pem` , store these credentials as a Secret that will be referenced by the MultiClusterIngress resource.```\nkubectl -n whereami create secret tls SECRET_NAME --key key.pem --cert cert.pem\n```\n- Save the following FrontendConfig resource as `frontendconfig.yaml` . See [Configuring FrontendConfig resources](/kubernetes-engine/docs/how-to/ingress-configuration#configuring_ingress_features_through_frontendconfig_parameters) for more information on the supported fields within a FrontendConfig.```\napiVersion: networking.gke.io/v1beta1kind: FrontendConfigmetadata:\u00a0 name: frontend-redirect-tls-policy\u00a0 namespace: whereamispec:\u00a0 sslPolicy: tls-12-policy\u00a0 redirectToHttps:\u00a0 \u00a0 enabled: true\n```This FrontendConfig will enable HTTPS redirects and an SSL policy that enforces a minimum TLS version of 1.2.\n- Deploy `frontendconfig.yaml` into your config cluster.```\nkubectl apply -f frontendconfig.yaml --context MCI_CONFIG_CLUSTER\n```Replace the `` with the name of your [config cluster](/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup#specifying_a_config_cluster) .\n- Save the following MultiClusterIngress as `mci-frontendconfig.yaml` .```\napiVersion: networking.gke.io/v1kind: MultiClusterIngressmetadata:\u00a0 name: foo-ingress\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/frontend-config: frontend-redirect-tls-policy\u00a0 \u00a0 networking.gke.io/static-ip: STATIC_IP_ADDRESSspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 serviceName: default-backend\u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\u00a0 \u00a0 \u00a0 rules:\u00a0 \u00a0 \u00a0 - host: foo.example.com\u00a0 \u00a0 \u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: whereami-mcs\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\u00a0 \u00a0 \u00a0 tls:\u00a0 \u00a0 \u00a0 - secretName: SECRET_NAME\n```- Replace``with a static global IP address that you have already provisioned.\n- Replace``with the Secret where your`foo.example.com`certificate is stored.\nThere are two requirements when enabling HTTPS redirects:- TLS must be enabled, either through the`spec.tls`field or through the pre-shared certificate annotation`networking.gke.io/pre-shared-certs`. The MultiClusterIngress won't deploy if HTTPS redirects is enabled but HTTPS is not.\n- A static IP must be referenced through the`networking.gke.io/static-ip`annotation. Static IPs are required when enabling HTTPS on a MultiClusterIngress.\n **Note:** Deployment of the application or MultiClusterService is not shown in this example, but they must also be created for this to fully work.\n- Deploy the MultiClusterIngress to your config cluster.```\nkubectl apply -f mci-frontendconfig.yaml --context MCI_CONFIG_CLUSTER\n```\n- Wait a minute or two and inspect `foo-ingress` .```\nkubectl describe mci foo-ingress --context MCI_CONFIG_CLUSTER\n```A successful output resembles the following:- The`Cloud Resources`status is populated with resource names\n- The`VIP`field is populated with the load balancer IP address\n```\nName:   foobar-ingress\nNamespace: whereami\n...\nStatus:\n Cloud Resources:\n Backend Services:\n  mci-otn9zt-8080-whereami-bar\n  mci-otn9zt-8080-whereami-default-backend\n  mci-otn9zt-8080-whereami-foo\n Firewalls:\n  mci-otn9zt-default-l7\n Forwarding Rules:\n  mci-otn9zt-fw-whereami-foobar-ingress\n  mci-otn9zt-fws-whereami-foobar-ingress\n Health Checks:\n  mci-otn9zt-8080-whereami-bar\n  mci-otn9zt-8080-whereami-default-backend\n  mci-otn9zt-8080-whereami-foo\n Network Endpoint Groups:\n  zones/europe-west1-b/networkEndpointGroups/k8s1-1869d397-multi-cluste-mci-default-backend-svc--80-9e362e3d\n  zones/europe-west1-b/networkEndpointGroups/k8s1-1869d397-multi-cluster--mci-bar-svc-067a3lzs8-808-89846515\n  zones/europe-west1-b/networkEndpointGroups/k8s1-1869d397-multi-cluster--mci-foo-svc-820zw3izx-808-8bbcb1de\n  zones/us-central1-b/networkEndpointGroups/k8s1-a63e24a6-multi-cluste-mci-default-backend-svc--80-a528cc75\n  zones/us-central1-b/networkEndpointGroups/k8s1-a63e24a6-multi-cluster--mci-bar-svc-067a3lzs8-808-36281739\n  zones/us-central1-b/networkEndpointGroups/k8s1-a63e24a6-multi-cluster--mci-foo-svc-820zw3izx-808-ac733579\n Target Proxies:\n  mci-otn9zt-whereami-foobar-ingress\n  mci-otn9zt-whereami-foobar-ingress\n URL Map: mci-otn9zt-rm-whereami-foobar-ingress\n VIP:  34.149.29.76\nEvents:\n Type  Reason Age    From        Message\n ----  ------ ----    ----        ------ Normal UPDATE 38m (x5 over 62m) multi-cluster-ingress-controller whereami/foobar-ingress\n```\n- Verify that HTTPS redirects function correctly by sending an HTTP request through `curl` .```\ncurl VIP\n```Replace `` with the MultiClusterIngress IP address.The output should show that the request was redirected to the HTTPS port which indicates that redirects are functioning correctly.\n- Verify that the TLS policy functions correctly by sending an HTTPS request using TLS version 1.1. Because DNS is not configured for this domain, use the `--resolve` option to tell `curl` to resolve the IP address directly.```\ncurl https://foo.example.com --resolve foo.example.com:443:VIP --cacert CERT_FILE -v\n```This step requires the certificate PEM file used to secure the MultiClusterIngress. A successful output will look similar to the following:```\n...\n* SSL connection using TLSv1.2 / ECDHE-RSA-CHACHA20-POLY1305\n* ALPN, server accepted to use h2\n* Server certificate:\n* subject: O=example; CN=foo.example.com\n* start date: Sep 1 10:32:03 2021 GMT\n* expire date: Aug 27 10:32:03 2022 GMT\n* common name: foo.example.com (matched)\n* issuer: O=example; CN=foo.example.com\n* SSL certificate verify ok.\n* Using HTTP2, server supports multi-use\n* Connection state changed (HTTP/2 confirmed)\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\n* Using Stream ID: 1 (easy handle 0x7fa10f00e400)\n> GET / HTTP/2\n> Host: foo.example.com\n> User-Agent: curl/7.64.1\n> Accept: */*\n>\n* Connection state changed (MAX_CONCURRENT_STREAMS == 100)!\n< HTTP/2 200\n< content-type: application/json\n< content-length: 308\n< access-control-allow-origin: *\n< server: Werkzeug/1.0.1 Python/3.8.6\n< date: Wed, 01 Sep 2021 11:39:06 GMT\n< via: 1.1 google\n< alt-svc: clear\n<\n{\"cluster_name\":\"gke-us\",\"host_header\":\"foo.example.com\",\"metadata\":\"foo\",\"node_name\":\"gke-gke-us-default-pool-22cb07b1-r5r0.c.mark-church-project.internal\",\"pod_name\":\"foo-75ccd9c96d-dkg8t\",\"pod_name_emoji\":\"\ud83d\udc5e\",\"project_id\":\"mark-church-project\",\"timestamp\":\"2021-09-01T11:39:06\",\"zone\":\"us-central1-b\"}\n* Connection #0 to host foo.example.com left intact\n* Closing connection 0\n```The response code is 200 and TLSv1.2 is being used which indicates that everything is functioning properly.Next you can verify that the SSL policy enforces the correct TLS version by attempting to connect with TLS 1.1. Your SSL policy must be configured for a minimum version of 1.2 for this step to work.\n- Send the same request from the previous step, but enforce a TLS version of 1.1.```\ncurl https://foo.example.com --resolve foo.example.com:443:VIP -v \\\u00a0 --cacert CERT_FILE \\\u00a0 --tls-max 1.1\n```A successful output will look similar to the following:```\n* Added foo.example.com:443:34.149.29.76 to DNS cache\n* Hostname foo.example.com was found in DNS cache\n* Trying 34.149.29.76...\n* TCP_NODELAY set\n* Connected to foo.example.com (34.149.29.76) port 443 (#0)\n* ALPN, offering h2\n* ALPN, offering http/1.1\n* successfully set certificate verify locations:\n* CAfile: cert.pem\n CApath: none\n* TLSv1.1 (OUT), TLS handshake, Client hello (1):\n* TLSv1.1 (IN), TLS alert, protocol version (582):\n* error:1400442E:SSL routines:CONNECT_CR_SRVR_HELLO:tlsv1 alert protocol version\n* Closing connection 0\ncurl: (35) error:1400442E:SSL routines:CONNECT_CR_SRVR_HELLO:tlsv1 alert protocol version\n```The failure to complete the TLS handshake indicates that the SSL policy has blocked TLS 1.1 successfully.\n### Creating a static IP\n- Allocate a static IP:```\ngcloud compute addresses create ADDRESS_NAME --global\n```Replace `` with the name of the static IP to allocate.The output contains the complete URL of the address you created, similar to the following:```\nCreated [https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/addresses/ADDRESS_NAME].\n```\n- View the IP address you just created:```\ngcloud compute addresses list\n```The output is similar to the following:```\nNAME   ADDRESS/RANGE TYPE  STATUS\nADDRESS_NAME STATIC_IP_ADDRESS EXTERNAL RESERVED\n```This output includes:- The``you defined.\n- The``allocated.\n- Update the `mci.yaml` file with the static IP:```\napiVersion: networking.gke.io/v1kind: MultiClusterIngressmetadata:\u00a0 name: whereami-ingress\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/static-ip: STATIC_IP_ADDRESSspec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 serviceName: whereami-mcs\u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\n```Replace the `` with either:- The allocated IP address, similar to:`34.102.201.47`\n- The complete URL of the address you created, similar to:`\"https://www.googleapis.com/compute/v1/projects/` `` `/global/addresses/` `` `\"`\nThe `` is not the resource name ( `` ).\n- Redeploy the `MultiClusterIngress` resource:```\nkubectl apply -f mci.yaml\n```The output is similar to the following:```\nmulticlusteringress.networking.gke.io/whereami-ingress configured\n```\n- Follow the steps in [Validating a successful deployment status](#deployment_validation) to verify that the deployment is serving on the `` .\n### Pre-shared certificates\n[Pre-shared certificates](/kubernetes-engine/docs/how-to/ingress-multi-ssl#using_pre-shared_certificates) are certificates uploaded to Google Cloud that can be used by the load balancer for TLS termination instead of certificates stored in Kubernetes Secrets. These certificates are uploaded out of band from GKE to Google Cloud and referenced by a `MultiClusterIngress` resource. Multiple certificates, either through pre-shared certs or Kubernetes secrets, are also supported.\nUsing the certificates in Multi Cluster Ingress requires the `networking.gke.io/pre-shared-certs` annotation and the names of the certs. When multiple certificates are specified for a given `MultiClusterIngress` , a predetermined order governs which cert is presented to the client.\nYou can list the available SSL certificates by running:\n```\ngcloud compute ssl-certificates list\n```\nThe following example describes client traffic to one of the specified hosts that matches the Common Name of the pre-shared certs so the respective certificate that matches the domain name will be presented.\n```\nkind: MultiClusterIngressmetadata:\u00a0 name: shopping-service\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/pre-shared-certs: \"domain1-cert, domain2-cert\"spec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 rules:\u00a0 \u00a0 \u00a0 - host: my-domain1.gcp.com\u00a0 \u00a0 \u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: domain1-svc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 443\u00a0 \u00a0 \u00a0 - host: my-domain2.gcp.com\u00a0 \u00a0 \u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: domain2-svc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 443\n```\n### Google-managed Certificates\n[Google-managed Certificates](/load-balancing/docs/ssl-certificates/google-managed-certs) are supported on `MultiClusterIngress` resources through the `networking.gke.io/pre-shared-certs` annotation. Multi Cluster Ingress supports the attachment of Google-managed certificates to a `MultiClusterIngress` resource, however unlike single-cluster Ingress, the [declarative generation of a Kubernetes ManagedCertificate resource](/kubernetes-engine/docs/how-to/managed-certs#setting_up_the_managed_certificate) is not supported on `MultiClusterIngress` resources. The original creation of the Google-managed certificate must be done directly through the `compute ssl-certificates create` API before you can attach it to a `MultiClusterIngress` . That can be done following these steps:\n- Create a Google-managed Certificate as [in step 1 here.](/load-balancing/docs/ssl-certificates/google-managed-certs#create-ssl) Don't move to step 2 as Multi Cluster Ingress will attach this certificate for you.```\ngcloud compute ssl-certificates create my-google-managed-cert \\\u00a0 \u00a0 --domains=my-domain.gcp.com \\\u00a0 \u00a0 --global\n```\n- Reference the name of the certificate in your `MultiClusterIngress` using the `networking.gke.io/pre-shared-certs` annotation.```\nkind: MultiClusterIngressmetadata:name: shopping-servicenamespace: whereamiannotations:\u00a0 networking.gke.io/pre-shared-certs: \"my-google-managed-cert\"spec:template:\u00a0 spec:\u00a0 \u00a0 rules:\u00a0 \u00a0 - host: my-domain.gcp.com\u00a0 \u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 \u00a0 - backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 serviceName: my-domain-svc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 servicePort: 8080\n```\nThe preceding manifest attaches the certificate to your `MultiClusterIngress` so that it can terminate traffic for your backend GKE clusters. Google Cloud will [automatically renew your certificate](/load-balancing/docs/ssl-certificates/google-managed-certs#renewal) prior to certificate expiry. Renewals occur transparently and does not require any updates to Multi Cluster Ingress.\n### Application protocols\nThe connection from the load balancer proxy to your application uses HTTP by default. Using `networking.gke.io/app-protocols` annotation, you can configure the load balancer to use HTTPS or HTTP/2 when it forwards requests to your application. In the `annotation` field of the following example, `http2` refers to the `MultiClusterService` port name and `HTTP2` refers to the protocol that the load balancer uses.\n```\nkind: MultiClusterServicemetadata:\u00a0 name: shopping-service\u00a0 namespace: whereami\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/app-protocols: '{\"http2\":\"HTTP2\"}'spec:\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - port: 443\u00a0 \u00a0 \u00a0 \u00a0 name: http2\n```\n### BackendConfig\nRefer to the section [above](#backendconfig_support) on how to configure the annotation.\n## What's next\n- [Read the GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- Learn more about [setting up HTTP Load Balancing with Ingress](/kubernetes-engine/docs/tutorials/http-balancer) .\n- Implement [Multi Cluster Ingress with end to end HTTPS](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/multi-cluster/mci-https-e2e) .", "guide": "Google Kubernetes Engine (GKE)"}