{"title": "Google Kubernetes Engine (GKE) - Auto-repair nodes", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair", "abstract": "# Google Kubernetes Engine (GKE) - Auto-repair nodes\nThis page provides information about node auto-repair in Google Kubernetes Engine (GKE) clusters.\n", "content": "## Overview\nhelps keep the nodes in your GKE cluster in a healthy, running state. When enabled, GKE makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended time period, GKE initiates a repair process for that node.\n## Settings for Autopilot and Standard\nAutopilot clusters always automatically repair nodes. You can't disable this setting.\nIn Standard clusters, node auto-repair is enabled by default for new node pools. You can [disable auto repair](#disable) for an existing node pool, however we recommend keeping the default configuration.\n## Repair criteria\nGKE uses the node's health status to determine if a node needs to be repaired. A node reporting a `Ready` status is considered healthy. GKE triggers a repair action if a node reports consecutive unhealthy status reports for a given time threshold. An unhealthy status can mean:\n- A node reports a`NotReady`status on consecutive checks over the given time threshold (approximately 10 minutes).\n- A node does not report any status at all over the given time threshold (approximately 10 minutes).\n- A node's boot disk is out of disk space for an extended time period (approximately 30 minutes).\nYou can manually check your node's health signals at any time by using the `kubectl get nodes` command.\n## Node repair process\nIf GKE detects that a node requires repair, the node is drained and re-created. GKE waits one hour for the drain to complete. If the drain doesn't complete, the node is shut down and a new node is created.\nIf multiple nodes require repair, GKE might repair nodes in parallel. GKE balances the number of repairs depending on the size of the cluster and the number of broken nodes. GKE will repair more nodes in parallel on a larger cluster, but fewer nodes as the number of unhealthy nodes grows.\nIf you disable node auto-repair at any time during the repair process, in- progress repairs are cancelled and continue for any node currently under repair.\n**Note:** Modifications on the boot disk of a node VM do not persist across node re-creations. To preserve modifications across node re-creation, use a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) .\n**Note:** Node auto-repair uses a set of signals, including signals from the [Node Problem Detector](https://github.com/kubernetes/node-problem-detector) . The Node Problem Detector is enabled by default on nodes that use [Container-Optimized OS](/container-optimized-os/docs/how-to/monitoring) and Ubuntu images.\n### Node repair history\nGKE generates a log entry for automated repair events. You can check the logs by running the following command:\n```\ngcloud container operations list\n```\n### Node auto repair in TPU nodes\nIf you use [multi-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) , which is a node pool that contains two or more interconnected TPU VMs, the node auto repair criteria is different. If a TPU node in a multi-host TPU slice node pool is unhealthy and requires auto repair, the node pool is recreated. To learn more about the TPU node conditions, see [TPU node auto repair](/kubernetes-engine/docs/how-to/tpus#node-auto-repair) .\n## Enable auto-repair for an existing Standard node pool\nYou enable node auto-repair on a basis.\nIf auto-repair is disabled on an existing node pool in a Standard cluster, use the following instructions to enable it:\n```\ngcloud container node-pools update POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --enable-autorepair\n```\nReplace the following:- ``: the name of your node pool.\n- ``: the name of your Standard cluster.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the cluster. For zonal clusters, use the`--zone` ``option.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to modify.\n- On the **Node pool details** page, click **Edit** .\n- Under **Management** , select the **Enable auto-repair** checkbox.\n- Click **Save** .## Verify node auto-repair is enabled for a Standard node pool\nNode auto-repair is enabled on a basis. You can verify that a node pool in your cluster has node auto-repair enabled with the Google Cloud CLI or the Google Cloud console.\nDescribe the node pool:\n```\ngcloud container node-pools describe NODE_POOL_NAME \\--cluster=CLUSTER_NAME\n```\nIf node auto-repair is enabled, the output of the command will include these lines:\n```\nmanagement:\n ...\n autoRepair: true\n```- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- On the Google Kubernetes Engine page, click the name of the cluster of the node pool you want to inspect.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to inspect.\n- Under **Management** , in the **Auto-repair** field, verify that auto-repair is enabled.## Disable node auto-repair\nYou can disable node auto-repair for an existing node pool in a Standard cluster by using the gcloud CLI or the Google Cloud console.\n**Note:** You can only disable auto-repair with the gcloud CLI for a node pool in a Standard cluster enrolled in a release channel.\n```\ngcloud container node-pools update POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --no-enable-autorepair\n```\nReplace the following:- ``: the name of your node pool.\n- ``: the name of your Standard cluster.\n- ``: the [Compute Engine region](/compute/docs/regions-zones#available) for the cluster. For zonal clusters, use the`--zone` ``option.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to modify.\n- On the **Node pool details** page, click **Edit** .\n- Under **Management** , clear the **Enable auto-repair** checkbox.\n- Click **Save** .## What's next\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .", "guide": "Google Kubernetes Engine (GKE)"}