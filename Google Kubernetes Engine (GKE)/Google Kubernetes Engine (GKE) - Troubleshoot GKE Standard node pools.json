{"title": "Google Kubernetes Engine (GKE) - Troubleshoot GKE Standard node pools", "url": "https://cloud.google.com/kubernetes-engine/docs/troubleshooting/node-pools", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshoot GKE Standard node pools\nThis page shows you how to resolve issues with GKE Standard mode node pools.\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## Node pool creation issues\nThis section lists issues that might occur when creating new node pools in Standard clusters and provides suggestions for how you might fix them.\n### Node pool creation fails due to resource availability\nThe following issue occurs when you create a node pool with specific hardware in a Google Cloud zone that doesn't have enough hardware available to meet your requirements.\nTo validate that node pool creation failed because a zone didn't have enough resources, check your logs for relevant error messages.\n- Go to Logs Explorer in the Google Cloud console: [Go to Logs Explorer](https://console.cloud.google.com/logs/query) \n- In the **Query** field, specify the following query:```\nlog_id(cloudaudit.googleapis.com/activity)\nresource.labels.cluster_name=\"CLUSTER_NAME\"\nprotoPayload.status.message:(\"ZONE_RESOURCE_POOL_EXHAUSTED\" OR \"does not have enough resources available to fulfill the request\" OR \"resource pool exhausted\" OR \"does not exist in zone\")\n```Replace `` with the name of your GKE cluster.\n- Click **Run query** .\nYou might see one of the following error messages:\n- `resource pool exhausted`\n- `The zone does not have enough resources available to fulfill the request. Try a different zone, or try again later.`\n- `ZONE_RESOURCE_POOL_EXHAUSTED`\n- `ZONE_RESOURCE_POOL_EXHAUSTED_WITH_DETAILS`\n- `Machine type with name '<code><var>MACHINE_NAME</var></code>' does not exist in zone '<code><var>ZONE_NAME</var></code>'`To resolve this issue, try the following suggestions:\n- Ensure that the selected Google Cloud region or zone has the specific hardware that you need. Use the [Compute Engine availability table](/compute/docs/regions-zones#available) to check whether specific zones support specific hardware. Choose a different Google Cloud region or zone for your nodes that might have better availability of the hardware that you need.\n- Create the node pool with smaller machine types. Increase the number of nodes in the node pool so that the total compute capacity remains the same.\n- Use [Compute Engine capacity reservation](/compute/docs/instances/reservations-overview) to reserve the resources in advance.\n- Use best-effort provisioning, described in the following section, to successfully create the node pool if it can provision at least a specified minimum number of nodes out of the requested number.\n**Best-effort provisioning**\nFor certain hardware, you can use , which tells GKE to successfully create the node pool if it can provision at least a specified minimum number of nodes. GKE continues attempting to provision the remaining nodes to satisfy the original request over time. To tell GKE to use best-effort provisioning, use the following command:\n```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --node-locations=ZONE1,ZONE2,... \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE\u00a0 \u00a0 --best-effort-provision \\\u00a0 \u00a0 --min-provision-nodes=MINIMUM_NODES\n```\nReplace the following:\n- ``: the name of the new node pool.\n- ``: the Compute Engine zones for the nodes. These zones must support the selected hardware.\n- ``: the Compute Engine engine machine type for the nodes. For example,`a2-highgpu-1g`.\n- ``: the minimum number of nodes for GKE to provision and successfully create the node pool. If omitted, the default is`1`.\n**Note:** Best-effort provisioning is not available for TPUs and for Standard nodes that use a [compact placement policy](/kubernetes-engine/docs/how-to/compact-placement#compact-placement-standard) .\nFor example, consider a scenario in which you need 10 nodes with attached NVIDIA A100 40GB GPUs in `us-central1-c` . According to the [GPU regions and zones availability table](/compute/docs/gpus/gpu-regions-zones#gpu_regions_and_zones) , this zone supports A100 GPUs. To avoid node pool creation failure if 10 GPU machines aren't available, you use best-effort provisioning.\n```\ngcloud container node-pools create a100-nodes \\\n --cluster=ml-cluster \\\n --node-locations=us-central1-c \\\n --num-nodes=10 \\\n --machine-type=a2-highgpu-1g \\\n --accelerator=type=nvidia-tesla-a100,count=1 \\\n --best-effort-provision \\\n --min-provision-nodes=5\n```\nGKE creates the node pool even if only five GPUs are available in `us-central1-c` . Over time, GKE attempts to provision more nodes until there are 10 nodes in the node pool.\n## Migrate workloads between node pools\nUse the following instructions to migrate workloads from one node pool to another node pool. If you want to change the machine attributes of the nodes in your node pool, see [Vertically scale by changing the node machineattributes](/kubernetes-engine/docs/how-to/node-pools#change-machine-attributes) .\n### How to migrate Pods to a new node pool\nTo migrate Pods to a new node pool, you must do the following:\n- **Cordon the existing node pool:** This operation marks the nodes in the existing node pool as . Kubernetes stops scheduling new Pods to these nodes once you mark them as .\n- **Drain the existing node pool:** This operation evicts the workloads running on the nodes of the existing node pool gracefully.\nThese steps cause Pods running in your existing node pool to gracefully terminate. Kubernetes reschedules them onto other available nodes.\n**Note:** If your workload has a LoadBalancer service with `externalTrafficPolicy` set to `Local` , then cordoning the existing node pool might cause the workload to become unreachable. Either change the `externalTrafficPolicy` to `Cluster` or ensure the workload is re-deployed into the new node pool before cordoning the existing pool.\nTo make sure Kubernetes terminates your applications gracefully, your containers should handle the [SIGTERM](http://man7.org/linux/man-pages/man7/signal.7.html) signal. Use this approach to close active connections to clients and commit or rollback database transactions in a clean way. In your Pod manifest, you can use the `spec.terminationGracePeriodSeconds` field to specify how long Kubernetes must wait before stopping containers in the Pod. This defaults to 30 seconds. You can read more about [Podtermination](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) in the Kubernetes documentation.\nYou can cordon and drain nodes using the `kubectl cordon` and `kubectl drain` commands.\n**Note:** Kubernetes does not reschedule Pods that are not managed by a controller such as Deployment, ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet. Such Pods prevent `kubectl drain` commands from running, therefore you must deploy your Pods using these controllers. Run `kubectl drain` with `--force` option to clean up some GKE system Pods, such as kube-proxy and fluend-cloud-logging.\n### Create node pool and migrate workloads\nTo migrate your workloads to a new node pool, create the new node pool, then cordon and drain the nodes in the existing node pool:\n- [Add a node pool](/kubernetes-engine/docs/how-to/node-pools#add) to your cluster.Verify that the new node pool is created by running the following command:```\ngcloud container node-pools list --cluster CLUSTER_NAME\n```\n- Run the following command to see which node the Pods are running on (see the `NODE` column):```\nkubectl get pods -o=wide\n```\n- Get a list of nodes in the existing node pool, replacing `` with the name:```\nkubectl get nodes -l cloud.google.com/gke-nodepool=EXISTING_NODE_POOL_NAME\n```\n- Run the `kubectl cordon` `` command (substitute `` with the names from the previous command). The following shell command iterates each node in the existing node pool and marks them as unschedulable:```\nfor node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=EXISTING_NODE_POOL_NAME -o=name); do\u00a0 kubectl cordon \"$node\";done\n```\n- Optionally, update your workloads running on the existing node pool to add a nodeSelector for the label `cloud.google.com/gke-nodepool:NEW_NODE_POOL_NAME` , where `` is the name of the new node pool. This ensures that GKE places those workloads on nodes in the new node pool.\n- Drain each node by evicting Pods with an allotted graceful termination period of 10 seconds:```\nfor node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=EXISTING_NODE_POOL_NAME -o=name); do\u00a0 kubectl drain --force --ignore-daemonsets --delete-emptydir-data --grace-period=GRACEFUL_TERMINATION_SECONDS \u00a0\"$node\";done\n```Replace `` with the required amount of time for graceful termination.\n- Run the following command to see that the nodes in the existing node pool have `SchedulingDisabled` status in the node list:```\nkubectl get nodes\n```Additionally, you should see that the Pods are now running on the nodes in the new node pool:```\nkubectl get pods -o=wide\n```\n- Delete the existing node pool if don't need it anymore:```\ngcloud container node-pools delete default-pool --cluster CLUSTER_NAME\n```## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}