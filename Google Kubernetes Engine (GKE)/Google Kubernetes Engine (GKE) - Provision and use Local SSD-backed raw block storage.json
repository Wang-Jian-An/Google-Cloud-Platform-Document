{"title": "Google Kubernetes Engine (GKE) - Provision and use Local SSD-backed raw block storage", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd-raw", "abstract": "# Google Kubernetes Engine (GKE) - Provision and use Local SSD-backed raw block storage\nThis page explains how to provision Local SSD storage on Google Kubernetes Engine (GKE) clusters, and how to configure workloads to consume data from Local SSD-backed raw block storage attached to nodes in your cluster.\nUsing this Local SSD option gives you more control over the underlying storage and lets you build your own node-level cache for Pods to deliver better performance for your applications. You can also customize this option by installing a file system on Local SSD disks by [running a DaemonSet to configure RAID andformat disks](#run-local-volume-static-provisioner) as needed.\n**Note:** Local SSD volumes require machine type `n1-standard-1` or larger; the default machine type, `e2-medium` is not supported. To learn more, see [machine types](/compute/docs/machine-resource) in the Compute Engine documentation.\nTo learn more about Local SSD support for raw block access on GKE, see [About local SSDs](/kubernetes-engine/docs/concepts/local-ssd#block-access) .\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Create a cluster or node pool with Local SSD-backed raw block storage\nUse the gcloud CLI with the `--local-nvme-ssd-block` option to create a cluster with Local SSD-backed raw block storage.\n**Note:** To use Local SSD-backed raw block storage, you must manually configure the devices. Local SSD disks are not formatted for RAID. You are also responsible for ensuring capacity on nodes and handling potential disruptions from co-located workloads. If you use [local PersistentVolume](https://kubernetes.io/docs/concepts/storage/volumes/#local) , scheduling is integrated. If you don't need data to persist across the Pod lifecycle, or if you don't have special performance requirements, use the [--ephemeral-storage-local-ssd](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd) option which should fit most use cases.\nThe gcloud CLI command you run to create the cluster or node pool depends on which [machine series generation](/kubernetes-engine/docs/concepts/local-ssd#machine) the machine type you are using belongs to. For example, N1 and N2 machine types belong to a first and second generation machine series respectively, while C3 machine types belong to a third generation machine series.\n### Create a cluster with Local SSD\nIf you use a machine type from a first or second generation machine series, you create your cluster by specifying the `--local-nvme-ssd-block count=` `` option. The option specifies the number of [Local SSD disks](/compute/docs/disks/local-ssd) to attach to each node. The maximum number [varies by machine type and region](/compute/docs/disks#local_ssd_machine_type_restrictions) .\nTo create a cluster:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --local-nvme-ssd-block count=NUMBER_OF_DISKS \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --release-channel CHANNEL_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the number of Local SSD disks to provision on each node. The maximum number of disks [varies by machine type and region](/compute/docs/disks#local_ssd_machine_type_restrictions) .\n- ``: the first or second generation machine type to use. Specifying this field is required, because you can't use Local SSDs with the default`e2-medium`type.\n- ``: a [release channel](/kubernetes-engine/docs/concepts/release-channels) that includes GKE versions later than 1.25.3-gke.1800.\nIf you use a machine type from a third generation machine series, use the `--local-nvme-ssd-block` option, without a count field, to create a cluster. GKE automatically provisions Local SSD capacity for your cluster based on the VM shape. The maximum number [varies by machine type and region](/compute/docs/disks#local_ssd_machine_type_restrictions) .\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --cluster-version CLUSTER_VERSION \\\u00a0 \u00a0 --local-nvme-ssd-block\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the machine type to use from a third generation machine series.\n- ``: a [GKE cluster version](/kubernetes-engine/docs/concepts/local-ssd#3gen) that supports Local SSD on machines types from a third generation machine series.\n### Create a node pool with Local SSD\nTo create a node pool that uses Local SSD disks for raw block access, run the following command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --local-nvme-ssd-block count=NUMBER_OF_DISKS\n```\nReplace the following:- ``: the name of your new node pool.\n- ``: the name of the cluster.\n- ``: the first or second generation machine type to use. Specifying this field is required, as Local SSD cannot be used with the default`e2-medium`type.\n- ``: the number of Local SSD disks to provision on each node. The maximum number of disks [varies by machine type and region](/compute/docs/disks#local_ssd_machine_type_restrictions) .\nIf you use a machine type from a third generation machine series, use the `--local-nvme-ssd-block` option, without a count field, to create a cluster:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --node-version NODE_VERSION \\\u00a0 \u00a0 --local-nvme-ssd-block\n```\nReplace the following:- ``: the name of the new node pool.\n- ``: the name of the cluster.\n- ``: the machine type to use from a third generation machine type.\n- ``: a [GKE node pool version](/kubernetes-engine/docs/concepts/local-ssd#3gen) that supports Local SSD on machine types from a third generation machine series.\nNodes in the node pool are created with a `cloud.google.com/gke-local-nvme-ssd=true` label. You can verify the labels by running the following command:\n```\nkubectl describe node NODE_NAME\n```\nFor each Local SSD attached to the node pool, the host OS creates a symbolic link (symlink) to access the disk under an ordinal folder, and a symlink with a universally unique identifier (UUID). For example, if you create a node pool with three local SSDs using the `--local-nvme-ssd-block` option, the host OS creates the following symlinks for the disks:\n- `/dev/disk/by-id/google-local-ssd-block0`\n- `/dev/disk/by-id/google-local-ssd-block1`\n- `/dev/disk/by-id/google-local-ssd-block2`\nCorrespondingly, the host OS also creates the following symlinks with UUIDs for the disks:\n- `/dev/disk/by-uuid/google-local-ssds-nvme-block/local-ssd-` ``\n- `/dev/disk/by-uuid/google-local-ssds-nvme-block/local-ssd-` ``\n- `/dev/disk/by-uuid/google-local-ssds-nvme-block/local-ssd-` ``\nThis ensures that the disks can be accessed using a unique identifier.\n## Access Local SSD volumes\nThe following example shows how you can access Local SSD-backed raw block storage.\n### Local PersistentVolumes\nLocal SSD volumes can be mounted as Pods using [PersistentVolumes](/kubernetes-engine/docs/concepts/persistent-volumes) .\nYou can create PersistentVolumes from Local SSD by manually creating a PersistentVolume, or by running the [local volume static provisioner](#run-local-volume-static-provisioner) .\n**Note:** When using [release channels](/kubernetes-engine/docs/concepts/release-channels) , auto-upgrade and auto-repair cannot be disabled. We do not recommend using local PersistentVolumes with clusters in a release channel.\n- [Cluster autoscaling](/kubernetes-engine/docs/concepts/cluster-autoscaler) and [dynamic provisioning](/kubernetes-engine/docs/concepts/persistent-volumes#dynamic_provisioning) are not supported with local PersistentVolumes.\n- Upgrading a GKE cluster or repairing nodes deletes the Compute Engine instances, which also deletes all data on the Local SSD disks.\n- Don't enable [node auto-upgrades](/kubernetes-engine/docs/concepts/node-auto-upgrades) or [node auto-repair](/kubernetes-engine/docs/concepts/node-auto-repair) for clusters or node pools using Local SSD for persistent data. You must back up your application data first, then restore the data to a new cluster or node pool.\n**Caution:** Clusters and node pools created with the Google Cloud console have node auto-upgrade enabled by default. To disable, refer to [node auto-upgrades](/kubernetes-engine/docs/concepts/node-auto-upgrades) .\n- Local PersistentVolume objects are not automatically cleaned up when a node is deleted, upgraded, repaired, or scaled down. We recommend that you periodically scan and delete stale Local PersistentVolume objects associated with deleted nodes.You can manually create a PersistentVolume for each Local SSD on each node in your cluster.\nUse the `nodeAffinity` field in a PersistentVolume object to reference a Local SSD on a specific node. The following example shows the PersistentVolume specification for Local SSD on nodes running Linux:\n```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 name: \"example-local-pv\"spec:\u00a0 capacity:\u00a0 \u00a0 storage: 375Gi\u00a0 accessModes:\u00a0 - \"ReadWriteOnce\"\u00a0 persistentVolumeReclaimPolicy: \"Retain\"\u00a0 storageClassName: \"local-storage\"\u00a0 local:\u00a0 \u00a0 path: \"/mnt/disks/ssd0\"\u00a0 nodeAffinity:\u00a0 \u00a0 required:\u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 - key: \"kubernetes.io/hostname\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: \"In\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"gke-test-cluster-default-pool-926ddf80-f166\"\n```\nIn this example, the Local SSD disks are manually configured for RAID and formatted, then mounted at `/mnt/disks/ssd0` on node `gke-test-cluster-default-pool-926ddf80-f166` . The nodeAffinity field is used to help assign workloads to nodes with Local SSDs that are manually configured for RAID. If you only have one node in your cluster or if you've configured RAID for all nodes, the nodeAffinity field is not needed.\nThe corresponding PersistenVolumeClaim specification looks like the following:\n```\n\u00a0 kind: PersistentVolumeClaim\u00a0 apiVersion: v1\u00a0 metadata:\u00a0 \u00a0 name: ssd-local-claim\u00a0 spec:\u00a0 \u00a0 accessModes:\u00a0 \u00a0 - ReadWriteOnce\u00a0 \u00a0 storageClassName: local-storage\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 storage: 37Gi\n```\nIf you delete the PersistentVolume, you must manually erase the data from the disk.\n**Note:** The local volume static provisioner is a community project and not maintained or supported by Google. It also cannot be used with Windows.\nYou can create PersistentVolumes for Local SSD automatically using the local volume static provisioner. The provisioner is a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) that manages the Local SSD disks on each node, creates and deletes the PersistentVolumes for them, and cleans up the data on the Local SSD disks when the PersistentVolume is released.\nTo run the local volume static provisioner:\n- Use a DaemonSet to configure RAID and format the disks:- Download the [gke-daemonset-raid-disks.yaml](https://raw.githubusercontent.com/kubernetes-sigs/sig-storage-local-static-provisioner/master/examples/gke-daemonset-raid-disks.yaml) specification.\n- Deploy the RAID disks DaemonSet. The DaemonSet sets a RAID 0 array on all Local SSD disks and formats the device to an `ext4` filesystem.```\nkubectl create -f gke-daemonset-raid-disks.yaml\n```\n- Download the [gke-nvme-ssd-block-raid.yaml](https://raw.githubusercontent.com/kubernetes-sigs/sig-storage-local-static-provisioner/master/helm/generated_examples/gke-nvme-ssd-block-raid.yaml) specification, and modify the specification's namespace fields as needed.The specification includes these resources:- ServiceAccount for the provisioner\n- ClusterRole and ClusterRoleBindings for permissions to:- Create and Delete PersistentVolume objects\n- Get Node objects\n- ConfigMap with provisioner settings for GKE\n- DaemonSet for running the provisioner\n- Deploy the provisioner:```\nkubectl create -f gke-nvme-ssd-block-raid.yaml\n```After the provisioner is running successfully, it creates a PersistentVolume object for the RAID Local SSD device in the cluster.\n- Save the following PersistentVolumeClaim manifest as `provisioner-pvc-example.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: PVC_NAMEspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 50Gi\u00a0 storageClassName: nvme-ssd-block\n```Replace `` with the name of your PersistentVolumeClaim.\n- Create the PersistentVolumeClaim:```\nkubectl create -f provisioner-pvc-example.yaml\n```\n- Save the following Pod manifest as `provisioner-pod-example.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: POD_NAMEspec:\u00a0 containers:\u00a0 - name: \"shell\"\u00a0 \u00a0 image: \"ubuntu:14.04\"\u00a0 \u00a0 command: [\"/bin/sh\", \"-c\"]\u00a0 \u00a0 args: [\"echo 'hello world' > /cache/test.txt && sleep 1 && cat /cache/test.txt && sleep 3600\"]\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 - mountPath: /cache\u00a0 \u00a0 \u00a0 name: local-ssd-storage\u00a0 volumes:\u00a0 - name: local-ssd-storage\u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 claimName: PVC_NAME\n```Replace `` with the name of your Pod.\n- Create the Pod:```\nkubectl create -f provisioner-pod-example.yaml\n```**Note:** If you followed the instructions to [run the local volume static provisioner](#run-local-volume-static-provisioner) , its manifest provides an appropriate StorageClass definition. **If you use thelocal volume static provisioner, you do not need to perform the steps in thissection.**\nFor improved scheduling, we recommend that you also create a StorageClass with `volumeBindingMode: WaitForFirstConsumer` . This delays PersistentVolumeClaim binding until Pod scheduling, so that a Local SSD is chosen from an appropriate node that can run the Pod. This enhanced scheduling behavior considers Pod CPU and memory requests, node affinity, Pod affinity and anti-affinity, and multiple PersistentVolumeClaim requests, along with which nodes have available local SSDs, when selecting a node for a runnable Pod.\nThis example uses delayed volume binding mode:\n```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: \"local-nvme\"provisioner: \"kubernetes.io/no-provisioner\"volumeBindingMode: \"WaitForFirstConsumer\"\n```\nTo create a StorageClass with delayed binding, save the YAML manifest to a local file and apply it to the cluster using the following command:\n```\nkubectl apply -f filename\n```\n## Troubleshooting\nFor troubleshooting instructions, refer to [Troubleshooting storage in GKE](/kubernetes-engine/docs/troubleshooting/troubleshooting-gke-storage#local-ssds) .\n## What's next\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .", "guide": "Google Kubernetes Engine (GKE)"}