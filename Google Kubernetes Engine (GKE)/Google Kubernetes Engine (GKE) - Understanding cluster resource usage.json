{"title": "Google Kubernetes Engine (GKE) - Understanding cluster resource usage", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-usage-metering", "abstract": "# Google Kubernetes Engine (GKE) - Understanding cluster resource usage\nThis page explains how to use GKE usage metering to understand the usage profiles of Google Kubernetes Engine (GKE) clusters, and tie usage to individual teams or business units within your organization. GKE usage metering has no impact on billing for your project; it lets you understand resource usage at a granular level.\n**Note:** We recommend that you use [GKE cost allocation](/kubernetes-engine/docs/how-to/cost-allocations) instead of GKE usage metering. GKE cost allocation lets you distribute the costs of a cluster to its users.\n", "content": "## Overview\nGKE usage metering tracks information about the resource requests and actual resource usage of your cluster's workloads. Currently, GKE usage metering tracks information about CPU, GPU, TPU, memory, storage, and optionally network egress. You can differentiate resource usage by using Kubernetes [namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) , [labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) , or a combination of both.\nData is stored in BigQuery, where you can query it directly or export it for analysis with external tools such as [Looker Studio](/bigquery/docs/visualize-looker-studio) .\nGKE usage metering is helpful for scenarios such as the following:\n- Tracking per-tenant resource requests and actual resource consumption in a multi-tenant cluster where each tenant operates within a given namespace.\n- Determining the resource consumption of a workload running in a given cluster, by assigning a unique label to the Kubernetes objects associated with the workload.\n- Identifying workloads whose resource requests differ significantly from their actual resource consumption, so that you can more efficiently allocate resources for each workload.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Limitations\nYou can use the sample BigQuery queries and Looker Studio template to join GKE usage metering data with exported Google Cloud billing data in BigQuery. This lets you estimate a cost breakdown by cluster, namespace, and labels.\nGKE usage metering data is purely advisory, and doesn't affect your Google Cloud bill. For billing data, your Google Cloud billing invoice is the sole source of truth.\nThe following limitations apply:\n- Special contract discounts or credits are not accounted for.\n- Resources created outside the scope of GKE are not tracked by namespace or label.\n- Only labels from Pod and PersistentVolumeClaim objects are tracked by usage reporting.\n- Only [dynamically provisioned PersistentVolumes](/kubernetes-engine/docs/concepts/persistent-volumes#dynamic_provisioning) are supported.\n- Only pd-standard and pd-ssd [disk types](/compute/docs/disks#disk-types) are supported. GKE usage metering might include costs for regional versions of both disk types under the same SKU.\n- Looker Studio does not support the visualization of [machine types capable of bursting](/compute/docs/machine-types#cpu-bursting) .\n- You can only export data to a BigQuery dataset that is in the same project as your cluster.\n- You must not use ports 27304, 47082 and 47083, because these ports are reserved by network egress tracking.\n- Custom`StorageClass`objects are not supported.\n- Network egress metering is not supported for Windows Server nodes.\n- Network egress metering is not supported for Shared VPC or VPC Network Peering.\n- Network egress metering is not supported for clusters with more than 150 nodes.\n### Prerequisites\nBefore you use GKE usage metering, you must meet the following prerequisites:\n- To track actual resource consumption, the cluster must use GKE 1.14.2-gke.3 or later.\n- If you are using [E2 or N2 machine types](/compute/docs/machine-types#machine_types) , the cluster version must be GKE 1.15.11-gke.9 or later.\n- [Billing export for BigQuery](/billing/docs/how-to/export-data-bigquery) is enabled. Charges are associated with [BigQuery usage](/bigquery/pricing) .\n- Version 250.0.0 or later of the`gcloud`command is required. Use`gcloud --version`to check.\n- You must [enable the BigQuery API](/bigquery/docs/enable-transfer-service#creating_a_project_and_enabling_the_api) in your Google Cloud project. If you first enabled GKE after July 2018, the API is already enabled.## Enable GKE usage metering\nTo enable GKE usage metering, you first [create a BigQuery dataset](#create-dataset) for either a single cluster, multiple clusters in the project, or the entire project. For more information about choosing a mapping between datasets and clusters, see [Choosing one or more BigQuery datasets](#dataset-cluster-mapping) .\nNext, you [enable GKE usage metering](#enable-usage-metering) when creating a new cluster or by modifying an existing cluster.\nOptionally, you can create a [Looker Studio dashboard](#view_in_data-studio) to visualize the resource usage of your clusters.\n### Create the BigQuery dataset\nTo use GKE usage metering for clusters in your Google Cloud project, you first create the BigQuery dataset, and then configure clusters to use it. You can use a single BigQuery dataset to store information about resource usage for multiple clusters in the same project.\nVisit [Creating Datasets](/bigquery/docs/datasets#create-dataset) for more details. Set the `Default table expiration` for the dataset to `Never` so that the table doesn't expire. If a table expires, it is recreated automatically as an empty table.\n**Note:** This feature writes to the BigQuery dataset that you provide with the Google service account ( `service-` `` `@container-engine-robot.iam.gserviceaccount.com` ) and with the [Kubernetes Engine Service Agent role](/iam/docs/understanding-roles#service-agent-roles-roles) .\n**Warning:** If you delete a BigQuery dataset or table that a cluster is using to log GKE usage metering data, Cloud Logging shows transient warnings such as `Failed to upload a record to BigQuery` . To resolve the warning, re-create the dataset or configure the cluster to use a different dataset. Your historical data will be lost.\n### Enable GKE usage metering for a cluster\nYou can enable GKE usage metering on a new or existing cluster by using either the `gcloud` command or the Google Cloud console.\nEnabling GKE usage metering also enables resource consumption metering by default. To selectively disable resource consumption metering while continuing to track resource requests, see the specific instructions for enabling GKE usage metering using the `gcloud` command, in this topic.\nNetwork egress metering is disabled by default. To enable it, see the caveats and instructions in [Optional: Enabling network egress metering](#enable-network-egress-metering) in this topic.\nYou can create a cluster by using the gcloud CLI or the Google Cloud console.\nTo create a cluster with GKE usage metering enabled, run the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --resource-usage-bigquery-dataset RESOURCE_USAGE_DATASET\n```\nReplace the following:- ``: the name of your GKE cluster.\n- ``: the name of your BigQuery dataset.\nResource consumption metering is enabled by default. To disable it and only track resource requests, add the flag `--no-enable-resource-consumption- metering` to the preceding command. **You also need to modify the example queriesin the rest of this topic so that they do not query for resourceconsumption.** \n **Note:** Network egress metering is disabled by default. To enable it, see the caveats and instructions in [Optional: Enabling network egress metering](#enable-network-egress-metering) in this topic.\nIf needed, the required tables are created within the BigQuery dataset when the cluster starts.\nTo create a cluster with GKE usage metering enabled:\n **Note:** When using the Google Cloud console, it is not possible to enable GKE usage metering while selectively disabling resource consumption metering. If you need to do this, use the `gcloud` instructions instead.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- From the navigation pane, under **Cluster** , click **Features** .\n- Select **Enable GKE usage metering** .\n- Enter the name of your **BigQuery dataset** .\n- Optional: select **Enable network egress metering** after reviewing the caveats and instructions in [Optional: Enabling network egress metering](#enable-network-egress-metering) .\n- Continue configuring your cluster, then click **Create** .To enable GKE usage metering on an existing cluster, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --resource-usage-bigquery-dataset RESOURCE_USAGE_DATASET\n```\nResource consumption metering is enabled by default. To disable it and only track resource requests, add the flag `--no-enable-resource-consumption- metering` to the preceding command. **You also need to modify the example queries inthe rest of this topic so that they do not query for resource consumption.** \n **Note:** Network egress metering is disabled by default. To enable it, see the caveats and instructions in [Optional: Enabling network egress metering](#enable-network-egress-metering) in this topic.\nYou can also change the dataset an existing cluster uses to store its usage metering data by changing the value of the `--resource-usage-bigquery-dataset` flag.\nIf needed, a table is created within the BigQuery dataset when the cluster is updated.\n **Note:** When using Google Cloud console, it is not possible to enable GKE usage metering while selectively disabling resource consumption metering. If you need to do this, use the `gcloud` instructions instead.- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to the cluster you want to modify, click **Actions** , then click **Edit** .\n- Under **Features** , click **Edit** next to **GKE usage metering** .\n- Select **Enable GKE usage metering** .\n- Enter the name of the BigQuery dataset.\n- Optional: select **Enable network egress metering** after reviewing the caveats and instructions in [Optional: Enabling network egress metering](#enable-network-egress-metering) .\n- Click **Save Changes** ..By default, network egress data is not collected or exported. Measuring network egress requires a network metering agent (NMA) running on each node. The NMA runs as a privileged Pod, consumes some resources on the node (CPU, memory, and disk space), and enables the [nf_conntrack_acct sysctl flag](https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt) on the kernel (for connection tracking flow accounting).\nIf you are comfortable with these caveats, you can enable network egress tracking for use with GKE usage metering. To enable network egress tracking, include the `--enable-network-egress-metering` option when creating or updating your cluster, or select **Enable network egress metering** when enabling GKE usage metering in the Google Cloud console.\nTo disable network egress metering, add the flag `--no-enable-network-egress-metering` when updating your cluster with the [command line](#existing) . Alternatively, you can clear **Enable network egress metering** in the GKE usage metering section of the cluster in the Google Cloud console.\n## Verify that GKE usage metering is enabled\nTo verify that GKE usage metering is enabled on a cluster, and to confirm which BigQuery dataset stores the cluster's resource usage data, run the following command:\n```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --format=\"value(resourceUsageExportConfig)\"\n```\nThe output is empty if GKE usage metering is not enabled, and otherwise shows the BigQuery dataset used by the cluster, as in the following example output:\n```\nbigqueryDestination={u'datasetId': u'test_usage_metering_dataset'}\n```\n## Choose one or more BigQuery datasets\nA dataset can hold GKE usage metering data for one or more clusters in your project. Whether you use one or many datasets depends on your security needs:\n- A single dataset for the entire project simplifies administration.\n- A dataset per cluster lets you to delegate granular access to the datasets.\n- A dataset per related group of clusters lets you to find the right mix of simplicity and granularity for your needs.## Visualize GKE usage metering data using a Looker Studio dashboard\nYou can visualize your GKE usage metering data using a [Looker Studio dashboard](/bigquery/docs/visualize-looker-studio) . This lets you to filter your data by cluster name, namespace, or label. You can also adjust the reporting period dynamically. If you have experience with Looker Studio and BigQuery, you can create a customized dashboard. You can also clone a dashboard that we created specifically for GKE usage metering.\n**Note:** You may see discrepancies between GKE usage metering data and Cloud Billing data, due to upload latency. Batches of Cloud Billing data take up to 5 hours to appear in BigQuery, while GKE usage metering data appears in BigQuery roughly every hour.\nYou can [use the dashboard](#use_dashboard) to visualize resource requests and consumption on your clusters over time.\n**Note:** Looker Studio is not supported by Cloud Customer Care. For more information, see the [Looker Studio Help Center](https://support.google.com/looker-studio) .\n### Prerequisites\n- Enable [Exporting Google Cloud billing data to BigQuery](/billing/docs/how-to/export-data-bigquery) if it is not already enabled.During this process, you create a dataset, but the table within the dataset can take up to 5 hours to appear and start populating. When the table appears, its name is `gcp_billing_export_v1_` `` .\n- Enable [GKE usage metering](/kubernetes-engine/docs/how-to/cluster-usage-metering#enabling) on at least one cluster in the project. Note the name you chose for the BigQuery dataset.\n- Enable [Looker Studio](https://lookerstudio.google.com/c/navigation/reporting) if it's not already enabled.\n- Gather the following information, which is needed to configure the dashboard:- Cloud Billing export dataset ID and data table\n- GKE usage metering dataset ID\n- Ensure that you have version 2.0.58 or later of the [BigQuery CLI](https://cloud.google.com/bigquery/docs/bq-command-line-tool) . To check the version, run `bq version` , and `gcloud components update` to update your BigQuery CLI.\n- The commands in this section should be run in a Linux terminal or in Cloud Shell.\n### Create the BigQuery cost breakdown table\n- Download one of the following query templates:- If you enabled consumption metering, download [this template](/static/kubernetes-engine/docs/how-to/usage_metering_query_template_request_and_consumption.sql) .\n- If you didn't enable consumption metering, download [this template](/static/kubernetes-engine/docs/how-to/usage_metering_query_template_request_only.sql) .\nIf you are using Cloud Shell, copy this file into the directory where you perform the following commands.\n- Run the following command to set environment variables:```\nexport GCP_BILLING_EXPORT_TABLE_FULL_PATH=YOUR_BILLING_EXPORT_TABLE_PATHexport USAGE_METERING_PROJECT_ID=YOUR_USAGE_METERING_PROJECT_IDexport USAGE_METERING_DATASET_ID=YOUR_USAGE_METERING_DATASET_IDexport USAGE_METERING_START_DATE=YOUR_USAGE_METERING_START_DATEexport COST_BREAKDOWN_TABLE_ID=YOUR_COST_BREAKDOWN_TABLE_IDexport USAGE_METERING_QUERY_TEMPLATE=YOUR_TEMPLATE_PATHexport USAGE_METERING_QUERY=YOUR_RENDERED_QUERY_PATH\n```Replace the following:- ``: the path to your generated billing export table. This table has a name similar to`PROJECT_ID.DATASET_ID.gcp_billing_export_v1_xxxx`.\n- ``: the name of your Google Cloud project.\n- ``: the name of the dataset you created in BigQuery, such as`all_billing_data`.\n- ``: the start date of your query in the form`YYYY-MM-DD`.\n- ``: the name of a new table that you chose, such as`usage_metering_cost_breakdown`. This table is used as input to Looker Studio.\n- ``: the name of the query template you downloaded, either`usage_metering_query_template_request_and_consumption.sql`or`usage_metering_query_template_request_only.sql`.\n- ``: the name of the path for the rendered query that you choose, such as`cost_breakdown_query.sql`.\nAs an example, your environment variables might resemble the following:```\nexport GCP_BILLING_EXPORT_TABLE_FULL_PATH=my-billing-project.all_billing_data.gcp_billing_export_v1_xxxxexport USAGE_METERING_PROJECT_ID=my-billing-projectexport USAGE_METERING_DATASET_ID=all_billing_dataexport USAGE_METERING_START_DATE=2022-05-01export COST_BREAKDOWN_TABLE_ID=usage_metering_cost_breakdownexport USAGE_METERING_QUERY_TEMPLATE=usage_metering_query_template_request_only.sqlexport USAGE_METERING_QUERY=cost_breakdown_query.sql\n```\n- Render the query from the template:```\nsed \\-e \"s/\\${fullGCPBillingExportTableID}/$GCP_BILLING_EXPORT_TABLE_FULL_PATH/\" \\-e \"s/\\${projectID}/$USAGE_METERING_PROJECT_ID/\" \\-e \"s/\\${datasetID}/$USAGE_METERING_DATASET_ID/\" \\-e \"s/\\${startDate}/$USAGE_METERING_START_DATE/\" \\\"$USAGE_METERING_QUERY_TEMPLATE\" \\> \"$USAGE_METERING_QUERY\"\n```\n- Create a new cost breakdown table that refreshes every 24 hours:```\nbq query \\--project_id=$USAGE_METERING_PROJECT_ID \\--use_legacy_sql=false \\--destination_table=$USAGE_METERING_DATASET_ID.$COST_BREAKDOWN_TABLE_ID \\--schedule='every 24 hours' \\--display_name=\"GKE Usage Metering Cost Breakdown Scheduled Query\" \\--replace=true \\\"$(cat $USAGE_METERING_QUERY)\"\n```For more information about scheduling queries, see [Set up scheduled queries](/bigquery/docs/scheduling-queries#set_up_scheduled_queries) .\n### Create the BigQuery data source\n- In Looker Studio, go to [Data Sources](https://lookerstudio.google.com/c/navigation/datasources) .\n- Click **Create** , and then click **Data source** .\n- Select **BigQuery** .\n- Name your data source. From the toolbar, click the words **Untitled Data Source** to replace the text with a descriptive name.\n- Select **Custom Query** and then select your project ID.\n- Paste the following query into the Query Editor:```\nSELECT\u00a0 *FROM\u00a0 `USAGE_METERING_PROJECT_ID.USAGE_METERING_DATASET_ID.COST_BREAKDOWN_TABLE_ID`\n```\n- Click **Connect** .\n### Create the Looker Studio dashboard\n- Copy the [GKE usage metering dashboard](https://lookerstudio.google.com/reporting/1g7Z1hjCIkAHmL4QStbWNvuSUe-eQ-vnk) into your project.\n- Click **More options** , and then clickcontent_copy **Make a copy** .\n- In the **Copy this report** dialog, from the **New data source** list, select the data source that you created.\n- Click **Copy report** .\nThe dashboard is created, and you can access it at any time in the list of [Looker Studio reports](https://lookerstudio.google.com/c/navigation/reporting) for your project.\n### Use the Looker Studio dashboard\nThe dashboard contains multiple reports:\n**Note:** Because of differences in the frequency of data availability between usage metering and Cloud Billing, data shown in the dashboard is not definitive and is informational only.\nYou can change pages using the navigation menu. You can change the timeframe for a page using the date picker. To share the report with members of your organization, or to revoke access, click **Share Report** .\nAfter you copy the report into your project, you can customize it by using the [Looker Studio report editor](https://support.google.com/looker-studio/answer/6371822) . Even if the report template provided by Google changes, your copy is unaffected.\n## Explore GKE usage metering data using BigQuery\nTo view data about resource requests using BigQuery, query the `gke_cluster_resource_usage` table within the relevant BigQuery dataset.\nTo view data about actual resource consumption, query the `gke_cluster_resource_consumption` table. Network egress consumption data remains in the `gke_cluster_resource_usage` because there is no concept of resource requests for egresses.\nFor more information about using queries in BigQuery, see [Running queries](/bigquery/docs/query-overview#running_queries) . The fields in the [schema](#schema) are stable, though more fields may be added in the future.\nThese queries are simple examples. Customize your query to find the data you need.\n### Query for resource requests\n```\nSELECT\u00a0 cluster_name,\u00a0 labels,\u00a0 usageFROM\u00a0 'CLUSTER_GCP_PROJECT.USAGE_METERING_DATASET.gke_cluster_resource_usage'WHERE\u00a0 namespace=\"NAMESPACE\"\n```\n### Query for resource consumption\n```\nSELECT\u00a0 cluster_name,\u00a0 labels,\u00a0 usageFROM\u00a0 'CLUSTER_GCP_PROJECT.USAGE_METERING_DATASET.gke_cluster_resource_consumption'WHERE\u00a0 namespace=\"NAMESPACE\"\n```\nReplace the following:\n- ``: the name of your Google Cloud project that contains the cluster that you want to query.\n- ``: the name of your usage metering table.\n- ``: the name of your namespace.\n### More examples\nExpand the following sections to see more sophisticated examples.\n## GKE usage metering schema in BigQuery\nThe following table describes the schema for the GKE usage metering tables in the BigQuery dataset. If your cluster is running a version of GKE that supports resource consumption metering and resource requests, an additional table is created with the same schema.\n| Field    | Type  | Description                                                                              |\n|:--------------------|:----------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| cluster_location | STRING | The name of the Compute Engine zone or region in which the GKE cluster resides.                                                             |\n| cluster_name  | STRING | The name of the GKE cluster.                                                                          |\n| namespace   | STRING | The Kubernetes namespace from which the usage is generated.                                                                  |\n| resource_name  | STRING | The name of the resource, such as \"cpu\", \"memory\", and \"storage\".                                                                |\n| sku_id    | STRING | The SKU ID of the underlying Google Cloud cloud resource.                                                                  |\n| start_time   | TIMESTAMP | The UNIX timestamp of when the usage began.                                                                      |\n| end_time   | TIMESTAMP | The UNIX timestamp of when the usage ended.                                                                      |\n| fraction   | FLOAT  | The fraction of a cloud resource used by the usage. For a dedicated cloud resource that is solely used by a single namespace, the fraction is always 1.0. For resources shared among multiple namespaces, the fraction is calculated as the requested amount divided by the total capacity of the underlying cloud resource. |\n| cloud_resource_size | INTEGER | The size of the underlying Google Cloud resource. For example, the size of vCPUs on a n1-standard-2 instances is 2.                                                    |\n| labels.key   | STRING | The key of a Kubernetes label associated with the usage.                                                                   |\n| labels.value  | STRING | The value of a Kubernetes label associated with the usage.                                                                  |\n| project.id   | STRING | The ID of the project in which the GKE cluster resides.                                                                   |\n| usage.amount  | FLOAT  | The quantity of usage.unit used.                                                                         |\n| usage.unit   | STRING | The base unit in which resource usage is measured. For example, the base unit for standard storage is byte-seconds.                                                    |\nThe units for GKE usage metering must be interpreted in the following way:\n- The CPU `usage.unit` is , which is the total CPU time that a Pod requested or utilized. For example, if we have two Pods that each request 30 CPU and run for 15 minutes then the aggregate amount of the request table is 54,000 seconds (2 Pods * 30 CPU * 15 minutes * 60 seconds / minute).\n- The memory `usage.unit` is , which is the integral of memory over time that a Pod requested or utilized. For example, if we have two Pods that each request 30 GiB and run for 15 minutes then the aggregate amount of the request table is 5.798+13 byte-seconds (2 Pods * 30 GiB * 15 minutes * 60 seconds / minute * 1073741824 bytes / GiB).## Understanding when GKE usage metering data is written to BigQuery\nThere are two conditions when GKE usage metering writes usage records to BigQuery metrics:\n- The Pod phase changes to`succeeded`or`failed`, or when the Pod is deleted.\n- The hourly schedule's timestamp to write records is reached while the Pod is still running.GKE usage metering generates an hourly schedule where it writes Pod usage records to BigQuery for all currently running Pods. The schedule's timestamp is not the same across all clusters.If you have multiple Pods running at that timestamp, you'll find multiple usage records with the same `end_time` . These usage records' `end_time` indicate the hourly schedule's timestamp.Also, if you have multiple Pods that have been running for multiple hours, you also have a set of usage records with an `end_time` that matches the `start_time` of another set of usage records.## Disable GKE usage metering\nTo disable GKE usage metering on a cluster, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --clear-resource-usage-bigquery-dataset\n```\n **Note:** The BigQuery dataset is preserved. You can remove it if you don't need the data and no cluster is using it.\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to the cluster you want to modify, click **Actions** , then click **Edit** .\n- Under **Features** , click **Edit** next to **GKE usage metering** .\n- Clear **Enable GKE usage metering** .\n- Click **Save Changes** .## What's next\n- [Learn more about cluster multi-tenancy](/kubernetes-engine/docs/concepts/multitenancy-overview) \n- [Learn about Cloud Billing](https://cloud.google.com/billing/docs/) \n- [Learn about Looker Studio](https://support.google.com/looker-studio/answer/6283323)", "guide": "Google Kubernetes Engine (GKE)"}