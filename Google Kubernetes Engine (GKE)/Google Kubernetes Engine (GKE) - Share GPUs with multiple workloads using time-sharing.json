{"title": "Google Kubernetes Engine (GKE) - Share GPUs with multiple workloads using time-sharing", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/timesharing-gpus", "abstract": "# Google Kubernetes Engine (GKE) - Share GPUs with multiple workloads using time-sharing\nThis page shows you how to let multiple workloads get access to a single NVIDIA\u00ae GPU hardware accelerator in your Google Kubernetes Engine (GKE) nodes. To learn more about how time-sharing works, as well as limitations and examples of when you should use time-sharing GPUs, refer to [Time-sharing GPUs on GKE](/kubernetes-engine/docs/concepts/timesharing-gpus) .\n", "content": "## Who should use this guide\nThe instructions in this topic apply to you if you are one of the following:\n- **Platform administrator** : Creates and manages a GKE cluster, plans infrastructure and resourcing requirements, and monitors the cluster's performance.\n- **Application developer** : Designs and deploys workloads on GKE clusters. If you want instructions for requesting time-shared GPUs, refer to [Deploy workloads that use time-shared GPUs](#deploy) .## Requirements and limitations\n- You can enable time-sharing GPUs on GKE Standard clusters and node pools running GKE version 1.23.7-gke.1400 and later.\n- You can't update existing clusters or node pools to enable time-sharing GPUs. Instead, [create a new node pool](#enable-node-pool) with time-sharing enabled.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have sufficient [NVIDIA Tesla GPU](/compute/docs/gpus#nvidia_t4_gpus) quota. If you need more quota, refer to [Requesting an increase in quota](/compute/quotas#requesting_additional_quota) .\n- Plan your time-sharing GPU capacity based on the resource needs of the workloads and the capacity of the underlying GPU.## Enable time-sharing GPUs on GKE clusters and node pools\nAs a platform administrator, you must enable time-sharing GPUs on a GKE Standard cluster before developers can deploy workloads to use the GPUs. To enable time-sharing, you must do the following:\n- [Enable time-sharing GPUs on a GKE cluster](#enable-cluster) .\n- [Install NVIDIA GPU device drivers (if required)](#install-drivers) .\n- [Verify the GPU resources available on your nodes](#verify-sharing) .\n### Enable time-sharing GPUs on a GKE cluster\nYou can enable time-sharing GPUs when you create GKE Standard clusters. The default node pool in the cluster has the feature enabled. You still need to enable time-sharing GPUs when you manually create new node pools in that cluster.\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --cluster-version=CLUSTER_VERSION \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --accelerator=type=GPU_TYPE,count=GPU_QUANTITY,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=CLIENTS_PER_GPU,gpu-driver-version=DRIVER_VERSION\n```\nReplace the following:\n- ``: the name of your new cluster.\n- ``: the Compute Engine [region](/compute/docs/regions-zones#available) for your new cluster. For zonal clusters, specify`--zone=` ``.\n- ``: the GKE version for the cluster control plane and nodes. Use GKE version 1.23.7-gke.1400 or later. Alternatively, specify a [release channel](/kubernetes-engine/docs/release-notes#current_versions) with that GKE version by using the`--release-channel=` ``flag.\n- ``: the Compute Engine machine type for your nodes.- For H100 GPUs, use an [A3 machine type](/compute/docs/accelerator-optimized-machines#a3-vms) \n- For A100 GPUs, use an [A2 machine type](/compute/docs/accelerator-optimized-machines#a2-vms) \n- For L4 GPUs, use a [G2 machine type](/compute/docs/accelerator-optimized-machines#g2-vms) \n- For all other GPUs, use an [N1 machine type](/compute/docs/machine-types#n1_machine_types) \n- ``: the GPU type, which must be an [NVIDIA Tesla GPU platform](/compute/docs/gpus#nvidia_gpus_for_compute_workloads) such as`nvidia-tesla-v100`.\n- ``: the number of physical GPUs to attach to each node in the default node pool.\n- ``: the maximum number of containers that can share each physical GPU.\n- ``: the NVIDIA driver version to install. Can be one of the following:- `default`: Install the default driver version for your GKE version.\n- `latest`: Install the latest available driver version for your GKE version. Available only for nodes that use Container-Optimized OS.\n- `disabled`: Skip automatic driver installation. You **must** [manually install a driver](#install-drivers) after you create the node pool. If you omit`gpu-driver-version`, this is the default option.\n### Enable time-sharing GPUs on a GKE node pool\nYou can enable time-sharing GPUs when you manually create new node pools in a GKE cluster.\n```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --accelerator=type=GPU_TYPE,count=GPU_QUANTITY,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=CLIENTS_PER_GPU,gpu-driver-version=DRIVER_VERSION\n```\nReplace the following:\n- ``: the name of your new node pool.\n- ``: the name of your cluster, which must run GKE version 1.23.7-gke.1400 or later.\n- ``: the Compute Engine [region](/compute/docs/regions-zones#available) of your cluster. For zonal clusters, specify`--zone=` ``.\n- ``: the Compute Engine machine type for your nodes.- For H100 GPUs, use an [A3 machine type](/compute/docs/accelerator-optimized-machines#a3-vms) \n- For A100 GPUs, use an [A2 machine type](/compute/docs/accelerator-optimized-machines#a2-vms) \n- For L4 GPUs, use a [G2 machine type](/compute/docs/accelerator-optimized-machines#g2-vms) \n- For all other GPUs, use an [N1 machine type](/compute/docs/machine-types#n1_machine_types) \n- ``: the GPU type, which must be an [NVIDIA Tesla GPU platform](/compute/docs/gpus#nvidia_gpus_for_compute_workloads) such as`nvidia-tesla-v100`.\n- ``: the number of physical GPUs to attach to each node in the node pool.\n- ``: the maximum number of containers that can share each physical GPU.\n- `` : the NVIDIA driver version to install. Can be one of the following:- `default`: Install the default driver version for your GKE version.\n- `latest`: Install the latest available driver version for your GKE version. Available only for nodes that use Container-Optimized OS.\n- `disabled`: Skip automatic driver installation. You **must** [manually install a driver](#install-drivers) after you create the node pool. If you omit`gpu-driver-version`, this is the default option.\n **Note:** The `gpu-driver-version` option is only available for GKE version 1.27.2-gke.1200 and later. In earlier versions, omit this flag and [manually install a driver](#install-drivers) after you create the node pool.\n### Install NVIDIA GPU device drivers\nBefore you proceed, connect to your cluster by running the following command:\n```\ngcloud container clusters get-credentials CLUSTER_NAME\n```\nIf you chose to disable automatic driver installation when creating the cluster, or if you use a GKE version earlier than 1.27.2-gke.1200, you must manually install a compatible NVIDIA driver to manage the time-sharing division of the physical GPUs. To install the drivers, you deploy a GKE installation DaemonSet that sets the drivers up.\nFor instructions, refer to [Installing NVIDIA GPU device drivers](/kubernetes-engine/docs/how-to/gpus#installing_drivers) .\nIf you plan to use node auto-provisioning in your cluster, you must also configure node auto-provisioning with the scopes that allow GKE to install the GPU device drivers for you. For instructions, refer to [Using node auto-provisioning with GPUs](/kubernetes-engine/docs/how-to/gpus#using_node_auto-provisioning_with_gpus) .\n### Verify the GPU resources available on your nodes\nTo verify that the number of GPUs visible in your nodes matches the number you specified when you enabled time-sharing, describe your nodes:\n```\nkubectl describe nodes NODE_NAME\n```\nThe output is similar to the following:\n```\n...\nCapacity:\n ...\n nvidia.com/gpu:    3\nAllocatable:\n ...\n nvidia.com/gpu:    3\n```\nIn this example output, the number of GPU resources on the node is `3` because the value that was specified for `max-shared-clients-per-gpu` was `3` and the `count` of physical GPUs to attach to the node was `1` . As another example, if the `count` of physical GPUs was `2` , the output would show `6` allocatable GPU resources, three on each physical GPU.\n## Deploy workloads that use time-shared GPUs\nAs an application operator who is deploying GPU workloads, you can select time-shared GPU nodes by specifying the appropriate node labels in a `nodeSelector` in your manifests. When planning your requests, review the [request limits](/kubernetes-engine/docs/concepts/timesharing-gpus#request-limits) to ensure that GKE doesn't reject your deployments.\nTo deploy a workload to consume time-sharing GPUs, you need to do the following:\n- Add a `nodeSelector` to your Pod manifest for the following labels:- `cloud.google.com/gke-gpu-sharing-strategy: time-sharing`: selects nodes that use time-sharing GPUs.\n- `cloud.google.com/gke-max-shared-clients-per-gpu: \"` `` `\"`: selects nodes that allow a specific number of containers to share the underlying GPU.\n- Add the `nvidia.com/gpu=1` GPU resource request to your container specification, in `spec.containers.resources.limits` .\nFor example, the following steps show you how to deploy three Pods to a time-sharing GPU node pool. GKE allocates a time-shared GPU to each container. The containers print the UUID of the GPU that's attached to that container.\n- Save the following manifest as `gpu-timeshare.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: cuda-simplespec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: cuda-simple\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: cuda-simple\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-gpu-sharing-strategy: time-sharing\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-max-shared-clients-per-gpu: \"3\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: cuda-simple\u00a0 \u00a0 \u00a0 \u00a0 image: nvidia/cuda:11.0.3-base-ubi7\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /usr/local/nvidia/bin/nvidia-smi -L; sleep 300\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\n```\n- Apply the manifest:```\nkubectl apply -f gpu-timeshare.yaml\n```\n- Check that all Pods are running:```\nkubectl get pods -l=app=cuda-simple\n```\n- Check the logs for any Pod to view the UUID of the GPU:```\nkubectl logs POD_NAME\n```The output is similar to the following:```\nGPU 0: Tesla V100-SXM2-16GB (UUID: GPU-0771302b-eb3a-6756-7a23-0adcae8efd47)\n```\n- If your nodes have one physical GPU attached, check the logs for any other Pod on the same node to verify that the GPU UUID is the same:```\nkubectl logs POD2_NAME\n```The output is similar to the following:```\nGPU 0: Tesla V100-SXM2-16GB (UUID: GPU-0771302b-eb3a-6756-7a23-0adcae8efd47)\n```## Use time-sharing GPUs with multi-instance GPUs\nAs a platform administrator, you might want to combine multiple GKE GPU features. Time-sharing GPUs works with [multi-instance GPUs](/kubernetes-engine/docs/how-to/gpus-multi) , which partition a single physical GPU into up to seven slices. These partitions are isolated from each other. You can configure time-sharing GPUs for each multi-instance GPU partition.\nFor example, if you set the `gpu-partition-size` to `1g.5gb` , the underlying GPU would be split into seven partitions. If you also set `max-shared-clients-per-gpu` to `3` , each partition would support up to three containers, for a total of 21 time-shared GPU devices available to allocate. To learn about how the `gpu-partition-size` converts to actual partitions, refer to [Multi-instance GPU partitions](/kubernetes-engine/docs/how-to/gpus-multi#multi-instance_gpu_partitions) .\nTo create a time-shared, multi-instance GPU cluster, run the following command:\n```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --accelerator=type=nvidia-tesla-a100,count=GPU_QUANTITY,gpu-partition-size=PARTITION_SIZE,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=CLIENTS_PER_GPU,gpu-driver-version=DRIVER_VERSION\n```\nReplace `` with the [multi-instance GPU partition size](/kubernetes-engine/docs/how-to/gpus-multi#multi-instance_gpu_partitions) that you want, such as `1g.5gb` .\n## What's next\n- Learn more about [Time-sharing GPUs on GKE](/kubernetes-engine/docs/concepts/timesharing-gpus) .\n- Learn more about [GPUs](/kubernetes-engine/docs/how-to/gpus) .\n- Learn more about [Running multi-instance GPUs](/kubernetes-engine/docs/how-to/gpus-multi) .\n- For more information about compute preemption for the NVIDIA GPU, refer to the [NVIDIA Pascal Tuning Guide](https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html#preemption) .", "guide": "Google Kubernetes Engine (GKE)"}