{"title": "Google Kubernetes Engine (GKE) - Use secondary boot disks to preload data or container images", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/data-container-image-preloading", "abstract": "# Google Kubernetes Engine (GKE) - Use secondary boot disks to preload data or container images\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nThis page shows you how to improve the initialization speed of your AI/ML workloads by using a secondary boot disk on your nodes. When a node has a secondary boot disk, you can preload data or container images. Google Kubernetes Engine (GKE) schedules the node and preloads this data to reduce the provisioning time.\n", "content": "## Overview\nStarting in version 1.28.3-gke.1067000, you can preload data or container images in new nodes. By configuring the node pool to use [image streaming](/kubernetes-engine/docs/how-to/image-streaming) and a secondary boot disk, you can tell GKE to provision the nodes and preload them with data, such as a machine learning model, or a container image as if they were locally cached on the disk. Using preloaded data or a container image in a secondary disk has the following benefits for your workloads:\n- Faster autoscaling\n- Reduced latency when pulling large images\n- Quicker recovery from disruptions like maintenance events and system errors## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Enable the Container File System API. [EnableContainer File System API](https://console.cloud.google.com/apis/library/containerfilesystem.googleapis.com?q=container%20file%20system) \n- Ensure that your cluster has access to the disk image to load in the nodes.## Requirements\nThe following requirements apply to using secondary boot disk:\n- The feature is available in GKE version of 1.28.3-gke.106700 and later.\n- When you modify the disk image, you must create a new node pool. Updating the disk image on existing nodes is not supported.## Configure the secondary boot disk\nThe following sections describe how to configure the secondary boot disk:\n- [Preload data](#preload-data) \n- [Preload a container image](#preload-container-image) \n### Preload data\nBefore you create the GKE cluster and node pool with a secondary boot disk, we recommend that you prepare the disk image when the data is ready during build time, ideally automated in a CI/CD pipeline.\nCreate a custom disk image as the data source by completing the following steps:\n- [Create a VM with a blank disk](/compute/docs/instances/create-start-instance#create_a_vm_instance_with_additional_non-boot_disks) .\n- [SSH into the VM](/compute/docs/connect/standard-ssh) .- [Mount the blank disk](/compute/docs/disks/format-mount-disk-linux#mounting) .\n- [Download the data onto the blank disk](/storage/docs/downloading-objects#downloading-an-object) .\n- [Create a custom image from the disk](/compute/docs/images/create-custom#create_image) .You can configure a secondary boot disk by using the gcloud CLI:\n- Create a GKE Standard cluster with image streaming enabled by using the `--enable-image-streaming` flag:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location LOCATION \\\u00a0 \u00a0 --enable-image-streaming\n```Replace the following:- : The name of your cluster.\n- : The [cluster location](/compute/docs/regions-zones/viewing-regions-zones) .\n- Create a node pool with a secondary boot disk by using the `--secondary-boot-disk=disk-image` flag:```\ngcloud beta container node-pools create NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --location LOCATION \\\u00a0 \u00a0 --enable-image-streaming \\\u00a0 \u00a0 --secondary-boot-disk=disk-image=global/images/DATA_DISK IMAGE\n```Replace the with the path of the secondary disk. For example, `/mnt/disks/gke-secondary-disks/gke-model-data-disk.`GKE creates a node pool where each node has a secondary disk with preloaded data. This attaches and mounts the secondary boot disk onto the node.\n- Optionally, you can mount the secondary disk image in the Pod containers using a hostPath volume mount. Use the following manifest to define a Pod resources and use a hostPath volume mount to preload the data disk in its containers:```\napiVersion: v1kind: Podmetadata:\u00a0 name: pod-namespec:\u00a0 containers:\u00a0 ...\u00a0 volumeMounts:\u00a0 - mountPath: /usr/local/data_path_sbd\u00a0 \u00a0 name: data_path_sbd...volumes:\u00a0 - name: data_path_sbd\u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 path: DISK_IMAGE_NAME\n```Replace the with the path of the secondary disk. For example, `/mnt/disks/gke-secondary-disks/gke-model-data-disk.`\n### Preload the container image\nIn this section, you create a disk image with a preloaded container image, a cluster, and a node pool. Each node creates a secondary disk with a preloaded container image:\n- Create your [Cloud Storage buckets](/storage/docs/creating-buckets) to store the disk image.\n- Create a disk image. In this guide, you use the [gke-disk-image-builder](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/gke-disk-image-builder) :```\ngo run ./cli \\\u00a0 \u00a0 --project-name=PROJECT_ID \\\u00a0 \u00a0 --image-name=DISK_IMAGE_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --gcs-path=gs://BUCKET_NAME \\\u00a0 \u00a0 --disk-size-gb=10 \\\u00a0 \u00a0 --container-image=docker.io/library/nginx:latest\n```Replace the following:- : The name of your Google Cloud project.\n- : The name of the image of the disk. For example,`nginx-python-image`.\n- : The [cluster location](/compute/docs/regions-zones/viewing-regions-zones) .\n- : The name of the Cloud Storage bucket that contains the disk image. For example, `gke-secondary-disk-image-logs/` .GKE pulls the container images on a disk and then creates an image from that disk. GKE stores the image in the Cloud Storage bucket. **Note:** When you create a disk image with [gke-disk-image-builder](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/gke-disk-image-builder) , Google Cloud creates multiple resources to complete the process (for example, a VM instance, a temporary disk, and disk). After its execution, the image builder cleans up all the resources except the disk image that you created.\n- Create a GKE Standard cluster with image streaming enabled:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --enable-image-streaming\n```\n- Create a node pool with a secondary boot disk:```\ngcloud beta container node-pools create NODE_POOL_NAME \\--cluster=CLUSTER_NAME \\--location=LOCATION \\ \\--enable-image-streaming \\--secondary-boot-disk=disk-image=global/images/DISK_IMAGE_NAME, mode=CONTAINER_IMAGE_CACHE\n```\n- Add a `nodeSelector` to your Pod template:```\nnodeSelector:\u00a0 \u00a0 cloud.google.com/gke-nodepool: NODE_POOL_NAME\n```\n- Confirm that the secondary boot disk cache is in use:```\nkubectl get events --all-namespaces\n```The output is similar to the following:```\n75s   Normal  SecondaryDiskCachin\nnode/gke-pd-cache-demo-default-pool-75e78709-zjfm Image\ngcr.io/k8s-staging-jobsejt/pytorch-mnist:latest is backed by secondary disk cache\n```The expected image pull latency for the cached container image should be no more than a few seconds, regardless of image size. You can check the image pull latency by running the following command:```\nkubectl describe pod POD_NAME\n```Replace `` with the name of the Pod.The output is similar to following:```\n\u2026\n Normal Pulled  15m kubelet   Successfully pulled image \"docker.io/library/nginx:latest\" in 0.879149587s\n\u2026\n```## What's next\n- Use [Use Image streaming to pull container images](/kubernetes-engine/docs/how-to/image-streaming) to pull container images by streaming the image data as your workloads need.\n- See [Improve workload efficiency using NCCL Fast Socket](/kubernetes-engine/docs/how-to/nccl-fast-socket) to learn how to use the [NVIDIA Collective Communication Library (NCCL) Fast Socket plugin](https://github.com/google/nccl-fastsocket) .", "guide": "Google Kubernetes Engine (GKE)"}