{"title": "Google Kubernetes Engine (GKE) - Serve an LLM with multiple GPUs in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-multiple-gpu", "abstract": "# Google Kubernetes Engine (GKE) - Serve an LLM with multiple GPUs in GKE\nThis tutorial shows you how to serve a large language model (LLM) with GPUs in Google Kubernetes Engine (GKE). This tutorial creates a GKE cluster that uses multiple L4 GPUs and prepares the GKE infrastructure to serve any of the following models:- [Llama 2 70b](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) \n- [Mixtral 8x7b](https://mistral.ai/news/mixtral-of-experts/) \n- [Falcon 40b](https://falconllm.tii.ae/falcon-40b.html) \nDepending on the data format of the model, the number of GPUs varies. In this tutorial, each model uses two L4 GPUs. To learn more, see [Calculating the amount of GPUs](#calculate-gpus) .\nBefore you complete this tutorial in GKE, we recommend that you learn [About GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) .", "content": "## ObjectivesThis tutorial is intended for MLOps or DevOps engineers or platform administrator that want to use GKE orchestration capabilities for serving LLMs.\nThis tutorial covers the following steps:- Create a cluster and node pools.\n- Prepare your workload.\n- Deploy your workload.\n- Interact with the LLM interface.\n## Before you beginBefore you start, make sure you have performed the following tasks:- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- If you want to use the Llama 2 70b model, ensure you have the following:- Access and an active license for the [Meta Llama models](https://huggingface.co/meta-llama/Llama-2-7b-hf) .\n- A [HuggingFace token](https://huggingface.co/docs/hub/security-tokens) .\n **Warning:** Getting access and approval for the Llama model might take up to three days.\n## Prepare your environment\n- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=us-central1\n```Replace the with your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) . **Note:** If your Cloud Shell instance disconnects throughout the tutorial execution, repeat the preceding step.\n### Create a GKE cluster and node poolYou can serve LLMs on GPUs in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience. To choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\n- In Cloud Shell, run the following command:```\ngcloud container clusters create-auto l4-demo \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid\n```GKE creates an Autopilot cluster with CPU and GPU nodes as requested by the deployed workloads.\n- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials l4-demo --region=${REGION}\n```\n- In Cloud Shell, run the following command to create a Standard cluster that uses [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) :```\ngcloud container clusters create l4-demo --location ${REGION} \\\u00a0 --workload-pool ${PROJECT_ID}.svc.id.goog \\\u00a0 --enable-image-streaming \\\u00a0 --node-locations=$REGION-a \\\u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 --machine-type n2d-standard-4 \\\u00a0 --num-nodes 1 --min-nodes 1 --max-nodes 5 \\\u00a0 --release-channel=rapid\n``` **Note:** The `--node-locations` flag might have to be adjusted based on which region you choose. Check which [zones the L4 GPUs](/compute/docs/gpus#nvidia_l4_vws_gpus) are available if you change the `us-central1` region.The cluster creation might take several minutes.\n- Run the following command to create a [node pool](/kubernetes-engine/docs/concepts/node-pools) for your cluster:```\ngcloud container node-pools create g2-standard-24 --cluster l4-demo \\\u00a0 --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\u00a0 --machine-type g2-standard-24 \\\u00a0 --enable-autoscaling --enable-image-streaming \\\u00a0 --num-nodes=0 --min-nodes=0 --max-nodes=3 \\\u00a0 --node-locations $REGION-a,$REGION-c --region $REGION --spot\n```GKE creates the following resources for the LLM:- A public Google Kubernetes Engine (GKE) Standard edition cluster.\n- A node pool with`g2-standard-24`machine type scaled down to 0 nodes. You aren't charged for any GPUs until you launch Pods. that request GPUs. This node pool provisions [Spot VMs](/kubernetes-engine/docs/how-to/spot-vms) , which are priced lower than the default standard Compute Engine VMs and provide no guarantee of availability. You can remove the`--spot`flag from this command, and the`cloud.google.com/gke-spot`node selector in the`text-generation-inference.yaml`config to use on-demand VMs.\n- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials l4-demo --region=${REGION}\n```\n## Prepare your workloadThe following section shows how to set up your workload depending on the model you want to use:\n- Set the default environment variables:```\nexport HF_TOKEN=HUGGING_FACE_TOKEN\n```Replace the `` with your HuggingFace token.\n- Create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) for the HuggingFace token:```\nkubectl create secret generic l4-demo \\\u00a0 \u00a0 --from-literal=HUGGING_FACE_TOKEN=${HF_TOKEN} \\\u00a0 \u00a0 --dry-run=client -o yaml > hf-secret.yaml\n```\n- Apply the manifest:```\nkubectl apply -f hf-secret.yaml\n```\n- Create the following `text-generation-inference.yaml` manifest: [  ai-ml/llm-multiple-gpus/llama2-70b/text-generation-inference.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/llama2-70b/text-generation-inference.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/llama2-70b/text-generation-inference.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: llmspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: llm\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: llm\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: llm\u00a0 \u00a0 \u00a0 \u00a0 image: ghcr.io/huggingface/text-generation-inference:1.4.3\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"60Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"60Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: meta-llama/Llama-2-70b-chat-hf\u00a0 \u00a0 \u00a0 \u00a0 - name: NUM_SHARD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8080\"\u00a0 \u00a0 \u00a0 \u00a0 - name: QUANTIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: bitsandbytes-nf4\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: l4-demo\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: HUGGING_FACE_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: ephemeral-volume\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 \u00a0 - name: ephemeral-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeClaimTemplate:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: ephemeral\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 accessModes: [\"ReadWriteOnce\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storageClassName: \"premium-rwo\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 150Gi\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: \"nvidia-l4\"\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-spot: \"true\"\n```In this manifest:- `NUM_SHARD`must be`2`because the model requires two NVIDIA L4 GPUs.\n- `QUANTIZE`is set to`bitsandbytes-nf4`which means that the model is loaded in 4 bit instead of 32 bits. This allows GKE to reduce the amount of GPU memory needed and improves the inference speed. However, the model accuracy can decrease. To learn how to calculate the GPUs to request, see [Calculating the amount of GPUs](#calculate-gpus) .\n- Apply the manifest:```\nkubectl apply -f text-generation-inference.yaml\n```The output is similar to the following:```\ndeployment.apps/llm created\n```\n- Verify the status of the model:```\nkubectl get deploy\n```The output is similar to the following:```\nNAME   READY UP-TO-DATE AVAILABLE AGE\nllm   1/1  1   1   20m\n```\n- View the logs from the running deployment:```\nkubectl logs -l app=llm\n```The output is similar to the following:```\n{\"timestamp\":\"2024-03-09T05:08:14.751646Z\",\"level\":\"INFO\",\"message\":\"Warming up model\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":291}\n{\"timestamp\":\"2024-03-09T05:08:19.961136Z\",\"level\":\"INFO\",\"message\":\"Setting max batch total tokens to 133696\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":328}\n{\"timestamp\":\"2024-03-09T05:08:19.961164Z\",\"level\":\"INFO\",\"message\":\"Connected\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":329}\n{\"timestamp\":\"2024-03-09T05:08:19.961171Z\",\"level\":\"WARN\",\"message\":\"Invalid hostname, defaulting to 0.0.0.0\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":343}\n```\n- Create the following `text-generation-inference.yaml` manifest: [  ai-ml/llm-multiple-gpus/mixtral-8x7b/text-generation-inference.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/mixtral-8x7b/text-generation-inference.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/mixtral-8x7b/text-generation-inference.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: llmspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: llm\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: llm\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: llm\u00a0 \u00a0 \u00a0 \u00a0 image: ghcr.io/huggingface/text-generation-inference:1.4.3\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"5\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"5\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: mistralai/Mixtral-8x7B-Instruct-v0.1\u00a0 \u00a0 \u00a0 \u00a0 - name: NUM_SHARD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8080\"\u00a0 \u00a0 \u00a0 \u00a0 - name: QUANTIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: bitsandbytes-nf4\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: ephemeral-volume\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 \u00a0 - name: ephemeral-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeClaimTemplate:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: ephemeral\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 accessModes: [\"ReadWriteOnce\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storageClassName: \"premium-rwo\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 100Gi\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: \"nvidia-l4\"\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-spot: \"true\"\n```In this manifest:- `NUM_SHARD`must be`2`because the model requires two NVIDIA L4 GPUs.\n- `QUANTIZE`is set to`bitsandbytes-nf4`which means that the model is loaded in 4 bit instead of 32 bits. This allows GKE to reduce the amount of GPU memory needed and improves the inference speed. However, this may reduce model accuracy. To learn how to calculate the GPUs to request, see [Calculating the amount of GPUs](#calculate-gpus) .\n- Apply the manifest:```\nkubectl apply -f text-generation-inference.yaml\n```The output is similar to the following:```\ndeployment.apps/llm created\n```\n- Verify the status of the model:```\nwatch kubectl get deploy\n```The output is similar to the following when the deployment is ready. To exit the watch, type `CTRL + C` :```\nNAME   READY UP-TO-DATE AVAILABLE AGE\nllm   1/1  1   1   10m\n```\n- View the logs from the running deployment:```\nkubectl logs -l app=llm\n```The output is similar to the following:```\n{\"timestamp\":\"2024-03-09T05:08:14.751646Z\",\"level\":\"INFO\",\"message\":\"Warming up model\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":291}\n{\"timestamp\":\"2024-03-09T05:08:19.961136Z\",\"level\":\"INFO\",\"message\":\"Setting max batch total tokens to 133696\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":328}\n{\"timestamp\":\"2024-03-09T05:08:19.961164Z\",\"level\":\"INFO\",\"message\":\"Connected\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":329}\n{\"timestamp\":\"2024-03-09T05:08:19.961171Z\",\"level\":\"WARN\",\"message\":\"Invalid hostname, defaulting to 0.0.0.0\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":343}\n```\n- Create the following `text-generation-inference.yaml` manifest: [  ai-ml/llm-multiple-gpus/falcon-40b/text-generation-inference.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/falcon-40b/text-generation-inference.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/falcon-40b/text-generation-inference.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: llmspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: llm\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: llm\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: llm\u00a0 \u00a0 \u00a0 \u00a0 image: ghcr.io/huggingface/text-generation-inference:1.4.3\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"60Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"60Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: tiiuae/falcon-40b-instruct\u00a0 \u00a0 \u00a0 \u00a0 - name: NUM_SHARD\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8080\"\u00a0 \u00a0 \u00a0 \u00a0 - name: QUANTIZE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: bitsandbytes-nf4\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: ephemeral-volume\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 \u00a0 - name: ephemeral-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeClaimTemplate:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: ephemeral\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 accessModes: [\"ReadWriteOnce\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storageClassName: \"premium-rwo\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 storage: 175Gi\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: \"nvidia-l4\"\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-spot: \"true\"\n```In this manifest:- `NUM_SHARD`must be`2`because the model requires two NVIDIA L4 GPUs.\n- `QUANTIZE`is set to`bitsandbytes-nf4`which means that the model is loaded in 4 bit instead of 32 bits. This allows GKE to reduce the amount of GPU memory needed and improves the inference speed. However, the model accuracy can decrease. To learn how to calculate the GPUs to request, see [Calculating the amount of GPUs](#calculate-gpus) .\n- Apply the manifest:```\nkubectl apply -f text-generation-inference.yaml\n```The output is similar to the following:```\ndeployment.apps/llm created\n```\n- Verify the status of the model:```\nwatch kubectl get deploy\n```The output is similar to the following when the deployment is ready. To exit the watch, type `CTRL + C` :```\nNAME   READY UP-TO-DATE AVAILABLE AGE\nllm   1/1  1   1   10m\n```\n- View the logs from the running deployment:```\nkubectl logs -l app=llm\n```The output is similar to the following:```\n{\"timestamp\":\"2024-03-09T05:08:14.751646Z\",\"level\":\"INFO\",\"message\":\"Warming up model\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":291}\n{\"timestamp\":\"2024-03-09T05:08:19.961136Z\",\"level\":\"INFO\",\"message\":\"Setting max batch total tokens to 133696\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":328}\n{\"timestamp\":\"2024-03-09T05:08:19.961164Z\",\"level\":\"INFO\",\"message\":\"Connected\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":329}\n{\"timestamp\":\"2024-03-09T05:08:19.961171Z\",\"level\":\"WARN\",\"message\":\"Invalid hostname, defaulting to 0.0.0.0\",\"target\":\"text_generation_router\",\"filename\":\"router/src/main.rs\",\"line_number\":343}\n```### Create a Service of type ClusterIP\n- Create the following `llm-service.yaml` manifest:```\napiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: llm\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 80\u00a0 \u00a0 \u00a0 targetPort: 8080\n```\n- Apply the manifest:```\nkubectl apply -f llm-service.yaml\n```\n### Deploy a chat interfaceUse [Gradio](https://www.gradio.app/docs/interface) to build a web application that lets you interact with your model. Gradio is a Python library that has a ChatInterface wrapper that creates user interfaces for chatbots.\n- Create a file named `gradio.yaml` : [  ai-ml/llm-multiple-gpus/llama2-70b/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/llama2-70b/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/llama2-70b/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 strategy: \u00a0 \u00a0 type: Recreate\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"tgi\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"llama-2-70b\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"[INST] prompt [/INST]\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"prompt\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradio-servicespec:\u00a0 type: LoadBalancer\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 - port: 80\u00a0 \u00a0 targetPort: 7860\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Find the external IP address of the Service:```\nkubectl get svc\n```The output is similar to the following:```\nNAME    TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE\ngradio-service LoadBalancer 10.24.29.197 34.172.115.35 80:30952/TCP 125m\n```\n- Copy the external IP address from the `EXTERNAL-IP` column.\n- View the model interface from your web browser by using the external IP address with the exposed port:```\nhttp://EXTERNAL_IP\n```\n- Create a file named `gradio.yaml` : [  ai-ml/llm-multiple-gpus/mixtral-8x7b/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/mixtral-8x7b/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/mixtral-8x7b/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 strategy: \u00a0 \u00a0 type: Recreate\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"tgi\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"mixtral-8x7b\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"[INST] prompt [/INST]\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"prompt\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradio-servicespec:\u00a0 type: LoadBalancer\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 - port: 80\u00a0 \u00a0 targetPort: 7860\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Find the external IP address of the Service:```\nkubectl get svc\n```The output is similar to the following:```\nNAME    TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE\ngradio-service LoadBalancer 10.24.29.197 34.172.115.35 80:30952/TCP 125m\n```\n- Copy the external IP address from the `EXTERNAL-IP` column.\n- View the model interface from your web browser by using the external IP address with the exposed port:```\nhttp://EXTERNAL_IP\n```\n- Create a file named `gradio.yaml` : [  ai-ml/llm-multiple-gpus/falcon-40b/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/falcon-40b/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-multiple-gpus/falcon-40b/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 strategy: \u00a0 \u00a0 type: Recreate\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"tgi\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"falcon-40b-instruct\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"User: prompt\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"Assistant: prompt\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradio-servicespec:\u00a0 type: LoadBalancer\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 - port: 80\u00a0 \u00a0 targetPort: 7860\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Find the external IP address of the Service:```\nkubectl get svc\n```The output is similar to the following:```\nNAME    TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)  AGE\ngradio-service LoadBalancer 10.24.29.197 34.172.115.35 80:30952/TCP 125m\n```\n- Copy the external IP address from the `EXTERNAL-IP` column.\n- View the model interface from your web browser by using the external IP address with the exposed port:```\nhttp://EXTERNAL_IP\n``` **Success:** At this point, you have deployed an LLM using L4 GPUs in GKE.## Calculating the amount of GPUsThe amount of GPUs depends on the value of the `QUANTIZE` flag. In this tutorial, `QUANTIZE` is set to `bitsandbytes-nf4` , which means that the model is loaded in 4 bits.\nA 70 billion parameter model would require a minimum of 40 GB of GPU memory which equals to 70 billion times 4 bits (70 billion x 4 bits= 35 GB) and considers a 5 GB of overhead. In this case, a single L4 GPU wouldn't have enough memory. Therefore, the examples in this tutorial use L4 GPU of memory (2 x 24 = 48 GB). This configuration is sufficient for running Falcon 40b or Llama 2 70b in L4 GPUs.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the clusterTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, delete the GKE cluster:\n```\ngcloud container clusters delete l4-demo --region ${REGION}\n```## What's next\n- [Learn more about G2 VMs with NVIDIA L4 GPUs](/blog/products/compute/introducing-g2-vms-with-nvidia-l4-gpus) \n- [Train a model with GPUs on GKE Standard mode](/kubernetes-engine/docs/quickstarts/train-model-gpus-standard)", "guide": "Google Kubernetes Engine (GKE)"}