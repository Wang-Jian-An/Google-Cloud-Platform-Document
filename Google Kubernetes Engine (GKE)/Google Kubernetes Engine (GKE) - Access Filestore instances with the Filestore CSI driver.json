{"title": "Google Kubernetes Engine (GKE) - Access Filestore instances with the Filestore CSI driver", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/filestore-csi-driver", "abstract": "# Google Kubernetes Engine (GKE) - Access Filestore instances with the Filestore CSI driver\nThe Filestore CSI driver is the primary way for you to use [Filestoreinstances](/filestore/docs/overview) with Google Kubernetes Engine (GKE). The Filestore CSI driver provides a fully-managed experience powered by the open source [Google Cloud Filestore CSI driver](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver) .\nThe Filestore CSI driver version is tied to Kubernetes minor version numbers. The Filestore CSI driver version is typically the latest driver available at the time that the Kubernetes minor version is released. The drivers update automatically when the cluster is upgraded to the latest GKE patch.\n**Note:** Because the Filestore CSI driver and some of the other associated CSI components are deployed as separate containers, they incur resource usage (VM CPU, memory, and boot disk) on Kubernetes nodes. VM CPU usage is typically tens of millicores and memory usage is typically tens of MiB. Boot disk usage is mostly incurred by the logs of the CSI driver and other system containers in the Deployment.\n", "content": "## Benefits\nThe Filestore CSI driver provides the following benefits:\n- You have access to fully-managed NFS storage through the Kubernetes APIs ( [kubectl](https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api) ).\n- You can use the GKE Filestore CSI driver to dynamically provision your PersistentVolumes.\n- You can use volume snapshots with the GKE Filestore CSI driver. [CSI volume snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) can be used to create [Filestore backups](/filestore/docs/backups) .A Filestore backup creates a differential copy of the file share, including all file data and metadata, and stores it separate from the instance. You can restore this copy to a new Filestore instance only. Restoring to an existing Filestore instance is not supported. You can use the CSI volume snapshot API to trigger Filestore backups, by adding a `type:backup` field in the [volume snapshot class](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/examples/kubernetes/backups-restore/backup-volumesnapshotclass.yaml#L7) .\n- You can use [volume expansion](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/volume-expansion) with the GKE Filestore CSI driver. Volume expansion lets you resize your volume's capacity.\n- You can access existing Filestore instances by [using pre-provisioned Filestore instances in Kubernetes workloads](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/docs/kubernetes/pre-provisioned-pv.md) . You can also dynamically create or delete Filestore instances and use them in Kubernetes workloads with a [StorageClass](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/examples/kubernetes/demo-sc.yaml) or a [Deployment](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/examples/kubernetes/demo-deployment.yaml) .\n- Supports [Filestore multishares for GKE](/filestore/docs/multishares) . This feature lets you create a Filestore instance and allocate multiple smaller NFS-mounted PersistentVolumes for it simultaneously across any number of GKE clusters.## Requirements\n- To use the Filestore CSI driver, your clusters must use GKE version 1.21 or later.\n- To use the Filestore multishares capability, your clusters must use GKE version 1.23 or later.\n- The Filestore CSI driver is supported for clusters using Linux only; Windows Server nodes are not supported.\n- The minimum instance size for Filestore is at least 1 TiB. The minimum instance size depends on the Filestore service tier you selected. To learn more, see [Service tiers](/filestore/docs/service-tiers) .\n- Filestore uses the NFSv3 file system protocol on the Filestore instance and supports any NFSv3-compatible client.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Cloud Filestore API and   the Google Kubernetes Engine API.\n- [    Enable APIs   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com,file.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- If you want to use Filestore on a Shared VPC network, see the additional setup instructions in [Use Filestore with Shared VPC](#shared-vpc-filestore) .## Enable the Filestore CSI driver on a new cluster\n**Note:** The Filestore CSI driver is enabled by default for Autopilot clusters.\nTo enable the Filestore CSI driver CSI driver when creating a new Standard cluster, follow these steps with Google Cloud CLI or the Google Cloud console.\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --addons=GcpFilestoreCsiDriver \\\u00a0 \u00a0 --cluster-version=VERSION\n```\nReplace the following:- ``: the name of your cluster.\n- ``: the GKE version number. **You must select a version of 1.21 or higher to use this feature.** Alternatively, you can use the`--release-channel`flag and specify a [release channel](/sdk/gcloud/reference/container/clusters/create#--release-channel) .\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Choose the **Standard** cluster mode, then click **Configure** .\n- Configure the cluster as desired.\n- From the navigation pane, under **Cluster** , click **Features** .\n- Select the **Enable Filestore CSI driver** checkbox.\n- Click **Create** .\nIf you want to use Filestore on a Shared VPC network, see [Enable the Filestore CSI driver on a new cluster with Shared VPC](#shared-vpc-enable) .\nAfter you enable the Filestore CSI driver, you can use the driver in Kubernetes volumes using the driver and provisioner name: `filestore.csi.storage.gke.io` .\n## Enable the Filestore CSI driver on an existing cluster\n**Note:** The Filestore CSI driver is enabled by default for Autopilot clusters.\nTo enable the Filestore CSI driver in existing clusters, use the Google Cloud CLI or the Google Cloud console.\nTo enable the driver on an existing cluster, complete the following steps:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0--update-addons=GcpFilestoreCsiDriver=ENABLED\n```\nReplace `` with the name of the existing cluster.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Features** , next to the **Filestore CSI driver** field, click **Edit Filestore CSI driver** .\n- Select the **Enable Filestore CSI driver** checkbox.\n- Click **Save Changes** .## Disable the Filestore CSI driver\nYou can disable the Filestore CSI driver on an existing Autopilot or Standard cluster by using the Google Cloud CLI or the Google Cloud console.\n**Note:** We strongly recommend not to manually disable the Filestore CSI driver on Autopilot clusters. Doing so causes any Pods using PersistentVolumes owned by the driver to fail to terminate. New pods attempting to use those PersistentVolumes will also fail to start.\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --update-addons=GcpFilestoreCsiDriver=DISABLED \\\u00a0 \u00a0 --region REGION\n```\nReplace the following values:- ``: the name of the existing cluster.\n- ``: the region for your cluster (such as,`us-central1`).\n- In the Google Cloud console, go to the Google Kubernetes Engine menu. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Under **Features** , next to the **Filestore CSI driver** field, click **Edit Filestore CSI driver** .\n- Clear the **Enable Filestore CSI driver** checkbox.\n- Click **Save Changes** .## Access pre-existing Filestore instances using the Filestore CSI driver\nThis section describes the typical process for using a Kubernetes volume to access pre-existing Filestore instances using Filestore CSI driver in GKE:\n### Create a PersistentVolume and a PersistentVolumeClaim to access the instance\n- Create a manifest file like the one shown in the following example, and name it `preprov-filestore.yaml` :```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 name: PV_NAMEspec:\u00a0 storageClassName: \"\"\u00a0 capacity:\u00a0 \u00a0 storage: 1Ti\u00a0 accessModes:\u00a0 \u00a0 - ReadWriteMany\u00a0 persistentVolumeReclaimPolicy: Retain\u00a0 volumeMode: Filesystem\u00a0 csi:\u00a0 \u00a0 driver: filestore.csi.storage.gke.io\u00a0 \u00a0 volumeHandle: \"modeInstance/FILESTORE_INSTANCE_LOCATION/FILESTORE_INSTANCE_NAME/FILESTORE_SHARE_NAME\"\u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 ip: FILESTORE_INSTANCE_IP\u00a0 \u00a0 \u00a0 volume: FILESTORE_SHARE_NAME---kind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 \u00a0 - ReadWriteMany\u00a0 storageClassName: \"\"\u00a0 volumeName: PV_NAME\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 1Ti\n```\n- To create the `PersistentVolumeClaim` and `PersistentVolume` resources based on the `preprov-filestore.yaml` manifest file, run the following command:```\nkubectl apply -f preprov-filestore.yaml\n```\nThen, proceed to [create a Deployment that consumes the volume](#deployment) .\n## Create a volume using the Filestore CSI driver\nThe following sections describe the typical process for using a Kubernetes volume backed by a Filestore CSI driver in GKE:\n- [Create a StorageClass](#storage-class) \n- [Use a PersistentVolumeClaim to access the volume](#pvc) \n- [Create a Deployment that consumes the volume](#deployment) \n### Create a StorageClass\nAfter you enable the Filestore CSI driver, GKE automatically installs the following [StorageClasses](/kubernetes-engine/docs/concepts/persistent-volumes#storageclasses) for provisioning Filestore instances:\n- `zonal-rwx`, using the [zonal Filestore tier](/filestore/docs/service-tiers#zonal_with_capacity_bands) . Only available for the higher capacity band. This StorageClass is available in GKE clusters running version 1.27 or later.\n- `enterprise-rwx`, using the [enterprise Filestore tier](/filestore/docs/service-tiers#enterprise_tier) , where each Kubernetes PersistentVolume maps to a Filestore instance. This StorageClass is available in GKE clusters running version 1.23 or later.\n- `enterprise-multishare-rwx`, using the [enterprise Filestore tier](/filestore/docs/service-tiers#enterprise_tier) , where each Kubernetes PersistentVolume maps to a share of a given Filestore instance. This StorageClass is available in GKE clusters running version 1.23 or later. To learn more, see [Filestore multishares for Google Kubernetes Engine](/filestore/docs/multishares) .\n- `standard-rwx`, using the [Basic HDD Filestore service tier](/filestore/docs/service-tiers#basic_hdd_and_basic_ssd_tiers) .\n- `premium-rwx`, using the [Basic SSD Filestore service tier](/filestore/docs/service-tiers#basic_hdd_and_basic_ssd_tiers) .\nYou can find the name of your installed `StorageClass` by running the following command:\n```\nkubectl get sc\n```\nYou can also install a different `StorageClass` that uses the Filestore CSI driver by adding `filestore.csi.storage.gke.io` in the `provisioner` field.\n**Note:** Filestore needs to know on which network to create the new instance. The automatically installed storage classes use the default network created for GKE clusters. If you have deleted this network or wish to use a different network, you must create a new storage class as described below. Otherwise, the automatically installed storage classes will not work.\n- Save the following manifest as `filestore-example-class.yaml` :```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: filestore-exampleprovisioner: filestore.csi.storage.gke.iovolumeBindingMode: ImmediateallowVolumeExpansion: trueparameters:\u00a0 tier: standard\u00a0 network: default\n```From the manifest, consider the following parameter configuration:- Setting`volumeBindingMode`to`Immediate`allows the provisioning of the volume to begin immediately. This is possible because Filestore instances are accessible from any zone. Therefore GKE does not need to know the zone where the Pod is scheduled, in contrast with Compute Engine persistent disk. When set to`WaitForFirstConsumer`, GKE begins provisioning only after the Pod is scheduled. For more information, see [VolumeBindingMode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode) .\n- Any [tier](/filestore/docs/service-tiers) can be specified in the`tier`parameter (for example,`standard`,`premium`,`zonal`,or`enterprise`). + The`network`parameter can be used when provisioning Filestore instances on non-default VPCs. Non-default VPCs require special firewall rules to be set up.\n- To create a `StorageClass` resource based on the `filestore-example-class.yaml` manifest file, run the following command:```\nkubectl create -f filestore-example-class.yaml\n```\nIf you want to use Filestore on a Shared VPC network, see [Create a StorageClass when using the Filestore CSI driver with Shared VPC](#shared-vpc-access) .\n### Use a PersistentVolumeClaim to access the volume\nYou can create a `PersistentVolumeClaim` resource that references the Filestore CSI driver's `StorageClass` .\nYou can use either a pre-installed or custom `StorageClass` .\nThe following example manifest file creates a `PersistentVolumeClaim` that references the `StorageClass` named `filestore-example` .\n- Save the following manifest file as `pvc-example.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 - ReadWriteMany\u00a0 storageClassName: filestore-example\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 1Ti\n```\n- To create a `PersistentVolume` resource based on the `pvc-example.yaml` manifest file, run the following command:```\nkubectl create -f pvc-example.yaml\n```\n### Create a Deployment that consumes the volume\nThe following example Deployment manifest consumes the `PersistentVolume` resource named `pvc-example.yaml` .\nMultiple Pods can share the same `PersistentVolumeClaim` resource.\n- Save the following manifest as `filestore-example-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: web-server-deployment\u00a0 labels:\u00a0 \u00a0 app: nginxspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /usr/share/nginx/html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: mypvc\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: mypvc\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: podpvc---kind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 \u00a0 - ReadWriteMany\u00a0 storageClassName: filestore-example\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 1Ti\n```\n- To create a Deployment based on the `filestore-example-deployment.yaml` manifest file, run the following command:```\nkubectl apply -f filestore-example-deployment.yaml\n```\n- Confirm the Deployment was successfully created:```\nkubectl get deployment\n```It might take a while for Filestore instances to complete provisioning. Before that, deployments will not report a `READY` status. You can check the progress by monitoring your PVC status by running the following command:```\nkubectl get pvc\n```You should see the PVC reach a `BOUND` status, when the volume provisioning completes.## Label Filestore instances\nYou can use [labels](/filestore/docs/managing-labels) to group related instances and store metadata about an instance. A label is a key-value pair that helps you organize your Filestore instances. You can attach a label to each resource, then filter the resources based on their labels.\nYou can provide labels by using the `labels` key in `StorageClass.parameters` . A Filestore instance can be labeled with information about what `PersistentVolumeClaim` / `PersistentVolume` the instance was created for. Custom label keys and values must comply with the label [naming convention](/resource-manager/docs/creating-managing-labels#requirements) . See the Kubernetes [storage class example](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/examples/kubernetes/sc-labels.yaml) to apply custom labels to the Filestore instance.\n## Use fsgroup with Filestore volumes\nKubernetes uses `fsGroup` to change permissions and ownership of the volume to match a user-requested `fsGroup` in the Pod's [SecurityContext](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod) . An `fsGroup` is a supplemental group that applies to all containers in a Pod. You can [apply an fsgroup ](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver/blob/master/docs/kubernetes/fsgroup.md) to volumes provisioned by the Filestore CSI driver.\n## Use Filestore with Shared VPC\nThis section covers how to use a Filestore instance on a Shared VPC network from a service project.\n### Set up a cluster with Shared VPC\nTo set up your clusters with a Shared VPC network, follow these steps:\n- [Create a host and service project](/kubernetes-engine/docs/how-to/cluster-shared-vpc#overview) .\n- [Enable the Google Kubernetes Engine API on both your host and service projects](/kubernetes-engine/docs/how-to/cluster-shared-vpc#enabling_the_api_in_your_projects) .\n- [In your host project, create a network and a subnet](/kubernetes-engine/docs/how-to/cluster-shared-vpc#creating_a_network_and_two_subnets) .\n- [Enable Shared VPC in the host project](/kubernetes-engine/docs/how-to/cluster-shared-vpc#enabling_and_granting_roles) .\n- [On the host project, grant the HostServiceAgent user role binding for the service project's GKE service account](/kubernetes-engine/docs/how-to/cluster-shared-vpc#grant_host_service_agent_role) .\n- [Enable private service access on the Shared VPC network](/filestore/docs/shared-vpc#enable_private_service_access_on_the_network) .\n### Enable the Filestore CSI driver on a new cluster with Shared VPC\nTo enable the Filestore CSI driver on a new cluster with Shared VPC, follow these steps:\n- Verify the usable subnets and secondary ranges. When creating a cluster, you must specify a subnet and the secondary IP address ranges to be used for the cluster's Pods and Service.```\ngcloud container subnets list-usable \\\u00a0 \u00a0--project=SERVICE_PROJECT_ID \\\u00a0 \u00a0--network-project=HOST_PROJECT_ID\n```The output is similar to the following:```\nPROJECT \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 REGION \u00a0 \u00a0 \u00a0 NETWORK \u00a0 \u00a0 SUBNET \u00a0RANGEHOST_PROJECT_ID \u00a0us-central1 \u00a0shared-net \u00a0tier-1 \u00a010.0.4.0/22\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 SECONDARY_RANGE_NAME \u2502 IP_CIDR_RANGE \u2502 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502 tier-1-pods \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u2502 10.4.0.0/14 \u00a0 \u2502 usable for pods or services \u2502\u2502 tier-1-services \u00a0 \u00a0 \u00a0\u2502 10.0.32.0/20 \u00a0\u2502 usable for pods or services \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n- Create a GKE cluster. The following examples show how you can use gcloud CLI to create an Autopilot or Standard cluster configured for Shared VPC. The following examples use the network, subnet, and range names from [Creating a network and two subnets](/kubernetes-engine/docs/how-to/cluster-shared-vpc#creating_a_network_and_two_subnets) .\n```\ngcloud container clusters create-auto tier-1-cluster \\\u00a0 \u00a0--project=SERVICE_PROJECT_ID \\\u00a0 \u00a0--region=COMPUTE_REGION \\\u00a0 \u00a0--network=projects/HOST_PROJECT_ID/global/networks/NETWORK_NAME \\\u00a0 \u00a0--subnetwork=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/SUBNET_NAME \\\u00a0 \u00a0--cluster-secondary-range-name=tier-1-pods \\\u00a0 \u00a0--services-secondary-range-name=tier-1-services\n```\n```\ngcloud container clusters create tier-1-cluster \\\u00a0 \u00a0--project=SERVICE_PROJECT_ID \\\u00a0 \u00a0--zone=COMPUTE_REGION \\\u00a0 \u00a0--enable-ip-alias \\\u00a0 \u00a0--network=projects/HOST_PROJECT_ID/global/networks/NETWORK_NAME \\\u00a0 \u00a0--subnetwork=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/SUBNET_NAME \\\u00a0 \u00a0--cluster-secondary-range-name=tier-1-pods \\\u00a0 \u00a0--services-secondary-range-name=tier-1-services \\\u00a0 \u00a0--addons=GcpFilestoreCsiDriver\n```\n- Create firewall rules to allow communication between nodes, Pods, and Services in your cluster. The following example shows how you can create a firewall rule named `my-shared-net-rule-2` .```\ngcloud compute firewall-rules create my-shared-net-rule-2 \\\u00a0 \u00a0--project HOST_PROJECT_ID \\\u00a0 \u00a0--network=NETWORK_NAME \\\u00a0 \u00a0--allow=tcp,udp \\\u00a0 \u00a0--direction=INGRESS \\\u00a0 \u00a0--source-ranges=10.0.4.0/22,10.4.0.0/14,10.0.32.0/20\n```In the example, the source ranges IP values come from the previous step where you verified the usable subnets and secondary ranges.\n### Create a StorageClass when using the Filestore CSI driver with Shared VPC\nThe following example shows how you can create a StorageClass when using the Filestore CSI driver with Shared VPC:\n```\ncat <<EOF | kubectl apply -f -apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: filestore-sharedvpc-exampleprovisioner: filestore.csi.storage.gke.ioparameters:\u00a0 network: \"projects/HOST_PROJECT_ID/global/networks/SHARED_VPC_NAME\"\u00a0 connect-mode: PRIVATE_SERVICE_ACCESS\u00a0 reserved-ip-range: RESERVED_IP_RANGE_NAMEallowVolumeExpansion: trueEOF\n```\nReplace the following:\n- ``: the ID or name of the host project of the Shared VPC network.\n- ``: the name of the Shared VPC network you created earlier.\n- ``: the name of the specific reserved IP address range to provision Filestore instance in. This field is optional. If a reserved IP address range is specified, it must be a named address range instead of a direct CIDR value.\nIf you want to provision a volume backed by Filestore multishares on GKE clusters running v1.23 or later, see [Optimize storage withFilestore multishares for GKE](/filestore/docs/optimize-multishares#dynamic_provisioning_on_a_shared_vpc) .\n## Reconnect Filestore single share volumes\nIf you are using Filestore with the basic HDD, basic SSD, or enterprise (single share) tier, you can follow these instructions to reconnect your existing Filestore instance to your GKE workloads.\n- Find the details of your pre-provisioned Filestore instance by following the instructions in [Getting information about a specific instance](/filestore/docs/getting-instance-information#get-instance) .\n- Redeploy your PersistentVolume specification. In the `volumeAttributes` field, modify the following fields to use the same values as your Filestore instance from step 1:- `ip`: Modify this value to the pre-provisioned Filestore instance IP address.\n- `volume`: Modify this value to the pre-provisioned Filestore instance's share name.\n- Redeploy your PersistentVolumeClaim specification. In the `volumeName` make sure you reference the same PersistentVolume name as from step 2.\n- Check the binding status of your PersistentVolumeClaim and PersistentVolume by running `kubectl get pvc` .\n- Redeploy your Pod specification and ensure that your Pod is able to access the Filestore share again.## What's next\n- [Learn how to deploy a stateful Filestore workload on GKE](/kubernetes-engine/docs/tutorials/stateful-workload) .\n- [Learn how to share a Filestore enterprise instance with multiple Persistent Volumes](/filestore/docs/multishares) .\n- [Learn how to use volume expansion](/kubernetes-engine/docs/how-to/persistent-volumes/volume-expansion) .\n- [Learn how to use volume snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) .\n- [Read more about the CSI driver on GitHub](https://github.com/kubernetes-sigs/gcp-filestore-csi-driver) .", "guide": "Google Kubernetes Engine (GKE)"}