{"title": "Google Kubernetes Engine (GKE) - Provision extra compute capacity for rapid Pod scaling", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/capacity-provisioning", "abstract": "# Google Kubernetes Engine (GKE) - Provision extra compute capacity for rapid Pod scaling\nThis page shows you how to reserve extra compute capacity in your Google Kubernetes Engine (GKE) clusters so that your workloads can rapidly scale up during high traffic events without waiting for new nodes to start. You can use these instructions to reserve compute overhead on a consistently available basis, or in advance of specific events.\n", "content": "## Why spare capacity provisioning is useful\nGKE Autopilot clusters and Standard clusters with node auto-provisioning create new nodes when there are no existing nodes with the capacity to run new Pods. Each new node takes approximately 80 to 120 seconds to boot. GKE waits until the node has started before placing pending Pods on the new node, after which the Pods can boot. In Standard clusters, you can alternatively create a new node pool manually that has the extra capacity that you need to run new Pods. This page applies to clusters that use a node autoscaling mechanism such as Autopilot or node auto-provisioning.\nIn some cases, you might want your Pods to boot faster during scale-up events. For example, if you're launching a new expansion for your popular live-service multiplayer game, the faster boot times for your game server Pods might reduce queue times for players logging in on launch day. As another example, if you run an ecommerce platform and you're planning on a flash sale for a limited time, you expect bursts of traffic for the duration of the sale.\nSpare capacity provisioning is compatible with , which lets Pods temporarily use resources that were requested by other Pods on the node, if that capacity is available and unused by other Pods. To use bursting, set your resource limits higher than your resource requests or don't set resource limits. For details, see [Configure Pod bursting in GKE](/kubernetes-engine/docs/how-to/pod-bursting-gke) .\n## How spare capacity provisioning works in GKE\nTo provision spare capacity, you can use [Kubernetes PriorityClasses](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass) and placeholder Pods. A PriorityClass lets you tell GKE that some workloads are a lower priority than other workloads. You can deploy placeholder Pods that use a low priority PriorityClass and request the compute capacity that you need to reserve. GKE adds capacity to the cluster by creating new nodes to accommodate the placeholder Pods.\nWhen your production workloads scale up, GKE evicts the lower-priority placeholder Pods and schedules the new replicas of your production Pods (which use a higher priority PriorityClass) in their place. If you have multiple low-priority Pods that have different priority levels, GKE evicts the lowest priority Pods first.\n### Capacity provisioning methods\nDepending on your use case, you can provision extra capacity in your GKE clusters in one of the following ways:\n- **Consistent capacity provisioning** : Use a Deployment to create a specific number of low priority placeholder Pods that constantly run in the cluster. When GKE evicts these Pods to run your production workloads, the Deployment controller ensures that GKE provisions more capacity to recreate the evicted low priority Pods. This method provides consistent capacity overhead across multiple scale-up and scale-down events, until you delete the Deployment.\n- **Single use capacity provisioning** : Use a Job to run a specific number of low priority parallel placeholder Pods for a specific period of time. When that time has passed or when GKE evicts all the Job replicas, the reserved capacity stops being available. This method provides a specific amount of available capacity for a specific period.## Pricing\nIn GKE Autopilot, you're charged for the resource requests of your running Pods, including the low priority workloads that you deploy. For details, see [Autopilot pricing](/kubernetes-engine/pricing#autopilot_mode) .\nIn GKE Standard, you're charged for the underlying Compute Engine VMs that GKE provisions, regardless of whether Pods use that capacity. For details, see [Standard pricing](/kubernetes-engine/pricing#standard_mode)\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have a [GKE Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster#create_an_autopilot_cluster) , or a GKE Standard cluster with node auto-provisioning enabled.\n- Read the [Considerations](#considerations) for capacity provisioning to ensure that you choose appropriate values in your capacity requests.## Create a PriorityClass\nTo use either of the methods described in [Capacity provisioning methods](#methods) , you first need to create the following PriorityClasses:\n- **Default PriorityClass** : A global default PriorityClass that's assigned to any Pod that doesn't explicitly set a different PriorityClass in the Pod specification. Pods with this default PriorityClass can evict Pods that use a lower PriorityClass.\n- **Low PriorityClass** : A non-default PriorityClass set to the lowest priority possible in GKE. Pods with this PriorityClass can be evicted to run Pods with higher PriorityClasses.\n- Save the following manifest as `priorityclasses.yaml` :```\napiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata:\u00a0 name: low-priorityvalue: -10preemptionPolicy: NeverglobalDefault: falsedescription: \"Low priority workloads\"---apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata:\u00a0 name: default-priorityvalue: 0preemptionPolicy: PreemptLowerPriorityglobalDefault: truedescription: \"The global default priority.\"\n```This manifest includes the following fields:- `preemptionPolicy`: Specifies whether or not Pods using a PriorityClass can evict lower priority Pods. The`low-priority`PriorityClass uses`Never`, and the`default`PriorityClass uses`PreemptLowerPriority`.\n- `value` : The priority for Pods that use the PriorityClass. The `default` PriorityClass uses `0` . The `low-priority` PriorityClass uses `-1` . In Autopilot, you can set this to any value that's less than the `default` PriorityClass priority.In Standard, if you set this value to less than `-10` , Pods that use that PriorityClass won't trigger new node creation and remain in Pending.For help deciding on appropriate values for priority, see [Choose a priority](#choose-priority) .\n- `globalDefault` : Specifies whether or not GKE assigns the PriorityClass to Pods that don't explicitly set a PriorityClass in the Pod specification. The `low-priority` PriorityClass uses `false` , and the `default` PriorityClass uses `true` .\n- Apply the manifest:```\nkubectl apply -f priorityclasses.yaml\n```## Provision extra compute capacity\nThe following sections show an example in which you provision capacity for a single event or consistently over time.\n### Use a Deployment for consistent capacity provisioning\n- Save the following manifest as `capacity-res-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: capacity-res-deployspec:\u00a0 replicas: 10\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: reservation\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: reservation\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 priorityClassName: low-priority\u00a0 \u00a0 \u00a0 terminationGracePeriodSeconds: 0\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: ubuntu\u00a0 \u00a0 \u00a0 \u00a0 image: ubuntu\u00a0 \u00a0 \u00a0 \u00a0 command: [\"sleep\"]\u00a0 \u00a0 \u00a0 \u00a0 args: [\"infinity\"]\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 500m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 500Mi\n```This manifest includes the following fields:- `spec.replicas`: Change this value to meet your requirements.\n- `spec.resources.requests`: Change the CPU and memory requests to meet your requirements. Use the guidance in [Choose capacity sizing](#choose-sizing) to help you decide on appropriate request values.\n- `spec.containers.command`and`spec.containers.args`: Tell the Pods to remain active until evicted by GKE.\n- Apply the manifest:```\nkubectl apply -f capacity-res-deployment.yaml\n```\n- Get the Pod status:```\nkubectl get pods -l app=reservation\n```Wait until all the replicas have a status of `Running` .\n### Use a Job for single event capacity provisioning\n- Save the following manifest as `capacity-res-job.yaml` :```\napiVersion: batch/v1kind: Jobmetadata:\u00a0 name: capacity-res-jobspec:\u00a0 parallelism: 4\u00a0 backoffLimit: 0\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 priorityClassName: low-priority\u00a0 \u00a0 \u00a0 terminationGracePeriodSeconds: 0\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: ubuntu-container\u00a0 \u00a0 \u00a0 \u00a0 image: ubuntu\u00a0 \u00a0 \u00a0 \u00a0 command: [\"sleep\"]\u00a0 \u00a0 \u00a0 \u00a0 args: [\"36000\"]\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"16\"\u00a0 \u00a0 \u00a0 restartPolicy: Never\n```This manifest includes the following fields:- `spec.parallelism`: Change to the number of Jobs you want to run in parallel to reserve capacity.\n- `spec.backoffLimit: 0`: Prevent the Job controller from recreating evicted Jobs.\n- `template.spec.resources.requests`: Change the CPU and memory requests to meet your requirements. Use the guidance in [Considerations](#considerations) to help you decide on appropriate values.\n- `template.spec.containers.command`and`template.spec.containers.args`: Tell the Jobs to remain active for the period of time, in seconds, during which you need the extra capacity.\n- Apply the manifest:```\nkubectl apply -f capacity-res-job.yaml\n```\n- Get the Job status:```\nkubectl get jobs\n```Wait until all the Jobs have a status of `Running` .## Test the capacity provisioning and eviction\nTo verify that capacity provisioning works as expected, do the following:\n- In your terminal, watch the status of the capacity provisioning workloads:- For Deployments, run the following command:```\nkubectl get pods --label=app=reservation -w\n```\n- For Jobs, run the following command:```\nkubectl get Jobs -w\n```\n- Open a **new** terminal window and do the following:- Save the following manifest as `test-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: helloweb\u00a0 labels:\u00a0 \u00a0 app: hellospec:\u00a0 replicas: 5\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: hello\u00a0 \u00a0 \u00a0 tier: web\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: hello\u00a0 \u00a0 \u00a0 \u00a0 tier: web\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 400m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 400Mi\n```\n- Apply the manifest:```\nkubectl apply -f test-deployment.yaml\n```\n- In the original terminal window, note that GKE terminates some of the capacity provisioning workloads to schedule your new replicas, similar to the following example:```\nNAME           READY STATUS RESTARTS AGE\ncapacity-res-deploy-6bd9b54ffc-5p6wc   1/1  Running 0   7m25s\ncapacity-res-deploy-6bd9b54ffc-9tjbt   1/1  Running 0   7m26s\ncapacity-res-deploy-6bd9b54ffc-kvqr8   1/1  Running 0   2m32s\ncapacity-res-deploy-6bd9b54ffc-n7zn4   1/1  Running 0   2m33s\ncapacity-res-deploy-6bd9b54ffc-pgw2n   1/1  Running 0   2m32s\ncapacity-res-deploy-6bd9b54ffc-t5t57   1/1  Running 0   2m32s\ncapacity-res-deploy-6bd9b54ffc-v4f5f   1/1  Running 0   7m24s\nhelloweb-85df88c986-zmk4f     0/1  Pending 0   0s\nhelloweb-85df88c986-lllbd     0/1  Pending 0   0s\nhelloweb-85df88c986-bw7x4     0/1  Pending 0   0s\nhelloweb-85df88c986-gh8q8     0/1  Pending 0   0s\nhelloweb-85df88c986-74jrl     0/1  Pending 0   0s\ncapacity-res-deploy-6bd9b54ffc-v6dtk 1/1  Terminating 0   2m47s\ncapacity-res-deploy-6bd9b54ffc-kvqr8 1/1  Terminating 0   2m47s\ncapacity-res-deploy-6bd9b54ffc-pgw2n 1/1  Terminating 0   2m47s\ncapacity-res-deploy-6bd9b54ffc-n7zn4 1/1  Terminating 0   2m48s\ncapacity-res-deploy-6bd9b54ffc-2f8kx 1/1  Terminating 0   2m48s\n...\nhelloweb-85df88c986-lllbd    0/1  Pending  0   1s\nhelloweb-85df88c986-gh8q8    0/1  Pending  0   1s\nhelloweb-85df88c986-74jrl    0/1  Pending  0   1s\nhelloweb-85df88c986-zmk4f    0/1  Pending  0   1s\nhelloweb-85df88c986-bw7x4    0/1  Pending  0   1s\nhelloweb-85df88c986-gh8q8    0/1  ContainerCreating 0   1s\nhelloweb-85df88c986-zmk4f    0/1  ContainerCreating 0   1s\nhelloweb-85df88c986-bw7x4    0/1  ContainerCreating 0   1s\nhelloweb-85df88c986-lllbd    0/1  ContainerCreating 0   1s\nhelloweb-85df88c986-74jrl    0/1  ContainerCreating 0   1s\nhelloweb-85df88c986-zmk4f    1/1  Running    0   4s\nhelloweb-85df88c986-lllbd    1/1  Running    0   4s\nhelloweb-85df88c986-74jrl    1/1  Running    0   5s\nhelloweb-85df88c986-gh8q8    1/1  Running    0   5s\nhelloweb-85df88c986-bw7x4    1/1  Running    0   5s\n```This output shows that your new Deployment took five seconds to change from Pending to Running.## Considerations for capacity provisioning\n### Consistent capacity provisioning\n- Evaluate how many placeholder Pod replicas you need and the size of the requests in each replica. The low priority replicas should requestthe same capacity as your largest production workload, so that those workloads can fit in the capacity reserved by your low priority workload.\n- If you operate large numbers of production workloads at scale, consider setting the resource requests of your placeholder Pods to values that provision enough capacity to run multiple production workloads instead of just one.\n### Single use capacity provisioning\n- Set the length of time for the placeholder Jobs to persist to the time during which you need additional capacity. For example, if you want the additional capacity for a 24 hour game launch day, set the length of time to 86400 seconds. This ensures that the provisioned capacity doesn't last longer than you need it.\n- Set a [maintenance window](/kubernetes-engine/docs/how-to/maintenance-windows-and-exclusions#maintenance-window) for the same period of time that you're reserving the capacity. This prevents your low priority Jobs from being evicted during a node upgrade. Setting a maintenance window is also a good practice when you're anticipating high demand for your workload.\n- If you operate large numbers of production workloads at scale, consider setting the resource requests of your placeholder Jobs to values that provision enough capacity to run multiple production workloads instead of just one.\nCapacity is only provisioned for a single scaling event. If you scale up and use the capacity, then scale down, that capacity is no longer available for another scale-up event. If you anticipate multiple scale-up and scale-down events, use the consistent capacity reservation method and adjust the size of the reservation as needed. For example, making the Pod requests larger ahead of an event, and lower or zero after.\n### Choose a priority\n**Set the priority in your PriorityClasses to less than 0.**\nYou can define multiple PriorityClasses in your cluster to use with workloads that have different requirements. For example, you could create a PriorityClass with a -10 priority for single-use capacity provisioning and a PriorityClass with a -9 priority for consistent capacity provisioning. You could then provision consistent capacity using the PriorityClass with -9 priority and, when you want more capacity for a special event, you could deploy new Jobs that use the PriorityClass with -10 priority. GKE evicts the lowest priority workloads first.\n**Caution:** In Standard clusters with node auto-provisioning, if you deploy workloads with a priority less than -10, GKE won't create new nodes to run the workloads.\nYou can also use other PriorityClasses to run low priority non-production workloads that perform actual tasks, such as fault-tolerant batch workloads, at a priority that's lower than your production workloads but higher than your placeholder Pods. For example, -5.\n### Choose capacity sizing\n**Set replica counts and resource requests of your placeholder workload togreater than or equal to the capacity that your production workloads might needwhen scaling up.**\nThe total capacity provisioned is based on the number of placeholder Pods that you deploy and the resource requests of each replica. If your scale-up requires more capacity than GKE provisioned for your placeholder Pods, some of your production workloads remain in `Pending` until GKE can provision more capacity.\n## What's next\n- [Learn how to separate your workloads from each other](/kubernetes-engine/docs/how-to/workload-separation) \n- [Learn how to optimize autoscaling your workloads based on metrics](/kubernetes-engine/docs/concepts/custom-and-external-metrics)", "guide": "Google Kubernetes Engine (GKE)"}