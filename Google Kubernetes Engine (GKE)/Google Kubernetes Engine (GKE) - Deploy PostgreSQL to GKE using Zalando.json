{"title": "Google Kubernetes Engine (GKE) - Deploy PostgreSQL to GKE using Zalando", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/zalando", "abstract": "# Google Kubernetes Engine (GKE) - Deploy PostgreSQL to GKE using Zalando\nThe guide shows you how to use the [Zalando Postgres operator](https://github.com/zalando/postgres-operator) to deploy Postgres clusters to Google Kubernetes Engine (GKE).\n [PostgreSQL](https://www.postgresql.org/) is a powerful, open source object-relational database system with several decades of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\nThis guide is intended for platform administrators, cloud architects, and operations professionals interested in running PostgreSQL as a database application on GKE instead of using [Cloud SQL for PostgreSQL](/kubernetes-engine/docs/concepts/database-options) .", "content": "## Objectives\n- Plan and deploy GKE infrastructure for Postgres\n- Deploy and configure the Zalando Postgres operator\n- Configure Postgres using the operator to ensure availability, security, observability and performance\n### BenefitsZalando offers the following benefits:- A declarative and Kubernetes-native way to manage and configure the PostgreSQL clusters\n- High availability provided by [Patroni](https://patroni.readthedocs.io/en/latest/) \n- Backup management support using [Cloud Storage buckets](https://postgres-operator.readthedocs.io/en/latest/administrator/#google-cloud-platform-setup) \n- Rolling updates on Postgres cluster changes, including quick minor version updates\n- Declarative [User](https://github.com/zalando/postgres-operator/blob/master/docs/user.md#defining-database-roles-in-the-operator) management with password generation and rotation using custom resources\n- Support for [TLS](https://postgres-operator.readthedocs.io/en/latest/user/#custom-tls-certificates) , certificate rotation, and [connection pools](https://postgres-operator.readthedocs.io/en/latest/user/#connection-pooler) \n- Cluster [cloning](https://postgres-operator.readthedocs.io/en/latest/user/#clone-directly) and data replication\n### Deployment architectureIn this tutorial, you use the Zalando Postgres operator to deploy and configure a highly available Postgres cluster to GKE. The cluster has one leader replica and two standby (read-only) replicas managed by [Patroni](https://github.com/zalando/patroni) . Patroni is an open source solution maintained by Zalando to provide high-availability and auto-failover capabilities to Postgres. In case of leader failure, one standby replica is automatically promoted to a leader role.\nYou also deploy a highly available regional GKE cluster for Postgres, with multiple >Kubernetes nodes spread across several availability zones. This setup helps ensure fault tolerance, scalability, and geographic redundancy. It allows for rolling updates and maintenance while providing SLAs for uptime and availability. For more information, see [Regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) .\nThe following diagram shows a Postgres cluster running on multiple nodes and zones in a GKE cluster:In the diagram, the Postgres `StatefulSet` is deployed across three nodes in three different zones. You can control how GKE deploys to nodes by setting the required Pod [affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) rules on the [postgresql](https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml) custom resource specification. If one zone fails, using the recommended configuration, GKE reschedules Pods on other available nodes in your cluster. For persisting data, you use SSD disks ( `premium-rwo` [StorageClass](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver#create_a_storageclass) ), which are [recommended](/compute/docs/disks/performance) in most cases for highly loaded databases due to their [low latency and high IOPS](/compute/docs/disks/performance#type_comparison) .## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin [Cloud Shell](/shell) is preinstalled with the software you need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) . If you don't use Cloud Shell, you must install the gcloud CLI.- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/storage.objectViewer, roles/container.admin, roles/iam.serviceAccountAdmin, roles/compute.admin, roles/gkebackup.admin, roles/monitoring.viewer` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.\n### Set up your environmentTo set up your environment, follow these steps- Set environment variables:```\nexport PROJECT_ID=PROJECT_ID\nexport KUBERNETES_CLUSTER_PREFIX=postgres\nexport REGION=us-central1\n```Replace `` with your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory:```\ncd kubernetes-engine-samples/databases/postgres-zalando\n```\n## Create your cluster infrastructureIn this section, you run a Terraform script to create a private, highly-available, regional GKE cluster.\nYou can install the operator using a [Standard or Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode) cluster.\nThe following diagram shows a private regional Standard GKE cluster deployed across three different zones:Deploy this infrastructure:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply \\\u00a0 -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- A node pools with auto scaling enabled (one to two nodes per zone, one node per zone minimum)\n- A`ServiceAccount`with logging and monitoring permissions\n- Backup for GKE for disaster recovery\n- Google Cloud Managed Service for Prometheus for cluster monitoring\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 14 added, 0 changed, 0 destroyed.\n...\n```\nThe following diagram shows a private regional Autopilot GKE cluster:Deploy the infrastructure:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply \\\u00a0 -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- A`ServiceAccount`with logging and monitoring permission\n- Google Cloud Managed Service for Prometheus for cluster monitoring\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 12 added, 0 changed, 0 destroyed.\n...\n```\n### Connect to the clusterConfigure `kubectl` to communicate with the cluster:\n```\ngcloud container clusters get-credentials ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the Zalando operator to your clusterDeploy the Zalando operator to your Kubernetes cluster using a Helm chart.- Add the Zalando operator Helm Chart repository:```\nhelm repo add postgres-operator-charts https://opensource.zalando.com/postgres-operator/charts/postgres-operator\n```\n- Create a namespace for the Zalando operator and the Postgres cluster:```\nkubectl create ns postgreskubectl create ns zalando\n```\n- Deploy the Zalando operator using the Helm command-line tool:```\nhelm install postgres-operator postgres-operator-charts/postgres-operator -n zalando \\\u00a0 \u00a0 --set configKubernetes.enable_pod_antiaffinity=true \\\u00a0 \u00a0 --set configKubernetes.pod_antiaffinity_preferred_during_scheduling=true \\\u00a0 \u00a0 --set configKubernetes.pod_antiaffinity_topology_key=\"topology.kubernetes.io/zone\" \\\u00a0 \u00a0 --set configKubernetes.spilo_fsgroup=\"103\"\n```You can't configure `podAntiAffinity` settings directly on the custom resource representing the Postgres cluster. Instead, set `podAntiAffinity` settings globally for all Postgres clusters in the operator settings.\n- Check the deployment status of the Zalando operator using Helm:```\nhelm ls -n zalando\n```The output is similar to the following:```\nNAME     NAMESPACE REVISION UPDATED        STATUS  CHART      APP VERSION\npostgres-operator zalando  1   2023-10-13 16:04:13.945614 +0200 CEST deployed postgres-operator-1.10.1 1.10.1\n```\n## Deploy PostgresThe basic configuration for the Postgres cluster instance includes the following components:- Three Postgres replicas: one leader and two standby replicas.\n- CPU resource allocation of one CPU request and two CPU limits, with 4\u00a0GB memory requests and limits.\n- Tolerations,`nodeAffinities`, and`topologySpreadConstraints`configured for each workload, ensuring proper distribution across Kubernetes nodes, utilizing their respective node pools and different availability zones.\nThis configuration represents the minimal setup required to create a production-ready Postgres cluster.\nThe following manifest describes a Postgres cluster:\n [  databases/postgres-zalando/manifests/01-basic-cluster/my-cluster.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/postgres-zalando/manifests/01-basic-cluster/my-cluster.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/postgres-zalando/manifests/01-basic-cluster/my-cluster.yaml) \n```\napiVersion: \"acid.zalan.do/v1\"kind: postgresqlmetadata:\u00a0 name: my-clusterspec:\u00a0 dockerImage: ghcr.io/zalando/spilo-15:3.0-p1\u00a0 teamId: \"my-team\"\u00a0 numberOfInstances: 3\u00a0 users:\u00a0 \u00a0 mydatabaseowner:\u00a0 \u00a0 - superuser\u00a0 \u00a0 - createdb\u00a0 \u00a0 myuser: []\u00a0 databases:\u00a0 \u00a0 mydatabase: mydatabaseowner\u00a0 postgresql:\u00a0 \u00a0 version: \"15\"\u00a0 \u00a0 parameters:\u00a0 \u00a0 \u00a0 shared_buffers: \"32MB\"\u00a0 \u00a0 \u00a0 max_connections: \"10\"\u00a0 \u00a0 \u00a0 log_statement: \"all\"\u00a0 \u00a0 \u00a0 password_encryption: scram-sha-256\u00a0 volume:\u00a0 \u00a0 size: 5Gi\u00a0 \u00a0 storageClass: premium-rwo\u00a0 enableShmVolume: true\u00a0 podAnnotations:\u00a0 \u00a0 cluster-autoscaler.kubernetes.io/safe-to-evict: \"true\"\u00a0 tolerations:\u00a0 - key: \"app.stateful/component\"\u00a0 \u00a0 operator: \"Equal\"\u00a0 \u00a0 value: \"postgres-operator\"\u00a0 \u00a0 effect: NoSchedule\u00a0 nodeAffinity:\u00a0 \u00a0 preferredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 - weight: 1\u00a0 \u00a0 \u00a0 preference:\u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 - key: \"app.stateful/component\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"postgres-operator\"\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 memory: 4Gi\u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 memory: 4Gi\u00a0 sidecars:\u00a0 \u00a0 - name: exporter\u00a0 \u00a0 \u00a0 image: quay.io/prometheuscommunity/postgres-exporter:v0.14.0\u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 - --collector.stat_statements\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - name: exporter\u00a0 \u00a0 \u00a0 \u00a0 containerPort: 9187\u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 500m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 256M\u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 100m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 256M\u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 - name: \"DATA_SOURCE_URI\"\u00a0 \u00a0 \u00a0 \u00a0 value: \"localhost/postgres?sslmode=require\"\u00a0 \u00a0 \u00a0 - name: \"DATA_SOURCE_USER\"\u00a0 \u00a0 \u00a0 \u00a0 value: \"$(POSTGRES_USER)\"\u00a0 \u00a0 \u00a0 - name: \"DATA_SOURCE_PASS\"\u00a0 \u00a0 \u00a0 \u00a0 value: \"$(POSTGRES_PASSWORD)\"\n```\nThis manifest has the following fields:- `spec.teamId`: a prefix for the cluster objects that you choose\n- `spec.numberOfInstances`: the total number of instances for a cluster\n- `spec.users`: the user list with [privileges](https://www.postgresql.org/docs/current/sql-createrole.html) \n- `spec.databases`: the database list in the format`dbname: ownername`\n- `spec.postgresql`: postgres parameters\n- `spec.volume`: Persistent Disk parameters\n- `spec.tolerations`: the tolerations Pod template that allows cluster Pods to be scheduled on`pool-postgres`nodes\n- `spec.nodeAffinity`: the`nodeAffinity`Pod template that tells GKE that cluster Pods prefer to be scheduled on`pool-postgres`nodes.\n- `spec.resources`: requests and limits for cluster Pods\n- `spec.sidecars`: a list of sidecar containers, which contains`postgres-exporter`\nFor more information, see [Cluster manifest reference](https://postgres-operator.readthedocs.io/en/latest/reference/cluster_manifest/) in the Postgres documentation.\n### Create a basic Postgres cluster\n- Create a new Postgres cluster using the basic configuration:```\nkubectl apply -n postgres -f manifests/01-basic-cluster/my-cluster.yaml\n```This command creates a PostgreSQL Custom Resource of the Zalando operator with:- CPU and memory requests and limits\n- Taints and affinities to distribute the provisioned Pod replicas across GKE nodes.\n- A database\n- Two users with database owner permissions\n- A user with no permissions\n- Wait for GKE to start the required workloads:```\nkubectl wait pods -l cluster-name=my-cluster \u00a0--for condition=Ready --timeout=300s -n postgres\n```This command might take a few minutes to complete.\n- Verify that GKE created the Postgres workloads:```\nkubectl get pod,svc,statefulset,deploy,pdb,secret -n postgres\n```The output is similar to the following:```\nNAME         READY STATUS RESTARTS AGE\npod/my-cluster-0      1/1  Running 0   6m41s\npod/my-cluster-1      1/1  Running 0   5m56s\npod/my-cluster-2      1/1  Running 0   5m16s\npod/postgres-operator-db9667d4d-rgcs8 1/1  Running 0   12m\nNAME      TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nservice/my-cluster   ClusterIP 10.52.12.109 <none>  5432/TCP 6m43s\nservice/my-cluster-config ClusterIP None  <none>  <none> 5m55s\nservice/my-cluster-repl  ClusterIP 10.52.6.152 <none>  5432/TCP 6m43s\nservice/postgres-operator ClusterIP 10.52.8.176 <none>  8080/TCP 12m\nNAME      READY AGE\nstatefulset.apps/my-cluster 3/3 6m43s\nNAME        READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/postgres-operator 1/1  1   1   12m\nNAME            MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE\npoddisruptionbudget.policy/postgres-my-cluster-pdb 1    N/A    0     6m44s\nNAME               TYPE    DATA AGE\nsecret/my-user.my-cluster.credentials.postgresql.acid.zalan.do Opaque    2 6m45s\nsecret/postgres.my-cluster.credentials.postgresql.acid.zalan.do Opaque   2 6m44s\nsecret/sh.helm.release.v1.postgres-operator.v1     helm.sh/release.v1 1  12m\nsecret/standby.my-cluster.credentials.postgresql.acid.zalan.do Opaque    2 6m44s\nsecret/zalando.my-cluster.credentials.postgresql.acid.zalan.do Opaque    2 6m44s\n```\nThe operator creates the following resources:- A Postgres StatefulSet, which controls three Pod replicas for Postgres\n- A`PodDisruptionBudgets`, ensuring a minimum of one available replica\n- The`my-cluster`Service, which targets the leader replica only\n- The`my-cluster-repl`Service, which exposes the Postgres port for incoming connections and for replication between Postgres replicas\n- The`my-cluster-config`headless Service, to get the list of running Postgres Pod replicas\n- Secrets with user credentials for accessing the database and replication between Postgres nodes\n## Authenticate to PostgresYou can create Postgres users and assign them database permissions. For example, the following manifest describes a custom resource that assigns users and roles:\n```\napiVersion: \"acid.zalan.do/v1\"kind: postgresqlmetadata:\u00a0 name: my-clusterspec:\u00a0 ...\u00a0 users:\u00a0 \u00a0 mydatabaseowner:\u00a0 \u00a0 - superuser\u00a0 \u00a0 - createdb\u00a0 \u00a0 myuser: []\u00a0 databases:\u00a0 \u00a0 mydatabase: mydatabaseowner\n```\nIn this manifest:- The`mydatabaseowner`user has the [SUPERUSER and CREATEDB](https://www.postgresql.org/docs/16/sql-createrole.html) roles, which permit full administrator rights (i.e. manage Postgres configuration, create new databases, tables, and users). You shouldn't share this user with clients. For example Cloud SQL [doesn't allow](/sql/docs/postgres/users) customers to have access to users with the`SUPERUSER`role.\n- The`myuser`user has no roles assigned. This follows the [best practice](https://wiki.postgresql.org/wiki/Client_Authentication) of using the`SUPERUSER`to create users with least privileges. Granular rights are granted to`myuser`by`mydatabaseowner`. To maintain security, you should only share`myuser`credentials with client applications.\n### Store passwordsYou should use the `scram-sha-256` [recommended method for storing passwords](https://www.postgresql.org/docs/current/auth-password.html) . For example, the following manifest describes a custom resource that specifies `scram-sha-256` encryption using the `postgresql.parameters.password_encryption` field:\n```\napiVersion: \"acid.zalan.do/v1\"kind: postgresqlmetadata:\u00a0 name: my-clusterspec:\u00a0 ...\u00a0 postgresql:\u00a0 \u00a0 parameters:\u00a0 \u00a0 \u00a0 password_encryption: scram-sha-256\n```\n### Rotate user credentialsYou can [rotate user credentials](https://postgres-operator.readthedocs.io/en/latest/administrator/#password-rotation-in-k8s-secrets) that are stored in Kubernetes Secrets with Zalando. For example, the following manifest describes a custom resource that defines user credential rotation using the `usersWithSecretRotation` field:\n```\napiVersion: \"acid.zalan.do/v1\"kind: postgresqlmetadata:\u00a0 name: my-clusterspec:\u00a0 ...\u00a0 usersWithSecretRotation:\u00a0 - myuser\u00a0 - myanotheruser\u00a0 - ...\n```\n### Authentication example: connect to PostgresThis section shows you how to deploy an example Postgres client and connect to the database using the password from a Kubernetes Secret.- Run the client Pod to interact with your Postgres cluster:```\nkubectl apply -n postgres -f manifests/02-auth/client-pod.yaml\n```The credentials of the `myuser` and `mydatabaseowner` users are taken from the related Secrets and mounted as environment variables to the Pod.\n- Connect to the Pod when it is ready:```\nkubectl wait pod postgres-client --for=condition=Ready --timeout=300s -n postgreskubectl exec -it postgres-client -n postgres -- /bin/bash\n```\n- Connect to Postgres and attempt to create a new table using `myuser` credentials:```\nPGPASSWORD=$CLIENTPASSWORD psql \\\u00a0 -h my-cluster \\\u00a0 -U $CLIENTUSERNAME \\\u00a0 -d mydatabase \\\u00a0 -c \"CREATE TABLE test (id serial PRIMARY KEY, randomdata VARCHAR ( 50 ) NOT NULL);\"\n```The command should fail with an error similar to the following:```\nERROR: permission denied for schema public\nLINE 1: CREATE TABLE test (id serial PRIMARY KEY, randomdata VARCHAR...\n```The command fails because users without assigned privileges by default can only login to Postgres and list databases.\n- Create a table with `mydatabaseowner` credentials and grant **all** privileges on the table to `myuser` :```\nPGPASSWORD=$OWNERPASSWORD psql \\\u00a0 -h my-cluster \\\u00a0 -U $OWNERUSERNAME \\\u00a0 -d mydatabase \\\u00a0 -c \"CREATE TABLE test (id serial PRIMARY KEY, randomdata VARCHAR ( 50 ) NOT NULL);GRANT ALL ON test TO myuser;GRANT ALL ON SEQUENCE test_id_seq TO myuser;\"\n```The output is similar to the following:```\nCREATE TABLE\nGRANT\nGRANT\n```\n- Insert random data into the table using `myuser` credentials:```\nfor i in {1..10}; do\u00a0 DATA=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13)\u00a0 PGPASSWORD=$CLIENTPASSWORD psql \\\u00a0 -h my-cluster \\\u00a0 -U $CLIENTUSERNAME \\\u00a0 -d mydatabase \\\u00a0 -c \"INSERT INTO test(randomdata) VALUES ('$DATA');\"done\n```The output is similar to the following:```\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\nINSERT 0 1\n```\n- Get the values that you inserted:```\nPGPASSWORD=$CLIENTPASSWORD psql \\\u00a0 -h my-cluster \\\u00a0 -U $CLIENTUSERNAME \\\u00a0 -d mydatabase \\\u00a0 -c \"SELECT * FROM test;\"\n```The output is similar to the following:```\nid | randomdata\n----+-------------- 1 | jup9HYsAjwtW4\n 2 | 9rLAyBlcpLgNT\n 3 | wcXSqxb5Yz75g\n 4 | KoDRSrx3muD6T\n 5 | b9atC7RPai7En\n 6 | 20d7kC8E6Vt1V\n 7 | GmgNxaWbkevGq\n 8 | BkTwFWH6hWC7r\n 9 | nkLXHclkaqkqy\n 10 | HEebZ9Lp71Nm3\n(10 rows)\n```\n- Exit the Pod shell:```\nexit\n```\n## Understand how Prometheus collects metrics for your Postgres clusterThe following diagram shows how Prometheus metrics collecting works:In the diagram, a GKE private cluster contains:- A Postgres Pod that gathers metrics on path`/`and port`9187`\n- Prometheus-based collectors that process the metrics from the Postgres Pod\n- A`PodMonitoring`resource that sends metrics to Cloud Monitoring\nGoogle Cloud Managed Service for Prometheus supports metrics collection in the Prometheus format. Cloud Monitoring uses an [integrated dashboard](/stackdriver/docs/managed-prometheus/exporters/postgresql) for Postgres metrics.\nZalando exposes cluster metrics in the Prometheus format using the [postgres_exporter component](https://github.com/prometheus-community/postgres_exporter) as a [sidecar container](https://postgres-operator.readthedocs.io/en/latest/user/#sidecar-support) .- Create the [PodMonitoring](/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring) resource to scrape metrics by `labelSelector` :```\nkubectl apply -n postgres -f manifests/03-prometheus-metrics/pod-monitoring.yaml\n```\n- In the Google Cloud console, go to the **GKE Clusters Dashboard** page. [Go to GKE Clusters Dashboard](https://console.cloud.google.com/monitoring/dashboards/resourceList/gmp_gke_cluster) The dashboard shows a non-zero metrics ingestion rate.\n- In the Google Cloud console, go to the **Dashboards** page. [Go to Dashboards](https://console.cloud.google.com/monitoring/dashboards) \n- Open the **PostgreSQL Prometheus Overview dashboard** . The dashboard shows the number of fetched rows. It might take several minutes for the dashboard to auto-provision.\n- Connect to the client Pod:```\nkubectl exec -it postgres-client -n postgres -- /bin/bash\n```\n- Insert random data:```\nfor i in {1..100}; do\u00a0 DATA=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 13)\u00a0 PGPASSWORD=$CLIENTPASSWORD psql \\\u00a0 -h my-cluster \\\u00a0 -U $CLIENTUSERNAME \\\u00a0 -d mydatabase \\\u00a0 -c \"INSERT INTO test(randomdata) VALUES ('$DATA');\"done\n```\n- Refresh the page. The **Rows** and **Blocks** graphs update to show the actual database state.\n- Exit the Pod shell:```\nexit\n```\n## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete individual resources\n- Set environment variables.```\nexport PROJECT_ID=${PROJECT_ID}export KUBERNETES_CLUSTER_PREFIX=postgresexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform \u00a0-chdir=terraform/FOLDER destroy \\\u00a0 -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,zone)\")\n```\n- Delete the disks:```\nfor i in $disk_list; do\u00a0 disk_name=$(echo $i| cut -d'|' -f1)\u00a0 disk_zone=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0 echo \"Deleting $disk_name\"\u00a0 gcloud compute disks delete $disk_name --zone $disk_zone --quietdone\n```\n- Delete the GitHub repository:```\nrm -r ~/kubernetes-engine-samples/\n```\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}