{"title": "Google Kubernetes Engine (GKE) - Harden workload isolation with GKE Sandbox", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods", "abstract": "# Google Kubernetes Engine (GKE) - Harden workload isolation with GKE Sandbox\nThis page describes how to use [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) to protect the host kernel on your nodes when containers in the Pod execute unknown or untrusted code, or need extra isolation from the node.\n", "content": "## GKE Sandbox availability\nGKE Sandbox is ready to use in Autopilot clusters running GKE version 1.27.4-gke.800 and later. To start deploying Autopilot workloads in a sandbox, skip to [Working with GKE Sandbox](#working_with) .\nTo use GKE Sandbox in new or existing GKE Standard clusters, you must manually enable GKE Sandbox on the cluster.\n### Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Enable GKE Sandbox on a new Standard cluster\nThe default node pool, which is created when you create a new cluster, can't use GKE Sandbox if it's the only node pool in the cluster, because GKE-managed system workloads must run separately from untrusted sandboxed workloads. To enable GKE Sandbox during cluster creation, you must add at least one extra node pool to the cluster.\nTo view your clusters, visit the Google Kubernetes Engine menu in the Google Cloud console.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- : From the navigation menu, under **Cluster** , click **Features** and select the following check boxes so that gVisor messages are logged:- **Cloud Logging** \n- **Cloud Monitoring** \n- **Managed Service for Prometheus** \n- Click **Add NodePool** .\n- From the navigation menu, under **Node Pools** , expand the new node pool and click **Nodes** .\n- Configure the following settings for the node pool:- From the **Image type** drop-down list, select **Container-Optimized\nOS with Containerd (cos_containerd)** . This is the only supported image type for GKE Sandbox.\n- Under **Machine Configuration** , select a **Series** and **Machine type** . **Note:** GKE Sandbox does not support the `e2-micro` , `e2-small` , and `e2-medium` machine types.\n- From the navigation menu, under the name of the node pool you are configuring, click **Security** and select the **Enable sandbox withgVisor** checkbox.\n- Continue to configure the cluster and node pools as needed.\n- Click **Create** .\nGKE Sandbox can't be enabled for the default node pool, and it isn't possible to create additional node pools at the same time as you create a new cluster using the `gcloud` command. Instead, create your cluster as you normally would. Although optional, it's recommended that you [enableLogging and Monitoring](/stackdriver/docs/solutions/gke/installing#installing) so that gVisor messages are logged.\nNext, use the `gcloud container node-pools create` command, and set the `-- sandbox` flag to `type=gvisor` . The node image type must be `cos_containerd` for GKE Sandbox.\n```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 --cluster=CLUSTER_NAME \\\u00a0 --node-version=NODE_VERSION \\\u00a0 --machine-type=MACHINE_TYPE \\\u00a0 --image-type=cos_containerd \\\u00a0 --sandbox type=gvisor\n```\nReplace the following variables:- ``: the name of your new node pool.\n- ``: the name of your cluster.\n- ``: the version to use for the node pool.\n- ``: the [type of machine](/compute/docs/machine-types) to use for the nodes. GKE Sandbox does not support the`e2-micro`,`e2-small`, and`e2-medium`machine types.\n### Enable GKE Sandbox on an existing Standard cluster\nYou can enable GKE Sandbox on an existing Standard cluster by adding a new node pool and enabling the feature for that node pool.\nTo create a new node pool with GKE Sandbox enabled:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Click **Add NodePool** .\n- Configure the **Node pool details** page as selected.\n- From the navigation menu, click **Nodes** and configure the following settings:- From the **Image type** drop-down list, select **Container-Optimized\nOS with Containerd (cos_containerd)** . This is the only supported image type for GKE Sandbox.\n- Under **Machine Configuration** , select a **Series** and **Machine type** . **Note:** GKE Sandbox does not support the `e2-micro` , `e2-small` , and `e2-medium` machine types.\n- From the navigation menu, click **Security** and select the **Enablesandbox with gVisor** checkbox.\n- Click **Create** .\nTo create a new node pool with GKE Sandbox enabled, use a command like the following:\n```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 --cluster=CLUSTER_NAME \\\u00a0 --machine-type=MACHINE_TYPE \\\u00a0 --image-type=cos_containerd \\\u00a0 --sandbox type=gvisor\n```\nThe node image type must be `cos_containerd` for GKE Sandbox.\nIt is optional but recommended that you enable Cloud Logging and Cloud Monitoring on the cluster, so that gVisor messages are logged. These services are enabled by default for new clusters.\nYou can use the Google Cloud console to enable these features on an existing cluster.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Features** , in the **Cloud Logging** field, click **Edit Cloud Logging** .\n- Select the **Enable Cloud Logging** checkbox.\n- Click **Save Changes** .\n- Repeat the same steps for the **Cloud Monitoring** and **Managed Service for Prometheus** fields to enable those features.## Use GKE Sandbox in Autopilot and Standard\nIn Autopilot clusters and in Standard clusters with GKE Sandbox enabled, you request a sandboxed environment for a Pod by specifying the `gvisor` RuntimeClass in the Pod specification.\nFor Autopilot clusters, ensure that you're running GKE version 1.26.0-gke.2500 or later.\n### Running an application in a sandbox\nTo make a Deployment run on a node with GKE Sandbox enabled, set its `spec.template.spec.runtimeClassName` to `gvisor` , as shown in the following example:\n```\n# httpd.yamlapiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: httpd\u00a0 labels:\u00a0 \u00a0 app: httpdspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: httpd\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: httpd\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 runtimeClassName: gvisor\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: httpd\u00a0 \u00a0 \u00a0 \u00a0 image: httpd\n```\nCreate the Deployment:\n```\nkubectl apply -f httpd.yaml\n```\nThe Pod is deployed to a node with GKE Sandbox enabled. To verify the deployment, find the node where the Pod is deployed:\n```\nkubectl get pods\n```\nThe output is similar to the following:\n```\nNAME     READY STATUS RESTARTS AGE\nhttpd-db5899bc9-dk7lk 1/1  Running 0   24s\n```\nFrom the output, find the name of the Pod in the output, and then check the value for RuntimeClass:\n```\nkubectl get pods POD_NAME -o jsonpath='{.spec.runtimeClassName}'\n```\nThe output is `gvisor` .\nAlternatively, you can list the RuntimeClass of each Pod, and look for the Pods where it is set to `gvisor` :\n```\nkubectl get pods -o jsonpath=$'{range .items[*]}{.metadata.name}: {.spec.runtimeClassName}\\n{end}'\n```\nThe output is the following:\n```\nPOD_NAME: gvisor\n```\nThis method of verifying that the Pod is running in a sandbox is trustworthy because it does not rely on any data within the sandbox itself. Anything reported from within the sandbox is untrustworthy, because it could be defective or malicious.\n### Running a regular Pod along with sandboxed Pods\nThe steps in this section apply to Standard mode workloads. You don't need to run regular Pods alongside sandbox Pods in Autopilot mode, because the [Autopilot pricing model](/kubernetes-engine/pricing#autopilot_mode) eliminates the need to manually optimize the number of Pods scheduled on nodes.\nAfter enabling GKE Sandbox on a node pool, you can run trusted applications on those nodes without using a sandbox by using node taints and tolerations. These Pods are referred to as \"regular Pods\" to distinguish them from sandboxed Pods.\nRegular Pods, just like sandboxed Pods, are prevented from accessing other Google Cloud services or cluster metadata. This prevention is part of the node's configuration. If your regular Pods or sandboxed Pods require access to Google Cloud services, use [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) .\nRunning untrusted code on the same nodes as critical system services comes with potential risk, even if the untrusted code is running in a sandbox. Consider these risks when designing your applications.\nGKE Sandbox adds the following label and taint to nodes that can run sandboxed Pods:\n```\nlabels:\u00a0 sandbox.gke.io/runtime: gvisor\n```\n```\ntaints:- effect: NoSchedule\u00a0 key: sandbox.gke.io/runtime\u00a0 value: gvisor\n```\nIn addition to any node affinity and toleration settings in your Pod manifest, GKE Sandbox applies the following node affinity and toleration to all Pods with `RuntimeClass` set to `gvisor` :\n```\naffinity:\u00a0 nodeAffinity:\u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 - key: sandbox.gke.io/runtime\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - gvisor\n```\n```\ntolerations:\u00a0 - effect: NoSchedule\u00a0 \u00a0 key: sandbox.gke.io/runtime\u00a0 \u00a0 operator: Equal\u00a0 \u00a0 value: gvisor\n```\nTo schedule a regular Pod on a node with GKE Sandbox enabled, manually apply the node affinity and toleration described earlier in your Pod manifest.\n- If your pod **can** run on nodes with GKE Sandbox enabled, add the toleration.\n- If your pod **must** run on nodes with GKE Sandbox enabled, add both the node affinity and toleration.\nFor example, the following manifest modifies the manifest used in [Running an application in a sandbox](#sandboxed-application) so that it runs as a regular Pod on a node with sandboxed Pods, by removing the runtimeClass and adding both the taint and toleration described earlier.\n```\n# httpd-no-sandbox.yamlapiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: httpd-no-sandbox\u00a0 labels:\u00a0 \u00a0 app: httpdspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: httpd\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: httpd\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: httpd\u00a0 \u00a0 \u00a0 \u00a0 image: httpd\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: sandbox.gke.io/runtime\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - gvisor\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 \u00a0 - effect: NoSchedule\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: sandbox.gke.io/runtime\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: gvisor\n```\nFirst, verify that the Deployment is not running in a sandbox:\n```\nkubectl get pods -o jsonpath=$'{range .items[*]}{.metadata.name}: {.spec.runtimeClassName}\\n{end}'\n```\nThe output is similar to:\n```\nhttpd-db5899bc9-dk7lk: gvisor\nhttpd-no-sandbox-5bf87996c6-cfmmd:\n```\nThe `httpd` Deployment created earlier is running in a sandbox, because its runtimeClass is `gvisor` . The `httpd-no-sandbox` Deployment has no value for runtimeClass, so it is not running in a sandbox.\nNext, verify that the non-sandboxed Deployment is running on a node with GKE Sandbox by running the following command:\n```\nkubectl get pod -o jsonpath=$'{range .items[*]}{.metadata.name}: {.spec.nodeName}\\n{end}'\n```\nThe name of the node pool is embedded in the value of `nodeName` . Verify that the Pod is running on a node in a node pool with GKE Sandbox enabled.\n**Note:** If the regular Pod is unschedulable, verify that the taint and toleration is set correctly in the Pod manifest.\n## Verifying metadata protection\nTo validate the assertion that metadata is protected from nodes that can run sandboxed Pods, you can run a test:\n- Create a sandboxed Deployment from the following manifest, using `kubectl apply -f` . It uses the `fedora` image, which includes the `curl` command. The Pod runs the `/bin/sleep` command to ensure that the Deployment runs for 10000 seconds.```\n# sandbox-metadata-test.yamlapiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: fedora\u00a0 labels:\u00a0 \u00a0 app: fedoraspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: fedora\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: fedora\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 runtimeClassName: gvisor\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: fedora\u00a0 \u00a0 \u00a0 \u00a0 image: fedora\u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sleep\",\"10000\"]\n```\n- Get the name of the Pod using `kubectl get pods` , then use `kubectl exec` to connect to the Pod interactively.```\nkubectl exec -it POD_NAME /bin/sh\n```You are connected to a container running in the Pod, in a `/bin/sh` session.\n- Within the interactive session, attempt to access a URL that returns cluster metadata:```\ncurl -s \"http://169.254.169.254/computeMetadata/v1/instance/attributes/kube-env\" -H \"Metadata-Flavor: Google\"\n```The command hangs and eventually times out, because the packets are silently dropped.\n- Press **Ctrl+C** to terminate the `curl` command, and type `exit` to disconnect from the Pod.\n- Remove the `RuntimeClass` line from the YAML manifest and redeploy the Pod using `kubectl apply -f` `` . The sandboxed Pod is terminated and recreated on a node without GKE Sandbox.\n- Get the new Pod name, connect to it using `kubectl exec` , and run the `curl` command again. This time, results are returned. This example output is truncated.```\nALLOCATE_NODE_CIDRS: \"true\"\nAPI_SERVER_TEST_LOG_LEVEL: --v=3\nAUTOSCALER_ENV_VARS: kube_reserved=cpu=60m,memory=960Mi,ephemeral-storage=41Gi;...\n...\n```Type `exit` to disconnect from the Pod.\n- Remove the deployment:```\nkubectl delete deployment fedora\n```## Disabling GKE Sandbox\nYou can't disable GKE Sandbox in GKE Autopilot clusters or in GKE Standard node pools. If you want to stop using GKE Sandbox, [delete the node pool](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool) .\n## What's next\n- Learn more about [managing node pools](/kubernetes-engine/docs/how-to/node-pools) .\n- Read the [security overview](/kubernetes-engine/docs/concepts/security-overview) .", "guide": "Google Kubernetes Engine (GKE)"}