{"title": "Google Kubernetes Engine (GKE) - Right-size your GKE workloads at scale", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/right-size-workloads-at-scale", "abstract": "# Google Kubernetes Engine (GKE) - Right-size your GKE workloads at scale\nThis tutorial shows you how to right-size your Google Kubernetes Engine (GKE) workloads with VPA recommendations and usage metrics.\n#", "content": "## Understand why resource rightsizing is importantUnder-provisioning can starve your containers of the necessary resources to run your applications, making them slow and unreliable. Over-provisioning doesn't impact the performance of your applications but might increase your monthly bill.\nThe following table describes the implications of under-provisioning and over-provisioning CPU and memory:\n| Resource | Provisioning status | Risk  | Explanation                |\n|:-----------|:----------------------|:------------|:-------------------------------------------------------------------------|\n| CPU  | Over     | Cost  | Increases the cost of your workloads by reserving unnecessary resources. |\n| CPU  | Under     | Performance | Can cause workloads to slow down or become unresponsive.     |\n| CPU  | Not set    | Reliability | CPU can be throttled to 0 causing your workloads to become unresponsive. |\n| Memory  | Over     | Cost  | Increases the cost of your workloads by reserving unnecessary resources. |\n| Memory  | Under     | Reliability | Can cause applications to terminate with an out of memory (OOM) error. |\n| Memory  | Not set    | Reliability | kubelet can stop your Pods, at any time, and mark them as failed.  |## ObjectivesIn this tutorial, you will learn how to:- Deploy a sample application.\n- Export GKE recommendations metrics from Monitoring to BigQuery.\n- Use BigQuery and Looker Studio to view GKE container recommendations across projects.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Monitoring](/monitoring/pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [BigQuery](/bigquery/pricing) \n- [Cloud Run](/run/pricing) \n- [Cloud Build](/build/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n### Set up your project [Cloud Shell](/shell) is preinstalled with the software you need for this tutorial, including [Docker](https://www.docker.com/) , [kubectl](https://kubernetes.io/docs/reference/kubectl/) , [gcloud CLI](/sdk/gcloud) , and [Terraform](/docs/terraform) . If you don't use Cloud Shell, you must install the gcloud CLI.- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/serviceusage.serviceUsageAdmin, roles/container.clusterAdmin, roles/iam.serviceAccountAdmin, roles/iam.securityAdmin, roles/container.admin` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.\n### Set up your environmentTo set up your environment, follow these steps- Set environment variables:```\nexport PROJECT_ID=PROJECT_IDexport REGION=us-central1export ZONE=us-central1-fexport IMAGE=$REGION-docker.pkg.dev/$PROJECT_ID/main/vpa-recs-image:latest\n```Replace `` with your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Set the default environment variables:```\ngcloud config set project $PROJECT_IDgcloud config set compute/region $REGIONgcloud config set compute/zone $ZONE\n```\n- Clone the code repository.```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory.```\ncd kubernetes-engine-samples/cost-optimization/gke-vpa-recommendations\n```\n### (Optional) Set up the sample applicationThis is an optional section to deploy a sample application. To use an existing cluster, ensure that [Cloud Monitoring is configured on your cluster](/stackdriver/docs/solutions/gke/installing) .\nTo simulate a realistic environment, you will use a setup script to deploy [Online Boutique](https://github.com/GoogleCloudPlatform/microservices-demo) .\nThe following steps install the sample application and modify the default configuration. For example, the instructions configure the [Horizontal Pod Autoscaler (HPA)](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) for some workloads and changes resource requests and limits.- Run the setup script:```\n./scripts/setup.sh\n```The setup script does the following:- Creates a GKE cluster.\n- Deploys the Online Boutique sample application.\n- Updates Pod CPU and memory resource requests.\n- Configures a HorizontalPodAutoscaler resource for the`adservice`workloads to simulate a realistic environment.\nThe setup script might take up to 10 minutes to complete.\n- Verify that the sample application is ready:```\nkubectl get deployment\n```The output is similar to the following:```\nNAME     READY UP-TO-DATE AVAILABLE AGE\nadservice    2/2  2   2   4m54s\ncartservice    1/1  1   1   4m55s\ncheckoutservice   1/1  1   1   4m56s\ncurrencyservice   1/1  1   1   4m55s\nemailservice   1/1  1   1   4m56s\nfrontend    1/1  1   1   4m55s\nloadgenerator   1/1  1   1   4m55s\npaymentservice   1/1  1   1   4m55s\nproductcatalogservice 1/1  1   1   4m55s\nrecommendationservice 1/1  1   1   4m56s\nredis-cart    1/1  1   1   4m54s\nshippingservice   1/1  1   1   4m54s\n```\n **Warning:** Workloads with HPA enabled may provide incorrect recommendations if they frequently trigger scaling events.## Create a repositoryCreate the repository to store the metric exporter image.- Create a new Docker repository:```\ngcloud artifacts repositories create main --repository-format=docker \\\u00a0 \u00a0 --location=$REGION \\\u00a0 \u00a0 --description=\"docker repository\" \n```\n- Setup authentication to Docker repositories:```\ngcloud auth configure-docker $REGION-docker.pkg.dev\n```\n- Deploy the image by running the following command:```\ngcloud builds submit metrics-exporter --region=$REGION --tag $IMAGE\n```\n## Deploy the applicationIn the following section, you use Terraform to perform the following tasks:- Create a Service Account and assign the permissions required to manage and interact with Google Cloud resources.\n- Grant the monitoring viewer, BigQuery data editor, BigQuery data owner, BigQuery job user, and Cloud Run invoker roles to the Service Account.\n- Deploy a Cloud Run job that pulls a Docker image from the Artifact Registry and runs it with the specified configuration.\n- Create a Cloud Scheduler job that triggers the Cloud Run service daily.\n- Create a BigQuery dataset, table and view to store metrics data and recommendations.\n### Configure Terraform\n- Set configuration environment variables:```\nexport TF_VAR_BIGQUERY_DATASET=gke_metrics_datasetexport TF_VAR_BIGQUERY_TABLE=gke_metricsexport TF_VAR_RECOMMENDATION_WINDOW_SECONDS=1209600export TF_VAR_RECOMMENDATION_DISTANCE=86400export TF_VAR_LATEST_WINDOW_SECONDS=600export TF_VAR_METRIC_WINDOW=259200export TF_VAR_METRIC_DISTANCE=600\n```This command includes the following:- `TF_VAR_BIGQUERY_DATASET`and`TF_VAR_BIGQUERY_TABLE`: hold the GKE metric data.\n- `TF_VAR_RECOMMENDATION_WINDOW_SECONDS`: the timeframe for VPA recommendations. Defaults to 1,209,600 seconds, or 14 days.\n- `TF_VAR_RECOMMENDATION_DISTANCE`: the interval at which VPA recommendation data points are returned. Defaults to 86,400 seconds, or every 1 day.\n- `TF_VAR_LATEST_WINDOW_SECONDS`: the timeframe for obtaining the most recent requested and limit resource values. Defaults to 600 seconds, or 10 minutes.\n- `METRIC_WINDOW`: establishes the timeframe for GKE usage and utilization metrics. Defaults to 25,9200 seconds, or 3 days.\n- `METRIC_DISTANCE`: the interval at which data points are returned. Defaults to 600 seconds, or every 10 minutes.\nAdjust these values based on the needs of your workloads. Example, for batch workloads which run once a month, update `TF_VAR_RECOMMENDATION_WINDOW_SECONDS` and `METRIC_WINDOW` to `2592000` seconds (30 days).\n### Deploy the Terraform configuration\n- Initialize, validate and apply your configuration:```\nterraform -chdir=terraform initterraform -chdir=terraform validateterraform -chdir=terraform apply -var project_id=$PROJECT_ID -var region=$REGION -var image=$IMAGE\n```This command provides an execution plan and asks for your approval before making any changes. Review the plan, and if everything is as expected, type `yes` to proceed.After the apply command completes successfully, your resources are created and managed by Terraform.\n- Manually run the Cloud Scheduler job:```\ngcloud scheduler jobs run recommendation-schedule --location ${REGION}\n```\n### Verify the Deployment\n- Select the **Logs** tab on the `workload-recommendations` details page.\n- Verify the metrics logs are being processed in the Cloud Run console: [Go to Cloud Run](https://console.cloud.google.com/run/jobs/details/us-central1/workload-recommendations/logs) The logs show metrics being written to BigQuery. The output should be similar to the following:```\nINFO - Building Row\nINFO - Successfully wrote 12 rows to BigQuery table [PROJECT_ID].gke_metric_dataset.gke_metrics.\nINFO - Run Completed\n```If output does not match, wait five minutes and then run the command `gcloud scheduler jobs run recommendation-schedule --location $REGION` .\n## View the container recommendation in BigQuery\n- Go to the BigQuery page in the Google Cloud console: [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Verify the data is visible in the `gke_metrics` table and the `container_recommendations` view. Depending on the number of workloads, it might take a few minutes to write all metrics to BigQuery.\n- In the query editor, select all rows in the `container_recommendations` view:```\nSELECT * FROM `PROJECT_ID.gke_metrics_dataset.container_recommendations`\n```This program extracts the following metrics from cloud monitoring:- **Workload details** : the project ID, cluster name, controller and container name.\n- **CPU/memory usage and utilization** : the amount of CPU and memory that is being used by the workload, as well as the percentage of CPU and memory that is being utilized.\n- **Requested and limits** : the amount of CPU and memory that was requested for the workload, as well as the maximum amount of CPU and memory that is allowed for the workload.\n- **CPU and memory workload recommendations** : recommendations for how much CPU and memory should be allocated to the workload to ensure that it runs smoothly, based on VPA recommendations for Deployments and on actual usage and target utilizations for non-Deployment objects.\n **Warning:** Failing or restarting workloads cause recommendations to be incorrect. Ensure workloads are healthy before updating resource configurations.## Visualize recommendations in Looker StudioLooker Studio is a free, self-service business intelligence platform that lets you build and consume data visualizations, dashboards, and reports. With Looker Studio, you can connect to your data, create visualizations, and share your insights with others.\nUse Looker Studio to visualize data in the BigQuery `container_recommendations` view:- Open the [Workload Rightsizing dashboard template](https://lookerstudio.google.com/reporting/cfc33a6f-f251-4c22-815f-af449d1d9b10/page/tEnnC/preview) \n- Click **Use my own data** .\n- Select your project.\n- For Dataset, select`gke_metric_dataset`.\n- For Table, select`container_recommendations`.\n- Click **Add** .\n- Click **Add to Report** .\n### Looker Studio template detailsThe Looker Studio template details page provides the following information:- **GKE Workload Rightsizing Overview** : provides an overview of your clusters, including:- The number of Best Effort and Burstable workloads that are at risk of reliability and performance issues.\n- Potential CPU and memory resource savings. Positive values indicate over-provisioning, while negative values indicate under-provisioning.\n- **Workload Recommendations** : Provides recommendations for workload CPU and memory requests and limits.\n- **GKE Workloads at Risk** : Shows workloads that are at the greatest risk of experiencing reliability and performance issues.\n- **History - Workload Rightsizing - How are we doing?** : Provides an historical view of how well workload rightsizing and reducing the number of Best Effort workloads has been implemented.\n### CPU requested and limit container recommendationIf the workloads CPU requested and limit values are equal, the QoS is considered Guaranteed, and the CPU recommendation is set to the maximum within the window period of 14 days. Otherwise, the 95th percentile of the CPU requested recommendation within 14 days is used.\nWhen the CPU request and limit values are equal, the recommendation for CPU limit is set to the maximum CPU request VPA recommendation for Deployment objects only and the CPU usage with a target utilization of 70%. If the request and limit of the workload are not identical, the existing limit ratio is used.\n### Memory requested and limit container recommendationMemory recommendations use the maximum VPA recommendation for Deployments objects only and the maximum memory usage with a target utilization of 80% to ensure the workloads reliability. You can update the target utilization values in the `container_recommendation` view's query.\nIt is [best practice to use the same amount of memory for requests and limits](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#set_appropriate_resource_requests_and_limits) because memory is an incompressible resource. When memory is exhausted, the Pod must be taken down. To avoid having Pods taken down and destabilizing your environment, you must set the requested memory to the memory limit.\n### Prioritizing recommendationsA priority value is assigned to each row to surface workloads which require immediate attention based on the recommendations. The units of CPU and memory are different. To normalize the units, the [E2 machine type on-demand price](/compute/all-pricing) ratio between predefined CPU and memory is used as an approximation to convert memory units to CPU units.\nThe priority is calculated using the following formula:\n```\npriority = (CPU requested - CPU recommendation) + ((memory requested memory recommendation) / (vCPUs on-demand pricing /memory on-demand pricing ))\n```\nFor Autopilot, the total resources requested by your deployment configuration should be within the supported [minimum and maximum values](/kubernetes-engine/docs/concepts/autopilot-resource-requests#min-max-requests) .## View VPA recommendations for multiple projectsTo view VPA container recommendations across multiple projects, use a new [project as a scoping project](/monitoring/settings#create-multi) .\nWhen deploying this project in your production environment, add all projects you want to analyze to the new project's metrics scope.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to avoid billing is to delete the project you created for the tutorial.\n **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\nDelete a Google Cloud project:\n```\ngcloud projects delete PROJECT_ID\n```## What's next\n- Learn more about GKE cost optimization in [Best practices for running cost-optimized Kubernetes applications on GKE](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) .\n- Find design recommendations and best practices to optimize the cost of Google Cloud workloads in [Google Cloud Architecture Framework: Cost optimization](/architecture/cost-efficiency-on-google-cloud) .\n- Learn more about cost-optimizing your cluster at low-demand periods in [Reducing costs by scaling down GKE clusters during off-peak hours](/architecture/reducing-costs-by-scaling-down-gke-off-hours) .\n- Learn more about GKE cost optimization in [Monitoring GKE clusters for cost optimization using Monitoring](/kubernetes-engine/docs/tutorials/reducing-costs-by-scaling-down-gke-off-hours) .", "guide": "Google Kubernetes Engine (GKE)"}