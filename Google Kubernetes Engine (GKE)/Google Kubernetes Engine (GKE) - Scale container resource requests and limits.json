{"title": "Google Kubernetes Engine (GKE) - Scale container resource requests and limits", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling", "abstract": "# Google Kubernetes Engine (GKE) - Scale container resource requests and limits\nThis page explains how you can analyze and adjust the [CPU requests](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/) and [memory requests](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/) of a container in a Google Kubernetes Engine (GKE) cluster using [vertical Pod autoscaling](/kubernetes-engine/docs/concepts/verticalpodautoscaler) .\nYou can scale container resources manually through the Google Cloud console, analyze resources using a `VerticalPodAutoscaler` object, or configure automatic scaling using [vertical Pod autoscaling](/kubernetes-engine/docs/concepts/verticalpodautoscaler) .\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Analyze resource requests\nThe Vertical Pod Autoscaler automatically analyzes your containers and provides suggested resource requests. You can view these resource requests using the Google Cloud console, Cloud Monitoring, or Google Cloud CLI.\n**Caution:** In some cases, when you access the vertical Pod autoscaling recommendations, GKE initializes the Vertical Pod Autoscaler controller. This might cause a control plane restart.\nTo view suggested resource requests in the Google Cloud console, you must have an existing workload deployed that is at least 24 hours old. Some suggestions might not be available or relevant for certain workloads, such as those created within the last 24 hours, standalone Pods, and apps written in Java.- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- In the workloads list, click the name of the workload you want to scale.\n- Click **Actions > Scale > Edit resource requests** .The Analyze resource utilization data section shows historic usage data that the Vertical Pod Autoscaler controller analyzed to create the suggested resource requests in the Adjust resource requests and limits section.\nTo view suggested resource requests in Cloud Monitoring, you must have an existing workload deployed.- Go to the **Metrics Explorer** page in the Google Cloud console. [Go to Metrics Explorerr](https://console.cloud.google.com/monitoring/metrics-explorer) \n- Click **Configuration** .\n- Expand the **Select a Metric** menu.\n- In the **Resource** menu, select **Kubernetes Scale** .\n- In the **Metric category** menu, select **Autoscaler** .\n- In the **Metric** menu, select **Recommended per replicate request bytes** and **Recommended per replica request core** .\n- Click **Apply** .\nTo view suggested resource requests, you must create a `VerticalPodAutoscaler` object and a Deployment.- For Standard clusters, enable vertical Pod autoscaling for your cluster. For Autopilot clusters, vertical Pod autoscaling is enabled by default.```\ngcloud container clusters update CLUSTER_NAME --enable-vertical-pod-autoscaling\n```Replace `` with the name of your cluster.\n- Save the following manifest as `my-rec-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-rec-deploymentspec:\u00a0 replicas: 2\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: my-rec-deployment\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: my-rec-deployment\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: my-rec-container\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\n```This manifest describes a `Deployment` that does not have CPU or memory requests. The `containers.name` value of `my-rec-deployment` specifies that all Pods in the Deployment belong to the `VerticalPodAutoscaler` .\n- Apply the manifest to the cluster:```\nkubectl create -f my-rec-deployment.yaml\n```\n- Save the following manifest as `my-rec-vpa.yaml` :```\napiVersion: autoscaling.k8s.io/v1kind: VerticalPodAutoscalermetadata:\u00a0 name: my-rec-vpaspec:\u00a0 targetRef:\u00a0 \u00a0 apiVersion: \"apps/v1\"\u00a0 \u00a0 kind: \u00a0 \u00a0 \u00a0 Deployment\u00a0 \u00a0 name: \u00a0 \u00a0 \u00a0 my-rec-deployment\u00a0 updatePolicy:\u00a0 \u00a0 updateMode: \"Off\"\n```This manifest describes a `VerticalPodAutoscaler` . The `updateMode` value of `Off` means that when Pods are created, the Vertical Pod Autoscaler controller analyzes the CPU and memory needs of the containers and records those recommendations in the `status` field of the resource. The Vertical Pod Autoscaler controller does not automatically update the resource requests for running containers.\n- Apply the manifest to the cluster:```\nkubectl create -f my-rec-vpa.yaml\n```\n- After some time, view the `VerticalPodAutoscaler` :```\nkubectl get vpa my-rec-vpa --output yaml\n```The output is similar to the following:```\n...\n recommendation:\n containerRecommendations:\n - containerName: my-rec-container\n  lowerBound:\n  cpu: 25m\n  memory: 262144k\n  target:\n  cpu: 25m\n  memory: 262144k\n  upperBound:\n  cpu: 7931m\n  memory: 8291500k\n...\n```This output shows recommendations for CPU and memory requests.## Set Pod resource requests manually\nYou can set Pod resource requests manually using the Google Cloud CLI or the Google Cloud console.\n**Caution:** Applying vertical Pod autoscaling recommendations causes workload disruptions.\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- In the workloads list, click the name of the workload you want to scale.\n- Click **Actions > Scale > Edit resource requests** .- The **Adjust resource requests and limits** section shows the current CPU and memory requests for each container as well as suggested CPU and memory requests.\n- Click **Apply Latest Suggestions** to view suggested requests for each container.\n- Click **Save Changes** .\n- Click **Confirm** .\nTo set resource requests for a Pod, set the requests.cpu and memory.cpu values in your Deployment manifest.In this example, you manually modify the Deployment created in [Analyze resource requests](#analyze) with suggested resource requests.- Save the following example manifest as `my-adjusted-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-rec-deploymentspec:\u00a0 replicas: 2\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: my-rec-deployment\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: my-rec-deployment\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: my-rec-container\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 25m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 256Mi\n```This manifest describes a Deployment that has two Pods. Each Pod has one container that requests 25 milliCPU and 256 MiB of memory.\n- Apply the manifest to the cluster:```\nkubectl apply -f my-adjusted-deployment.yaml\n```\nYou can also apply changes manually by performing the following steps:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- In the workloads list, click the name of the workload you want to scale.\n- Click **Actions > Scale > Edit resource requests** .\n- Configure your container requests.\n- Click **Get Equivalent YAML** .\n- Click **Download Workload** or copy and paste the manifest into a file named `resource-adjusted.yaml` .\n- Apply the manifest to your cluster:```\nkubectl create -f resource-adjusted.yaml\n```## Set Pod resource requests automatically\nVertical Pod autoscaling uses the `VerticalPodAutoscaler` object to automatically set resource requests on Pods when the `updateMode` is `Auto` . You can configure a `VerticalPodAutoscaler` using the gcloud CLI or the Google Cloud console.\n**Caution:** Enabling or disabling vertical Pod autoscaling causes a control plane restart.\nTo set resource requests automatically, you must have a cluster with the vertical Pod autoscaling feature enabled. Autopilot clusters have the vertical Pod autoscaling feature enabled by default.## Enable Vertical Pod Autoscaling\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- In the **Automation** section, click **Edit** for the **Vertical Pod Autoscaling** option.\n- Select the **Enable Vertical Pod Autoscaling** checkbox.\n- Click **Save changes** .\n## Configure Vertical Pod Autoscaling\n- Go to the **Workloads** page in Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- In the workloads list, click the name of the Deployment you want to configure vertical Pod autoscaling for.\n- Click **Actions > Autoscale > Vertical pod autoscaling** .\n- Choose an autoscaling mode:- **Auto mode** : Vertical Pod autoscaling updates CPU and memory requests during the life of a Pod.\n- **Initial mode** : Vertical Pod autoscaling assigns resource requests only at Pod creation and never changes them later.\n- (Optional) Set container policies. This option lets you ensure that the recommendation is never set above or below a specified resource request.- Click **Add Policy** .\n- Select **Auto** for **Edit container mode** .\n- In **Controlled resources** , select which resources you want to autoscale the container on.\n- Click **Add Rule** to set one or more minimum or maximum ranges for the container's resource requests:- **Min. allowed Memory** : the minimum amount of memory that the container should always have, in MiB.\n- **Min. allowed CPU** : the minimum amount of CPU that the container should always have, in mCPU.\n- **Max allowed Memory** : the maximum amount of memory that the container should always have, in MiB.\n- **Max allowed CPU** : the maximum amount of CPU that the container should always have, in mCPU.\n- Click **Done** .\n- Click **Save** .\nTo set resource requests automatically, you must use a cluster that has the vertical Pod autoscaling feature enabled. Autopilot clusters have the feature enabled by default.- For Standard clusters, enable vertical Pod autoscaling for your cluster:```\ngcloud container clusters update CLUSTER_NAME --enable-vertical-pod-autoscaling\n```Replace `` with the name of your cluster.\n- Save the following manifest as `my-auto-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-auto-deploymentspec:\u00a0 replicas: 2\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: my-auto-deployment\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: my-auto-deployment\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: my-container\u00a0 \u00a0 \u00a0 \u00a0 image: registry.k8s.io/ubuntu-slim:0.1\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 100m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 50Mi\u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sh\"]\u00a0 \u00a0 \u00a0 \u00a0 args: [\"-c\", \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"]\n```This manifest describes a Deployment that has two Pods. Each Pod has one container that requests 100 milliCPU and 50 MiB of memory.\n- Apply the manifest to the cluster:```\nkubectl create -f my-auto-deployment.yaml\n```\n- List the running Pods:```\nkubectl get pods\n```The output shows the names of the Pods in `my-deployment` :```\nNAME       READY  STATUS    RESTARTS AGE\nmy-auto-deployment-cbcdd49fb-d6bf9 1/1  Running   0   8s\nmy-auto-deployment-cbcdd49fb-th288 1/1  Running   0   8s\n```\n- Save the following manifest as `my-vpa.yaml` :```\napiVersion: autoscaling.k8s.io/v1kind: VerticalPodAutoscalermetadata:\u00a0 name: my-vpaspec:\u00a0 targetRef:\u00a0 \u00a0 apiVersion: \"apps/v1\"\u00a0 \u00a0 kind: \u00a0 \u00a0 \u00a0 Deployment\u00a0 \u00a0 name: \u00a0 \u00a0 \u00a0 my-auto-deployment\u00a0 updatePolicy:\u00a0 \u00a0 updateMode: \"Auto\"\n```This manifest describes a `VerticalPodAutoscaler` with the following properties:- `targetRef.name`: specifies that any Pod that is controlled by a Deployment named`my-deployment`belongs to this`VerticalPodAutoscaler`.\n- `updateMode: Auto`: specifies that the Vertical Pod Autoscaler controller can delete a Pod, adjust the CPU and memory requests, and then start a new Pod.\nYou can also configure vertical Pod autoscaling to assign resource requests only at Pod creation time, using `updateMode: \"Initial\"` .\n- Apply the manifest to the cluster:```\nkubectl create -f my-vpa.yaml\n```\n- Wait a few minutes, and view the running Pods again:```\nkubectl get pods\n```The output shows that the Pod names have changed:```\nNAME         READY  STATUS    RESTARTS AGE\nmy-auto-deployment-89dc45f48-5bzqp 1/1  Running   0   8s\nmy-auto-deployment-89dc45f48-scm66 1/1  Running   0   8s\n```If the Pod names have not changed, wait a bit longer, and then view the running Pods again.## View information about a Vertical Pod Autoscaler\nTo view details about a Vertical Pod Autoscaler, do the following:\n- Get detailed information about one of your running Pods:```\nkubectl get pod POD_NAME --output yaml\n```Replace `` with the name of one of your Pods that you retrieved in the previous step.The output is similar to the following:```\napiVersion: v1\nkind: Pod\nmetadata:\n annotations:\n vpaUpdates: 'Pod resources updated by my-vpa: container 0: cpu capped to node capacity, memory capped to node capacity, cpu request, memory request'\n...\nspec:\n containers:\n ...\n resources:\n  requests:\n  cpu: 510m\n  memory: 262144k\n ...\n```This output shows that the Vertical Pod Autoscaler controller has a memory request of 262144k and a CPU request of 510 milliCPU.\n- Get detailed information about the `VerticalPodAutoscaler` :```\nkubectl get vpa my-vpa --output yaml\n```The output is similar to the following:```\n...\n recommendation:\n containerRecommendations:\n - containerName: my-container\n  lowerBound:\n  cpu: 536m\n  memory: 262144k\n  target:\n  cpu: 587m\n  memory: 262144k\n  upperBound:\n  cpu: 27854m\n  memory: \"545693548\"\n```This output shows recommendations for CPU and memory requests and includes the following properties:- `target`: specifies that for the container to run optimally, it should request 587 milliCPU and 26,2144 kilobytes of memory.\n- `lowerBound`and`upperBound`: vertical Pod autoscaling uses these properties to decide whether to delete a Pod and replace it with a new Pod. If a Pod has requests less than the lower bound or greater than the upper bound, the Vertical Pod Autoscaler deletes the Pod and replaces it with a Pod that meets the target attribute.\n## Opt out specific containers\nYou can opt out specific containers from vertical Pod autoscaling using the gcloud CLI or the Google Cloud console.\nTo opt out specific containers from vertical Pod autoscaling, you must have a cluster with the vertical Pod autoscaling feature enabled. Autopilot clusters have the vertical Pod autoscaling feature enabled by default.## Enable Vertical Pod Autoscaling\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- In the **Automation** section, click **Edit** for the **Vertical Pod Autoscaling** option.\n- Select the **Enable Vertical Pod Autoscaling** checkbox.\n- Click **Save changes** .\n## Configure Vertical Pod Autoscaling\n- Go to the **Workloads** page in Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- In the workloads list, click the name of the Deployment you want to configure vertical Pod autoscaling for.\n- Click **Actions > Autoscale > Vertical pod autoscaling** .\n- Choose an autoscaling mode:- **Auto mode** : Vertical Pod autoscaling updates CPU and memory requests during the life of a Pod.\n- **Initial mode** : Vertical Pod autoscaling assigns resource requests only at Pod creation and never changes them later.\n- Click **Add Policy** .\n- Select the container you want to opt out.\n- For **Edit container mode** , select **Off** .\n- Click **Done** .\n- Click **Save** .\nTo opt out specific containers from vertical Pod autoscaling, perform the following steps:- Save the following manifest as `my-opt-vpa.yaml` :```\napiVersion: autoscaling.k8s.io/v1kind: VerticalPodAutoscalermetadata:\u00a0 name: my-opt-vpaspec:\u00a0 targetRef:\u00a0 \u00a0 apiVersion: \"apps/v1\"\u00a0 \u00a0 kind: \u00a0 \u00a0 \u00a0 Deployment\u00a0 \u00a0 name: \u00a0 \u00a0 \u00a0 my-opt-deployment\u00a0 updatePolicy:\u00a0 \u00a0 updateMode: \"Auto\"\u00a0 resourcePolicy:\u00a0 \u00a0 containerPolicies:\u00a0 \u00a0 - containerName: my-opt-sidecar\u00a0 \u00a0 \u00a0 mode: \"Off\"\n```This manifest describes a `VerticalPodAutoscaler` . The `mode: \"Off\"` value turns off recommendations for the container `my-opt-sidecar` .\n- Apply the manifest to the cluster:```\nkubectl apply -f my-opt-vpa.yaml\n```\n- Save the following manifest as `my-opt-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-opt-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: my-opt-deployment\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: my-opt-deployment\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: my-opt-container\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 - name: my-opt-sidecar\u00a0 \u00a0 \u00a0 \u00a0 image: busybox\u00a0 \u00a0 \u00a0 \u00a0 command: [\"sh\",\"-c\",\"while true; do echo Doing sidecar stuff!; sleep 60; done\"]\n```\n- Apply the manifest to the cluster:```\nkubectl apply -f my-opt-deployment.yaml\n```\n- After some time, view the Vertical Pod Autoscaler:```\nkubectl get vpa my-opt-vpa --output yaml\n```The output shows recommendations for CPU and memory requests:```\n...\n recommendation:\n containerRecommendations:\n - containerName: my-opt-container\n...\n```In this output, there are only recommendations for one container. There are no recommendations for `my-opt-sidecar` .The Vertical Pod Autoscaler never updates resources on opted out containers. If you wait a few minutes, the Pod recreates but only one container has updated resource requests.## What's next\n- Learn more about [Vertical Pod autoscaling](/kubernetes-engine/docs/concepts/verticalpodautoscaler) .\n- Learn [best practices for running cost-optimized Kubernetes applications on GKE](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#vertical_pod_autoscaler) .\n- Learn how to [Assign CPU Resources to containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/) .\n- Learn how to [Assign memory resources to containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/) .", "guide": "Google Kubernetes Engine (GKE)"}