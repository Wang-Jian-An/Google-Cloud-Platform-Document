{"title": "Google Kubernetes Engine (GKE) - Run GPUs in GKE Standard node pools", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gpus", "abstract": "# Google Kubernetes Engine (GKE) - Run GPUs in GKE Standard node pools\nThis page shows you how to use NVIDIA\u00ae graphics processing unit (GPU) hardware accelerators in your Google Kubernetes Engine (GKE) Standard clusters' [nodes](/kubernetes-engine/docs/concepts/cluster-architecture#nodes) . For more information about GPUs in GKE, refer to [About GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) .\nYou can also use GPUs directly in your Autopilot Pods. For instructions, refer to [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .\n", "content": "## Overview\nWith GKE, you can create [node pools](/kubernetes-engine/docs/concepts/node-pools) equipped with NVIDIA Tesla\u00ae [K80](https://www.nvidia.com/en-gb/data-center/tesla-k80/) , [P100](http://www.nvidia.com/object/tesla-p100.html) , [P4](http://images.nvidia.com/content/pdf/tesla/184457-Tesla-P4-Datasheet-NV-Final-Letter-Web.pdf) , [V100](https://www.nvidia.com/en-us/data-center/tesla-v100/) , [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/) , [L4](https://www.nvidia.com/en-us/data-center/l4/) , and [A100](https://www.nvidia.com/en-us/data-center/a100/) GPUs. GPUs provide compute power to drive deep-learning tasks such as image recognition, natural language processing, as well as other compute-intensive tasks such as video transcoding and image processing.\n**Note:** To learn more about use cases for GPUs, refer to Google Cloud's [GPUs](/gpu) page.\nYou can also use GPUs with [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) if your workloads can tolerate frequent node disruptions. Using Spot VMs reduces the price of running GPUs. To learn more, refer to [Using Spot VMs with GPU node pools](/kubernetes-engine/docs/concepts/spot-vms#spot-vms-gpus) .\nAs of version 1.29.2-gke.1108000 you can now create GPU node pools on GKE Sandbox. For more information, see [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods#gpus) and [GKE Sandbox Configuration](/kubernetes-engine/docs/how-to/sandbox-pods) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Requirements\nGPUs on GKE have the following requirements:\n- **Kubernetes version** : For node pools using the [Container-Optimized OS](/kubernetes-engine/docs/concepts/node-images) node image, GPU nodes are available in GKE [version](/kubernetes-engine/versioning-and-upgrades#available_versions) 1.9 or higher. For node pools using the Ubuntu node image, GPU nodes are available in GKE version 1.11.3 or higher.\n- **GPU quota** : You must have [Compute Engine GPU quota](/compute/quotas#gpu_quota) in your selected zone before you can create GPU nodes. To ensure that you have enough GPU quota in your project, refer to [Quotas](https://console.cloud.google.com/iam-admin/quotas) in the Google Cloud console.If you require additional GPU quota, you must [request GPU quota](#gpu_quota) in the Google Cloud console. If you have an established billing account, your project should automatically receive quota after you submit the quota request. **Note:** By default, Free Trial accounts do not receive GPU quota.\n- **NVIDIA GPU drivers** : When creating a cluster or a node pool, you can tell GKE to automatically install a driver version based on your GKE version. If you don't tell GKE to automatically install GPU drivers, you  [manually install the drivers](#installing_drivers) .\n- **A100 GPUs** : A100 GPUs are only supported on [a2 machinetypes](/compute/docs/accelerator-optimized-machines#a2-vms) , and requires GKE version 1.18.6-gke.3504 or higher. You must ensure that you have enough quota for the underlying A2 machine type to use the A100 GPU.\n- **L4 GPUs** :- You must use GKE version 1.22.17-gke.5400 or later.\n- The GKE version that you choose must include NVIDIA driver version 525 or later in Container-Optimized OS. If driver version 525 or later isn't the default or the latest version in your GKE version, you must [manually install](#installing_drivers) a supported driver on your nodes.\n### Limitations\nBefore using GPUs on GKE, keep in mind the following limitations:\n- You cannot add GPUs to existing node pools.\n- GPU nodes cannot be [live migrated](/compute/docs/instances/live-migration#gpusmaintenance) during maintenance events.\n- The GPU type you can use depends on the machine series, as follows:- [A3 machine series](/compute/docs/accelerator-optimized-machines#a3-vms) : H100 GPUs.\n- [A2 machine series](https://cloud.google.com/compute/docs/accelerator-optimized-machines#a2-vms) : A100 GPUs.\n- [G2 machine series](/compute/docs/accelerator-optimized-machines#g2-vms) : L4 GPUs.\n- [N1 machine series](/compute/docs/machine-types#n1_machine_types) : All GPUs except A100 and L4.\nYou should ensure that you have enough quota in your project for the machine series that corresponds to your selected GPU type and quantity.\n- GPUs are not supported in Windows Server node pools.\n- In GKE versions 1.22 to 1.25, the cluster autoscaler only supports basic scaling up and down of nodes with L4 GPUs. This limitation doesn't apply to GKE version 1.26 and later.\n- For H100 GPUs, to use local SSDs for Pod storage, you must explicitly specify the **exact** number of local SSDs to attach to the underlying A3 VM by using the `--ephemeral-storage-local-ssd=count=` `` flag for ephemeral storage or the `--local-nvme-ssd-block=count=` `` flag for block access. If you omit this flag, you won't be able to use the Local SSDs in your Pods. **These flags are only required if you want to use Local SSD fordata access.** The supported machine size in GKE is `a3-highgpu-8g` , and the corresponding Local SSD count is `16` .\n### Availability\nGPUs are available in specific [regions and zones](/compute/docs/regions-zones) . When you [requestGPU quota](#request_quota) , consider the regions in which you intend to run your clusters.\nFor a complete list of applicable regions and zones, refer to [GPUs on Compute Engine](/compute/docs/gpus) .\nYou can also see GPUs available in your zone using the Google Cloud CLI. To see a list of all GPU accelerator types supported in each zone, run the following command:\n```\ngcloud compute accelerator-types list\n```\n### Pricing\nFor GPU pricing information, refer to the [pricing table](/compute/gpus-pricing) on the Google Cloud GPU page.\n## GPU quota\nYour GPU quota is the total number of GPUs that can run in your [Google Cloud project](/resource-manager/docs/cloud-platform-resource-hierarchy#projects) . To create clusters with GPUs, your project must have sufficient GPU quota.\nYour GPU quota should be at least equivalent to the total number of GPUs you intend to run in your cluster. If you enable [cluster autoscaling](/kubernetes-engine/docs/concepts/cluster-autoscaler) , you should request GPU quota at least equivalent to your cluster's maximum number of nodes multiplied by the number of GPUs per node.\nFor example, if you create a cluster with three nodes that runs two GPUs per node, your project requires at least six GPU quota.\n### Requesting GPU quota\nTo request GPU quota, use the Google Cloud console. For more information about requesting quotas, refer to [GPU quotas](/compute/resource-usage#gpu_quota) in the Compute Engine documentation.\nTo search for GPU quota and submit a quota request, use the Google Cloud console:\n- Go to the IAM & Admin **Quotas** page in the Google Cloud console. [Go to Quotas](https://console.cloud.google.com/iam-admin/quotas) \n- In the **Filter** box, do the following:- Select the **Quota** property, enter the name of the [GPU model](/compute/docs/gpus#nvidia_gpus_for_compute_workloads) , and press **Enter** .\n- (Optional) To apply more advanced filters to narrow the results, select the **Dimensions (e.g. locations)** property, add the name of the [region or zone you are using](/compute/docs/gpus/gpu-regions-zones) , and press **Enter** .\n- From the list of GPU quotas, select the quota you want to change.\n- Click **Edit Quotas** . A request form opens.\n- Fill the **New quota limit** field for each quota request.\n- Fill the **Request description** field with details about your request.\n- Click **Next** .\n- In the **Override confirmation** dialog, click **Confirm** .\n- In the **Contact details** screen, enter your name and a phone number that the approvers might use to complete your quota change request.\n- Click **Submit request** .\n- You receive a confirmation email to track the quota change.## Running GPUs\nTo run GPUs in GKE Standard clusters, create a node pool with attached GPUs. When you add a GPU node pool to an existing cluster that already runs a non-GPU node pool, GKE automatically [taints](/kubernetes-engine/docs/how-to/node-taints) the GPU nodes with the following node taint:\n- **Key:** `nvidia.com/gpu`\n- **Effect:** `NoSchedule`\n**Note:** GKE only adds this taint if there is a non-GPU node pool in the cluster. If you add a GPU node pool to a cluster where all the existing node pools are GPU node pools, or if you create a new cluster where the default node pool has GPUs attached, the above taint will **not** be added to the GPU nodes. If you add a non-GPU node pool to the cluster in the future, GKE does **not** retroactively apply this taint to existing GPU nodes.\nAdditionally, GKE automatically applies the corresponding tolerations to Pods requesting GPUs by running the [ExtendedResourceToleration](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration) admission controller.\nThis causes only Pods requesting GPUs to be scheduled on GPU nodes, which enables more efficient autoscaling: your GPU nodes can quickly scale down if there are not enough Pods requesting GPUs.\nFor better cost-efficiency, reliability, and availability of GPUs on GKE, we recommend the following actions:\n- Create separate GPU node pools. For each node pool, limit the node location to the zones where the GPUs you want are available.\n- Enable autoscaling in each node pool.\n- Use regional clusters to improve availability by replicating the Kubernetes control plane across zones in the region.\n- Tell GKE to automatically install either the default or latest GPU drivers on the node pools so that you don't need to manually install and manage your driver versions.\n### Create a GPU node pool\nTo create a separate GPU node pool in an existing cluster you can use the Google Cloud console or the Google Cloud CLI. You can also use Terraform for provisioning your GKE clusters and GPU node pool.\nYou can tell GKE to automatically install the default or latest NVIDIA driver version that corresponds to your Container-Optimized OS version.\nTo create a node pool with GPUs in a cluster, run the following command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 --accelerator type=GPU_TYPE,count=AMOUNT,gpu-driver-version=DRIVER_VERSION \\\u00a0 [--machine-type MACHINE_TYPE] \\\u00a0 --region COMPUTE_REGION --cluster CLUSTER_NAME \\\u00a0 --node-locations COMPUTE_ZONE1[,COMPUTE_ZONE2] \\\u00a0 [--enable-autoscaling \\\u00a0 \u00a0--min-nodes MIN_NODES \\\u00a0 \u00a0--max-nodes MAX_NODES] \\\u00a0 [--ephemeral-storage-local-ssd=count=SSD_COUNT]\n```\nReplace the following:- ``: the name you choose for the node pool.\n- ``: the GPU type. Can be one of the following:- `nvidia-tesla-k80`\n- `nvidia-tesla-p100`\n- `nvidia-tesla-p4`\n- `nvidia-tesla-v100`\n- `nvidia-tesla-t4`\n- `nvidia-tesla-a100`\n- `nvidia-a100-80gb`\n- `nvidia-l4`\n- `nvidia-h100-80gb`\n- `` : the NVIDIA driver version to install. Can be one of the following:- `default`: Install the default driver version for your GKE version.\n- `latest`: Install the latest available driver version for your GKE version. Available only for nodes that use Container-Optimized OS.\n- `disabled`: Skip automatic driver installation. You **must** [manually install a driver](#installing_drivers) after you create the node pool. If you omit`gpu-driver-version`, this is the default option.\n **Note:** The `gpu-driver-version` option is only available for GKE version 1.27.2-gke.1200 and later. In earlier versions, omit this flag and [manually install a driver](#installing_drivers) after you create the node pool. If you upgrade an existing cluster or node pool to this version or later, GKE automatically installs the default driver version that corresponds to the GKE version, unless you specify differently when you start the upgrade.\n- `` : the number of GPUs to attach to nodes in the node pool.\n- `` : the Compute Engine machine type for the nodes. Required for the following GPU types:- `nvidia-h100`: [A3 machine type](/compute/docs/accelerator-optimized-machines#a3-vms) \n- `nvidia-tesla-a100`or`nvidia-a100-80gb`: [A2 machine type](/compute/docs/accelerator-optimized-machines#a2-vms) \n- `nvidia-l4`: [G2 machine type](/compute/docs/accelerator-optimized-machines#g2-vms) .\nFor all other GPUs, this flag is optional.\n- `` : the cluster's Compute Engine region, such as `us-central1` . Choose a region that has at least one zone where the requested GPUs are available.\n- `` : the name of the cluster in which to create the node pool.\n- `` `,` `` `,[...]` : the specific [zones](/compute/docs/regions-zones#available) where GKE creates the GPU nodes. The zones must be in the same region as the cluster, specified by the `--region` flag. The GPU types that you define must be [available](#availability) in each selected zone. We recommend that you always use the `--node-locations` flag when creating the node pool to specify the zone or zones containing the requested GPUs.\n- `` : the minimum number of nodes for each zone in the node pool at any time. This value is relevant only if the `--enable-autoscaling` flag is used.\n- `` : the maximum number of nodes for each zone in the node pool at any time. This value is relevant only if the `--enable-autoscaling` flag is used.\n- `` : the number of Local SSDs to attach for ephemeral storage. This flag is required to use Local SSDs in A3 machine types with H100 GPUs.\nFor example, the following command creates a highly-available autoscaling node pool, `p100` , with two P100 GPUs for each node, in the regional cluster `p100-cluster` . GKE automatically installs the default drivers on those nodes.\n```\ngcloud container node-pools create p100 \\\u00a0 --accelerator type=nvidia-tesla-p100,count=2,gpu-driver-version=default \\\u00a0 --region us-central1 --cluster p100-cluster \\\u00a0 --node-locations us-central1-c \\\u00a0 --min-nodes 0 --max-nodes 5 --enable-autoscaling\n```\nTo create a node pool with GPUs:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add Node Pool** .\n- Optionally, on the **Node pool details** page, select the **Enable autoscaling** checkbox.\n- Configure your node pool as desired.\n- From the navigation pane, select **Nodes** .\n- Under **Machine configuration** , click **GPU** .\n- Select a **GPU type** and **Number of GPUs** to run on each node.\n- Read the warning and select **I understand the limitations** .\n- In the **GPU Driver installation** section, select one of the following methods:- **Google-managed** : GKE automatically installs a driver. If you select this option, choose one of the following from the **Version** drop-down:- **Default** : Install the default driver version.\n- **Latest** : Install the latest available driver version.\n- **Customer-managed** : GKE doesn't install a driver. You **must** manually install a compatible driver using the instructions in [Installing NVIDIA GPU device drivers](#installing_drivers) .\n- Click **Create** .\nYou can create a regional cluster with Terraform with GPUs using a [Terraform module](https://github.com/terraform-google-modules/terraform-google-kubernetes-engine) .- Set the Terraform variables by including the following block in the `variables.tf` file:```\nvariable \"project_id\" {\u00a0 default \u00a0 \u00a0 = PROJECT_ID\u00a0 description = \"the gcp_name_short project where GKE creates the cluster\"}variable \"region\" {\u00a0 default \u00a0 \u00a0 = CLUSTER_REGION\u00a0 description = \"the gcp_name_short region where GKE creates the cluster\"}variable \"zone\" {\u00a0 default \u00a0 \u00a0 = \"COMPUTE_ZONE1,COMPUTE_ZONE2\"\u00a0 description = \"the GPU nodes zone\"}variable \"cluster_name\" {\u00a0 default \u00a0 \u00a0 = \"CLUSTER_NAME\"\u00a0 description = \"the name of the cluster\"}variable \"gpu_type\" {\u00a0 default \u00a0 \u00a0 = \"GPU_TYPE\"\u00a0 description = \"the GPU accelerator type\"}variable \"gpu_driver_version\" {\u00a0 default = \"DRIVER_VERSION\"\u00a0 description = \"the NVIDIA driver version to install\"}variable \"machine_type\" {\u00a0 default = \"MACHINE_TYPE\"\u00a0 description = \"The Compute Engine machine type for the VM\"}\n```Replace the following:- ``: your project ID.\n- ``: the name of the GKE cluster.\n- ``: the [compute region](/compute/docs/regions-zones#available) for the cluster.\n- `` `,` `` `,[...]`: the specific [zones](/compute/docs/regions-zones#available) where GKE creates the GPU nodes. The zones must be in the same region specified by the`region`variable. These zones must have the GPU types you defined available. To learn which zones have GPUs, see [Availability](#availability) . You should use the`node_locations`variable when creating the GPU node pool to specify the zone or zones containing the requested GPUs.\n- `` : the GPU type. Can be one of the following:- `nvidia-tesla-k80`\n- `nvidia-tesla-p100`\n- `nvidia-tesla-p4`\n- `nvidia-tesla-v100`\n- `nvidia-tesla-t4`\n- `nvidia-tesla-a100`\n- `nvidia-a100-80gb`\n- `nvidia-l4`\n- `nvidia-h100-80gb`\n- `` : the GPU driver version for GKE to automatically install. This field is optional. The following values are supported:- `INSTALLATION_DISABLED`: Disable automatic GPU driver installation. You **must** manually install drivers to run your GPUs.\n- `DEFAULT`: Automatically install the default driver version for your node operating system version.\n- `LATEST`: Automatically install the latest available driver version for your node OS version.If you omit this field, GKE doesn't automatically install a driver. This field isn't supported in node pools that use node auto-provisioning. To manually install a driver, see [Manually install NVIDIA GPU drivers](#installing_drivers) in this document. * `` : the Compute Engine machine type for the nodes. Required for the following GPU types:- `nvidia-h100`: [A3 machine type](/compute/docs/accelerator-optimized-machines#a3-vms) \n- `nvidia-tesla-a100`or`nvidia-a100-80gb`: [A2 machine type](/compute/docs/accelerator-optimized-machines#a2-vms) \n- `nvidia-l4`: [G2 machine type](/compute/docs/accelerator-optimized-machines#g2-vms) .\nFor all other GPUs, this flag is optional.\n- Add the following block to your Terraform configuration:```\nprovider \"google\" {\u00a0 project = var.project_id\u00a0 region \u00a0= var.region}resource \"google_container_cluster\" \"ml_cluster\" {\u00a0 name \u00a0 \u00a0 = var.cluster_name\u00a0 location = var.region\u00a0 node_locations = [var.zone]}resource \"google_container_node_pool\" \"gpu_pool\" {\u00a0 name \u00a0 \u00a0 \u00a0 = google_container_cluster.ml_cluster.name\u00a0 location \u00a0 = var.region\u00a0 cluster \u00a0 \u00a0= google_container_cluster.ml_cluster.name\u00a0 node_count = 3\u00a0 autoscaling {\u00a0 \u00a0 total_min_node_count = \"1\"\u00a0 \u00a0 total_max_node_count = \"5\"\u00a0 }\u00a0 management {\u00a0 \u00a0 auto_repair \u00a0= \"true\"\u00a0 \u00a0 auto_upgrade = \"true\"\u00a0 }\u00a0 node_config {\u00a0 \u00a0 oauth_scopes = [\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/logging.write\",\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/monitoring\",\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/devstorage.read_only\",\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/trace.append\",\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/service.management.readonly\",\u00a0 \u00a0 \u00a0 \"https://www.googleapis.com/auth/servicecontrol\",\u00a0 \u00a0 ]\u00a0 \u00a0 labels = {\u00a0 \u00a0 \u00a0 env = var.project_id\u00a0 \u00a0 }\u00a0 \u00a0 guest_accelerator {\u00a0 \u00a0 \u00a0 type \u00a0= var.gpu_type\u00a0 \u00a0 \u00a0 count = 1\u00a0 \u00a0 \u00a0 gpu_driver_installation_config {\u00a0 \u00a0 \u00a0 \u00a0 gpu_driver_version = var.gpu_driver_version\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 image_type \u00a0 = \"cos_containerd\"\u00a0 \u00a0 machine_type = var.machine_type\u00a0 \u00a0 tags \u00a0 \u00a0 \u00a0 \u00a0 = [\"gke-node\", \"${var.project_id}-gke\"]\u00a0 \u00a0 disk_size_gb = \"30\"\u00a0 \u00a0 disk_type \u00a0 \u00a0= \"pd-standard\"\u00a0 \u00a0 metadata = {\u00a0 \u00a0 \u00a0 disable-legacy-endpoints = \"true\"\u00a0 \u00a0 }\u00a0 }}\n```\nTerraform calls Google Cloud APIs to set create a new cluster with a node pool that uses GPUs. The node pool initially has three nodes and autoscaling is enabled. To learn more about Terraform, see the [google_container_node_pool resource spec](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster) .\n **Note:** Remove all the resources defined in the configuration file so that you don't incur any further costs: `terraform destroy` .\nYou can also create a new cluster with GPUs and specify zones using the [--node-locations](/sdk/gcloud/reference/container/clusters/create#--node-locations) flag. However, we recommend that you create a separate GPU node pool in an existing cluster, as shown in this section.\n### Manually install NVIDIA GPU drivers\nIf you chose to disable automatic device driver installation when you created a GPU node pool, or if you're using a GKE version earlier than the minimum supported version for automatic installation, you must manually install a compatible NVIDIA GPU driver on the nodes. Google provides a DaemonSet that you can apply to install the drivers. On GPU nodes that use Container-Optimized OS, you also have the option of selecting between the default GPU driver version or a newer version.\nWe recommend that you use automatic driver installation when possible by specifying the `gpu-driver-version` option in the `--accelerator` flag when you create your Standard cluster. If you used the installation DaemonSet to manually install GPU drivers **on or before January 25, 2023** , you might need to re-apply the DaemonSet to get a version that ignores nodes that use automatic driver installation.\n**Note:** To run the installation DaemonSet, the GPU node pool requires the `https://www.googleapis.com/auth/devstorage.read_only` scope for communicating with [Cloud Storage](/storage) . Without this scope, downloading of the installation DaemonSet manifest will fail. This scope is one of the default [scopes](/sdk/gcloud/reference/container/clusters/create#--scopes) , which is typically added when you create the cluster.\nThe following instructions show you how to install the drivers on Container-Optimized OS (COS) and Ubuntu nodes, and using Terraform.\nTo deploy the installation [DaemonSet](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml) and install the default GPU driver version, run the following command:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n```\nAlternatively, to install the newer GPU driver version (see table below), run the following command:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n```\nThe installation takes several seconds to complete. Once installed, the NVIDIA GPU device plugin surfaces NVIDIA GPU capacity via Kubernetes APIs.\nEach version of Container-Optimized OS image has at least one supported NVIDIA GPU driver version. See the [release notes](/container-optimized-os/docs/release-notes) of the major Container-Optimized OS LTS milestones for the default supported version.\nThe following table lists the available driver versions in each GKE version:\n| GKE version | NVIDIA driver      |\n|--------------:|:-----------------------------------|\n|   1.26 | R470(default), R510, or R525  |\n|   1.25 | R470(default), R510, or R525  |\n|   1.24 | R470(default), R510, or R525  |\n|   1.23 | R450(default), R470, R510, or R525 |\n|   1.22 | R450(default), R470, R510, or R525 |\n|   1.21 | R450(default), R470, or R510  |\n|   1.2 | R450(default), R470    |\n **Note:** R510 is available in GKE versions in [2022-R24](/kubernetes-engine/docs/release-notes#October_05_2022) or later.\n **Note:** GPU support requires v1.11.3 or higher for Ubuntu nodes.\nTo deploy the installation [DaemonSet](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded.yaml) for all GPUs **except** NVIDIA L4 GPUs, run the following command:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded.yaml\n```\nThe installation takes several seconds to complete. Once installed, the NVIDIA GPU device plugin surfaces NVIDIA GPU capacity via Kubernetes APIs.\nFor NVIDIA L4 GPUs, install the `R525` driver instead by using the following command:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded-R525.yaml\n```\nThe following table lists the available driver versions in each GKE version:\n| GKE version | NVIDIA driver |\n|--------------:|:----------------|\n|   1.27 | R470   |\n|   1.26 | R470   |\n|   1.25 | R470   |\n|   1.24 | R470   |\n|   1.23 | R470   |\n|   1.22 | R450   |\n|   1.21 | R450   |\n|   1.2 | R450   |\nYou can use Terraform to install the default GPU driver version based on the type of nodes. In both cases, you must configure the [kubectl_manifest Terraform resource type](https://registry.terraform.io/providers/alekc/kubectl/latest/docs/resources/kubectl_manifest) .- To install the [DaemonSet on COS](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml) , add the following block in your Terraform configuration:```\n\u00a0 data \"http\" \"nvidia_driver_installer_manifest\" {\u00a0 \u00a0 url = \"https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\"\u00a0 }\u00a0 resource \"kubectl_manifest\" \"nvidia_driver_installer\" {\u00a0 \u00a0 yaml_body = data.http.nvidia_driver_installer_manifest.body\u00a0 }\n```\n- To install [DaemonSet on Ubuntu](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded.yaml) , add the following block in your Terraform configuration:```\n\u00a0 data \"http\" \"nvidia_driver_installer_manifest\" {\u00a0 \u00a0 url = \"https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/ubuntu/daemonset-preloaded.yaml\"\u00a0 }\u00a0 resource \"kubectl_manifest\" \"nvidia_driver_installer\" {\u00a0 \u00a0 yaml_body = data.http.nvidia_driver_installer_manifest.body\u00a0 }\n```## Using node auto-provisioning with GPUs\nWhen using node auto-provisioning with GPUs, the auto-provisioned node pools by default do not have sufficient scopes to install the drivers. To grant the required scopes, modify the default scopes for node auto-provisioning to add `logging.write` , `monitoring` , `devstorage.read_only` , and `compute` , such as in the following example.\n**Note:** You can't use the `gpu-driver-version` flag to enable automatic driver installation in auto-provisioned node pools. Only manually created node pools support automatic driver installation.\n```\ngcloud container clusters update CLUSTER_NAME --enable-autoprovisioning \\\u00a0 \u00a0 --min-cpu=1 --max-cpu=10 --min-memory=1 --max-memory=32 \\\u00a0 \u00a0 --autoprovisioning-scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/compute\n```\nTo learn more about auto-provisioning, see [Using node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\n## Configuring Pods to consume GPUs\nYou use a [resource limit](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) to configure Pods to consume GPUs. You specify a resource limit in a [Pod specification](https://v1-25.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core) using the following key-value pair\n- **Key:** `nvidia.com/gpu`\n- **Value:** Number of GPUs to consume\n**Note:** `alpha.kubernetes.io/nvidia-gpu` is not supported as a resource name in GKE. Use `nvidia.com/gpu` as the resource name instead.\nBelow is an example of a Pod specification that consumes GPUs:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n name: my-gpu-pod\nspec:\n containers:\n - name: my-gpu-container\n image: nvidia/cuda:11.0.3-runtime-ubuntu20.04\n command: [\"/bin/bash\", \"-c\", \"--\"]\n args: [\"while true; do sleep 600; done;\"]\n resources:\n  limits:\n  nvidia.com/gpu: 2\n```\n## Consuming multiple GPU types\nIf you want to use multiple GPU accelerator types per cluster, you must create multiple node pools, each with their own accelerator type. GKE attaches a unique [node selector](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector) to GPU nodes to help place GPU workloads on nodes with specific GPU types:\n- **Key:** `cloud.google.com/gke-accelerator`\n- **Value:** `nvidia-tesla-k80`,`nvidia-tesla-p100`,`nvidia-tesla-p4`,`nvidia-tesla-v100`,`nvidia-tesla-t4`,`nvidia-tesla-a100`,`nvidia-a100-80gb`,`nvidia-h100-80gb`, or`nvidia-l4`.\nYou can target particular GPU types by adding this node selector to your workload's Pod specification. For example:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n name: my-gpu-pod\nspec:\n containers:\n - name: my-gpu-container\n image: nvidia/cuda:11.0.3-runtime-ubuntu20.04\n command: [\"/bin/bash\", \"-c\", \"--\"]\n args: [\"while true; do sleep 600; done;\"]\n resources:\n  limits:\n  nvidia.com/gpu: 2\n nodeSelector:\n cloud.google.com/gke-accelerator: nvidia-tesla-k80\n```\n## Upgrade node pools using accelerators (GPUs and TPUs)\nGKE [automatically upgrades](/kubernetes-engine/docs/concepts/cluster-upgrades#upgrading_automatically) Standard clusters, including node pools. You can also [manuallyupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_nodes) node pools if you want your nodes on a later version sooner. To control how upgrades work for your cluster, use [releasechannels](/kubernetes-engine/docs/concepts/release-channels) , [maintenancewindows andexclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) , and [rolloutsequencing](/kubernetes-engine/docs/concepts/about-rollout-sequencing) .\nYou can also configure a [node upgradestrategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) for your node pool, such as [surgeupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) or [blue-greenupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) . By configuring these strategies, you can ensure that the node pools are upgraded in a way that achieves the optimal balance between speed and disruption for your environment. For [multi-host TPU slice nodepools](/kubernetes-engine/docs/concepts/tpus#node_pool) , instead of using the configured node upgrade strategy, GKE atomically recreates the entire node pool in a single step. To learn more, see the definition of in [Terminology related to TPU inGKE](/kubernetes-engine/docs/concepts/tpus#terminology) .\nUsing a node upgrade strategy will temporarily require GKE to provision additional resources, depending on the configuration. If Google Cloud has limited capacity for your node pool's resources\u2014for example, you're seeing [resource availability](/compute/docs/troubleshooting/troubleshooting-resource-availability) errors when trying to create more nodes with GPUs or TPUs\u2014see [Upgrade in aresource-constrainedenvironment](/kubernetes-engine/docs/how-to/node-upgrades-quota#upgrade-resource-constrained) .\n## About the NVIDIA CUDA-X libraries\n[CUDA](https://developer.nvidia.com/cuda-zone) is NVIDIA's parallel computing platform and programming model for GPUs. To use CUDA applications, the image that you use must have the libraries. To add the NVIDIA CUDA-X libraries, use any of the following methods:\n- **Recommended: ** Use an image with the NVIDIA CUDA-X libraries pre-installed. For example, you can use [Deep Learning Containers](/deep-learning-containers/docs/overview) . These containers pre-install the key data science frameworks, the NVIDIA CUDA-X libraries, and tools. Alternatively, [ the NVIDIA CUDA image](https://hub.docker.com/r/nvidia/cuda) contains only the NVIDIA CUDA-X libraries.\n- Build and use your own image. In this case, include the following values in the`LD_LIBRARY_PATH`environment variable in your container specification:- `/usr/local/cuda-` `` `/lib64`: the  location of the NVIDIA CUDA-X libraries on the node. Replace``with the CUDA-X image version that  you used. Some versions also contain debug utilities in`/usr/local/nvidia/bin`. For details, see [the NVIDIA CUDA image on DockerHub](https://hub.docker.com/r/nvidia/cuda) .\n- `/usr/local/nvidia/lib64`: the location of the  NVIDIA device drivers.To check the minimum GPU driver version required for your version of CUDA, see [CUDA Toolkit and Compatible Driver Versions](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver) . Ensure that the GKE patch version running on your nodes includes a GPU driver version that's compatible with your chosen CUDA version. For a list of GPU driver versions associated with GKE version, refer to the corresponding Container-Optimized OS page linked in the [GKE current versions table](/kubernetes-engine/docs/release-notes#current_versions) .\n## Monitor GPU nodes\nIf your GKE cluster has [system metrics](/stackdriver/docs/solutions/gke/managing-metrics#system-metrics) enabled, then the following metrics are available in [Cloud Monitoring](/monitoring/docs) to monitor your GPU workload performance:\n- **Duty Cycle (container/accelerator/duty_cycle): ** Percentage of time over the past sample period (10 seconds) during which the accelerator was actively processing. Between 1 and 100.\n- **Memory Usage (container/accelerator/memory_used): ** Amount of accelerator memory allocated in bytes.\n- **Memory Capacity (container/accelerator/memory_total): ** Total accelerator memory in bytes.\nYou can use predefined dashboards to monitor your clusters with GPU nodes. For more information, see [ View observability metrics](/kubernetes-engine/docs/how-to/view-observability-metrics) . For general information about monitoring your clusters and their resources, refer to [Observability for GKE](/kubernetes-engine/docs/concepts/observability) .\n### View usage metrics for workloads\nYou view your workload GPU usage metrics from the **Workloads** dashboard in the Google Cloud console.\nTo view your workload GPU usage, perform the following steps:\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload/overview) \n- Select a workload.\nThe Workloads dashboard displays charts for GPU memory usage and capacity, and GPU duty cycle.\n### View NVIDIA Data Center GPU Manager (DCGM) metrics\nYou can collect and visualize NVIDIA DCGM metrics with Standard clusters by using [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) .\nFor instructions on how to deploy DCGM and the Prometheus DCGM exporter for your Standard cluster, see [NVIDIA Data Center GPU Manager (DCGM)](/stackdriver/docs/managed-prometheus/exporters/nvidia-dcgm) in the Google Cloud Observability documentation.\n## What's next\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn how to use a minimum CPU platform for your nodes](/kubernetes-engine/docs/how-to/min-cpu-platform) .\n- [Learn how to create and set up a local deep learning container with Docker](/deep-learning-containers/docs/getting-started-local) .", "guide": "Google Kubernetes Engine (GKE)"}