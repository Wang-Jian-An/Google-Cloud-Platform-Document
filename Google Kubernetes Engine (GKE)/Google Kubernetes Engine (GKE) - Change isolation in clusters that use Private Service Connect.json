{"title": "Google Kubernetes Engine (GKE) - Change isolation in clusters that use Private Service Connect", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/change-cluster-isolation", "abstract": "# Google Kubernetes Engine (GKE) - Change isolation in clusters that use Private Service Connect\nThis page shows you how to change the network isolation for your cluster's control plane and cluster nodes. Changing the isolation mode of a cluster is only supported for clusters that use [Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) to privately connect the control plane and nodes.\n", "content": "## Why change cluster isolation\nBy default, when you create [clusters that use Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) , GKE assigns an an external IP address (external endpoint) to the control plane. This means that any VM with an external IP address can reach the control plane.\nIf you configure [authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) , you can limit the IP address ranges that have access to your cluster control plane, but the cluster control plane is still accessible from Google Cloud-owned IP addresses. For example, any VM with an external IP address assigned in Google Cloud can reach your control plane external IP address. However, a VM without the corresponding credentials cannot reach your nodes\n## Benefits\nNetwork isolation provides the following benefits:\n- You can configure in the same cluster a mix of private and public nodes. This can reduce costs for nodes that don't require an external IP address to access public services on the internet.\n- You can block control plane access from Google Cloud-owned IP addresses or from external IP addresses to fully isolate the cluster control plane.\nThis page shows you how to change this default behavior by taking the following actions:\n- Enabling or disabling access to the control plane from Google Cloud-owned IP addresses. This action prevents any VM with a Google Cloud-owned IP addresses from reaching your control plane. For more information, see [block control plane access from Google Cloud-owned IP addresses](#block-control-plane-access-from-gcp) .\n- Enabling or disabling public access to the external endpoint of the control plane. This action fully isolates your cluster and control plane is not accessible from any public IP addresses. For more information, see [isolate the cluster control plane](#isolate-cluster) .\n- Removing public IP addresses from nodes. This action fully isolates your workloads. For more information, see [isolate node pools](#isolate_node_pool) .## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Review the [Private Service Connect limitations](/kubernetes-engine/docs/how-to/change-cluster-isolation#limitations) .## Block control plane access from Google Cloud VMs, Cloud Run, and Cloud Functions\nBy default, if you created a cluster with Private Service Connect predefined as , the authorized networks feature is disabled by default.\nIf you created cluster with Private Service Connect predefined as , the authorized networks feature is enable by default. To learn what IP addresses can always access the GKE control plane, see [Access to control plane endpoints](/kubernetes-engine/docs/how-to/authorized-networks#access_to_control_plane_endpoints) .\nTo remove access to the control plane of your cluster from Google Cloud VMs, Cloud Run, and Cloud Functions use the gcloud CLI or Google Cloud console:\n- Update your cluster to use the `--no-enable-google-cloud-access` flag:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-google-cloud-access\n```Replace `` with the name of the cluster.\n- Confirm that the `--no-enable-google-cloud-access` flag is applied:```\ngcloud container clusters describe CLUSTER_NAME | grep \"gcpPublicCidrsAccessEnabled\"\n```The output is similar to the following:```\ngcpPublicCidrsAccessEnabled: false\n```\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Networking** , in the **Control plane authorized networks** field, click **Editcontrol plane authorized networks** .\n- Clear the **Allow access through Google Cloud public IP addresses** checkbox.\n- Click **Save Changes** .## Allow access to the control plane from Google Cloud-owned IP addresses\nTo allow access from public IP addresses [owned by Google Cloud](/compute/docs/faq#find_ip_range) to the cluster control plane, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-google-cloud-access\n```\nReplace `` with the name of the cluster.\nGoogle Cloud-owned IP addresses can access your cluster control plane.- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Networking** , in the **Control plane authorized networks** field, click **Editcontrol plane authorized networks** .\n- Check the **Allow access through Google Cloud public IP addresses** checkbox.\n- Click **Save Changes** .## Disable external access to the control plane in clusters that use Private Service Connect\n### Clusters created as public\nBy default, when you create a GKE public cluster, GKE assigns an external IP address (external endpoint) to the control plane. If you instruct GKE to unassign this external endpoint, GKE enables a private endpoint. Access to your control plane from external IP addresses is disabled except from Google Cloud services that run cluster management processes. For more information about the enabled private endpoint and its limitation, see [public clusters with Private Service Connect](/kubernetes-engine/docs/how-to/change-cluster-isolation#limitations) .\nTo change control plane isolation for a cluster created as public, use the gcloud CLI:\n**Note:** You can also isolate a cluster during its creation. For more information, see how to [create a cluster and configure control plane access](/kubernetes-engine/docs/how-to/authorized-networks#create_cluster_limit_cp_access) .\n- Update your cluster to use the `--enable-private-endpoint` flag:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-private-endpoint\n```Replace `` with the name of the public cluster.\n- Confirm that the `--enable-private-endpoint` flag is applied:```\ngcloud container clusters describe CLUSTER_NAME | grep \"enablePrivateEndpoint\"\n```The output is similar to the following:```\nenablePrivateEndpoint:true\n```\n### Clusters created as private\nBy default, when you create a GKE private cluster, GKE assigns an external IP address (external endpoint) and an internal IP address (internal endpoint) to the control plane. You can unassign this external endpoint, but you can't unassign the internal endpoint.\nIf you instruct GKE to unassign the external endpoint, external access to your control plane from external IP addresses is disabled.\nTo remove the external endpoint in a cluster created as private, use the gcloud CLI or Google Cloud console:\n**Note:** You can also isolate a cluster during its creation. For more information, see how to [create a cluster and configure control plane access](/kubernetes-engine/docs/how-to/authorized-networks#create_cluster_limit_cp_access) .\n- Update your cluster to use the `--enable-private-endpoint` flag:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-private-endpoint\n```Replace `` with the name of the public cluster.\n- Confirm that the `--enable-private-endpoint` flag is applied:```\ngcloud container clusters describe CLUSTER_NAME | grep \"enablePrivateEndpoint\"\n```The output is similar to the following:```\nenablePrivateEndpoint: true\n```\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Cluster basics** , in the **External endpoint** field, click **Edit external control plane access** .\n- Clear the **Allow access through Google Cloud public IP addresses** checkbox.\n- Click **Save Changes** .## Enable external access to the control plane in clusters that use Private Service Connect\nTo assign an external IP address (external endpoint) to the control plane in clusters created as public or private, use the gcloud CLI or Google Cloud console:\nRun the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-private-endpoint\n```\nReplace `` with the name of the public cluster.\nExternal IP addresses can access your cluster control plane.- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Cluster basics** , in the **External endpoint** field, click **Edit external control plane access** .\n- Select the **Access control plane using its external IP address** checkbox.\n- Click **Save Changes** .## Isolate node pools\nYou can instruct GKE to provision node pools with only private IP addresses. After you update a public node pool to private mode, workloads requiring public internet access might fail. Before you change the nodes isolation, see the [Private Service Connect clusterslimitations](#psc_public_limitations) . You can edit this setting on clusters created as public or private:\n### Autopilot\nIn Autopilot clusters, add a taint on existing Pods so that GKE provisions them only on private nodes:\n- To request that GKE schedules a Pod on private nodes, add the following nodeSelector to your Pod specification:```\n\u00a0cloud.google.com/private-node=true\n```GKE recreates your Pods on private nodes. To avoid workload disruption, migrate each workload independently and monitor the migration.\n- If you are using Shared VPC, enable [Private Google Access](/vpc/docs/private-google-access) after changing the cluster isolation mode. If you are using [Cloud NAT](/nat/docs/gke-example) , you don't need to enable Private Google Access.\n### Standard\nTo provision nodes through private IP addresses in an existing node pool, run the following command:\n```\n\u00a0 gcloud container node-pools update NODE_POOL_NAME \\\u00a0 \u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 \u00a0 --enable-private-nodes\n```\nReplace the following:- ``: the name of the node pool that you want to edit.\n- `` : the name of the GKE cluster.If you are using Shared VPC, enable [Private Google Access](/vpc/docs/private-google-access) after changing the cluster isolation mode. If you are using [Cloud NAT](/nat/docs/gke-example) , you don't need to enable Private Google Access.\n- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the list of clusters, click the cluster name.\n- On the **Clusters** page, click the **Nodes** tab.\n- Under **Node Pools** , click the node pool name.\n- Click **Edit** .\n- Select the **Enable private nodes** checkbox.\n- Click **Save** .\n### Revert node pool isolation\nIn Standard clusters, to instruct GKE to provision node pools with public IP addresses, run the following command:\n```\ngcloud container node-pools update NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-private-nodes\n```\nReplace the following:\n- ``: the name of the node pool that you want to edit.\n- ``: the name of the GKE cluster.\nPublic IP addresses can access your cluster nodes.\n## Limitations\nBefore you change the cluster isolation mode, consider the following limitations:\n- You can only change the isolation mode for clusters that use [Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) .\n- Changing the isolation mode is not supported on public clusters running on [legacy networks](/vpc/docs/legacy) .\n- After you update a public node pool to private mode, workloads that require public internet access might fail in the following scenarios:- Clusters in a Shared VPC network where Private Google Access is not enabled. [Manually enable Private Google Access](/vpc/docs/configure-private-google-access#enabling-pga) to ensure GKE downloads the assigned node image. For clusters that aren't in a Shared VPC networks, GKE automatically enables Private Google Access.\n- Workloads that require access to the internet where Cloud NAT is not enabled or a custom NAT solution is not defined. To allow egress traffic to the internet, [enable Cloud NAT](/nat/docs/gke-example) or a custom NAT solution.\n### Private Service Connect clusters created as public or private\nClusters that were created as public and use Private Service Connect have a private endpoint enabled. In this private endpoint, the internal IP addresses in URLs for new or existing webhooks you configure are not supported. To mitigate this incompatibility do the following:\n- Set up a webhook with a private address by URL.\n- [Create a headless service without aselector](https://kubernetes.io/docs/concepts/services-networking/service/#without-selectors) .\n- Create a corresponding endpoint for the required destination.\n### Private Service Connect clusters created as public\nOnly in Private Service Connect clusters created as public, all private IP addresses from the cluster's network always can reach the cluster private endpoint. To learn more about control plane access, see [Authorized networks for control plane access](/kubernetes-engine/docs/how-to/authorized-networks#access_to_control_plane_endpoints) .\n**Warning:** Clusters with Private Service Connect use a Private Service Connect and a Private Service Connect . Both resources are named `gke-[cluster-name]-[cluster-hash:8]-[uuid:8]-pe` and permit the control plane and nodes to privately connect. GKE creates these resources automatically and at no cost. Don't remove these resources; otherwise, cluster network issues including downtime will occur. To determine if your cluster uses Private Service Connect, find the gcloud CLI command included in the [Private Service Connect overview](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) .\n## What's next\n- [Learn more about authorized networks for control plane access](/kubernetes-engine/docs/how-to/authorized-networks) .", "guide": "Google Kubernetes Engine (GKE)"}