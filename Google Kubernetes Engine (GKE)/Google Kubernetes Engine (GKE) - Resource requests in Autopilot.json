{"title": "Google Kubernetes Engine (GKE) - Resource requests in Autopilot", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-resource-requests", "abstract": "# Google Kubernetes Engine (GKE) - Resource requests in Autopilot\nThis page describes the maximum, minimum, and default resource requests that you can specify for your Google Kubernetes Engine (GKE) Autopilot workloads, and how Autopilot automatically modifies those requests to maintain workload stability.\n", "content": "## Overview of resource requests in Autopilot\nAutopilot uses the resource requests that you specify in your workload configuration to configure the nodes that run your workloads. Autopilot enforces minimum and maximum resource requests based on the compute class or the hardware configuration that your workloads use. If you don't specify requests for some containers, Autopilot assigns default values to let those containers run correctly.\nWhen you deploy a workload in an Autopilot cluster, GKE validates the workload configuration against the allowed minimum and maximum values for the selected [compute class](/kubernetes-engine/docs/concepts/autopilot-compute-classes) or hardware configuration (such as [GPUs](/kubernetes-engine/docs/how-to/autopilot-gpus) ). If your requests are less than the minimum, Autopilot automatically modifies your workload configuration to bring your requests within the allowed range. If your requests are greater than the maximum, Autopilot rejects your workload and displays an error message.\nThe following list summarizes the categories of resource requests:\n- [Default resource requests](#defaults) : Autopilot adds these if you don't specify your own requests for workloads\n- [Minimum and maximum resource requests](#min-max-requests) : Autopilot validates your specified requests to ensure that they're within these limits. If your requests are outside the limits, Autopilot modifies your workload requests.\n- [Workload separation and extended duration requests](#workload-separation) : Autopilot has different default values and different minimum values for workloads that you separate from each other, or for Pods that get extended protection from GKE-initiated eviction.\n- Resource requests for DaemonSets: Autopilot has different default, minimum, and maximum values for containers in DaemonSets.\n### How to request resources\nIn Autopilot, you request resources in your Pod specification. The supported minimum and maximum resources that you can request change based on the hardware configuration of the node on which the Pods run. To learn how to request specific hardware configurations, refer to the following pages:\n- [Choose compute classes for Autopilot Pods](/kubernetes-engine/docs/how-to/autopilot-compute-classes) \n- [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) ## Default resource requests\nIf you don't specify resource requests for some containers in a Pod, Autopilot applies default values. These defaults are suitable for many smaller workloads.\n**Note:** We recommend that you explicitly set your resource requests for each container to meet your application requirements, as these default values might not be sufficient, or optimal.\nAdditionally, Autopilot applies the following default resource requests regardless of the selected compute class or hardware configuration:\n- Containers in DaemonSets- CPU: 50 mCPU\n- Memory: 100 MiB\n- Ephemeral storage: 100 MiB\n- All other containers- Ephemeral storage: 1 GiBFor more information about Autopilot cluster limits, see [Quotas and limits](/kubernetes-engine/quotas#limits_per_cluster) .\n### Default requests for compute classes\nAutopilot applies the following default values to resources that are not defined in the Pod specification for Pods that run on [compute classes](/kubernetes-engine/docs/concepts/autopilot-compute-classes) :\n| Compute class | Resource   | Default request                                                                |\n|:----------------|:------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| General-purpose | CPU    | 0.5 vCPU                                                                  |\n| General-purpose | Memory   | 2 GiB                                                                   |\n| Balanced  | CPU    | 0.5 vCPU                                                                  |\n| Balanced  | Memory   | 2 GiB                                                                   |\n| Performance  | CPU    | C3 machine series: 2 vCPU C3 machine series with Local SSD: 2 vCPU C3D machine series: 2 vCPU C3D machine series with Local SSD: 4 vCPU H3 machine series: 80 vCPU C2 machine series: 2 vCPU C2D machine series: 2 vCPU T2A machine series: 2 vCPU T2D machine series: 2 vCPU |\n| Performance  | Memory   | C3 machine series: 8\u00a0GiB C3 machine series with Local SSD: 8\u00a0GiB C3D machine series: 8\u00a0GiB C3D machine series with Local SSD: 16\u00a0GiB H3 machine series: 320\u00a0GiB C2 machine series: 8\u00a0GiB C2D machine series: 8\u00a0GiB T2A machine series: 8\u00a0GiB T2D machine series: 8\u00a0GiB  |\n| Performance  | Ephemeral storage | C3 machine series: 1\u00a0GiB C3 machine series with Local SSD: 1\u00a0GiB C3D machine series: 1\u00a0GiB C3D machine series with Local SSD: 1\u00a0GiB H3 machine series: 1\u00a0GiB C2 machine series: 1\u00a0GiB C2D machine series: 1\u00a0GiB T2A machine series: 1\u00a0GiB T2D machine series: 1\u00a0GiB   |\n| Scale-Out  | CPU    | 0.5 vCPU                                                                  |\n| Scale-Out  | Memory   | 2 GiB                                                                   |\n### Default requests for other hardware configurations\nAutopilot applies the following default values to resources that are not defined in the Pod specification for Pods that run on nodes with specialized hardware, such as GPUs:\n| Hardware       | Resource   | Total default request               |\n|:-----------------------------------|:------------------|:--------------------------------------------------------------------------------|\n| H100 (80GB) GPUs nvidia-h100-80gb | CPU    | 8 GPUs: 200 vCPU                |\n| H100 (80GB) GPUs nvidia-h100-80gb | Memory   | 8 GPUs: 1400 GiB                |\n| H100 (80GB) GPUs nvidia-h100-80gb | Ephemeral storage | 8 GPUs: 1 GiB                 |\n| A100 (40GB) GPUs nvidia-tesla-a100 | CPU    | 1 GPU: 9 vCPU 2 GPUs: 20 vCPU 4 GPUs: 44 vCPU 8 GPUs: 92 vCPU 16 GPUs: 92 vCPU |\n| A100 (40GB) GPUs nvidia-tesla-a100 | Memory   | 1 GPU: 60 GiB 2 GPUs: 134 GiB 4 GPUs: 296 GiB 8 GPUs: 618 GiB 16 GPUs: 1250 GiB |\n| A100 (80GB) GPUs nvidia-a100-80gb | CPU    | 1 GPU: 9 vCPU 2 GPUs: 20 vCPU 4 GPUs: 44 vCPU 8 GPUs: 92 vCPU     |\n| A100 (80GB) GPUs nvidia-a100-80gb | Memory   | 1 GPU: 134 GiB 2 GPUs: 296 GiB 4 GPUs: 618 GiB 8 GPUs: 1250 GiB     |\n| A100 (80GB) GPUs nvidia-a100-80gb | Ephemeral storage | 1 GPU: 1 GiB 2 GPUs: 1 GiB 4 GPUs: 1 GiB 8 GPUs: 1 GiB       |\n| L4 GPUs nvidia-l4     | CPU    | 1 GPU: 2 vCPU 2 GPUs: 21 vCPU 4 GPUs: 45 vCPU 8 GPUs: 93 vCPU     |\n| L4 GPUs nvidia-l4     | Memory   | 1 GPU: 7 GiB 2 GPUs: 78 GiB 4 GPUs: 170 GiB 8 GPUs: 355 GiB      |\n| T4 GPUs nvidia-tesla-t4   | CPU    | 0.5 vCPU                  |\n| T4 GPUs nvidia-tesla-t4   | Memory   | 2 GiB                   |\n## Minimum and maximum resource requests\nThe total resources requested by your deployment configuration should be within the supported minimum and maximum values that Autopilot allows. The following conditions apply:\n- The **ephemeral storage request** must be between 10 MiB and 10 GiB for all compute classes and hardware configurations unless otherwise specified. For larger volumes, it is recommended to use [generic ephemeral volumes](/kubernetes-engine/docs/how-to/generic-ephemeral-volumes) which provide equivalent functionality and performance to ephemeral storage but with significantly more flexibility as they can be used with any GKE storage option. For example, the maximum size for a generic ephemeral volume using`pd-balanced`is 64 TiB.\n- For **DaemonSet Pods** , the minimum resource requests are as follows:- **Clusters that support bursting** : 1 mCPU per Pod, 2 MiB of memory per Pod, and 10 MiB of ephemeral storage per container in the Pod.\n- **Clusters that don't support bursting** : 10mCPU per Pod, 10 MiB of memory per Pod, and 10 MiB of ephemeral storage per container in the Pod.\nTo check whether your cluster supports bursting, see [Bursting availability in GKE](/kubernetes-engine/docs/how-to/pod-bursting-gke#availability-in-gke) .\n- The **CPU:memory ratio** must be within the allowed range for the selected compute class or hardware configuration. If your CPU:memory ratio is outside the allowed range, Autopilot automatically increases the smaller resource. For example, if you request 1 vCPU and 16 GiB of memory (1:16 ratio) for Pods running on the `Scale-Out` class, Autopilot increases the CPU request to 4 vCPUs, which changes the ratio to 1:4.\n### Minimums and maximums for compute classes\nThe following table describes the minimum, maximum, and allowed CPU:memory ratio for each compute class that Autopilot supports:\n| Compute class | CPU:memory ratio (vCPU:GiB) | Resource   | Minimum                                                            | Maximum                                                                      |\n|:----------------|:------------------------------|:------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| General-purpose | Between 1:1 and 1:6.5   | CPU    | The value depends on whether your cluster supports bursting, as follows: Clusters that support bursting: 50m CPU Clusters that don't support bursting: 250m CPU To check whether your cluster supports bursting, see Bursting availability in GKE. | 30 vCPU                                                                      |\n| General-purpose | Between 1:1 and 1:6.5   | Memory   | The value depends on whether your cluster supports bursting, as follows: Clusters that support bursting: 52 MiB Clusters that don't support bursting: 512 MiB To check whether your cluster supports bursting, see Bursting availability in GKE. | 110 GiB                                                                      |\n| Balanced  | Between 1:1 and 1:8   | CPU    | 0.25 vCPU                                                           | 222 vCPU If minimum CPU platform selected: Intel platforms: 126 vCPU AMD platforms: 222 vCPU                                                |\n| Balanced  | Between 1:1 and 1:8   | Memory   | 0.5 GiB                                                            | 851 GiB If minimum CPU platform selected: Intel platforms: 823 GiB AMD platforms: 851 GiB                                                 |\n| Performance  | nan       | CPU    | 0.001 vCPU                                                           | C3 machine series: 174 vCPU C3 machine series with Local SSD: 174 vCPU C3D machine series: 358 vCPU C3D machine series with Local SSD: 358 vCPU H3 machine series: 86 vCPU C2 machine series: 58 vCPU C2D machine series: 110 vCPU T2A machine series: 46 vCPU T2D machine series: 58 vCPU |\n| Performance  | nan       | Memory   | 1\u00a0MiB                                                            | C3 machine series: 1,345\u00a0GiB C3 machine series with Local SSD: 670\u00a0GiB C3D machine series: 2750\u00a0GiB C3D machine series with Local SSD: 1,375\u00a0GiB H3 machine series: 330\u00a0GiB C2 machine series: 218\u00a0GiB C2D machine series: 835\u00a0GiB T2A machine series: 172\u00a0GiB T2D machine series: 218\u00a0GiB |\n| Performance  | nan       | Ephemeral storage | 10\u00a0MiB                                                            | C3 machine series: 250\u00a0GiB C3 machine series with Local SSD: 10,000\u00a0GiB C3D machine series: 250\u00a0GiB C3D machine series with Local SSD: 10,000\u00a0GiB H3 machine series: 250\u00a0GiB C2 machine series: 250\u00a0GiB C2D machine series: 250\u00a0GiB T2A machine series: 250\u00a0GiB T2D machine series: 250\u00a0GiB |\n| Scale-Out  | 1:4       | CPU    | 0.25 vCPU                                                           | arm64: 43 vCPU amd64: 54 vCPU                                                                |\n| Scale-Out  | 1:4       | Memory   | 1 GiB                                                            | arm64: 172 GiB amd64: 216 GiB                                                                |\nTo learn how to request compute classes in your Autopilot Pods, refer to [Choose compute classes for Autopilot Pods](/kubernetes-engine/docs/how-to/autopilot-compute-classes) .\n### Minimums and maximums for other hardware configurations\nThe following table describes the minimum, maximum, and allowed CPU:memory ratio for Pods that run on nodes with specific hardware such as GPUs:\n| Hardware       | CPU:memory ratio (vCPU:GiB)         | Resource   | Minimum                   | Maximum                                           |\n|:-----------------------------------|:------------------------------------------------------------|:------------------|:--------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| H100 (80GB) GPUs nvidia-h100-80gb | nan               | CPU    | 8 GPUs: 0.001 vCPU                | 8 GPUs: 206 vCPU                                         |\n| H100 (80GB) GPUs nvidia-h100-80gb | nan               | Memory   | 8 GPUs: 1 MiB                 | 8 GPUs: 1795 GiB                                         |\n| H100 (80GB) GPUs nvidia-h100-80gb | nan               | Ephemeral storage | 8 GPU: 10 MiB                 | 8 GPUs: 5250 GiB                                         |\n| A100 (40GB) GPUs nvidia-tesla-a100 | Not enforced            | CPU    | 1 GPU: 9 vCPU 2 GPUs: 20 vCPU 4 GPUs: 44 vCPU 8 GPUs: 92 vCPU 16 GPUs: 92 vCPU | 1 GPU: 11 vCPU 2 GPUs: 22 vCPU 4 GPUs: 46 vCPU 8 GPUs: 94 vCPU 16 GPUs: 94 vCPU The sum of CPU requests of all DaemonSets that run on an A100 GPU node must not exceed 2 vCPU. |\n| A100 (40GB) GPUs nvidia-tesla-a100 | Not enforced            | Memory   | 1 GPU: 60 GiB 2 GPUs: 134 GiB 4 GPUs: 296 GiB 8 GPUs: 618 GiB 16 GPUs: 1250 GiB | 1 GPU: 74 GiB 2 GPUs: 148 GiB 4 GPUs: 310 GiB 8 GPUs: 632 GiB 16 GPUs: 1264 GiB The sum of memory requests of all DaemonSets that run on an A100 GPU node must not exceed 14 GiB. |\n| A100 (80GB) GPUs nvidia-a100-80gb | Not enforced            | CPU    | 1 GPU: 9 vCPU 2 GPUs: 20 vCPU 4 GPUs: 44 vCPU 8 GPUs: 92 vCPU     | 1 GPU: 11 vCPU 2 GPUs: 22 vCPU 4 GPUs: 46 vCPU 8 GPUs: 94 vCPU The sum of CPU requests of all DaemonSets that run on an A100 (80GB) GPU node must not exceed 2 vCPU.    |\n| A100 (80GB) GPUs nvidia-a100-80gb | Not enforced            | Memory   | 1 GPU: 134 GiB 2 GPUs: 296 GiB 4 GPUs: 618 GiB 8 GPUs: 1250 GiB     | 1 GPU: 148 GiB 2 GPUs: 310 GiB 4 GPUs: 632 GiB 8 GPUs: 1264 GiB The sum of memory requests of all DaemonSets that run on an A100 (80GB) GPU node must not exceed 14 GiB.   |\n| A100 (80GB) GPUs nvidia-a100-80gb | Not enforced            | Ephemeral storage | 1 GPU: 512 MiB 2 GPUs: 512 MiB 4 GPUs: 512 MiB 8 GPUs: 512 MiB     | 1 GPU: 280 GiB 2 GPUs: 585 GiB 4 GPUs: 1220 GiB 8 GPUs: 2540 GiB                             |\n| L4 GPUs nvidia-l4     | 1 GPU: Between 1:3.5 and 1:4 2, 4, and 8 GPUs: Not enforced | CPU    | 1 GPU: 2 vCPU 2 GPUs: 21 vCPU 4 GPUs: 45 vCPU 8 GPUs: 93 vCPU     | 1 GPU: 31 vCPU 2 GPUs: 23 vCPU 4 GPUs: 47 vCPU 8 GPUs: 95 vCPU The sum of CPU requests of all DaemonSets that run on an L4 GPU node must not exceed 2 vCPU.      |\n| L4 GPUs nvidia-l4     | 1 GPU: Between 1:3.5 and 1:4 2, 4, and 8 GPUs: Not enforced | Memory   | 1 GPU: 7 GiB 2 GPUs: 78 GiB 4 GPUs: 170 GiB 8 GPUs: 355 GiB      | 1 GPU: 115 GiB 2 GPUs: 86 GiB 4 GPUs: 177 GiB 8 GPUs: 363 GiB The sum of memory requests of all DaemonSets that run on an L4 GPU node must not exceed 14 GiB.      |\n| L4 GPUs nvidia-l4     | 1 GPU: Between 1:3.5 and 1:4 2, 4, and 8 GPUs: Not enforced | Ephemeral storage | All GPU quantities: 512 MiB              | All GPU quantities: 10 GiB                                       |\n| T4 GPUs nvidia-tesla-t4   | Between 1:1 and 1:6.25          | CPU    | 0.5 vCPU                  | 1 GPU: 46 vCPU 2 GPUs: 46 vCPU 4 GPUs: 94 vCPU                                  |\n| T4 GPUs nvidia-tesla-t4   | Between 1:1 and 1:6.25          | Memory   | 0.5 GiB                   | 1 GPU: 287.5 GiB 2 GPUs: 287.5 GiB 4 GPUs: 587.5 GiB                                |\n**Note:** All A100 (80GB) GPU nodes use local SSDs for node boot disks at fixed sizes based on the number of GPUs. You're billed separately for the attached Local SSDs. For details, see [Autopilot pricing](/kubernetes-engine/pricing#gpu-pods) . This doesn't apply to A100 (40GB) GPUs.\nTo learn how to request GPUs in your Autopilot Pods, refer to [Deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .\n### Resource requests for workload separation and extended duration\nAutopilot lets you manipulate Kubernetes scheduling and eviction behavior using methods such as the following:\n- Use [taints and tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) and [node selectors](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) to ensure that certain Pods only get placed on specific nodes. For details, see [Configure workload separation in GKE](/kubernetes-engine/docs/how-to/workload-separation) .\n- Use [Pod anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) to prevent Pods from co-locating on the same node. The default and minimum resource requests for workloads that use these methods to control scheduling behavior are higher than for workloads that don't.\n- Use an annotation to protect Pods from eviction caused by node auto-upgrades and scale-down events for up to seven days. For details, see [Extend the run time of Autopilot Pods](/kubernetes-engine/docs/how-to/extended-duration-pods) .\nIf your specified requests are less than the minimums, the behavior of Autopilot changes based on the method that you used, as follows:\n- **Taints, tolerations, selectors, and extended duration Pods** : Autopilot modifies your Pods to increase the requests when scheduling the Pods.\n- **Pod anti-affinity** : Autopilot rejects the Pod and displays an error message.\nThe following table describes the default requests and the minimum resource requests that you can specify. If a configuration or compute class isn't in this table, Autopilot doesn't enforce special minimum or default values.\n| Compute class | Resource | Default | Minimum |\n|:----------------|:-----------|:----------|:----------|\n| General-purpose | CPU  | 0.5 vCPU | 0.5 vCPU |\n| General-purpose | Memory  | 2 GiB  | 0.5 GiB |\n| Balanced  | CPU  | 2 vCPU | 1 vCPU |\n| Balanced  | Memory  | 8 GiB  | 4 GiB  |\n| Scale-Out  | CPU  | 0.5 vCPU | 0.5 vCPU |\n| Scale-Out  | Memory  | 2 GiB  | 2 GiB  |\n### Init containers\nInit containers run in serial and must complete before the application containers start. If you don't specify resource requests for your Autopilot init containers, GKE allocates the total resources available to the Pod to each init container. This behavior is different than in GKE Standard, where each init container can use any unallocated resources available on the on which the Pod is scheduled.\nUnlike application containers, GKE recommends that you don't specify resource requests for Autopilot init containers, so that each container gets the full resources available to the Pod. If you request less resources than the defaults, you constrain your init container. If you request more resources than the Autopilot defaults, you might increase your bill for the lifetime of the Pod.\n## Setting resource limits in Autopilot\nKubernetes lets you set both `requests` and `limits` for resources in your Pod specification. The behavior of your Pods changes depending on whether your `limits` are different than your `requests` , as described in the following table:\n| Values set      | Autopilot behavior                                                                                                   |\n|:---------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| requests equal to limits   | Pods use the Guaranteed QoS class. Note: Ephemeral storage limits must always be explicitly set equal to requests. GKE modifies your Pods to enforce this rule.                                                               |\n| requests set, limits not set  | The behavior depends on whether your cluster supports bursting, as follows: Clusters that support bursting: Pods can burst into available burstable capacity. Clusters that don't support bursting: GKE sets the limits equal to the requests To check whether your cluster supports bursting, see Bursting availability in GKE.                      |\n| requests not set, limits set  | Autopilot sets requests to the value of limits, which is the default Kubernetes behavior. Before: resources: limits: cpu: \"400m\" After: resources: requests: cpu: \"400m\" limits: cpu: \"400m\"                                                       |\n| requests less than limits  | The behavior depends on whether your cluster supports bursting, as follows: Clusters that support bursting: Pods can burst up to the value specified in limits. Clusters that don't support bursting: GKE sets the limits equal to the requests To check whether your cluster supports bursting, see Bursting availability in GKE.                      |\n| requests greater than limits  | Autopilot sets requests to the value of limits. Before: resources: requests: cpu: \"450m\" limits: cpu: \"400m\" After: resources: requests: cpu: \"400m\" limits: cpu: \"400m\"                                                           |\n| requests not set, limits not set | Autopilot sets requests to the default values for the compute class or hardware configuration. The behavior for limits depends on whether your cluster supports bursting, as follows: Clusters that support bursting: Autopilot doesn't set limits. Clusters that don't support bursting: GKE sets the limits equal to the requests To check whether your cluster supports bursting, see Bursting availability in GKE. |\nIn most situations, set adequate resource requests and equal limits for your workloads.\nFor workloads that temporarily need more resources than their steady-state, like during boot up or during higher traffic periods, set your limits higher than your requests to let the Pods burst. For details, see [Configure Pod bursting in GKE](/kubernetes-engine/docs/how-to/pod-bursting-gke) .\n## Automatic resource management in Autopilot\nIf your specified resource requests for your workloads are outside of the allowed ranges, or if you don't request resources for some containers, Autopilot modifies your workload configuration to comply with the allowed limits. Autopilot calculates resource ratios and the resource scale up requirements after applying default values to containers with no request specified.\n- **Missing requests:** If you don't request resources in some containers, Autopilot applies the default requests for the compute class or hardware configuration.\n- **CPU:memory ratio:** Autopilot scales up the smaller resource to bring the ratio within the allowed range.\n- **Ephemeral storage:** Autopilot modifies your ephemeral storage requests to meet the minimum amount required by each container. The cumulative value of storage requests across all containers cannot be more than the maximum allowed value. Autopilot scales the request down if the value exceeds the maximum.\n- **Requests below minimums** : If you request fewer resources than the allowed minimum for the selected hardware configuration, Autopilot automatically modifies the Pod to request at least the minimum resource value.\nBy default, when Autopilot automatically scales a resource up to meet a minimum or default resource value, GKE allocates the extra capacity to the in the Pod manifest. In GKE version 1.27.2-gke.2200 and later, you can tell GKE to allocate the extra resources to a specific container by adding the following to the `annotations` field in your Pod manifest:\n```\nautopilot.gke.io/primary-container: \"CONTAINER_NAME\"\n```\nReplace `` with the name of the container.\n### Resource modification examples\nThe following example scenario shows how Autopilot modifies your workload configuration to meet the requirements of your running Pods and containers.\n| Container number | Original request           | Modified request           |\n|-------------------:|:---------------------------------------------------------|:---------------------------------------------------------|\n|     1 | CPU: 30 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 50 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n### Multiple containers with total CPU < 0.05 vCPU\n| Container number | Original requests          | Modified requests          |\n|:--------------------|:--------------------------------------------------------|:-------------------------------------------------------|\n| 1     | CPU: 10 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 30 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| 2     | CPU: 10 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 10 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| 3     | CPU: 10 mvCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 10 mCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| Total Pod resources | nan              | CPU: 50 mCPU Memory: 1.5 GiB Ephemeral storage: 30 MiB |\n### Multiple containers with more than 0.25 vCPU total\nFor multiple containers with total resources >= 0.25 vCPU, the CPU is rounded to multiples of 0.25 vCPU and the extra CPU is added to the first container. In this example, the original cumulative CPU is 0.32 vCPU and is modified to a total of 0.5 vCPU.\n| Container number | Original requests          | Modified requests          |\n|:--------------------|:---------------------------------------------------------|:---------------------------------------------------------|\n| 1     | CPU: 0.17 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 0.35 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| 2     | CPU: 0.08 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 0.08 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| 3     | CPU: 0.07 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB | CPU: 0.07 vCPU Memory: 0.5 GiB Ephemeral storage: 10 MiB |\n| 4     | Init container, resources not defined     | Will receive Pod resources        |\n| Total Pod resources | nan              | CPU: 0.5 vCPU Memory: 1.5 GiB Ephemeral storage: 30 MiB |\n### Single container with memory too low for requested CPU\nIn this example, the memory is too low for the amount of CPU (1 vCPU:1 GiB minimum). The minimum allowed ratio for CPU to memory is 1:1. If the ratio is lower than that, the memory request is increased.\n| Container number | Original request         | Modified request         |\n|:--------------------|:----------------------------------------------------|:----------------------------------------------------|\n| 1     | CPU: 4 vCPU Memory: 1 GiB Ephemeral storage: 10 MiB | CPU: 4 vCPU Memory: 4 GiB Ephemeral storage: 10 MiB |\n| Total Pod resources | nan             | CPU: 4 vCPU Memory: 4 GiB Ephemeral storage: 10 MiB |\n## What's next\n- [Learn how to select compute classes in your Autopilotworkloads](/kubernetes-engine/docs/how-to/autopilot-compute-classes) .\n- [Learn more about the supported Autopilot compute classes](/kubernetes-engine/docs/concepts/autopilot-compute-classes) .\n- [Learn how to select GPUs in your Autopilot Pods](/kubernetes-engine/docs/how-to/autopilot-gpus) .", "guide": "Google Kubernetes Engine (GKE)"}