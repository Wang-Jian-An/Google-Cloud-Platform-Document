{"title": "Google Kubernetes Engine (GKE) - Maintenance windows and exclusions", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions", "abstract": "# Google Kubernetes Engine (GKE) - Maintenance windows and exclusions\nThis page describes [maintenance windows](#maintenance_windows) and [maintenanceexclusions](#exclusions) , which provide control over when cluster maintenance such as auto-upgrades can and cannot occur on your Google Kubernetes Engine (GKE) clusters. For example, a retail business could limit maintenance to only occur on weekday evenings, and could prevent automated maintenance during a key industry sales event.\n**Note:** Maintenance windows and exclusions only control GKE cluster maintenance, and don't impact the timing of maintenance for services on which GKE depends, including Compute Engine. To learn more, see [Other Google Cloud maintenance](#gcp-maintenance) .\n", "content": "## Overview\nMaintenance windows and exclusions give you fine-grained control over when automatic maintenance can occur on your clusters.\nA [maintenance window](#maintenance_windows) is a repeating window of time during which automatic maintenance is permitted.\nA [maintenance exclusion](#exclusions) is a non-repeating window of time during which automatic maintenance is forbidden.\nYou can configure maintenance windows and maintenance exclusions separately and independently. You can configure multiple maintenance exclusions.\n**Note:** To ensure that your clusters remain functional, your configured maintenance windows and maintenance exclusions do not block [control plane repair operations](#repairs) .\n### Examples of automatic maintenance\nGoogle performs maintenance tasks on your clusters as needed, or when you make a configuration change that re-creates nodes or networks in the cluster, such as the following:\n- [Auto-upgrades to cluster control planes](/kubernetes-engine/docs/concepts/cluster-upgrades#cluster_upgrades) in accordance with the [GKE versioning and support](/kubernetes-engine/versioning-and-upgrades) policy.\n- [Node auto-upgrades](/kubernetes-engine/docs/concepts/cluster-upgrades#upgrading_automatically) , if enabled.\n- User-initiated configuration changes that cause nodes to be re-created, such as [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\n- User-initiated configuration changes that fundamentally change the cluster's internal network topology, such as [optimizing IP address allocation](/kubernetes-engine/docs/how-to/flexible-pod-cidr) .\n[Zonal clusters](/kubernetes-engine/docs/concepts/types-of-clusters#zonal_clusters) cannot be modified during control plane configuration changes and cluster maintenance. This includes deploying workloads.\nEach of the other types of changes listed above can cause temporary disruptions while workloads are migrated to upgraded nodes.\n## Caveats\nMaintenance windows and exclusions can cause security patches to be delayed. GKE [reserves the right](/kubernetes-engine/docs/concepts/shared-responsibility) to override maintenance policies for [critical security vulnerabilities](/kubernetes-engine/docs/resources/security-patching#how_vulnerabilities_are_classified) . Before enabling maintenance windows and exclusions, make sure you understand the following caveats.\nGKE clusters and workloads can also be impacted by automatic maintenance on other, dependent services, such as Compute Engine. GKE maintenance windows and exclusions do not prevent automatic maintenance from other Google services, or services which install applications to the cluster, such as Cloud Deploy.\nGKE performs automated repairs on [control planes](/kubernetes-engine/docs/concepts/cluster-architecture#control_plane) . This includes processes like upscaling the control plane to an appropriate size or restarting the control plane to resolve issues. Most repairs ignore maintenance windows and exclusions because failing to perform the repairs can result in non-functional clusters. Repairing control planes cannot be disabled.\n**Note:** [Regional clusters](/kubernetes-engine/docs/concepts/types-of-clusters#regional_clusters) have multiple replicas of the control plane, allowing for high availability of the Kubernetes API server even during maintenance events.\nNodes also have [auto-repair functionality](/kubernetes-engine/docs/how-to/node-auto-repair) , but can be disabled.\nWhen you enable or modify features or options such as those that impact networking between the control planes and nodes, the nodes are recreated to apply the new configuration. Some examples of features that cause nodes to be recreated are as follows:\n- [Shielded nodes](/kubernetes-engine/docs/how-to/shielded-gke-nodes) \n- [Network policies](/kubernetes-engine/docs/how-to/network-policy) \n- [Intranode visibility](/kubernetes-engine/docs/how-to/intranode-visibility) \n- [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) \n- [Rotating the control plane's IP address](/kubernetes-engine/docs/how-to/ip-rotation) \n- [Rotating the control plane's credentials](/kubernetes-engine/docs/how-to/credential-rotation) \nIf you use maintenance windows or exclusions and you enable or modify a feature or option that requires nodes to be recreated, such as [cluster credentialrotation](/kubernetes-engine/docs/how-to/credential-rotation) , the new configuration is applied to the nodes **only when node maintenance is allowed** . If you prefer not to wait, you can manually apply the changes to the nodes by calling the [gcloud container clustersupgrade](/sdk/gcloud/reference/container/clusters/upgrade) command and passing the `--cluster-version` flag with the same GKE version that the node pool is already running. You must use the Google Cloud CLI for this workaround.\n## Maintenance windows\nallow you to control when automatic upgrades of control planes and nodes can occur, to mitigate potential transient disruptions to your workloads. Maintenance windows are useful for the following types of scenarios, among others:\n- **Off-peak hours:** You want to minimize the chance of downtime by scheduling automatic upgrades during off-peak hours when traffic is reduced.\n- **On-call:** You want to ensure that upgrades happen during working hours so that someone can monitor the upgrades and manage any unanticipated issues.\n- **Multi-cluster upgrades:** You want to roll out upgrades across multiple clusters in different regions one at a time at specified intervals.\nIn addition to automatic upgrades, Google may occasionally need to perform other maintenance tasks, and honors a cluster's maintenance window if possible.\nIf tasks run beyond the maintenance window, GKE attempts to pause the tasks, and attempts to resume those tasks during the next maintenance window.\nGKE reserves the right to roll out unplanned emergency upgrades outside of maintenance windows. Additionally, mandatory upgrades from deprecated or outdated software might automatically occur outside of maintenance windows.\n**Note:** You can also [manually upgrade your cluster](/kubernetes-engine/docs/how-to/upgrading-a-cluster) at any time. Manually-initiated upgrades begin immediately and ignore any maintenance windows.\nTo learn how to set up maintenance window for a new or existing cluster, see [Configure a maintenance window](/kubernetes-engine/docs/how-to/maintenance-windows-and-exclusions#maintenance-window) .\n### Restrictions\nMaintenance windows have the following restrictions:\nYou can only configure a single maintenance window per cluster. Configuring a new maintenance window overwrites the previous one.\nWhen configuring and viewing maintenance windows, times are shown differently depending on the tool you are using:\nWhen configuring maintenance windows using the more generic `--maintenance-window` flag, you cannot specify a time zone. UTC is used when using the gcloud CLI or the API, and the Google Cloud console displays times using the local time zone.\nWhen using more granular flags, such as `--maintenance-window-start` , you can specify the time zone as part of the value. If you omit the time zone, your local time zone is used. Times are always stored in UTC.\nWhen viewing information about your cluster, timestamps for maintenance windows may be shown in UTC or in your local time zone, depending on how you are viewing the information:\n- When using the Google Cloud console to view information about your cluster, times are always displayed in your local time zone.\n- When using the gcloud CLI to view information about your cluster, times are always shown in UTC.\nIn both cases, the `RRULE` is always in UTC. That means that if specifying, for example, days of the week, then those days are in UTC.\n## Maintenance exclusions\nWith , you can prevent automatic maintenance from occurring during a specific time period. For example, many retail businesses have business guidelines prohibiting infrastructure changes during the end-of-year holidays. As another example, if a company is using an API that is scheduled for deprecation, they can use maintenance exclusions to pause minor upgrades to give them time to migrate applications.\nFor known high-impact events, we recommend that you match any internal change restrictions with a maintenance exclusion that starts one week before the event and lasts for the duration of the event.\nExclusions have no recurrence. Instead, create each instance of a periodic exclusion separately.\nWhen exclusions and [maintenance windows](#maintenance_windows) overlap, exclusions have precedence.\nTo learn how to set up maintenance exclusions for a new or existing cluster, see [Configure a maintenance exclusion](/kubernetes-engine/docs/how-to/maintenance-windows-and-exclusions#configuring_a_maintenance_exclusion) .\n### Scope of maintenance to exclude\nNot only can you specify to prevent automatic maintenance on your cluster, you can restrict the of automatic updates that might occur. Maintenance exclusion scopes are useful for the following types of scenarios, among others:\n- **No upgrades - avoid any maintenance:** You want to temporarily avoid any change to your cluster during a specific period of time. This is the default scope.\n- **No minor upgrades - maintain current Kubernetes minor version** : You want to temporarily maintain the minor version of a cluster to avoid [API changes](/kubernetes-engine/docs/deprecations/apis-1-22) or validate the next minor version.\n- **No minor or node upgrades - prevent node pool disruption** : You want to temporarily avoid any eviction and rescheduling of your workloads because of node upgrades.\nThe following table lists the scope of automatic updates that you can restrict in a maintenance exclusion. The table also indicates what type of upgrades that occur ( [minor and/or patch](/kubernetes-engine/versioning#versioning_scheme) ). When upgrades occur, the VM(s) for the control plane and/or node pool restarts. For control planes, VM restarts may temporarily decrease the Kubernetes API Server availability, especially in zonal cluster topology with a single control plane. For nodes, VM restarts trigger Pod rescheduling which can temporarily disrupt existing workloads. You can [set your tolerance for workload disruption](/kubernetes-engine/docs/best-practices/upgrading-clusters#set-tolerance) using a [Pod Disruption Budget (PDB)](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) .\n| ('Scope', 'Scope')  | ('Control plane', 'Minor upgrade') | ('Control plane', 'Patch upgrade') | ('Control plane', 'VM disruption due to GKE maintenance') | ('Node pools', 'Minor upgrade') | ('Node pools', 'Patch upgrade') | ('Node pools', 'VM disruption due to GKE maintenance') |\n|:--------------------------|:-------------------------------------|:-------------------------------------|:------------------------------------------------------------|:----------------------------------|:----------------------------------|:---------------------------------------------------------|\n| No upgrades (default)  | No         | No         | No               | No        | No        | No              |\n| No minor upgrades   | No         | Yes         | Yes               | No        | Yes        | Yes              |\n| No minor or node upgrades | No         | Yes         | Yes               | No        | No        | No              |\nFor definitions on minor and patch versions, see [Versioning scheme](/kubernetes-engine/versioning#versioning_scheme) .\n### Multiple exclusions\nYou may set multiple exclusions on a cluster. These exclusions may have different scopes and may have overlapping time ranges. The [end-of-year holiday season use case](#example-shopping) is an example of overlapping exclusions, where both the \"No upgrades\" and \"No minor upgrades\" scopes are in use.\nWhen exclusions overlap, if any active exclusion (that is, current time is within the exclusion time period) blocks an upgrade, the upgrade will be postponed.\nUsing the [end-of-year holiday season use case](#example-shopping) , a cluster has the following exclusions specified:\n- No minor upgrades: September 30 - January 15\n- No upgrades: November 19 - December 4\n- No upgrades: December 15 - January 5\nAs a result of these overlapping exclusions, the following upgrades will be blocked on the cluster:\n- Patch upgrade to the node pool on November 25 (rejected by \"No upgrades\" exclusion)\n- Minor upgrade to the control plane on December 20 (rejected by \"No minor upgrades\" and \"No upgrades\" exclusion)\n- Patch upgrade to the control plane on December 25 (rejected by \"No upgrades\" exclusion)\n- Minor upgrade to the node pool on January 1 (rejected by \"No minor upgrades\" and \"No upgrades\" exclusion)\nThe following maintenance would be permitted on the cluster:\n- Patch upgrade to the control plane on November 10 (permitted by \"No minor upgrades\" exclusion)\n- VM disruption due to GKE maintenance on December 10 (permitted by \"No minor upgrades\" exclusion)\n### Exclusion expiration\nWhen an exclusion expires (that is, the current time has moved beyond the end time specified for the exclusion), that exclusion will no longer prevent GKE updates. Other exclusions that are still valid (not expired) will continue to prevent GKE updates.\nWhen no exclusions remain that prevent cluster upgrades, your cluster will gradually upgrade to the current default version in the cluster's release channel (or the static default for clusters in no release channel).\nIf your cluster is multiple minor versions behind the current default version after exclusion expiry, GKE will schedule one minor upgrade per month (upgrading both cluster control plane and nodes) until your cluster has reached the default version for the Release Channel. If you would like to return your cluster to the default version sooner, you can execute [manual upgrades](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_cp) .\n### Limitations\nMaintenance exclusions have the following limitations:\n- You can only restrict [the scope](#scope_of_maintenance_to_exclude) of automatic upgrades in a maintenance exclusion for clusters that are enrolled in a [release channel](/kubernetes-engine/docs/concepts/release-channels) . For Standard clusters [not enrolled in a release channel](/kubernetes-engine/docs/concepts/release-channels#no_channel) , you can only create a maintenance exclusion with [the default \"No upgrades\" scope](#scope_of_maintenance_to_exclude) .\n- You can add a maximum of three maintenance exclusions that exclude all upgrades (that is, a scope of \"no upgrades\"). These exclusions must be configured to allow for at least 48 hours of maintenance availability in a 32-day rolling window.\n- You can have a maximum of 20 maintenance exclusions for each cluster.\n- If you do not specify a scope in your exclusion, the scope defaults to \"no upgrades\".\n- The length of a maintenance exclusion has restrictions based on the specified exclusion scope:- **No upgrades** : Cannot exceed 30 days.\n- **No minor upgrades** : Cannot end more than 180 days after the exclusion creation date.\n- **No minor or node upgrades** : Cannot end more than 180 days after the exclusion creation date.\n- You can't configure a maintenance exclusion to include or exceed the [end of life date](/kubernetes-engine/docs/release-schedule) of the minor version. For example, with a cluster running a [minorversion](/kubernetes-engine/versioning#versioning_scheme) where the GKE release schedule states that its end of life date is June 5, 2023, you must set the end time of the maintenance exclusion to`2023-06-05T00:00:00Z`or earlier.\n**Important:** GKE began automatically migrating clusters to GKE version 1.24 after version 1.23 [reached end of life on July31,2023](/kubernetes-engine/docs/release-schedule#schedule-for-release-channels) . To learn more about how the migration process works, and how you can use a maintenance exclusion to temporarily prevent your nodes from being migrated to containerd node images, see [Temporarily delay the automatic migration tocontainerd nodeimages](/kubernetes-engine/docs/deprecations/docker-containerd#temporarily-delay) .\n### Usage examples\nHere are some example use cases for restricting the scope of updates that can occur.\nIn this example, the retail business does not want disruptions during the highest-volume sales periods, which is the four days encompassing [Black Friday](https://wikipedia.org/wiki/Black_Friday_(shopping)) through [Cyber Monday](https://wikipedia.org/wiki/Cyber_Monday) , and the month of December until the start of the new year. In preparation for the shopping season, the cluster administrator sets up the following exclusions:\n- **No minor upgrades** : Allow only patch updates on the control plane and nodes between September 30 - January 15.\n- **No upgrades** : Freeze all upgrades between November 19 - December 4.\n- **No upgrades** : Freeze all upgrades between December 15 - January 5.\nIf no other exclusion windows apply when the maintenance exclusion expires, the cluster is upgraded to a new GKE minor version if one was made available between September 30 and January 6.\nIn this example, a company is using the `CustomResourceDefinition` `apiextensions.k8s.io/v1beta1` API, which [will be removed in version 1.22](/kubernetes-engine/docs/deprecations/apis-1-22#customresourcedefinition-v122) . While the company is running versions earlier than 1.22, the cluster administrator sets up the following exclusion:\n- **No minor upgrades** : Freeze minor upgrades for three months while migrating customer applications from`apiextensions.k8s.io/v1beta1`to`apiextensions.k8s.io/v1`.In this example, a company is running a database that does not respond well to Pod evictions and rescheduling that occurs during a node pool upgrade. The cluster administrator sets up the following exclusion:\n- **No minor or node upgrades** : Freeze node upgrades for three months. When the company is ready to accept downtime for the database, they trigger a manual node upgrade.## What's next\n- Learn more about [upgrading a cluster or its nodes](/kubernetes-engine/docs/how-to/upgrading-a-cluster) .\n- Learn more about [Node upgrade strategies](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) .\n- Learn how to [Receive cluster notifications](/kubernetes-engine/docs/how-to/cluster-notifications) .", "guide": "Google Kubernetes Engine (GKE)"}