{"title": "Google Kubernetes Engine (GKE) - Viewing cluster autoscaler events", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility", "abstract": "# Google Kubernetes Engine (GKE) - Viewing cluster autoscaler events\nThis page describes the decisions the Google Kubernetes Engine (GKE) [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) makes about autoscaling.\nThe GKE cluster autoscaler emits visibility events, which are available as log entries in Cloud Logging.\nThe events described in this guide are separate from the Kubernetes events produced by the cluster autoscaler.\n", "content": "## Availability requirements\nThe ability to view logged events for cluster autoscaler is available in the following cluster versions:\n| Event type        | Cluster version   |\n|:----------------------------------------|:------------------------|\n| status, scaleUp, scaleDown, eventResult | 1.15.4-gke.7 and later |\n| nodePoolCreated, nodePoolDeleted.  | 1.15.4-gke.18 and later |\n| noScaleUp        | 1.16.6-gke.3 and later |\n| noScaleDown        | 1.16.8-gke.2 and later |\nTo see autoscaler events, you must enable [Cloud Logging](/stackdriver/docs/solutions/gke/installing) in your cluster. The events won't be produced if Logging is disabled.\n## Viewing events\nThe visibility events for the cluster autoscaler are stored in a Cloud Logging log, in the same project as where your GKE cluster is located. You can also view these events from the notifications in the Google Kubernetes Engine page in Google Cloud console.\nTo view the logs, perform the following:\n- In the Google Cloud console, go to the **Kubernetes Clusters** page. [Go to Kubernetes Clusters](https://console.cloud.google.com/kubernetes/list/overview) \n- Select the name of your cluster to views its **Cluster Details** page.\n- On the **Cluster Details** page, click on the **Logs** tab.\n- On the **Logs** tab, click on the **Autoscaler Logs** tab to view the logs.\n- (Optional) To apply more advanced filters to narrow the results, click the button with the arrow on the right side of the page to view the logs in Logs Explorer.To view the visibility event notifications on the Google Kubernetes Engine page, perform the following:\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console: [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Check the **Notifications** column for specific clusters to find notifications related to scaling.\n- Click on the notification for detailed information, recommended actions, and to access the logs for this event.## Types of events\nAll logged events are in the JSON format and can be found in the **jsonPayload** field of a log entry. All timestamps in the events are UNIX second timestamps.\nHere's a summary of the types of events emitted by the cluster autoscaler:\n| Event type  | Description                                      |\n|:----------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| status   | Occurs periodically and describes the size of all autoscaled node pools and the target size of all autoscaled node pools as observed by the cluster autoscaler. |\n| scaleUp   | Occurs when cluster autoscaler scales the cluster up.                            |\n| scaleDown  | Occurs when cluster autoscaler scales the cluster down.                           |\n| eventResult  | Occurs when a scaleUp or a scaleDown event completes successfully or unsuccessfully.                    |\n| nodePoolCreated | Occurs when cluster autoscaler with node auto-provisioning enabled creates a new node pool.                  |\n| nodePoolDeleted | Occurs when cluster autoscaler with node auto-provisioning enabled deletes a node pool.                   |\n| noScaleUp  | Occurs when there are unschedulable Pods in the cluster, and cluster autoscaler cannot scale the cluster up to accommodate the Pods.        |\n| noScaleDown  | Occurs when there are nodes that are blocked from being deleted by cluster autoscaler.                   |\n### Status event\nA `status` event is emitted periodically, and describes the actual size of all autoscaled node pools and the target size of all autoscaled node pools as observed by cluster autoscaler.\nThe following log sample shows a `status` event:\n```\n{\u00a0 \"status\": {\u00a0 \u00a0 \"autoscaledNodesCount\": 4,\u00a0 \u00a0 \"autoscaledNodesTarget\": 4,\u00a0 \u00a0 \"measureTime\": \"1582898536\"\u00a0 }}\n```\n### ScaleUp event\nA `scaleUp` event is emitted when the cluster autoscaler scales the cluster up. The autoscaler increases the size of the cluster's node pools by scaling up the underlying [Managed Instance Groups (MIGs)](/compute/docs/instance-groups#managed_instance_groups) for the node pools. To learn more about how scale up works, see [How does scale up work?](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work) in the Kubernetes Cluster Autoscaler FAQ.\nThe event contains information on which MIGs were scaled up, by how many nodes, and which unschedulable Pods triggered the event.\n**Note:** The fact that the Pods listed in this event triggered scaling up does not mean that the Pods end up being scheduled on the new nodes.\nThe list of triggering Pods is truncated to 50 arbitrary entries. The actual number of triggering Pods can be found in the `triggeringPodsTotalCount` field.\nThe following log sample shows a `scaleUp` event:\n```\n{\u00a0 \"decision\": {\u00a0 \u00a0 \"decideTime\": \"1582124907\",\u00a0 \u00a0 \"eventId\": \"ed5cb16d-b06f-457c-a46d-f75dcca1f1ee\",\u00a0 \u00a0 \"scaleUp\": {\u00a0 \u00a0 \u00a0 \"increasedMigs\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-a0c72690-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"default-pool\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"requestedNodes\": 1\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"triggeringPods\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"controller\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"apiVersion\": \"apps/v1\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"kind\": \"ReplicaSet\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-85958b848b\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-85958b848b-ptc7n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"namespace\": \"default\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"triggeringPodsTotalCount\": 1\u00a0 \u00a0 }\u00a0 }}\n```\n### ScaleDown event\nA `scaleDown` event is emitted when cluster autoscaler scales the cluster down. To learn more about how scale down works, see [How does scale down work?](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-down-work) in the Kubernetes Cluster Autoscaler FAQ.\nThe `cpuRatio` and `memRatio` fields describe the CPU and memory utilization of the node, as a percentage. This utilization is a sum of Pod requests divided by node allocatable, not real utilization.\nThe list of evicted Pods is truncated to 50 arbitrary entries. The actual number of evicted Pods can be found in the `evictedPodsTotalCount` field.\nUse the following query to verify if the cluster autoscaler scaled down the nodes\n```\nresource.type=\"k8s_cluster\" \\resource.labels.location=COMPUTE_REGION \\resource.labels.cluster_name=CLUSTER_NAME \\log_id(\"container.googleapis.com/cluster-autoscaler-visibility\") \\( \"decision\" NOT \"noDecisionStatus\" )\n```\nReplace the following:\n- `` : the name of the cluster.\n- `` : the cluster's Compute Engine region, such as `us-central1` .The following log sample shows a `scaleDown` event:\n```\n{\u00a0 \"decision\": {\u00a0 \u00a0 \"decideTime\": \"1580594665\",\u00a0 \u00a0 \"eventId\": \"340dac18-8152-46ff-b79a-747f70854c81\",\u00a0 \u00a0 \"scaleDown\": {\u00a0 \u00a0 \u00a0 \"nodesToBeRemoved\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"evictedPods\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"controller\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"apiVersion\": \"apps/v1\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"kind\": \"ReplicaSet\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"kube-dns-5c44c7b6b6\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"kube-dns-5c44c7b6b6-xvpbk\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"evictedPodsTotalCount\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"node\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cpuRatio\": 23,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"memRatio\": 5,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-c47ef39f-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"default-pool\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-f\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-c47ef39f-p395\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 }}\n```\nYou can also view the `scale-down` event on the nodes with no workload running (typically only system pods created by DaemonSets). Use the following query to see the event logs:\n```\nresource.type=\"k8s_cluster\" \\resource.labels.project_id=PROJECT_ID \\resource.labels.location=COMPUTE_REGION \\resource.labels.cluster_name=CLUSTER_NAME \\severity>=DEFAULT \\logName=\"projects/PROJECT_ID/logs/events\" \\(\"Scale-down: removing empty node\")\n```\nReplace the following:\n- `` : your project ID.\n- `` : the name of the cluster.\n- `` : the cluster's Compute Engine region, such as `us-central1` .\n### EventResult event\nAn `eventResult` event is emitted when a scaleUp or a scaleDown event completes successfully or unsuccessfully. This event contains a list of event IDs (from the `eventId` field in scaleUp or scaleDown events), along with error messages. An empty error message indicates the event completed successfully. A list of eventResult events are aggregated in the `results` field.\nTo diagnose errors, consult the [ScaleUp errors](#scaleup-errors) and [ScaleDown errors](#scaledown-errors) sections.\nThe following log sample shows an `eventResult` event:\n```\n{\u00a0 \"resultInfo\": {\u00a0 \u00a0 \"measureTime\": \"1582878896\",\u00a0 \u00a0 \"results\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"eventId\": \"2fca91cd-7345-47fc-9770-838e05e28b17\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"errorMsg\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"scale.down.error.failed.to.delete.node.min.size.reached\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"test-cluster-default-pool-5c90f485-nk80\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"eventId\": \"ea2e964c-49b8-4cd7-8fa9-fefb0827f9a6\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ]\u00a0 }}\n```\n### NodePoolCreated event\nA `nodePoolCreated` event is emitted when cluster autoscaler with [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) enabled creates a new node pool. This event contains the name of the created node pool and a list of its underlying [MIGs](/compute/docs/instance-groups#managed_instance_groups) . If the node pool was created because of a scaleUp event, the `eventId` of the corresponding scaleUp event is included in the `triggeringScaleUpId` field.\nThe following log sample shows a `nodePoolCreated` event:\n```\n{\u00a0 \"decision\": {\u00a0 \u00a0 \"decideTime\": \"1585838544\",\u00a0 \u00a0 \"eventId\": \"822d272c-f4f3-44cf-9326-9cad79c58718\",\u00a0 \u00a0 \"nodePoolCreated\": {\u00a0 \u00a0 \u00a0 \"nodePools\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"migs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-nap-n1-standard--b4fcc348-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"nap-n1-standard-1-1kwag2qv\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-f\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-nap-n1-standard--jfla8215-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"nap-n1-standard-1-1kwag2qv\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"nap-n1-standard-1-1kwag2qv\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"triggeringScaleUpId\": \"d25e0e6e-25e3-4755-98eb-49b38e54a728\"\u00a0 \u00a0 }\u00a0 }}\n```\n### NodePoolDeleted event\nA `nodePoolDeleted` event is emitted when cluster autoscaler with [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) enabled deletes a node pool.\nThe following log sample shows a `nodePoolDeleted` event:\n```\n{\u00a0 \"decision\": {\u00a0 \u00a0 \"decideTime\": \"1585830461\",\u00a0 \u00a0 \"eventId\": \"68b0d1c7-b684-4542-bc19-f030922fb820\",\u00a0 \u00a0 \"nodePoolDeleted\": {\u00a0 \u00a0 \u00a0 \"nodePoolNames\": [\u00a0 \u00a0 \u00a0 \u00a0 \"nap-n1-highcpu-8-ydj4ewil\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 }}\n```\n### NoScaleUp event\nA `noScaleUp` event is periodically emitted when there are unschedulable Pods in the cluster and cluster autoscaler cannot scale the cluster up to accommodate the Pods.\n- noScaleUp events are best-effort, that is, these events do not cover all possible reasons for why cluster autoscaler cannot scale up.\n- noScaleUp events are throttled to limit the produced log volume. Each persisting reason is only emitted every couple of minutes.\n- All the reasons can be arbitrarily split across multiple events. For example, there is no guarantee that all rejected MIG reasons for a single Pod group will appear in the same event.\n- The list of unhandled Pod groups is truncated to 50 arbitrary entries. The actual number of unhandled Pod groups can be found in the`unhandledPodGroupsTotalCount`field.The following fields help to explain why scaling up did not occur:\n- `reason`: Provides a global reason for why cluster autoscaler is prevented from scaling up. Refer to the [NoScaleUp top-levelreasons](#noscaleup-top-level-reasons) section for details.\n- `napFailureReason`: Provides a global reason preventing cluster autoscaler from provisioning additional node pools (for example, node auto-provisioning is disabled). Refer to the [NoScaleUp top-level node auto-provisioning reasons](#noscaleup-top-level-nap-reasons) section for details.\n- `skippedMigs[].reason`: Provides information about why a particular MIG was skipped. Cluster autoscaler skips some MIGs from consideration for any Pod during a scaling up attempt (for example, because adding another node would exceed cluster-wide [resource limits](/kubernetes-engine/docs/concepts/node-auto-provisioning#resource_limits) ). Refer to the [NoScaleUp MIG-level reasons](#noscaleup-mig-level-reasons) section for details.\n- `unhandledPodGroups`: Contains information about why a particular group of unschedulable Pods does not trigger scaling up. The Pods are grouped by their immediate controller. Pods without a controller are in groups by themselves. Each Pod group contains an arbitrary example Pod and the number of Pods in the group, as well as the following reasons:- `napFailureReasons`: Reasons why cluster autoscaler cannot provision a new node pool to accommodate this Pod group (for example, Pods have affinity constraints). Refer to the [NoScaleUp Pod-level node auto-provisioning reasons](#noscaleup-pod-group-level-nap-reasons) section for details\n- `rejectedMigs[].reason`: Per-MIG reasons why cluster autoscaler cannot increase the size of a particular MIG to accommodate this Pod group (for example, the MIG's node is too small for the Pods). Refer to the [NoScaleUp MIG-level reasons](#noscaleup-mig-level-reasons) section for details.\nThe following log sample shows a `noScaleUp` event:\n```\n{\u00a0 \"noDecisionStatus\": {\u00a0 \u00a0 \"measureTime\": \"1582523362\",\u00a0 \u00a0 \"noScaleUp\": {\u00a0 \u00a0 \u00a0 \"skippedMigs\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-nap-n1-highmem-4-fbdca585-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"nap-n1-highmem-4-1cywzhvf\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-f\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"reason\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"no.scale.up.mig.skipped\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max cluster cpu limit reached\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"unhandledPodGroups\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"napFailureReasons\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"no.scale.up.nap.pod.zonal.resources.exceeded\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"us-central1-f\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"podGroup\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"samplePod\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"controller\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"apiVersion\": \"v1\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"kind\": \"ReplicationController\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"memory-reservation2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"memory-reservation2-6zg8m\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"namespace\": \"autoscaling-1661\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"totalPodCount\": 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"rejectedMigs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-b1808ff9-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"default-pool\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-f\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"reason\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"no.scale.up.mig.failing.predicate\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"NodeResourcesFit\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Insufficient memory\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"unhandledPodGroupsTotalCount\": 1\u00a0 \u00a0 }\u00a0 }}\n```\n### NoScaleDown event\nA `noScaleDown` event is periodically emitted when there are nodes which are blocked from being deleted by cluster autoscaler.\n- Nodes that cannot be removed because their utilization is high are not included in noScaleDown events.\n- NoScaleDown events are best effort, that is, these events do not cover all possible reasons for why cluster autoscaler cannot scale down.\n- NoScaleDown events are throttled to limit the produced log volume. Each persisting reason will only be emitted every couple of minutes.\n- The list of nodes is truncated to 50 arbitrary entries. The actual number of nodes can be found in the`nodesTotalCount`field.The following fields help to explain why scaling down did not occur:\n- `reason`: Provides a global reason for why cluster autoscaler is prevented from scaling down (for example, a backoff period after recently scaling up). Refer to the [NoScaleDown top-level reasons](#noscaledown-top-level-reasons) section for details.\n- `nodes[].reason`: Provides per-node reasons for why cluster autoscaler is prevented from deleting a particular node (for example, there's no place to move the node's Pods to). Refer to the [NoScaleDown node-level reasons](#noscaledown-node-level-reasons) section for details.The following log sample shows a `noScaleDown` event:\n```\n{\u00a0 \"noDecisionStatus\": {\u00a0 \u00a0 \"measureTime\": \"1582858723\",\u00a0 \u00a0 \"noScaleDown\": {\u00a0 \u00a0 \u00a0 \"nodes\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"node\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cpuRatio\": 42,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-f74c1617-grp\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nodepool\": \"default-pool\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone\": \"us-central1-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"test-cluster-default-pool-f74c1617-fbhk\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"reason\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"no.scale.down.node.no.place.to.move.pods\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"nodesTotalCount\": 1,\u00a0 \u00a0 \u00a0 \"reason\": {\u00a0 \u00a0 \u00a0 \u00a0 \"messageId\": \"no.scale.down.in.backoff\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```\n## Troubleshooting scaling issues\nThis section provides guidance for how to troubleshoot scaling events.\n**Note:** Each node in a node pool is a virtual machine (VM) instance in a Compute Engine [Managed Instance Group (MIG)](/compute/docs/instance-groups#managed_instance_groups/) . In the following troubleshooting scenarios, remember that MIGs are the underlying infrastructure for your nodes.\n### Cluster not scaling up\n**Scenario** : I created a Pod in my cluster but it's stuck in the state for the past hour. Cluster autoscaler did not provision any new nodes to accommodate the Pod.\n**Solution** :\n- In the Logs Explorer, find the logging details for cluster autoscaler events, as described in the [Viewing events](#viewing_events) section.\n- Search for [scaleUp](#scaleup-event) events that contain the desired Pod in the `triggeringPods` field. You can filter the log entries, including filtering by a particular JSON field value. Learn more in [Advanced logs queries](/logging/docs/view/advanced-queries) .- Find an [EventResult](#eventresult-event) that contains the same`eventId`as the`scaleUp`event.\n- Look at the`errorMsg`field and consult the list of possible [scaleUperror messages](#scaleup-errors) .\n : For a `scaleUp` event, you discover the error is `\"scale.up.error.quota.exceeded\"` , which indicates that . To resolve the issue, you review your quota settings and increase the settings that are close to being exceeded. Cluster autoscaler adds a new node and the Pod is scheduled.\n- Otherwise, search for [noScaleUp](#noscaleup-event) events and review the following fields:- `unhandledPodGroups`: contains information about the Pod (or Pod's controller).\n- `reason`: provides global reasons indicating scaling up could be blocked.\n- `skippedMigs`: provides reasons why some MIGs might be skipped.\n- Refer to the following sections that contain possible reasons for `noScaleUp` events:- [NoScaleUp top-level reasons](#noscaleup-top-level-reasons) \n- [NoScaleUp top-level node auto-provisioning reasons](#noscaleup-top-level-nap-reasons) \n- [NoScaleUp MIG-level reasons](#noscaleup-mig-level-reasons) \n- [NoScaleUp Pod-group-level node auto-provisioning reasons](#noscaleup-pod-group-level-nap-reasons) \n **Note:** These lists should contain enough information for you to be able to resolve the issue, but if you don't know how to proceed, consult the [Kubernetes ClusterAutoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) . : You found a `noScaleUp` event for your Pod, and all MIGs in the `rejectedMigs` field have the same reason message ID of `\"no.scale.up.mig.failing.predicate\"` with two parameters: `\"NodeAffinity\"` and `\"node(s) did not match node selector\"` . After consulting the list of error messages, you discover that you \" \"; the parameters are the name of the failing predicate and the reason why it failed. To resolve the issue, you review the Pod spec, and discover that it has a node selector that doesn't match any MIG in the cluster. You delete the selector from the Pod spec and recreate the Pod. Cluster autoscaler adds a new node and the Pod is scheduled.\n- If there are no `noScaleUp` events, use other debugging methods to resolve the issue. **Note:** `noScaleUp` events are best-effort, and do not cover all possible cases.\n### Cluster not scaling down\n**Scenario** : I have a node in my cluster that has utilized only 10% of its CPU and memory for the past couple of days. Despite the low utilization, cluster autoscaler did not delete the node as expected.\n**Solution** :\n- In the Logs Explorer, find the logging details for cluster autoscaler events, as described in the [Viewing events](#viewing_events) section.\n- Search for [scaleDown](#scaledown-event) events that contain the desired node in the`nodesToBeRemoved`field. You can filter the log entries, including filtering by a particular JSON field value. Learn more in [Advanced logs queries](/logging/docs/view/advanced-queries) .- In the`scaleDown`event, search for an [EventResult](#eventresult-event) event that contains the associated`eventId`.\n- Look at the`errorMsg`field and consult the list of possible [scaleDown error messages](#scaledown-errors) .\n- Otherwise, search for [noScaleDown](#noscaledown-event) events that have the desired node in the`nodes`field. Review the`reason`field for any global reasons indicating that scaling down could be blocked.\n- Refer to the following sections that contain possible reasons for `noScaleDown` events:- [NoScaleDown top-level reasons](#noscaledown-top-level-reasons) \n- [NoScaleDown node-level reasons](#noscaledown-node-level-reasons) \n **Note:** These lists should contain enough information for you to be able to resolve the issue, but if you don't know how to proceed, consult the [Kubernetes Cluster Autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) . : You found a `noScaleDown` event that contains a per-node reason for your node. The message ID is `\"no.scale.down.node.pod.has.local.storage\"` and there is a single parameter: `\"test-single-pod\"` . After consulting the list of error messages, you discover this means that the . You consult the [Kubernetes Cluster Autoscaler FAQ](https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md) and find out that the solution is to add a `\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"` annotation to the Pod. After applying the annotation, cluster autoscaler scales down the cluster correctly.\n- If there are no `noScaleDown` events, use other debugging methods to resolve the issue. **Note:** `noScaleDown` events are best-effort, and do not cover all possible cases.## Messages\nThe events emitted by the cluster autoscaler use parameterized messages to provide explanations for the event. The `parameters` field is available with the `messageId` field, such as in [this example log for a NoScaleUp event](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility#example_7) .\nThis section provides descriptions for various `messageId` and its corresponding parameters. However, this section does not contain all possible messages, and may be extended at any time.\n**Note:** Visibility events provide a limited view into the behavior of cluster autoscaler. The purpose of the logs is to provide transparency into why cluster autoscaler makes decisions. **The events are not necessarily an indicationof system failure.** If the outcome is unexpected, use the following steps to mitigate the root cause.\n### ScaleUp errors\nError messages for `scaleUp` events are found in the corresponding `eventResult` event, in the `resultInfo.results[].errorMsg` field.\n| Message          | Description                                     | Mitigation                                   |\n|:-----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"scale.up.error.out.of.resources\"    | The scaleUp event failed because some of the MIGs could not be increased due to lack of resources. Parameters: Failing MIG IDs.        | Follow the resource availability troubleshooting steps.                       |\n| \"scale.up.error.quota.exceeded\"    | The scaleUp event failed because some of the MIGs could not be increased, due to exceeded Compute Engine quota. Parameters: Failing MIG IDs.    | Check the Errors tab of the MIG in Google Cloud console to see what quota is being exceeded. Follow the instructions to request a quota increase. |\n| \"scale.up.error.waiting.for.instances.timeout\" | The scaleUp event failed because instances in some of the MIGs failed to appear in time. Parameters: Failing MIG IDs.          | This message is transient. If it persists, engage Google Cloud Support for further investigation.             |\n| \"scale.up.error.ip.space.exhausted\"   | The scaleUp event failed because the cluster doesn't have enough unallocated IP address space to use to add new nodes or Pods. Parameters: Failing MIG IDs. | Refer to the troubleshooting steps to address the lack of IP address space for the nodes or pods.             |\n| \"scale.up.error.service.account.deleted\"  | The scaleUp event failed because a service account used by Cluster Autoscaler has been deleted. Parameters: Failing MIG IDs.        | Engage Google Cloud Support for further investigation.                       |\n### ScaleDown errors\nError messages for `scaleDown` events are found in the corresponding `eventResult` event, in the `resultInfo.results[].errorMsg` field.\n| Message             | Description                                  | Mitigation                                |\n|:----------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------|\n| \"scale.down.error.failed.to.mark.to.be.deleted\"   | The scaleDown event failed because a node could not be marked for deletion. Parameters: Failing node name.          | This message is transient. If it persists, engage Google Cloud Support for further investigation.          |\n| \"scale.down.error.failed.to.evict.pods\"     | The scaleDown event failed because some of the Pods could not be evicted from a node. Parameters: Failing node name.       | Review best practices for Pod Disruption Budgets to ensure that the rules allow for eviction of application replicas when acceptable. |\n| \"scale.down.error.failed.to.delete.node.min.size.reached\" | The scaleDown event failed because a node could not be deleted due to the cluster already being at minimal size. Parameters: Failing node name. | Review the minimum value set for node pool autoscaling and adjust the settings as necessary.           |\n### Reasons for a NoScaleUp event\nTop-level reason messages for `noScaleUp` events appear in the `noDecisionStatus.noScaleUp.reason` field. The message contains a top-level reason for why cluster autoscaler cannot scale the cluster up.\n| Message     | Description                                           | Mitigation                  | Unnamed: 3 |\n|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|-------------:|\n| \"no.scale.up.in.backoff\" | A noScaleUp occurred because scaling-up is in a backoff period (temporarily blocked). This is a transient message that may occur during scale up events with a large number of Pods. | If this message persists, engage Google Cloud Support for further investigation. |   nan |\nTop-level node auto-provisioning reason messages for `noScaleUp` events appear in the `noDecisionStatus.noScaleUp.napFailureReason` field. The message contains a top-level reason for why cluster autoscaler cannot provision new node pools.\n| Message     | Description                                                         | Mitigation                 |\n|:---------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|\n| \"no.scale.up.nap.disabled\" | Node auto-provisioning is not enabled at the cluster level. If node auto-provisioning is disabled, new nodes will not be automatically provisioned if the pending Pod has requirements that can't be satisfied by any existing node pools. | Review the cluster configuration and see Enabling Node auto-provisioning. |\nMIG-level reason messages for `noScaleUp` events appear in the `noDecisionStatus.noScaleUp.skippedMigs[].reason` and `noDecisionStatus.noScaleUp.unhandledPodGroups[].rejectedMigs[].reason` fields. The message contains a reason why cluster autoscaler cannot increase the size of a particular MIG.\n| Message        | Description                                          | Mitigation                       |\n|:------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------|\n| \"no.scale.up.mig.skipped\"   | Cannot scale up a MIG because it was skipped during the simulation. Parameters: human-readable reasons why it was skipped (for example, missing a pod requirement).    | Review the parameters included in the error message and address why the MIG was skipped.   |\n| \"no.scale.up.mig.failing.predicate\" | Cannot scale up a MIG because it does not meet the predicate requirements for the pending Pods. Parameters: Name of the failing predicate, human-readable reasons why it failed. | Review Pod requirements, such as affinity rules, taints or tolerations, and resource requirements. |\nPod-group-level node auto-provisioning reason messages for `noScaleUp` events appear in the `noDecisionStatus.noScaleUp.unhandledPodGroups[].napFailureReasons[]` field. The message contains a reason why cluster autoscaler cannot provision a new node pool to accommodate a particular Pod group.\n| Message          | Description                                                                          | Mitigation                                |\n|:-----------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|\n| \"no.scale.up.nap.pod.gpu.no.limit.defined\"  | Node auto-provisioning could not provision any node group because a pending Pod has a GPU request, but GPU resource limits are not defined at the cluster level. Parameters: Requested GPU type.                            | Review the pending Pod's GPU request, and update the cluster-level node auto-provisioning configuration for GPU limits.    |\n| \"no.scale.up.nap.pod.gpu.type.not.supported\" | Node auto-provisioning did not provision any node group for the Pod because it has requests for an unknown GPU type. Parameters: Requested GPU type.                                       | Check the pending Pod's configuration for the GPU type to ensure that it matches a supported GPU type.        |\n| \"no.scale.up.nap.pod.zonal.resources.exceeded\" | Node auto-provisioning did not provision any node group for the Pod in this zone because doing so would either violate the cluster-wide maximum resource limits, exceed the available resources in the zone, or there is no machine type that could fit the request. Parameters: Name of the considered zone. | Review and update cluster-wide maximum resource limits, the Pod resource requests, or the available zones for node auto-provisioning. |\n| \"no.scale.up.nap.pod.zonal.failing.predicates\" | Node auto-provisioning did not provision any node group for the Pod in this zone because of failing predicates. Parameters: Name of the considered zone, human-readable reasons why predicates failed.                          | Review the pending Pod's requirements, such as affinity rules, taints, tolerations, or resource requirements.       |\n### Reasons for a NoScaleDown event\nTop-level reason messages for `noScaleDown` events appear in the `noDecisionStatus.noScaleDown.reason` field. The message contains a top-level reason why cluster autoscaler cannot scale the cluster down.\n| Message      | Description                                            | Mitigation                                                                            |\n|:----------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"no.scale.down.in.backoff\" | A noScaleDown event occurred because scaling-down is in a backoff period (temporarily blocked). This event should be transient, and may occur when there has been a recent scale up event. | Follow the mitigation steps associated with the lower-level reasons for failure to scale down. When the underlying reasons are resolved, cluster autoscaler will exit backoff. If the message persists after addressing the underlying reasons, engage Google Cloud Support for further investigation.     |\n| \"no.scale.down.in.progress\" | A noScaleDown event occurred because scaling down is blocked until the previous node scheduled for removal is deleted.                  | This event should be transient, as the Pod will eventually be forcibly removed. If this message occurs frequently, you can review the gracefulTerminationPeriod value for the Pod(s) blocking scale down. If you would like to speed up the resolution, you can also forcibly delete the Pod if it is no longer needed. |\nNode-level reason messages for `noScaleDown` events appear in the `noDecisionStatus.noScaleDown.nodes[].reason` field. The message contains a reason why cluster autoscaler cannot remove a particular node.\n| Message            | Description                                                                                           | Mitigation                                                                              |\n|:------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"no.scale.down.node.scale.down.disabled.annotation\" | Node cannot be removed because it has a scale-down-disabled annotation.                                                                            | Review the annotation that is preventing scale down following the instructions in the Kubernetes Cluster Autoscaler FAQ.                                                   |\n| \"no.scale.down.node.node.group.min.size.reached\"  | Node cannot be removed because its node group is already at its minimum size.                                                                          | Review and adjust the minimum value set for node pool autoscaling.                                                                |\n| \"no.scale.down.node.minimal.resource.limits.exceeded\" | Scale down of an underutilized node is blocked because it would violate cluster-wide minimum resource limits set for node auto-provisioning.                                                           | Review the cluster-wide minimum resource limits.                                                                     |\n| \"no.scale.down.node.no.place.to.move.pods\"   | Scale down of an underutilized node is blocked because it is running a Pod which can't be moved to another node in the cluster.                                                              | If you expect that the Pod should be rescheduled, review the scheduling requirements of the Pods on the underutilized node to determine if they can be moved to another node in the cluster. This message is expected if you do not expect the Pod to be rescheduled as there are no other nodes on which it could be scheduled. |\n| \"no.scale.down.node.pod.not.backed.by.controller\"  | Pod is blocking scale down of an underutilized node because the Pod doesn't have a controller known to Kubernetes Cluster Autoscaler (ReplicationController, DaemonSet, Job, StatefulSet, or ReplicaSet). Learn more from the Kubernetes Cluster Autoscaler FAQ about what types of pods can prevent cluster autoscaler from removing a node. Parameters: Name of the blocking pod. | Set an annotation \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\" for the Pod or define a controller (ReplicationController, DaemonSet, Job, StatefulSet, or ReplicaSet).                                    |\n| \"no.scale.down.node.pod.has.local.storage\"   | Pod is blocking scale down because it requests local storage. Learn more from the Kubernetes Cluster Autoscaler FAQ about what types of Pods can prevent cluster autoscaler from removing a node. Parameters: Name of the blocking pod.                                   | Set an annotation \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\" for the Pod if the data in the local storage for the Pod is not critical.                                            |\n| \"no.scale.down.node.pod.not.safe.to.evict.annotation\" | Pod is blocking scale down because it has a \"not safe to evict\" annotation. See the Kubernetes Cluster Autoscaler FAQ for more details. Parameters: Name of the blocking pod.                                                  | If the Pod can be safely evicted, update the annotation to \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\".                                                    |\n| \"no.scale.down.node.pod.kube.system.unmovable\"  | Pod is blocking scale down because it's a non-DaemonSet, non-mirrored, Pod without a PodDisruptionBudget in the kube-system namespace. Parameters: Name of the blocking pod.                                                  | Follow the instructions in the Kubernetes Cluster Autoscaler FAQ to set a PodDisruptionBudget to enable cluster autoscaler to move Pods in the kube-system namespace.                                       |\n| \"no.scale.down.node.pod.not.enough.pdb\"    | Pod is blocking scale down because it doesn't have enough PodDisruptionBudget left. See the Kubernetes Cluster Autoscaler FAQ for more details. Parameters: Name of the blocking pod.                                                | Review the PodDisruptionBudget for the Pod, see best practices for PodDisruptionBudget. You may be able to resolve the message by scaling the application or changing the PodDisruptionBudget to allow for more unavailable Pods.                        |\n| \"no.scale.down.node.pod.controller.not.found\"   | Pod is blocking scale down because its controller (e.g. Deployment or ReplicaSet) can't be found.                                                                     | Review the logs to determine what actions were taken that left a Pod running after its controller was removed. To resolve, you can manually delete the Pod.                                          |\n| \"no.scale.down.node.pod.unexpected.error\"    | Scale down of an underutilized node is blocked because it has a Pod in an unexpected error state.                                                                     | Engage GCP Support for further investigation.                                                                     |\n## What's next\n- [Learn more about cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- [Learn about how to using node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) \n- [Learn about troubleshooting and resolving scaling issues](https://www.youtube.com/watch?v=IdCjKUZah-Q) .", "guide": "Google Kubernetes Engine (GKE)"}