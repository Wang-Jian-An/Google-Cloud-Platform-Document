{"title": "Google Kubernetes Engine (GKE) - Configuring multi-cluster Services", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-services", "abstract": "# Google Kubernetes Engine (GKE) - Configuring multi-cluster Services\nThis page shows you how to enable and use multi-cluster Services (MCS). To learn more about how MCS works and its benefits, see [Multi-cluster Services](/kubernetes-engine/docs/concepts/multi-cluster-services) .\nThe Google Kubernetes Engine (GKE) MCS feature extends the reach of the Kubernetes [Service](/kubernetes-engine/docs/concepts/service) beyond the cluster boundary and lets you discover and invoke Services across multiple GKE clusters. You can export a subset of existing Services or new Services.\nWhen you export a Service with MCS, that Service is then available across all of the clusters in your [fleet](/anthos/fleet-management/docs) .\n**Note:** For MCS to function correctly, GKE deploys Pods to your nodes that have elevated [RBAC permissions](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#determine-the-request-verb) , such as the ability to patch all deployments, services, and endpoints. These permissions are required for GKE to propagate endpoints between multiple GKE clusters.\n", "content": "## Google Cloud resources managed by MCS\nMCS manages the following components of Google Cloud:\n- **Cloud DNS** : MCS configures [Cloud DNS](/dns/docs/overview) zones and records for each exported Service in your [fleet clusters](/anthos/fleet-management/docs/fleet-creation) . This lets you connect to Services that are running in other clusters. These zones and records are created, read, updated, and deleted based on the Services that you choose to export across clusters. **Note:** Using Cloud DNS incurs additional charges. You are billed according to [Cloud DNS pricing](/dns/pricing) .\n- **Firewall rules** : MCS configures firewall rules that let Pods communicate with each other across clusters within your fleet. Firewall rules are created, read, updated, and deleted based on the clusters that you add to your fleet. These rules are similar to the rules that GKE creates to enable communication between Pods within a GKE cluster.\n- **Traffic Director** : MCS uses [Traffic Director](/traffic-director/docs/traffic-director-concepts) as a control plane to keep track of endpoints and their health across clusters.## Requirements\nMCS has the following requirements:\n- MCS only supports exporting services from VPC-native GKE clusters on Google Cloud. For more information, see [Creating a VPC-native cluster](/kubernetes-engine/docs/how-to/alias-ips) . You cannot use non VPC-native GKE Standard clusters.\n- Connectivity between clusters depends on clusters running within the same VPC network, in peered or shared VPC networks. Otherwise the calls to external services will not be able to cross the network boundary.\n- While a fleet might span multiple Google Cloud projects and VPC networks, a single multi-cluster service must be exported from a single project and a single VPC network.\n- MCS is not supported with [network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) .\n- Clusters must have the `HttpLoadBalancing` add-on enabled. Ensure that the `HttpLoadBalancing` add-on is enabled. The `HttpLoadBalancing` add-on is enabled by default and shouldn't be disabled.## Pricing\nMulti-cluster Services is included as part of the [GKE cluster management fee](/kubernetes-engine/pricing#google-kubernetes-engine-pricing) and has no extra cost for usage. You must enable the Traffic Director API, but MCS does not incur any Traffic Director endpoint charges. GKE Enterprise licensing is not required to use MCS.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Install the [Google Cloud SDK](/sdk/downloads) .\n- Enable the Google Kubernetes Engine API: [Enable Google Kubernetes Engine API](https://console.cloud.google.com/apis/library/container.googleapis.com?q=kubernetes%20engine) \n- Enable the MCS, fleet (hub), Resource Manager, Traffic Director, and Cloud DNS APIs:```\ngcloud services enable \\\u00a0 \u00a0 multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 gkehub.googleapis.com \\\u00a0 \u00a0 cloudresourcemanager.googleapis.com \\\u00a0 \u00a0 trafficdirector.googleapis.com \\\u00a0 \u00a0 dns.googleapis.com \\\u00a0 \u00a0 --project=PROJECT_ID\n```Replace `` with the project ID from the project where you plan to register your clusters to a fleet. **Note:** Non-project owners must be granted the `serviceusage.services.enable` permission before they can enable APIs.## Enabling MCS in your project\nMCS requires that participating GKE clusters be registered into the same fleet. Once the MCS feature is enabled for a fleet, any clusters can export Services between clusters in the fleet.\nWhile MCS does require registration to a fleet, it does not require you to enable the GKE Enterprise platform.\n### GKE Enterprise\nIf the GKE Enterprise API is enabled in your fleet host project as a prerequisite for using other GKE Enterprise components, then any clusters registered to the project's fleet are charged according to [GKE Enterprise pricing](/anthos/pricing) . This pricing model lets you use all GKE Enterprise features on registered clusters for a single per-vCPU charge. You can confirm if the GKE Enterprise API is enabled using the following command:\n```\ngcloud services list --project=PROJECT_ID | grep anthos.googleapis.com\n```\nIf the output is similar to the following, the full GKE Enterprise platform is enabled and any clusters registered to the fleet will incur GKE Enterprise charges:\n```\nanthos.googleapis.com      Anthos API\n```\nIf this is not expected then contact your project administrator.\nAn empty output indicates that GKE Enterprise is not enabled.\n**Warning:** Do not disable the GKE Enterprise API if there are other active GKE Enterprise components in use in your project or those components might experience an outage.\n## Enabling MCS on your GKE cluster\n- Enable the MCS feature for your project's fleet:```\ngcloud container fleet multi-cluster-services enable \\\u00a0 \u00a0 --project PROJECT_ID\n```Replace `` with the project ID from the project where you plan to register your clusters to a fleet. This is your .\n- [Register your GKE clusters to the fleet](/anthos/fleet-management/docs/register/gke) . We strongly recommend that you register your cluster with [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) enabled. If you do not enable workload identity federation for GKE, you need to register the cluster with a Google Cloud service account for authentication, and complete the additional steps in [Authenticating service accounts](#authenticating) .To register your cluster with workload identity federation for GKE, run the following command:```\ngcloud container fleet memberships register MEMBERSHIP_NAME \\\u00a0 \u00a0--gke-cluster CLUSTER_LOCATION/CLUSTER_NAME \\\u00a0 \u00a0--enable-workload-identity \\\u00a0 \u00a0--project PROJECT_ID\n```Replace the following:- ``: the [membership name](/anthos/fleet-management/docs/fleet-creation#about_fleet_membership) that you choose to uniquely represent the cluster in the fleet. Typically, a cluster's fleet membership name is the cluster's name, but you might need to specify a new name if another cluster with the original name already exists in the fleet.\n- ``: the zone or region where the cluster is located.\n- ``: the name of the cluster.\n- Grant the required Identity and Access Management (IAM) permissions for MCS Importer:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[gke-mcs/gke-mcs-importer]\" \\\u00a0 \u00a0 --role \"roles/compute.networkViewer\"\n```Replace `` with the project ID from the fleet host project.\n- Ensure that each cluster in the fleet has a namespace to share Services in. If needed, create a namespace by using the following command:```\nkubectl create ns NAMESPACE\n```Replace `` with a name for the namespace. **Note:** Since Services are not imported to clusters where their exporting namespace is not present, create a Service's namespace in any clusters from which a Service will be consumed. If a namespace is missing, you can add the namespace at any time, but might take up to five minutes for a previously exported Service to be imported to a newly created namespace.\n- To verify that MCS is enabled, run the following command:```\ngcloud container fleet multi-cluster-services describe \\\u00a0 \u00a0 --project PROJECT_ID\n```The output is similar to the following:```\ncreateTime: '2021-08-10T13:05:23.512044937Z'\nmembershipStates:\n projects/PROJECT_ID/locations/global/memberships/MCS_NAME:\n state:\n  code: OK\n  description: Firewall successfully updated\n  updateTime: '2021-08-10T13:14:45.173444870Z'\nname: projects/PROJECT_NAME/locations/global/features/multiclusterservicediscovery\nresourceState:\n state: ACTIVE\nspec: {}\n```If the value of `state` is not `ACTIVE` , see the [troubleshooting](#troubleshooting) section.\n### Authenticating service accounts\n**Important:** You do not need to do any steps in this section if you registered a cluster to a fleet with workload identity federation for GKE enabled.\nIf you registered your GKE clusters to a fleet using a service account, you need to take additional steps to authenticate the service account. MCS deploys a component called `gke-mcs-importer` . This component receives endpoint updates from Traffic Director, so as part of enabling MCS you need to grant your [service account](/iam/docs/service-accounts) permission to read information from Traffic Director.\nWhen you use a service account, you can use the Compute Engine default service account or your own node service account:\n- If you are using a [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) , enable the following scopes:- `https://www.googleapis.com/auth/compute.readonly`\n- `https://www.googleapis.com/auth/cloud-platform`\nTo learn more about enabling scopes, see [Changing the service account and access scopes for an instance](/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes) .\n- If you are using your [own node service account](/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa) , assign the `roles/compute.networkViewer` role to your service account.## Using MCS\nThe following sections show you how to use MCS. MCS uses the [Kubernetes multi-cluster services API](https://github.com/kubernetes/enhancements/tree/master/keps/sig-multicluster/1645-multi-cluster-services-api) .\n### Registering a Service for export\nTo register a Service for export to other clusters within your fleet, complete the following steps:\n- Create a `ServiceExport` object named `export.yaml` :```\n# export.yamlkind: ServiceExportapiVersion: net.gke.io/v1metadata:\u00a0namespace: NAMESPACE\u00a0name: SERVICE_EXPORT_NAME\n```Replace the following:- ``: the namespace of the`ServiceExport`object. This namespace must match the namespace of the Service that you are exporting.\n- ``: the name of a Service in your cluster that you want to export to other clusters within your fleet.\n- Create the `ServiceExport` resource by running the following command:```\nkubectl apply -f export.yaml\n```\nThe initial export of your Service takes approximately five minutes to sync to clusters registered in your fleet. After a Service is exported, subsequent endpoint syncs happen immediately.\nYou can export the same Service from multiple clusters to create a single highly available multi-cluster service endpoint with traffic distribution across clusters. Before you export Services that have the same name and namespace, ensure that you want them to be grouped in this manner. We recommend against exporting services in the `default` and `kube-system` namespaces because of the high probability of unintended name conflicts and the resulting unintended grouping. If you are exporting more than five services with the same name and namespace, traffic distribution on imported services might be limited to five exported services.\n### Consuming cross-cluster Services\nMCS only supports ClusterSetIP and headless Services. Only DNS \"A\" records are available.\nAfter you create a `ServiceExport` object, the following domain name resolves to your exported Service from any Pod in any fleet cluster:\n```\n SERVICE_EXPORT_NAME.NAMESPACE.svc.clusterset.local\n```\nThe output includes the following values:\n- ``and``: the values you define in your`ServiceExport`object.\nFor ClusterSetIP Services, the domain resolves to the `ClusterSetIP` . You can find this value by locating the `ServiceImport` object in a cluster in the namespace that the `ServiceExport` object was created in. The `ServiceImport` object is automatically created.\nFor example:\n```\nkind: ServiceImport\napiVersion: net.gke.io/v1\nmetadata:\n namespace: EXPORTED-SERVICE-NAMESPACE\n name: external-svc-SERVICE-EXPORT-TARGET\nstatus:\n ports:\n - name: https\n port: 443\n protocol: TCP\n targetPort: 443\n ips: CLUSTER_SET_IP\n```\nMCS creates an `Endpoints` object as part of importing a `Service` into a cluster. By investigating this object you can monitor the progress of a Service import. To find the name of the `Endpoints` object, look up the value of the annotation `net.gke.io/derived-service` on a `ServiceImport` object corresponding to your imported Service. For example:\n```\nkind: ServiceImport\napiVersion: net.gke.io/v1\nannotations: net.gke.io/derived-service: DERIVED_SERVICE_NAME\nmetadata:\n namespace: EXPORTED-SERVICE-NAMESPACE\n name: external-svc-SERVICE-EXPORT-TARGET\n```\nNext, look up the `Endpoints` object to check if MCS has already propagated the endpoints to the importing cluster. The `Endpoints` object is created in the same namespace as the `ServiceImport` object, under the name stored in the `net.gke.io/derived-service` annotation. For example:\n```\nkubectl get endpoints DERIVED_SERVICE_NAME -n NAMESPACE\n```\nReplace the following:\n- ` **DERIVED_SERVICE_NAME** `: the value of the annotation`net.gke.io/derived-service`on the`ServiceImport`object.\n- ``: the namespace of the`ServiceExport`object.\nYou can find out more on the healthiness status of the endpoints using the Traffic Director dashboard in Google Cloud console.\nFor headless Services, the domain resolves to the list of IP addresses of the endpoints in the exporting clusters. Each backend Pod with a hostname is also independently addressable with a domain name of the following form:\n```\n HOSTNAME.MEMBERSHIP_NAME.LOCATION.SERVICE_EXPORT_NAME.NAMESPACE.svc.clusterset.local\n```\nThe output includes the following values:\n- ``and``: the values you define in your`ServiceExport`object.\n- ``: the [unique identifier in the fleet](/anthos/fleet-management/docs/fleet-creation#about_fleet_membership) for the cluster that the Pod is in.\n- ``: the location of the membership. Memberships are either`global`, or their location is one of the regions or zones that the Pod is in, such as`us-central1`.\n- ``: the hostname of the Pod.\nYou can also address a backend Pod with a hostname exported from a cluster registered with a global Membership, using a domain name of the following format:\n```\nHOSTNAME.MEMBERSHIP_NAME.SERVICE_EXPORT_NAME.NAMESPACE.svc.clusterset.local\n```\n## Disabling MCS\nTo disable MCS, complete the following steps:\n- For each cluster in your fleet, delete each ServiceExport object that you created:```\nkubectl delete serviceexport SERVICE_EXPORT_NAME \\\u00a0 \u00a0 -n NAMESPACE\n```Replace the following:- ``: the name of your ServiceExport object.\n- ``: the namespace of the`ServiceExport`object.\n- [Unregister your clusters](/anthos/fleet-management/docs/unregister) from the fleet if they don't need to be registered for another purpose.\n- Disable the `multiclusterservicediscovery` feature:```\ngcloud container fleet multi-cluster-services disable \\\u00a0 \u00a0 --project PROJECT_ID\n```Replace `` with the project ID from the project where you registered clusters.\n- Disable the API for MCS:```\ngcloud services disable multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 --project PROJECT_ID\n```Replace `` with the project ID from the project where you registered clusters.## Limitations\nThe following limits are not enforced, and in some cases you can exceed these limits depending on the load in your clusters or project and the rate of endpoint churn. However, you might experience performance issues when these limitations are exceeded.\n- **Exporting clusters** : A single Service, identified by a namespaced name, can be safely exported from up to **5 clusters** simultaneously. Beyond that limit, it's possible that only a subset of endpoints can be imported to consuming clusters. You can export different Services from different subsets of clusters.\n- **The number of Pods behind a single Service** : It's safe if you keep below **250 Pods** behind a single Service. This is the same [limitation that single cluster Services](/kubernetes-engine/docs/concepts/planning-large-clusters#limits-best-practices-large-scale-clusters) have. With relatively static workloads and a small number of multi-cluster Services, it might be possible to significantly exceed this number into thousands of endpoints per Service. As with single cluster Services, all endpoints are watched by `kube-proxy` on every node. When going beyond this limit, especially when exporting from multiple clusters simultaneously, larger nodes might be required.\n- **The number of multi-cluster Services simultaneously exported** : We recommend that you simultaneously export no more than **50 uniqueService ports** , identified by a Service's namespaced name and declared ports. For example, exporting a Service that exposes ports 80 and 443 would count against 2 of the 50 unique Service port limit. Services with the same namespaced name exported from multiple clusters count as a single unique Service. The previously mentioned 2 port service would still only count against 2 ports if it were exported from 5 clusters simultaneously. Each multi-cluster Service counts toward your [Backend Services quota](https://console.cloud.google.com/iam-admin/quotas) , and each exporting cluster or zone creates a [network endpoint group (NEG)](/load-balancing/docs/negs) .\n- **Service types** : MCS only supports ClusterSetIP and Headless Services. NodePort and LoadBalancer Services are not supported and might lead to an unexpected behaviour.\n- **Using IPmasq Agent with MCS** : MCS operates as expected when you use a default or other non masqueraded Pod IP range.If you use a custom Pod IP range or a custom IPmasq agent ConfigMap, MCS traffic can be masqueraded. This prevents MCS from working because the firewall rules only allow traffic from Pod IPs.To avoid this issue, you should either use the default Pod IP range or specify all Pod IP ranges in the `nonMasqueradeCIDRs` field of the [IPmasqagent ConfigMap](/kubernetes-engine/docs/how-to/kubernetes-engine/docs/how-to/ip-masquerade-agent#checking_the_ip-masq-agent_configmap) . If you use Autopilot or you must use a non-default Pod IP range and cannot specify all Pod IP ranges in the ConfigMap, you should [use Egress NAT Policy to configure IP masquerade](/kubernetes-engine/docs/how-to/egress-nat-policy-ip-masq-autopilot) .\n### MCS with clusters in multiple projects\nYou cannot export a service if that service is already being exported by other clusters in a different project in the fleet with the same name and namespace. You can access the service in other clusters in the fleet in other projects, but those clusters cannot export the same service in the same namespace.\n## Troubleshooting\nThe following sections provide you with troubleshooting tips for MCS.\n### Viewing the featureState\nViewing the feature state can help you confirm if MCS was configured successfully. You can view the MCS feature state by using the following command:\n```\ngcloud container fleet multi-cluster-services describe\n```\nThe output is similar to the following:\n```\ncreateTime: '2021-08-10T13:05:23.512044937Z'\nmembershipStates:\n projects/PROJECT_ID/locations/global/memberships/MCS_NAME:\n state:\n  code: OK\n  description: Firewall successfully updated\n  updateTime: '2021-08-10T13:14:45.173444870Z'\nname: projects/PROJECT_NAME/locations/global/features/multiclusterservicediscovery\nresourceState:\n state: ACTIVE\nspec: {}\n```\nThe most helpful fields for troubleshooting are `code` and `description` .\nA code indicates the member's general state in relation to MCS. You can find these fields in the `state.code` field. There are three possible codes:\n- **OK** : The membership was successfully added to MCS and is ready to use.\n- **WARNING** : MCS is in the process of reconciling membership setup. The description field can provide more information about what caused this code.\n- **FAILED** : This membership was not added to MCS. Other memberships in the fleet with an `OK` code are not affected by this `FAILED` membership. The description field can provide more information about what caused this code.\n- **ERROR** : This membership is missing resources. Other memberships in the fleet with an `OK` code are not affected by this `ERROR` membership. The description field can provide more information about what caused this code.A description gives you further information about the membership's state in MCS. You can find these descriptions in the `state.description` field and you can see the following descriptions:\n- **Firewall successfully created** : This message indicates that the member's firewall rule was successfully created and or updated. The membership's code is `OK` .\n- **Firewall creation pending** : This message indicates that the member's firewall rule is pending creation or update. The membership's code is `WARNING` . This membership can experience issues updating and connecting to new multi-cluster Services and memberships added while the firewall rule is pending.\n- **GKE Cluster missing** : This message indicates that the registered GKE cluster is unavailable and or deleted. The membership's code is `ERROR` . This membership needs to be manually [unregistered from the fleet](/anthos/fleet-management/docs/unregister) after a GKE cluster is deleted.\n- **Project that member lives in is missing required permissionsand/or has not enabled all required APIs - additional setup steps arerequired** : This message indicates there are internal StatusForbidden (403) errors, and the membership's code is `FAILED` . This error occurs in the following scenarios:- **You have not enabled the necessary APIs in the member's project.** If the member cluster lives in a separate project than the fleet, see [cross-project setup](/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks) to ensure you have completed all necessary steps. If you have completed all steps, ensure that the following APIs are enabled in the registration project with the following commands:```\ngcloud services enable multiclusterservicediscovery.googleapis.com --project PROJECT_IDgcloud services enable dns.googleapis.com --project PROJECT_IDgcloud services enable trafficdirector.googleapis.com --project PROJECT_IDgcloud services enable cloudresourcemanager.googleapis.com --project PROJECT_ID\n```Replace `` with the project ID from the project where you registered clusters.\n- **The mcsd or gkehub service account requires more permissions in themember's project.** The `mcsd` and `gkehub` service accounts should automatically have been created in the fleet host project with all the required permissions. To verify the service accounts exist, run the following commands:```\ngcloud projects get-iam-policy PROJECT_ID | grep gcp-sa-mcsdgcloud projects get-iam-policy PROJECT_ID | grep gcp-sa-gkehub\n```Replace `` with the project ID from the fleet host project.\nThese commands should show you the full name of the `mcsd` and `gkehub` service accounts.\n- **Multiple VPCs detected in the hub - VPC must be peered with otherVPCs for successful connectivity** : This message occurs when clusters hosted in different VPCs are registered to the same fleet. Membership status is `OK` . A cluster's VPC network is defined by its [NetworkConfig's network](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#networkconfig) . Multi-cluster Services require a flat network, and these VPCs must be actively peered for multi-cluster Services to properly connect to each other. To learn more, see [Example VPC Network Peering setup](/vpc/docs/using-vpc-peering#example_vpc_network_peering_setup) .\n- **Member does not exist in the same project as hub - additionalsetup steps are required, errors may occur if not completed.** : This message reminds you that cross-project clusters require additional setup steps. Membership status is `OK` . Cross-project memberships are defined as a member cluster that is not in the same project as the fleet. For more information, see [cross-project setup](/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks) .\n- **Non-GKE clusters are currently not supported** : This message reminds you that MCS only supports GKE clusters. Non-GKE clusters cannot be added to MCS. Membership status is `FAILED` .## Known issues\n### MCS Services with multiple ports\nThere is a known issue with multi-cluster Services with multiple (TCP/UDP) ports on GKE Dataplane V2 where some endpoints are not programmed in the dataplane. This issue impacts GKE versions earlier than 1.26.3-gke.400.\nAs a workaround, when using GKE Dataplane V2, use multiple MCS with a single port instead of one MCS with multiple ports.\n### MCS with Shared VPC\nWith the current implementation of MCS, if you deploy more than one fleet in the same Shared VPC, metadata are shared between fleets. When a Service is created in one fleet, Service metadata is exported or imported in all other fleets that are part of the same Shared VPC and visible to the user.\nThis behavior will be fixed in an upcoming release of MCS.\n### Health check uses default port instead of containerPort\nWhen you deploy a Service with a `targetPort` field referencing a named port in a Deployment, MCS configures the default port for the health check instead of the specified `containerPort` .\nTo avoid this issue, use numerical values in the Service field `ports.targetPort` and the Deployment field `readinessProbe.httpGet.port` instead of named values.\nThis behavior will be fixed in an upcoming release of MCS.\n## What's next\n- Learn more about [Services](/kubernetes-engine/docs/concepts/service) .\n- Learn how to [expose apps with Services](/kubernetes-engine/docs/how-to/exposing-apps) .\n- Implement a basic [multi-cluster Services example](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/services/multi-cluster/mcs-basic) .", "guide": "Google Kubernetes Engine (GKE)"}