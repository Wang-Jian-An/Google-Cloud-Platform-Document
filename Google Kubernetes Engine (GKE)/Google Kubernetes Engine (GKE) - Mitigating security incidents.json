{"title": "Google Kubernetes Engine (GKE) - Mitigating security incidents", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/security-mitigations", "abstract": "# Google Kubernetes Engine (GKE) - Mitigating security incidents\nThis document describes common mitigations and responses to potential security incidents on your Google Kubernetes Engine (GKE) clusters and containers.\nThe suggestions in [Hardening your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster) can improve the security of your GKE workloads. Security incidents, however, can occur even when measures to protect your workloads are in place.\n", "content": "## Detecting incidents\nTo detect potential incidents, we recommend you set up a process that collects and monitors your workload's logs. Then, set up alerts based on abnormal events detected from logs. Alerts notify your security team when something unusual is detected. Your security team can then review the potential incident.\n### Generating alerts from logs\nYou can customize alerts based on specific metrics or actions. For example, alerting on high CPU usage on your GKE nodes may indicate they are compromised for cryptomining.\nAlerts should be generated where you aggregate your logs and metrics. For example, you can use GKE [Audit Logging](/kubernetes-engine/docs/how-to/audit-logging) in combination with [logs-based alerting](/logging/docs/logs-based-metrics) in Cloud Logging.\nTo learn more about security-relevant queries, see the [Audit Logging](/kubernetes-engine/docs/how-to/audit-logging#example_filters_for_your_admin_activity_log) documentation.\n## Responding to a security incident\nAfter you have been alerted to an incident, take action. Fix the vulnerability if you can. If you do not know the root cause of the vulnerability or do not have a fix ready, apply mitigations.\nThe mitigations you might take depend on the severity of the incident and your certainty that you have identified the issue.\nThis guide covers actions you can take after you detect an incident on a workload running on GKE. You could, in increasing order of severity:\n- [Snapshot the host VM's disk](#snapshot_the_vms_disk) . A snapshot lets you perform some forensics on the VM state at the time of the anomaly after the workload has been redeployed or deleted.\n- **Inspect the VM while the workload continues to run** . Connecting to the host VM or workload container can provide information about the attacker's actions. We recommend you reduce access before inspecting the live VM **.** \n- **Redeploy a container** . Redeploying ends currently running processes in the affected container and restarts them.\n- **Delete a workload** . Deleting the workload ends currently running processes in the affected container without a restart.\nThese mitigations are described in the following sections.\n## Before you begin\nThe methods used in this topic use the following information:\n- The name of the Pods you think have been compromised, or`POD_NAME`.\n- The name of the host VM running the container or Pods, or`NODE_NAME`.\nAlso, before taking any of the actions, consider if there will be a negative reaction from the attacker if they are discovered. The attacker may decide to delete data or destroy workloads. If the risk is too high, consider more drastic mitigations such as deleting a workload before performing further investigation.\n## Snapshot the VM's disk\nCreating a snapshot of the VM's disk allows forensic investigation after the workload has been redeployed or deleted. Snapshots can be created while disks are attached to running instances.\n**Note:** Snapshots only capture state written to disk. Snapshots do not capture contents of the VM's memory.\n- To snapshot your persistent disk, first find the disks attached to your VM. Run the following command and look at the `source` field:```\ngcloud compute instances describe NODE_NAME --zone COMPUTE_ZONE \\\u00a0 \u00a0 --format=\"flattened([disks])\"\n```\n- Look for the lines that contain `disks[NUMBER].source` . The output is similar to the following:```\ndisks[0].source: https://www.googleapis.com/compute/v1/projects/PROJECT_NAME/zones/COMPUTE_ZONE/disks/DISK_NAME\n```The disk name is the portion of the source name after the final slash. For example disk name is `gke-cluster-pool-1-abcdeffff-zgt8` .\n- To complete the snapshot, run the following command:```\ngcloud compute disks snapshot DISK_NAME\n```\nFor more information, see [Creating persistent disk snapshots](/compute/docs/disks/create-snapshots) in the Compute Engine documentation.\n## Inspect the VM while the workload continues to run\n**Caution:** In severe incidents, workloads on the same node or the same cluster may also be compromised. This is known as a container escape. Monitor all of your workloads for abnormal behavior and take appropriate actions.\nAlso, consider what access an attacker may have before taking action. If you suspect a container has been compromised and are concerned about informing the attacker, you can [connect to](https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/) the container and inspect it. Inspecting is useful for quick investigation before taking more disruptive actions. Inspecting is also the least disruptive approach to the workload, but it doesn't stop the incident.\nAlternatively, to avoid logging into a machine with a privileged credential, you can analyze your workloads by setting up live forensics (such as [GRR Rapid Response](https://grr-doc.readthedocs.io/en/latest/) ), on-node agents, or network filtering.\n### Reduce access before inspecting the live VM\nBy [cordoning](https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration) , [draining](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/) , and limiting network access to the VM hosting a compromised container, you can isolate the compromised container from the rest of your cluster. Limiting access to the VM reduces risk but does not prevent an attacker from moving laterally in your environment if they take advantage of a critical vulnerability.\nCordoning and draining a node moves workloads colocated with the compromised container to other VMs in your cluster. Cordoning and draining reduces an attacker's ability to impact other workloads on the same node. It does not necessarily prevent them from inspecting a workload's persistent state (for example, by inspecting container image contents).\n**Caution:** `kubectl cordon` marks a node as 'unschedulable', which has the side effect of the service controller removing the node from any LoadBalancer node target lists it was previously eligible for, effectively removing incoming load balancer traffic from the cordoned node(s). This issue applies to GKE versions 1.18 and earlier.\n- Use `kubectl` to cordon the node and ensure that no other Pods are scheduled on it:```\nkubectl cordon NODE_NAME\n``` **Caution:** Draining evicts other workloads from the same node, which might cause downtime. Using `kubectl drain` will respect PodDisruptionBudgets, but it assumes Pods are configured to be automatically re-created on other nodes after eviction.After cordoning the node, drain the node of other Pods.\n- Label the Pod that you are quarantining:```\nkubectl label pods POD_NAME quarantine=true\n```Replace `` with the name of the Pod that you want to quarantine.\n- Drain the node of Pods that are not labeled with `quarantine` :```\nkubectl drain NODE_NAME --pod-selector='!quarantine'\n```We recommended blocking both internal and external traffic from accessing the host VM. Next, allow inbound connections from a specific VM on your network or VPC to connect to the quarantined VM.\nThe first step is to abandon the VM from the [Managed Instance Group](/compute/docs/instance-groups) that owns it. Abandoning the VM prevents the node from being marked unhealthy and auto-repaired (re-created) before your investigation is complete.\nTo abandon the VM, run the following command:\n```\ngcloud compute instance-groups managed abandon-instances INSTANCE_GROUP_NAME \\\u00a0 \u00a0 --instances=NODE_NAME\n```\n### Firewall the VM\n[Creating a firewall](/vpc/docs/using-firewalls) between the affected container and other workloads in the same network helps prevent an attacker from moving into other parts of your environment while you conduct further analysis. Since you already [drained the VM](#reduce_access_before_inspecting_the_live_vm) of other containers, this only affects the quarantined container.\nThe following instructions on firewalling the VM prevents:\n- **New** outbound connections to other VMs in your cluster using an egress rule.\n- Inbound connections to the compromised VM using an ingress rule.\n**Note:** Firewalls have several limitations.- Adding firewall rules doesn't close existing connections. Removing the   VM's external IP address (described in the following section) breaks   existing connections from the external internet, although not from   inside your network.\n- An attacker who compromises a privileged container or breaks out of an   unprivileged container can access the VM's [metadata](/compute/docs/storing-retrieving-metadata) .   We recommend using [Shielded GKE nodes](/kubernetes-engine/docs/how-to/shielded-gke-nodes) to remove the privileged bootstrap keys from the metadata server.\nTo firewall the VM off from your other instances, follow these steps for the node that hosts the Pod you want to quarantine:\n- Tag the instance so you can apply a new firewall rule. **Note:** Check that the tag does not conflict with other VMs before applying it.```\ngcloud compute instances add-tags NODE_NAME \\\u00a0 \u00a0 --zone COMPUTE_ZONE \\\u00a0 \u00a0 --tags quarantine\n```\n- Create a firewall rule to deny all egress TCP traffic from instances with the `quarantine` tag:```\ngcloud compute firewall-rules create quarantine-egress-deny \\\u00a0 \u00a0 --network NETWORK_NAME \\\u00a0 \u00a0 --action deny \\\u00a0 \u00a0 --direction egress \\\u00a0 \u00a0 --rules tcp \\\u00a0 \u00a0 --destination-ranges 0.0.0.0/0 \\\u00a0 \u00a0 --priority 0 \\\u00a0 \u00a0 --target-tags quarantine\n```\n- Create a firewall rule to deny all ingress TCP traffic to instances with the `quarantine` tag. Give this ingress rule a `priority` of `1` , which lets you override it with another rule that allows SSH from a specified VM.```\ngcloud compute firewall-rules create quarantine-ingress-deny \\\u00a0 \u00a0 --network NETWORK_NAME \\\u00a0 \u00a0 --action deny \\\u00a0 \u00a0 --direction ingress \\\u00a0 \u00a0 --rules tcp \\\u00a0 \u00a0 --source-ranges 0.0.0.0/0 \\\u00a0 \u00a0 --priority 1 \\\u00a0 \u00a0 --target-tags quarantine\n```Removing the VM's external IP address breaks any existing network connections outside your VPC.\nTo remove the external address of a VM, perform the following steps:\n- Find and delete the access config that associates the external IP with the VM. First find the access config by describing the VM:```\ngcloud compute instances describe NODE_NAME \\\u00a0 \u00a0 --zone COMPUTE_ZONE --format=\"flattened([networkInterfaces])\"\n```Look for the lines that contain `name` and `natIP` . They look like the following:```\nnetworkInterfaces[0].accessConfigs[0].name:    ACCESS_CONFIG_NAME\nnetworkInterfaces[0].accessConfigs[0].natIP:    EXTERNAL_IP_ADDRESS\n```\n- Find the value of `natIP` that matches the external IP you want to remove. Note the name of the access config.\n- To remove the external IP, run the following command:```\ngcloud compute instances delete-access-config NODE_NAME \\\u00a0 \u00a0 --access-config-name \"ACCESS_CONFIG_NAME\"\n```\n### SSH to the host VM via an intermediate VM\nAfter you remove the host VM's external IP, you cannot ssh from outside your VPC. You access it from another VM in the same network. For the rest of this section, we refer to this as the **intermediate VM** .- An intermediate VM with access to the subnetwork of the host VM. If you do not already have one, [create a VM](/compute/docs/instances/create-start-instance) for this purpose.\n- The internal IP address of the intermediate VM.\n- An SSH public key from the intermediate VM. To learn more, see [Managing SSH Keys](/compute/docs/instances/adding-removing-ssh-keys#locatesshkeys) - Add the intermediate VM's public key to the host VM. For more information, see [Adding and Removing SSH keys](/compute/docs/instances/adding-removing-ssh-keys) in the Compute Engine documentation.\n- Add a tag to the intermediate VM. **Note:** Check that the tag does not conflict with other VMs in your network before applying it.```\ngcloud compute instances add-tags INTERMEDIATE_NODE_NAME \\\u00a0 --zone COMPUTE_ZONE \\\u00a0 --tags intermediate\n```\n- Add an ingress allow rule to override the deny rule you added earlier. To add the rule, run the following command.```\ngcloud compute firewall-rules create quarantine-ingress-allow \\\u00a0 \u00a0 --network NETWORK_NAME \\\u00a0 \u00a0 --action allow \\\u00a0 \u00a0 --direction ingress \\\u00a0 \u00a0 --rules tcp:22 \\\u00a0 \u00a0 --source-tags intermediate \\\u00a0 \u00a0 --priority 0 \\\u00a0 \u00a0 --target-tags quarantine\n```This rule allows incoming traffic on port 22 (SSH) from VMs in your network with the `intermediate` tag. It overrides the deny rule with a `priority` of `0` .\n- Connect to the quarantined VM with using its internal IP:```\nssh -i KEY_PATH USER@QUARANTINED_VM_INTERNAL_IP\n```Replace the following:- ``: the path to your SSH private key.\n- ``: your Google Cloud account's email address.\n- ``: the internal IP address.\n## Redeploy a container\nBy redeploying your container, you start a fresh copy of the container and delete the compromised container.\nYou redeploy a container by deleting the Pod that hosts it. If the Pod is managed by a higher-level Kubernetes construct (for example, a Deployment or DaemonSet), deleting the Pod schedules a new Pod. This Pod runs new containers.\nRedeploying makes sense when:\n- You already know the cause of the vulnerability.\n- You think it takes an attacker significant effort or time to compromise your container again.\n- You think that the container might quickly get compromised again and you don't want to take it offline, so you plan to place it in a [sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) to limit the impact.\n**Note:** **If you have a fix for the vulnerability, roll out the fix first, thenmigrate traffic to the replacement workload.** Redeploying a Pod does not fix security issues. An attacker might repeat the same exploit chain again. Attacks can continue until a fix has been deployed.\nWhen redeploying the workload, if the possibility of another compromise is high, consider placing the workload in a sandbox environment such as [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) . Sandboxing limits access to the host node kernel if the attacker compromises the container again.\nTo redeploy a container in Kubernetes, delete the Pod that contains it:\n```\nkubectl delete pods POD_NAME --grace-period=10\n```\nIf the containers in the deleted Pod continue to run, you can [delete the workload](#delete_a_workload) .\nTo redeploy the container within a sandbox, follow the instructions in [Harden workload isolation withGKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\n## Delete a workload\nDeleting a workload, such as a Deployment or DaemonSet, causes all of its member Pods to be deleted. All containers inside those Pods stop running. Deleting a workload can make sense when:\n- You want to stop an attack in progress.\n- You are willing to take the workload offline.\n- Stopping the attack immediately is more important than application uptime or forensic analysis.\n**Caution:** Deleting a workload takes the entire application offline.\nTo delete a workload, use `kubectl delete` `` . For example, to delete a Deployment, run the following command:\n```\nkubectl delete deployments DEPLOYMENT\n```\nIf deleting the workload doesn't delete all associated Pods or containers, you can manually delete the containers using the container runtimes's CLI tool, typically `docker` . If your nodes run [containerd](/kubernetes-engine/docs/concepts/using-containerd) , use `crictl` .\n**Note:** If you manually delete a container before deleting the workload, Kubernetes redeploys the container.\n**Warning: ** In GKE version 1.24 and later, Docker-based node image types are not supported. In GKE version 1.23, you also cannot create new node pools with Docker node image types. You must migrate to a containerd node image type. To learn more about this change, see [About the Docker node image deprecation](/kubernetes-engine/docs/deprecations/docker-containerd) .\nTo stop a container in using the Docker container runtime, you can use either `docker stop` or `docker kill` .\n`docker stop` stops the container by sending a `SIGTERM` signal to the root process, and waits 10 seconds for the process to exit by default. If the process hasn't exited in that time period, it then sends a `SIGKILL` signal. You can specify this grace period with the `--time` option.\n```\ndocker stop --time TIME_IN_SECONDS CONTAINER\n```\n`docker kill` is the fastest method to stop a container. It sends the `SIGKILL` signal immediately.\n```\ndocker kill CONTAINER\n```\nYou can also stop and remove a container in one command with `docker rm -f` :\n```\ndocker rm -f CONTAINER\n```\nIf you use the `containerd` runtime in GKE, you stop or remove containers with `crictl` .\nTo stop a container in `containerd` , run the following command:\n```\ncrictl stop CONTAINER\n```\nTo remove a container in `containerd` , run the following command:\n```\ncrictl rm -f CONTAINER\n```\n### Deleting the host VM\n**Warning:** Deleting a VM stops any workloads co-located on the node. [Cordon and drain](#cordon) the node first to limit disruption.\nIf you are unable to delete or remove the container, you can delete the virtual machine that hosts the affected container.\nIf the Pod is still visible, you can find the name of the host VM with the following command:\n```\nkubectl get pods --all-namespaces \\\u00a0 -o=custom-columns=POD_NAME:.metadata.name,INSTANCE_NAME:.spec.nodeName \\\u00a0 --field-selector=metadata.name=POD_NAME\n```\nTo delete the host VM, run the following `gcloud` command:\n```\ngcloud compute instance-groups managed delete-instances INSTANCE_GROUP_NAME \\\u00a0 \u00a0 --instances=NODE_NAME\n```\nAbandoning the instance from the Managed Instance Group reduces the size of the group by one VM. You can manually add one instance back to the group with the following command:\n```\ngcloud compute instance-groups managed resize INSTANCE_GROUP_NAME \\\u00a0 \u00a0 --size=SIZE\n```\n## What's next\n- [Performing forensics on containers](/blog/products/containers-kubernetes/best-practices-for-performing-forensics-on-containers) \n- [Hardening your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster) \n- [Forensic analysis for GKE applications](/solutions/security-controls-and-forensic-analysis-for-GKE-apps)", "guide": "Google Kubernetes Engine (GKE)"}