{"title": "Google Kubernetes Engine (GKE) - Harden your cluster's security", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster", "abstract": "# Google Kubernetes Engine (GKE) - Harden your cluster's security\nWith the speed of development in Kubernetes, there are often new security features for you to use. This page guides you through implementing our current guidance for hardening your Google Kubernetes Engine (GKE) cluster.\nThis guide prioritizes high-value security mitigations that require customer action at cluster creation time. Less critical features, secure-by-default settings, and those that can be enabled post-creation time are mentioned later in the document. For a general overview of security topics, read the [Security Overview](/kubernetes-engine/docs/concepts/security-overview) .\n**Note:** If you are creating new clusters in GKE, many of these protections are enabled by default. If you are upgrading existing clusters, make sure to regularly review this hardening guide and enable new features.Clusters created in the Autopilot mode implement many GKE hardening features by default.\nMany of these recommendations, as well as other common misconfigurations, can be automatically checked using [Security Health Analytics](/security-command-center/docs/list-of-security-health-analytics-findings#container_vulnerability_findings) .\nWhere the recommendations below relate to a [CIS GKE Benchmark Recommendation](/kubernetes-engine/docs/concepts/cis-benchmarks) , this is specified.\n", "content": "## Upgrade your GKE infrastructure in a timely fashion\n**Keeping the version of Kubernetes up to date is one of the simplest things youcan do to improve your security.** Kubernetes frequently introduces new security features and provides security patches.\nSee the [GKE securitybulletins](/kubernetes-engine/docs/security-bulletins) for information on security patches.\nIn Google Kubernetes Engine, the control planes are patched and upgraded for you automatically. [Node auto-upgrade](/kubernetes-engine/docs/how-to/node-auto-upgrades) also automatically upgrades nodes in your cluster.\n**Note:** Node auto-upgrade is enabled by default for clusters created using the Google Cloud console since June 2019, and for clusters created using the API starting November 11, 2019.\nIf you choose to disable node auto-upgrade, we recommend upgrading monthly on your own schedule. Older clusters should opt-in to node auto-upgrade and closely follow the [GKE securitybulletins](/kubernetes-engine/docs/security-bulletins) for critical patches.\nTo learn more, see [Auto-upgrading nodes](/kubernetes-engine/docs/how-to/node-auto-upgrades) .\n## Restrict network access to the control plane and nodes\n**You should limit exposure of your cluster control plane and nodes to theinternet.** These settings can only be set at cluster creation time.\nBy default the GKE cluster control plane and nodes have internet routable addresses that can be accessed from any IP address.\nFor the GKE cluster control plane, see [Creating a privatecluster](/kubernetes-engine/docs/how-to/private-clusters) . There are three different flavors of private clusters that can deliver network level protection:\n- **Public endpoint access disabled:** This is the most secure option as it prevents all internet access to both control planes and nodes. This is a good choice if you have configured your on-premises network to connect to Google Cloud using [Cloud Interconnect](/network-connectivity/docs/interconnect/concepts/overview) and [Cloud VPN](/network-connectivity/docs/vpn/concepts/overview) . Those technologies effectively connect your company network to your cloud VPC.\n- **Public endpoint access enabled, authorized\nnetworks enabled\n(recommended):** This option provides restricted access to the control plane from source IP addresses that you define. This is a good choice if you don't have existing VPN infrastructure or have remote users or branch offices that connect over the public internet instead of the corporate VPN and Cloud Interconnect or Cloud VPN.\n- **Public endpoint access enabled, authorized networks disabled:** This is the default and allows anyone on the internet to make network connections to the control plane.\nTo disable direct internet access to nodes, specify the gcloud CLIoption --enable-private-nodes at cluster creation.\nThis tells GKE to provision nodes with internal IP addresses, which means the nodes aren't directly reachable over the public internet.\nWe recommend clusters at least use [authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) and private nodes. This ensures the control plane is reachable by:\n- The allowed CIDRs in authorized networks.\n- Nodes within your cluster's VPC.\n- Google's internal production jobs that manage your control plane.\nThat corresponds to the following `gcloud` flags at cluster creation time:\n- `--enable-ip-alias`\n- `--enable-private-nodes`\n- `--enable-master-authorized-networks`## Use least-privilege firewall rules\n**Minimize the risk of unintended access by using the principle ofleast privilege for firewall rules**\nGKE creates default VPC firewall rules to enable system functionality and to enforce good security practices. For a full list of automatically created firewall rules, see [Automatically created firewall rules](/kubernetes-engine/docs/concepts/firewall-rules) .\nGKE creates these default firewall rules with a priority of 1000. If you create permissive firewall rules with a higher priority, for example an `allow-all` firewall rule for debugging, your cluster is at risk of unintended access.\nWhen you create firewall rules, use the principle of least privilege to provide access only for the required purpose. Ensure that your firewall rules don't conflict with the GKE default firewall rules when possible.\n## Group authentication\n**You should use groups to manage your users.** Using groups allows identities to be controlled using your Identity management system and Identity administrators. Adjusting the group membership negates the need to update your RBAC configuration whenever anyone is added or removed from the group.\nTo manage user permissions using Google Groups, you must enable [Google Groups for RBAC](/kubernetes-engine/docs/how-to/google-groups-rbac#enable) on your cluster. This allows you to manage users with the same permissions easily, while allowing your identity administrators to manage users centrally and consistently.\nSee [Google Groups for RBAC](/kubernetes-engine/docs/how-to/google-groups-rbac) for instructions on enabling Google Groups for RBAC.\n## Container node choices\nThe following sections describe secure node configuration choices.\n### Enable Shielded GKE Nodes\n**Shielded GKE Nodes provide strong, verifiable node identity and integrity toincrease the security of GKE nodes and should be enabled onall GKE clusters.**\nYou can enable Shielded GKE Nodes at cluster creation or update. Shielded GKE Nodes should be enabled with secure boot. Secure boot should not be used if you need third-party unsigned kernel modules. For instructions on how to enable Shielded GKE Nodes, and how to enable secure boot with Shielded GKE Nodes, see [Using Shielded GKE Nodes](/kubernetes-engine/docs/how-to/shielded-gke-nodes) .\n### Choose a hardened node image with the containerd runtime\nThe Container-Optimized OS with containerd ( [cos_containerd](/kubernetes-engine/docs/concepts/using-containerd) ) image is a variant of the Container-Optimized OS image with containerd as the main container runtime directly integrated with Kubernetes.\n[containerd](https://containerd.io/) is the core runtime component of Docker and has been designed to deliver core container functionality for the Kubernetes Container Runtime Interface (CRI). It is significantly less complex than the full Docker daemon, and therefore has a smaller attack surface.\nTo use the `cos_containerd` image in your cluster, see [Containerd images](/kubernetes-engine/docs/concepts/using-containerd) .\n**The cos_containerd image is the preferred image for GKEbecause it has been custom built, optimized, and hardened specifically for running containers.**\n## Enable workload identity federation for GKE\n[Workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) is the recommended way to authenticate to Google Cloud APIs.\nWorkload identity federation for GKE replaces the need to use [MetadataConcealment](/kubernetes-engine/docs/how-to/protecting-cluster-metadata#concealment) and as such, the two approaches are incompatible. The sensitive metadata protected by Metadata Concealment is also protected by workload identity federation for GKE.\n## Harden workload isolation with GKE Sandbox\n[GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) provides an extra layer of security to prevent malicious code from affecting the host kernel on your cluster nodes.\nYou can run containers in a environment to mitigate against most container escape attacks, also called local privilege escalation attacks. For past container escape vulnerabilities, refer to the [securitybulletins](/anthos/clusters/docs/security-bulletins) . This type of attack lets an attacker gain access to the host VM of the container, and therefore gain access to other containers on the same VM. A sandbox such as [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) can help limit the impact of these attacks.\nYou should consider sandboxing a workload in situations such as:\n- The workload runs untrusted code\n- You want to limit the impact if an attacker compromises a container in the workload.\nLearn how to use GKE Sandbox in [Harden workload isolation with GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\n## Enable security bulletin notifications\nWhen security bulletins are available that are relevant to your cluster, GKE publishes notifications about those events as messages to Pub/Sub topics that you configure. You can receive these notifications on a Pub/Sub subscription, [integrate with third-party services](/kubernetes-engine/docs/tutorials/cluster-notifications-slack) , and filter for the [notification types](/kubernetes-engine/docs/concepts/cluster-notifications#notification-types) you want to receive.\nFor more information about receiving security bulletins using GKE cluster notifications, see [Cluster notifications](/kubernetes-engine/docs/concepts/cluster-notifications) .\n## Permissions\n### Use least privilege IAM service accounts\nEach GKE node has an [Identity and Access Management (IAM) Service Account](/iam/docs/understanding-service-accounts) associated with it. By default, nodes are given the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) , which you can find by navigating to the [IAM section of the Google Cloud console](https://console.cloud.google.com/iam-admin/iam) . This account has broad access by default, making it useful to wide variety of applications, but it has more permissions than are required to run your Kubernetes Engine cluster. **You should create and use a minimally privileged service account foryour nodes to use instead of the Compute Engine default service account.**\nWith the launch of [workload identity federation for GKE](#workload_identity) , we suggest a more limited use case for the node service account. We expect the node service account to be used by system daemons responsible for logging, monitoring and similar tasks. Workloads in Pods should instead be provisioned identities with workload identity federation for GKE.\nGKE requires, at a minimum, the service account to have the `monitoring.viewer` , `monitoring.metricWriter` , `logging.logWriter` , `stackdriver.resourceMetadata.writer` , and `autoscaling.metricsWriter` roles. Read more about [monitoring roles](/monitoring/access-control) and [logging roles](/logging/docs/access-control) .\n**Note:** To edit IAM policies, you must enable the Cloud Resource Manager API in your project: ```gcloud services enable cloudresourcemanager.googleapis.com```\nThe following commands create an IAM service account with the minimum permissions required to operate GKE. You can also use the service account for resources in other projects. For instructions, refer to [Enabling service account impersonation across projects](/iam/docs/attach-service-accounts#enabling-cross-project) .\n```\ngcloud iam service-accounts create SA_NAME \\\u00a0 \u00a0 --display-name=DISPLAY_NAMEgcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/logging.logWritergcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/monitoring.metricWritergcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/monitoring.viewergcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/stackdriver.resourceMetadata.writergcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/autoscaling.metricsWriter\n```\nReplace the following:- ``: the name of the new service account.\n- ``: the display name for the new service account, which makes the account easier to identify.\n- ``: the project ID of the project in which you want to create the new service account.\n **Note:** This step requires [Config Connector](https://cloud.google.com/config-connector/docs/overview) . Follow the [installation instructions](https://cloud.google.com/config-connector/docs/how-to/install-upgrade-uninstall) to install Config Connector on your cluster.- To create the service account, download the following resource as `service-account.yaml` . [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/service-account.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMServiceAccountmetadata:\u00a0 name: [SA_NAME]spec:\u00a0 displayName: [DISPLAY_NAME]\n```Replace the following:- ``: the name of the new service account.\n- ``: the display name for the new service account, which makes the account easier to identify.\nThen, run:```\nkubectl apply -f service-account.yaml\n```\n- Apply the `logging.logWriter` role to the service account. Download the following resource as `policy-logging.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information. [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-logging.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-loggingspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/logging.logWriter\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n``````\nkubectl apply -f policy-logging.yaml\n```\n- Apply the `monitoring.metricWriter` role. Download the following resource as `policy-metrics-writer.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information. [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-metrics-writer.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-metrics-writerspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/monitoring.metricWriter\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n``````\nkubectl apply -f policy-metrics-writer.yaml\n```\n- Apply the `monitoring.viewer` role. Download the following resource as `policy-monitoring.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information. [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-monitoring.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-monitoringspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/monitoring.viewer\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n``````\nkubectl apply -f policy-monitoring.yaml\n```\n- Apply the `autoscaling.metricsWriter` role. Download the following resource as `policy-autoscaling-metrics-writer.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information. [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-autoscaling-metrics-writer.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-autoscaling-metrics-writerspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/autoscaling.metricsWriter\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n``````\nkubectl apply -f policy-autoscaling-metrics-writer.yaml\n```To use private images in [Artifact Registry](/artifact-registry) , grant the [Artifact Registry Reader role](/iam/docs/understanding-roles#artifactregistry.reader) ( `roles/artifactregistry.reader` ) to the service account.\n```\ngcloud artifacts repositories add-iam-policy-binding REPOSITORY_NAME \\\u00a0 \u00a0 --member=serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/artifactregistry.reader\n```\nReplace `` with the name of your Artifact Registry repository.\n **Note:** This step requires [Config Connector](https://cloud.google.com/config-connector/docs/overview) . Follow the [installation instructions](https://cloud.google.com/config-connector/docs/how-to/install-upgrade-uninstall) to install Config Connector on your cluster.\n **Note:** These steps assume that you use Config Connector to manage your Artifact Registry repository. If your repository doesn't exist as a Config Connector resource, these steps won't work.- Save the following manifest as `policy-artifact-registry-reader.yaml` : [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-artifact-registry-reader.yaml) ```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-artifact-registry-readerspec:\u00a0 member: serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com\u00a0 role: roles/artifactregistry.reader\u00a0 resourceRef:\u00a0 \u00a0 apiVersion: artifactregistry.cnrm.cloud.google.com/v1beta1\u00a0 \u00a0 kind: ArtifactRegistryRepository\u00a0 \u00a0 name: REPOSITORY_NAME\n```Replace the following:- : the name of your IAM service account.\n- : your Google Cloud project ID.\n- : the name of your Artifact Registry repository.\n- Grant the Artifact Registry Reader role to the service account:```\nkubectl apply -f policy-artifact-registry-reader.yaml\n```\nIf you use private images in [Container Registry](/container-registry) , you also need to grant access to those:\n**Note:** Container Registry is deprecated and scheduled for shutdown. Organizations that haven't used Container Registry prior to January 8, 2024 have new gcr.io repositories hosted on Artifact Registry by default. After May 15, 2024, Google Cloud projects without previous usage of Container Registry will only support hosting and managing images for the `gcr.io` domain in [Artifact Registry](/artifact-registry/docs) .Container Registry is scheduled for shutdown on March 18, 2025. For details on the deprecation, see [Container Registry deprecation](/container-registry/docs/deprecations/container-registry-deprecation) .\n```\ngsutil iam ch \\\u00a0 serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com:objectViewer \\\u00a0 gs://BUCKET_NAME\n```\nThe bucket that stores your images has the name `` of the form:- `artifacts.` `` `.appspot.com`for images pushed to a registry in the host`gcr.io`, or\n- `` `.artifacts.` `` `.appspot.com`\nReplace the following:- ``: your Google Cloud console project ID.\n- ``: the location of the storage bucket:- `us`for registries in the host`us.gcr.io`\n- `eu`for registries in the host`eu.gcr.io`\n- `asia`for registries in the host`asia.gcr.io`\nRefer to the [gsutil iam](/storage/docs/gsutil/commands/iam) documentation for more information about the command.\n **Note:** This step requires [Config Connector](https://cloud.google.com/config-connector/docs/overview) . Follow the [installation instructions](https://cloud.google.com/config-connector/docs/how-to/install-upgrade-uninstall) to install Config Connector on your cluster.\nApply the `storage.objectViewer` role to your service account. Download the following resource as `policy-object-viewer.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information.\n [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-object-viewer.yaml) \n```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-object-viewerspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/storage.objectViewer\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n```\n```\nkubectl apply -f policy-object-viewer.yaml\n```\nIf you want another human user to be able to create new clusters or node pools with this service account, you must grant them the Service Account User role on this service account:\n```\ngcloud iam service-accounts add-iam-policy-binding \\\n SA_NAME@PROJECT_ID.iam.gserviceaccount.com \\\n --member=user:USER \\\n --role=roles/iam.serviceAccountUser\n```\n **Note:** This step requires [Config Connector](https://cloud.google.com/config-connector/docs/overview) . Follow the [installation instructions](https://cloud.google.com/config-connector/docs/how-to/install-upgrade-uninstall) to install Config Connector on your cluster.\nApply the `iam.serviceAccountUser` role to your service account. Download the following resource as `policy-service-account-user.yaml` . Replace `[SA_NAME]` and `[PROJECT_ID]` with your own information.\n [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-config-connector/blob/HEAD/config/samples/tutorials/hardening-your-cluster/policy-service-account-user.yaml) \n```\napiVersion: iam.cnrm.cloud.google.com/v1beta1kind: IAMPolicyMembermetadata:\u00a0 name: policy-service-account-userspec:\u00a0 member: serviceAccount:[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com\u00a0 role: roles/iam.serviceAccountUser\u00a0 resourceRef:\u00a0 \u00a0 kind: Project\u00a0 \u00a0 name: [PROJECT_ID]\n```\n```\nkubectl apply -f policy-service-account-user.yaml\n```\nFor existing Standard clusters, you can now create a new node pool with this new service account. For Autopilot clusters, you must create a new cluster with the service account. For instructions, see [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) .\n- Create a node pool that uses the new service account:```\ngcloud container node-pools create NODE_POOL_NAME \\\n--service-account=SA_NAME@PROJECT_ID.iam.gserviceaccount.com \\\n--cluster=CLUSTER_NAME\n```\nIf you need your GKE cluster to have access to other Google Cloud services, you should use [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) .\n### Restrict access to cluster API discovery\nBy default, Kubernetes bootstraps clusters with a permissive set of [discovery ClusterRoleBindings](/kubernetes-engine/docs/how-to/role-based-access-control#default_discovery_roles) which give broad access to information about a cluster's APIs, including those of [CustomResourceDefinitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) .\nUsers should be aware that the `system:authenticated` Group included in the subjects of the `system:discovery` and `system:basic-user` ClusterRoleBindings can include any authenticated user (including any user with a Google account), and does not represent a meaningful level of security for clusters on GKE. For more information, see [Avoid default roles and groups](/kubernetes-engine/docs/best-practices/rbac#default-roles-groups) .\nThose wishing to harden their cluster's discovery APIs should consider one or more of the following:\n- Configure [Authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) to restrict access to set IP ranges.\n- Set up a [private cluster](/kubernetes-engine/docs/how-to/private-clusters) to restrict access to a VPC.\nIf neither of these options are suitable for your GKE use case, you should treat all API discovery information (namely the schema of CustomResources, APIService definitions, and discovery information hosted by extension API servers) as publicly disclosed.\nBoth of these options allow access to the API server IP address from Cloud Run and Cloud Functions. This access is being removed, so do not rely on these services to communicate with your API server. For more information, refer to the [Google Cloud blog post](/blog/products/identity-security/updates-coming-for-authorized-networks-and-cloud-runfunctions-on-gke) .\n### Use namespaces and RBAC to restrict access to cluster resources\n**Give teams least-privilege access to Kubernetes by creating separatenamespaces or clusters for each team and environment.** Assign cost centers and appropriate labels to each namespace for [accountability andchargeback](/kubernetes-engine/docs/how-to/cluster-usage-metering) . Only give developers the level of access to their namespace that they need to deploy and manage their application, especially in production. Map out the tasks that your users need to undertake against the cluster and define the permissions that they require to do each task.\nFor more information about creating namespaces, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/namespaces/) . For best practices when planning your RBAC configuration, see [Best practices for GKE RBAC](/kubernetes-engine/docs/best-practices/rbac) .\n[IAM](/iam) and [Role-based access control(RBAC)](/kubernetes-engine/docs/how-to/role-based-access-control) work together, and an entity must have sufficient permissions at either level to work with resources in your cluster.\n**Assign the appropriate IAMroles forGKE to groups and users to provide permissions at the projectlevel and use RBACto grant permissions on a cluster and namespace level.** To learn more, see [Accesscontrol](/kubernetes-engine/docs/concepts/access-control) .\n[ Enable access and view cluster resources by namespace](/kubernetes-engine/docs/how-to/restrict-resources-access-by-namespace)\n## Restrict traffic among Pods with a network policy\nBy default, all Pods in a cluster can communicate with each other. **You shouldcontrol Pod to Pod communication as needed for your workloads.**\nRestricting network access to services makes it much more difficult for attackers to move laterally within your cluster, and also offers services some protection against accidental or deliberate denial of service. Two recommended ways to control traffic are:\n- Use [Istio](https://istio.io/) . See [Installing Istio on Google Kubernetes Engine](/istio/docs/istio-on-gke/installing) if you're interested in load balancing, service authorization, throttling, quota, metrics and more.\n- Use [Kubernetes network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) . See [Creating a cluster network policy](/kubernetes-engine/docs/how-to/network-policy) . Choose this if you're looking for the basic access control functionality exposed by Kubernetes. To implement common approaches for restricting traffic using network policies, follow the [implementation guidefrom the GKE Enterprise Security Blueprints](https://github.com/GoogleCloudPlatform/anthos-security-blueprints/tree/master/restricting-traffic) . Also, the Kubernetes documentation has an excellent [walkthrough](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/) for a simple nginx deployment. Consider using [network policy logging](/kubernetes-engine/docs/how-to/network-policy-logging) to verify that your network policies are working as expected.\nIstio and network policy may be used together if there is a need to do so.\n## Secret management\nYou should provide an additional layer of protection for sensitive data, such as secrets, stored in [etcd](https://kubernetes.io/docs/concepts/overview/components/#etcd) . To do this **you need to configure a secrets manager that is integrated withGKE clusters.** Some solutions will work both in GKE and in GKE on VMware, and so may be more desirable if you are running workloads across multiple environments. If you choose to use an external secrets manager such as HashiCorp Vault, you'll want to have that set up before you create your cluster.\nYou have several options for secret management.\n- You can use Kubernetes secrets natively in GKE. Optionally, you can encrypt these at the application-layer with a key you manage, using [Application-layer secretsencryption](/kubernetes-engine/docs/how-to/encrypting-secrets) .\n- You can use a secrets manager such as [HashiCorpVault](https://www.vaultproject.io/) . When run in a hardened HA mode, this will provide a consistent, production-ready way to manage secrets. You can authenticate to HashiCorp Vault using either a Kubernetes service account or a Google Cloud service account. To learn more about using GKE with Vault, see [Running and connecting to HashiCorp Vault on Kubernetes](/blog/products/identity-security/exploring-container-security-running-and-connecting-to-hashicorp-vault-on-kubernetes) .\nGKE VMs are [encrypted at the storage layer bydefault](/security/encryption/default-encryption) , which includes etcd.\n## Use admission controllers to enforce policy\n[Admissioncontrollers](https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/) are plugins that govern and enforce how the cluster is used. They must be enabled to use some of the more advanced security features of Kubernetes and are an important part of the defence in depth approach to hardening your cluster\nBy default, Pods in Kubernetes can operate with capabilities beyond what they require. **You should constrain the Pod's capabilities to only those required forthat workload.**\nKubernetes supports numerous controls for restricting your Pods to execute with only explicitly granted capabilities. For example, [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) is available for clusters in fleets. Kubernetes also has the built-in [PodSecurity admission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) that lets you enforce the Pod Security Standards in individual clusters.\nPolicy Controller is a feature of GKE Enterprise that lets you enforce and validate security on GKE clusters at scale by using declarative policies. To learn how to use Policy Controller to enforce declarative controls on your GKE cluster, see [Install Policy Controller](/anthos-config-management/docs/how-to/installing-policy-controller) .\nThe [PodSecurity admission controller](https://kubernetes.io/docs/concepts/security/pod-security-admission/) lets you enforce pre-defined policies in specific namespaces or in the entire cluster. These policies correspond to the different [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) .\n## Restrict the ability for workloads to self-modify\nCertain Kubernetes workloads, especially system workloads, have permission to self-modify. For example, some workloads vertically autoscale themselves. While convenient, this can allow an attacker who has already compromised a node to escalate further in the cluster. For example, an attacker could have a workload on the node change itself to run as a more privileged service account that exists in the same namespace.\nIdeally, workloads should not be granted the permission to modify themselves in the first place. When self-modification is necessary, you can limit permissions by applying Gatekeeper or Policy Controller constraints, such as [NoUpdateServiceAccount](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/general/noupdateserviceaccount) from the open source Gatekeeper library, which provides several useful security policies.\nWhen you deploy policies, it is usually necessary to allow the controllers that manage the cluster lifecycle to bypass the policies. This is necessary so that the controllers can make changes to the cluster, such as applying cluster upgrades. For example, if you deploy the `NoUpdateServiceAccount` policy on GKE, you must set the following parameters in the `Constraint` :\n```\nparameters:\u00a0 allowedGroups:\u00a0 - system:masters\u00a0 allowedUsers:\u00a0 - system:addon-manager\n```\n## Restrict the use of the deprecated gcePersistentDisk volume type\nThe deprecated `gcePersistentDisk` volume type lets you mount a Compute Engine persistent disk to Pods. **We recommend that you restrict usage of thegcePersistentDisk volume type in your workloads** . GKE doesn't perform any IAM authorization checks on the Pod when mounting this volume type, although Google Cloud performs authorization checks when attaching the disk to the underlying VM. An attacker who already has the ability to create Pods in a namespace can therefore access the contents of Compute Engine persistent disks in your Google Cloud project.\nTo access and use Compute Engine persistent disks, use [PersistentVolumes](/kubernetes-engine/docs/concepts/persistent-volumes) and PersistentVolumeClaims instead. Apply security policies in your cluster that prevent usage of the `gcePersistentDisk` volume type.\nTo prevent usage of the `gcePersistentDisk` volume type, apply the Baseline or Restricted policy with the [PodSecurity admission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) , or you can define a custom constraint in [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) or in the Gatekeeper admission controller.\nTo define a custom constraint to restrict this volume type, do the following:\n- Install a policy-based admission controller such as Policy Controller or Gatekeeper OPA.\n [Install Policy Controller in your cluster](/anthos-config-management/docs/how-to/installing-policy-controller) .\nPolicy Controller is a paid feature for GKE users. Policy Controller is based on open source Gatekeeper, but you also get access to the full constraint template library, [policy bundles](/anthos-config-management/docs/concepts/policy-controller-bundles) , and integration with Google Cloud console dashboards to help observe and maintain your clusters. Policy bundles are opinionated best practices that you can apply to your clusters, including bundles based on recommendations like the [CIS Kubernetes Benchmark](/anthos-config-management/docs/how-to/using-cis-k8s-benchmark) .\n [Install Gatekeeper in your cluster](https://open-policy-agent.github.io/gatekeeper/website/docs/install) .\nFor Autopilot clusters, open the Gatekeeper `gatekeeper.yaml` manifest in a text editor. Modify the `rules` field in the `MutatingWebhookConfiguration` specification to replace wildcard ( `*` ) characters with specific API groups and resource names, such as in the following example:\n```\napiVersion: admissionregistration.k8s.io/v1kind: MutatingWebhookConfiguration...webhooks:- admissionReviewVersions:\u00a0 - v1\u00a0 - v1beta1\u00a0 ...\u00a0 rules:\u00a0 - apiGroups:\u00a0 \u00a0 - core\u00a0 \u00a0 - batch\u00a0 \u00a0 - apps\u00a0 \u00a0 apiVersions:\u00a0 \u00a0 - '*'\u00a0 \u00a0 operations:\u00a0 \u00a0 - CREATE\u00a0 \u00a0 - UPDATE\u00a0 \u00a0 resources:\u00a0 \u00a0 - Pod\u00a0 \u00a0 - Deployment\u00a0 \u00a0 - Job\u00a0 \u00a0 - Volume\u00a0 \u00a0 - Container\u00a0 \u00a0 - StatefulSet\u00a0 \u00a0 - StorageClass\u00a0 \u00a0 - Secret\u00a0 \u00a0 - ConfigMap\u00a0 sideEffects: None\u00a0 timeoutSeconds: 1\n```\nApply the updated `gatekeeper.yaml` manifest to your Autopilot cluster to install Gatekeeper. This is required because, as a built-in security measure, Autopilot disallows wildcard characters in mutating admission webhooks.\n- Deploy the built-in Pod Security Policy Volume Types ConstraintTemplate:```\nkubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/pod-security-policy/volumes/template.yaml\n```\n- Save the following Constraint with a list of allowed volume types as `constraint.yaml` :```\napiVersion: constraints.gatekeeper.sh/v1beta1kind: k8sPSPVolumeTypesmetadata:\u00a0 name: nogcepersistentdiskspec:\u00a0 match:\u00a0 \u00a0 kinds:\u00a0 \u00a0 \u00a0 - apiGroups: [\"\"]\u00a0 \u00a0 \u00a0 \u00a0 kinds: [\"Pods\"]\u00a0 parameters:\u00a0 \u00a0 volumes: [\"configMap\", \"csi\", \"projected\", \"secret\", \"downwardAPI\", \"persistentVolumeClaim\", \"emptyDir\", \"nfs\", \"hostPath\"]\n```This constraint restricts volumes to the list in the `spec.parameters.volumes` field.\n- Deploy the constraint:```\nkubectl apply -f constraint.yaml\n```## Monitor your cluster configuration\n**You should audit your cluster configurations for deviations from your definedsettings.**\nMany of the recommendations covered in this hardening guide, as well as other common misconfigurations, can be automatically checked using [Security HealthAnalytics](/security-command-center/docs/list-of-security-health-analytics-findings#container_vulnerability_findings) .\n## Secure defaults\nThe following sections describe options that are securely configured by default in new clusters. You should verify that preexisting clusters are configured securely.\n### Protect node metadata\nThe `v0.1` and `v1beta1` Compute Engine metadata server endpoints were deprecated and shutdown on **September 30, 2020** . These endpoints did not enforce [metadata query headers](/compute/docs/storing-retrieving-metadata#querying) . For the shutdown schedule, refer to [v0.1 and v1beta1 metadata server endpoints deprecation](/compute/docs/deprecations/v0.1-v1beta1-metadata-server) .\nSome practical attacks against Kubernetes rely on access to the VM's metadata server to extract credentials. These attacks are blocked if you are using [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) or [MetadataConcealment](/kubernetes-engine/docs/how-to/protecting-cluster-metadata#concealment) .\n### Leave legacy client authentication methods disabled\n**Note:** Basic authentication is deprecated and has been removed in GKE 1.19 and later.\nThere are several [methods of authenticating](/kubernetes-engine/docs/how-to/api-server-authentication) to the Kubernetes API server. In GKE, the supported methods are service account bearer tokens, OAuth tokens, and x509 client certificates. GKE manages authentication with `gcloud` for you using the OAuth token method, setting up the Kubernetes configuration, getting an access token, and keeping it up to date.\nPrior to GKE's integration with OAuth, a one-time generated x509 certificate or static password were the only available authentication methods, but are now not recommended and should be disabled. These methods present a wider surface of attack for cluster compromise and have been disabled by default since GKE version 1.12. If you are using legacy authentication methods, we recommend that you turn them off. Authentication with a static password is deprecated and has been removed since GKE version 1.19.\n**Existing clusters should move to OAuth.** If a long-lived credential is needed by a system external to the cluster we recommend you create a Google service account or a Kubernetes service account with the necessary privileges and export the key.\nTo update an existing cluster and remove the static password, see [Disabling authentication with a static password](/kubernetes-engine/docs/how-to/api-server-authentication#disabling_authentication_with_a_static_password) .\nCurrently, there is no way to remove the pre-issued client certificate from an existing cluster, but it has no permissions if RBAC is enabled and ABAC is disabled.\n### Leave Cloud Logging enabled\n**To reduce operational overhead and to maintain a consolidated view of your logs,implement a logging strategy that is consistent wherever your clusters aredeployed.** GKE Enterprise clusters are integrated with Cloud Logging by default and that should remain configured.\n**Note:** GKE has Cloud Logging and Cloud Monitoring configured by default. For more information, refer to [Observability in GKE](/monitoring/kubernetes-engine) .\nAll GKE clusters have [Kubernetes auditlogging](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/) enabled by default, which keeps a chronological record of calls that have been made to the Kubernetes API server. Kubernetes audit log entries are useful for investigating suspicious API requests, for collecting statistics, or for creating monitoring alerts for unwanted API calls.\nGKE clusters integrate [Kubernetes Audit Logging with CloudAudit Logs](/kubernetes-engine/docs/how-to/audit-logging) and [Cloud Logging](/logging/docs) . Logs can be [routed from Cloud Logging](/logging/docs/routing/overview#sinks) to your own logging systems.\n### Leave the Kubernetes web UI (Dashboard) disabled\n**You should not enable the Kubernetes web UI (Dashboard) when running onGKE.**\n**Note:** By default, the Kubernetes web UI (Dashboard) does not have admin access and is disabled in GKE 1.10 and later. In 1.15 and later, the Kubernetes web UI add-on `KubernetesDashboard` is not supported as a managed add-on in GKE. If you still want to run the Kubernetes web UI, follow the [Kubernetes web UI documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) to install it yourself.\nThe Kubernetes web UI (Dashboard) is backed by a highly privileged Kubernetes Service Account. The [Google Cloud console](https://console.cloud.google.com/kubernetes/) provides much of the same functionality, so you don't need these permissions.\nTo disable the Kubernetes web UI:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --update-addons=KubernetesDashboard=DISABLED\n```\n### Leave ABAC disabled\n**You should disable Attribute-Based Access Control (ABAC), and instead useRole-Based Access Control (RBAC) in GKE.**\nBy default, ABAC is disabled for clusters created using GKE version 1.8 and later. In Kubernetes, [RBAC](/kubernetes-engine/docs/how-to/role-based-access-control) is used to grant permissions to resources at the cluster and namespace level. RBAC allows you to define roles with rules containing a set of permissions. RBAC has significant security advantages over ABAC.\nIf you're still relying on ABAC, first review the [Prerequisites for usingRBAC](/kubernetes-engine/docs/how-to/role-based-access-control#before_you_begin) . If you upgraded your cluster from an older version and are using ABAC, you should update your access controls configuration:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --no-enable-legacy-authorization\n```\nTo create a new cluster with the above recommendation:\n```\ngcloud container clusters create CLUSTER_NAME \\\n --no-enable-legacy-authorization\n```\n### Leave the DenyServiceExternalIPs admission controller enabled\n**Do not disable the DenyServiceExternalIPs admission controller.**\nThe [DenyServiceExternalIPs](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips) admission controller blocks Services from using ExternalIPs and mitigates a [known security vulnerability](/anthos/clusters/docs/security-bulletins#gcp-2020-015) .\nThe `DenyServiceExternalIPs` admission controller is enabled by default on new clusters created on GKE versions 1.21 and later. For clusters upgrading to GKE versions 1.21 and later, you can enable the admission controller using the following command:\n```\ngcloud beta container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --no-enable-service-externalips\n```\n## What's next\n- Learn more about [GKE security in the Security Overview](/kubernetes-engine/docs/concepts/security-overview) .\n- Make sure you understand the [GKE shared responsibilitymodel](/blog/products/containers-kubernetes/exploring-container-security-the-shared-responsibility-model-in-gke-container-security-shared-responsibility-model-gke) .\n- Understand how to apply the [CIS GKE Benchmark](/kubernetes-engine/docs/concepts/cis-benchmarks) to your cluster.\n- Learn more about [access control](/kubernetes-engine/docs/concepts/access-control) in GKE.\n- Read the [GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- Read the [GKE multi-tenancy overview](/kubernetes-engine/docs/concepts/multitenancy-overview) .", "guide": "Google Kubernetes Engine (GKE)"}