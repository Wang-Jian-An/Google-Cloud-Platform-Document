{"title": "Google Kubernetes Engine (GKE) - Modern CI/CD with GKE: Build a CI/CD system", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/modern-cicd-gke-reference-architecture", "abstract": "# Google Kubernetes Engine (GKE) - Modern CI/CD with GKE: Build a CI/CD system\nThis reference architecture provides you with a method and initial infrastructure to build a modern continuous integration/continuous delivery (CI/CD) system using tools such as [Google Kubernetes Engine](/gke) , [Cloud Build](/build) , [Skaffold](https://skaffold.dev) , [kustomize](https://github.com/kubernetes-sigs/kustomize#kustomize) , [Config Sync](/anthos-config-management/docs/config-sync-overview) , [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) , [Artifact Registry](/artifact-registry) , and [Cloud Deploy](/deploy) .\nThis document is part of a series:- [Modern CI/CD with GKE: A software delivery framework](/kubernetes-engine/docs/tutorials/modern-cicd-gke-user-guide) \n- Modern CI/CD with GKE: Build a CI/CD system (this document)\n- [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) \nThis document is intended for enterprise architects and application developers, as well as IT security, DevOps, and Site Reliability Engineering teams. Some experience with automated deployment tools and processes is useful for understanding the concepts in this document.", "content": "## CI/CD workflowTo build out a modern CI/CD system, you first need to choose tools and services that perform the main functions of the system. This reference architecture focuses on implementing the core functions of a CI/CD system that are shown in the following diagram:This reference implementation uses the following tools for each component:- For source code management: GitHub- Stores application and configuration code.\n- Lets you review changes.\n- For application configuration management:`kustomize`- Defines the intended configuration of an application.\n- Lets you reuse and extend configuration primitives or blueprints.\n- For continuous integration: Cloud Build- Tests and validates source code.\n- Builds artifacts that the deployment environment consumes.\n- For continuous delivery: Cloud Deploy- Defines the rollout process of code across environments.\n- Provides rollback for failed changes.\n- For the infrastructure configuration: Config Sync- Consistently applies the cluster and policy configuration.\n- For policy enforcement: Policy Controller- Provides a mechanism that you can use to define what is allowed to run in a given environment based on the policies of the organization.\n- For container orchestration: Google Kubernetes Engine- Runs the artifacts that are built during CI.\n- Provides scaling, health checking, and rollout methodologies for workloads.\n- For container artifacts: Artifact Registry- Stores the artifacts (container images) that are built during CI.## ArchitectureThis section describes the CI/CD components that you implement by using this reference architecture: infrastructure, pipelines, code repositories and landing zones.\nFor a general discussion of these aspects of the CI/CD system, see [Modern CI/CD with GKE: A software delivery framework](/kubernetes-engine/docs/tutorials/modern-cicd-gke-user-guide) .\n### Reference Architecture variantsThe reference architecture has two deployment models:- A multi-project variant that is more like a production deployment with improved isolation boundaries\n- A single-project variant, which is useful for demonstrations\n **Note:** The diagrams in this document are based on multi-project reference architecture. They're applicable to single-project reference architecture. The exception is that all the Google Cloud resources are created in a single Google Cloud project in single-project architecture.The [multi-project](https://github.com/GoogleCloudPlatform/software-delivery-blueprint) version of the reference architecture simulates production-like scenarios. In these scenarios, different personas create infrastructure, CI/CD pipelines, and applications with proper isolation boundaries. These personas or teams can only access required resources.\nFor more information, see [Modern CI/CD with GKE: A software delivery framework](/kubernetes-engine/docs/tutorials/modern-cicd-gke-user-guide#isolation_boundaries) .\nFor details on how to install and apply this version of the reference architecture, see the [software delivery blueprint](https://github.com/GoogleCloudPlatform/software-delivery-blueprint/blob/main/README.md) The [single-project](https://github.com/GoogleCloudPlatform/software-delivery-blueprint/tree/single-project-blueprint) version of the reference architecture demonstrates how to set up the entire software delivery platform in a single Google Cloud project. This version can help any users who don't have elevated IAM roles install and try the reference architecture with just the owner role on a project. This document demonstrates the single-project version of the reference architecture.\n### Platform infrastructureThe infrastructure for this reference architecture consists of Kubernetes clusters to support development, staging, and production application environments. The following diagram shows the logical layout of the clusters:\n### Code repositoriesUsing this reference architecture, you set up repositories for operators, developers, platform, and security engineers.\nThe following diagram shows the reference architecture implementation of the different code repositories and how the operations, development, and security teams interact with the repositories:In this workflow, your operators can manage best practices for CI/CD and application configuration in the operator repository. When your developers onboard applications in the development repository, they automatically get best practices, business logic for the application, and any specialized configuration necessary for their application to properly operate. Meanwhile, your operations and security team can manage the consistency and security of the platform in the configuration and policy repositories.\n### Application landing zonesIn this reference architecture, the landing zone for an application is created when the application is provisioned. In the next document in this series, [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) , you provision a new application that creates its own landing zone. The following diagram illustrates the important components of the landing zones used in this reference architecture:Each namespace includes a service account that is used for workload identity federation for GKE to access services outside of the Kubernetes container, like a Cloud Storage or Spanner. The namespace also includes other resources like network policies to isolate or share boundaries with other namespaces or applications.\nThe namespace is created by the CD execution service account. We recommend that teams follow the principle of least privilege to help ensure that a CD execution service account can only access required namespaces.\nYou can define service account access in Config Sync and implement it by using Kubernetes role-based access control (RBAC) roles and role bindings. With this model in place, teams can deploy any resources directly into the namespaces they manage but are prevented from overwriting or deleting resources from other namespaces.\n## Objectives\n- Deploy the single-project reference architecture.\n- Explore the code repositories.\n- Explore the pipeline and infrastructure.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Google Kubernetes Engine (GKE)](/kubernetes-engine/pricing) \n- [Google Kubernetes Engine (GKE) Enterprise edition](/kubernetes-engine/pricing) for Config Sync and Policy Controller\n- [Cloud Build](/build/pricing) \n- [Artifact Registry](/artifact-registry/pricing) \n- [Cloud Deploy](/deploy/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Deploy the reference architecture\n- In Cloud Shell, set the project:```\ngcloud config set core/project PROJECT_ID\n```Replace `` with your Google Cloud project ID.\n- In Cloud Shell, clone the Git repository:```\ngit clone https://github.com/GoogleCloudPlatform/software-delivery-blueprint.gitcd software-delivery-blueprint/launch-scriptsgit checkout single-project-blueprint\n```\n- Create a [personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic) in GitHub with the following scopes:- `repo`\n- `delete_repo`\n- `admin:org`\n- `admin:repo_hook`\n- There is an empty file named `vars.sh` under `software-delivery-bluprint/launch-scripts` folder. Add the following content to the file:```\ncat << EOF >vars.shexport INFRA_SETUP_REPO=\"gke-infrastructure-repo\"export APP_SETUP_REPO=\"application-factory-repo\"export GITHUB_USER=GITHUB_USERexport TOKEN=TOKENexport GITHUB_ORG=GITHUB_ORGexport REGION=\"us-central1\"export SEC_REGION=\"us-west1\"export TRIGGER_TYPE=\"webhook\"EOF\n```Replace `` with the GitHub username.Replace `` with the GitHub personal access token.Replace `` with the name of the GitHub organization.\n- Run the `bootstrap.sh` script. If Cloud Shell prompts you for authorization, click **Authorize** :```\n./bootstrap.sh\n```The script bootstraps the software delivery platform.\n## Explore the code repositoriesIn this section, you explore the code repositories.\n### Sign in to GitHub\n- In a web browser, go to [github.com](https://github.com/) and sign in to your account.\n- Click the picture icon at the top of the interface.\n- Click **Your organizations** .\n- Choose the organization that you provided as input in the`vars.sh`file.\n- Click the **Repositories** tab.\n### Explore the starter, operator, configuration, and infrastructure repositoriesThe starter, operator, configuration, and infrastructure repositories are where operators and platform administrators define the common best practices for building on and operating the platform. These repositories are created under your GitHub organization when the reference architecture is bootstrapped.\n Starter repositories aid the adoption of CI/CD, infrastructure, and development best practices across the platform. For more information, see [Modern CI/CD with GKE: A software delivery framework](/kubernetes-engine/docs/tutorials/modern-cicd-gke-user-guide#starter_repositories) In the application starter repositories, your operators can codify and document best practices such as CI/CD, metrics collection, logging, container steps, and security for applications. Included in the reference architecture are examples of starter repositories for Go, Python, and Java applications.\nThe `app-template-python` , `app-template-java` and `app-template-golang` application starter repositories contain [boilerplate code](https://wikipedia.org/wiki/Boilerplate_code) that you can use to create new applications. In addition to creating new applications, you can create new templates based on application requirements. The application starter repositories provided by the reference architecture contain:- `kustomize` base and patches under the folder `k8s` .\n- Application source code.\n- A `Dockerfile` that describes how to build and run the application.\n- A `cloudbuild.yaml` file that describes the best practices for CI steps.\n- A `skaffold.yaml` file that describes the deployment steps.\nIn the next document in this series, [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) , you use the `app-template-python` repository to create a new application.In the infrastructure starter repositories, your operators and infrastructure administrators can codify and document best practices such as, CI/CD pipelines, IaC, metrics collection, logging, and security for infrastructure. Included in the reference architecture are examples of infrastructure starter repositories using Terraform. The `infra-template` infrastructure starter repository contains boilerplate code for Terraform that you can use to create the infrastructure resources that an application requires, like Cloud Storage bucket, or Spanner database, or others.In shared template repostories, infrastructure administrators and operators provide standard templates to perform tasks. There is a repository named `terraform-modules` provided with the reference architecture. The repository includes templated Terraform code to create various infrastructure resources.In the reference architecture, the operator repositories are the same as the application starter repositories. The operators manage the files required for both CI and CD in the application starter repositories. The reference architecture includes the `app-template-python` , `app-template-java` , and `app-template-golang` repositories.- These are starter templates and contain the base Kubernetes manifests for the applications running in Kubernetes on the platform. Operators can update the manifests in the starter templates as needed. Updates are picked up when an application is created.\n- The`cloudbuild.yaml`and`skaffold.yaml`files in these repositories store the best practices for running CI and CD respectively on the platform. Similar to the application configurations, operators can update and add steps to the best practices. Individual application pipelines are created using the latest steps.\nIn this reference implementation, operators use [kustomize](https://github.com/kubernetes-sigs/kustomize) to manage base configurations in the `k8s` folder of the starter repositories. Developers are then free to extend the manifests with application-specific changes such as resource names and configuration files. The `kustomize` tool supports configuration as data. With this methodology, `kustomize` inputs and outputs are Kubernetes resources. You can use the outputs from one modification of the manifests for another modification.\nThe following diagram illustrates a base configuration for a Spring Boot application:The configuration as data model in `kustomize` has a major benefit: when operators update the base configuration, the updates are automatically consumed by the developer's deployment pipeline on its next run without any changes on the developer's end.\nFor more information about using `kustomize` to manage Kubernetes manifests, see the [kustomize documentation](https://kubectl.docs.kubernetes.io/references/kustomize/) .Included in the reference architecture is an implementation of a configuration and policy repository that uses Config Sync and Policy Controller. The `acm-gke-infrastructure-repo` repository contains the configuration and policies that you deploy across the application environment clusters. The configuration defined and stored by platform admins in these repositories is important to ensuring the platform has a consistent look and feel to the operations and development teams.\nThe following sections discuss how the reference architecture implements configuration and policy repositories in more detail.In this reference implementation, you use [Config Sync](/anthos-config-management) to centrally manage the configuration of clusters in the platform and enforce policies. Centralized management lets you propagate configuration changes throughout the system.\nUsing Config Sync, your organization can register its clusters to sync their configuration from [a Git repository](/anthos-config-management/docs/concepts/repo) , a process known as . When you add new clusters, the clusters automatically sync to the latest configuration and continually reconcile the state of the cluster with the configuration in case anyone introduces out-of-band changes.\nFor more information about Config Sync, see its [documentation](/anthos-config-management/docs/config-sync-overview) .In this reference implementation, you use Policy Controller, which is based on [Open Policy Agent](https://www.openpolicyagent.org/) , to intercept and validate each request to the Kubernetes clusters in the platform. You can create policies by using the [Rego policy language](https://www.openpolicyagent.org/docs/latest/policy-language/) , which lets you fully control not only the types of resources submitted to the cluster but also their configuration.\nThe architecture in the following diagram shows a request flow for using Policy Controller to create a resource:You create and define rules in the Config Sync repository, and these changes are applied to the cluster. After that, new resource requests from either the CLI or API clients are validated against the constraints by the Policy Controller.\nFor more information about managing policies, see the [Policy Controller overview](/anthos-config-management/docs/concepts/policy-controller) .Included in the reference is an implementation of infrastructure repository using [Terraform](https://www.terraform.io/) . The `gke-infrastructure-repo` repository contains infrastructure as code to create GKE clusters for dev, staging, and production environments and configure Config Sync on them using the `acm-gke-infrastructure-repo` repository. `gke-infrastructure-repo` contains three branches, one for each dev, staging, and production environment. It also contains dev, staging, and production folders on each branch.## Explore the pipeline and infrastructureThe reference architecture creates a pipeline in the Google Cloud project. This pipeline is responsible for creating the shared infrastructure.\n### PipelineIn this section, you explore the infrastructure-as-code pipeline and run it to create the shared infrastructure including GKE clusters. The pipeline is a Cloud Build trigger named `create-infra` in the Google Cloud project that is linked to the infrastructure repository `gke-infrastructure-repo` . You follow GitOps methodology to create infrastructure as explained in the [Repeatable GCP Environments at Scale With Cloud Build Infra-As-Code Pipelines video](https://www.youtube.com/watch?v=3vfXQxWJazM) .\n`gke-infrastructure-repo` has dev, staging, and production branches. In the repository, there are also dev, staging, and production folders that correspond to these branches. There are branch protection rules on the repository ensuring that the code can only be pushed to the dev branch. To push the code to the staging and production branches you need to create a [pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) .\nTypically, someone who has access to the repository reviews the changes and then merges the pull request to make sure only the intended changes are being promoted to the higher branch. To let the individuals try out the blueprint, the branch protection rules have been relaxed in the reference architecture so that the repository administrator is be able to bypass the review and merge the pull request.\nWhen a push is made to `gke-infrastructure-repo` , it invokes the `create-infra` trigger. That trigger identifies the branch where the push happened and goes to the corresponding folder in the repository on that branch. Once it finds the corresponding folder, it runs Terraform using the files the folder contains. For example, if the code is pushed to the dev branch, the trigger runs Terraform on the dev folder of the dev branch to create a dev GKE cluster. Similarly, when a push happens to the `staging` branch, the trigger runs Terraform on the staging folder of the staging branch to create a staging GKE cluster.\nRun the pipeline to create GKE clusters:- In the Google Cloud console, go to the Cloud Build page. [Go to the Cloud Build page](https://console.cloud.google.com/cloud-build/triggers) - There are five Cloud Build webhook triggers. Look for the trigger with the name`create-infra`. This trigger creates the shared infrastructure including GKE clusters.\n- Click the trigger name. The trigger definition opens.\n- Click **OPEN EDITOR** to view the steps that the trigger runs.The other triggers are used when you onboard an application in [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) \n- In the Google Cloud console, go to the Cloud Build page. [Go to the Cloud Build history page](https://console.cloud.google.com/cloud-build/builds) Review the pipeline present on the history page. When you deployed software delivery platform using `bootstrap.sh` , the script pushed the code to the dev branch of the `gke-infrastructure-repo` repository that kicked-off this pipeline and created the dev GKE cluster. **Note:** If you don't see a pipeline already in Cloud Build history page, it means the webhook on `gke-infrastructure-repo` didn't invoke the Cloud Build trigger. This is rare but can happen if there are any issues going on with GitHub APIs. Go ahead and make a commit on the dev branch of the `gke-infrastructure-repo` repository when the issues are resolved and the webhook should invoke the Cloud Build trigger. Now, you should see a pipeline in Cloud Build history page. This pipeline creates dev GKE cluster.\n- To create a staging GKE cluster, submit a pull request from the dev branch to the staging branch:- Go to GitHub and navigate to the repository `gke-infrastructure-repo` .\n- Click **Pull requests** and then **New pull request** .\n- In the **Base** menu, choose **staging** and in the **Compare** menu, choose **dev** .\n- Click **Create pull request** .\n- If you are an administrator on the repository, merge the pull request. Otherwise, get the administrator to merge the pull request.\n- In the Google Cloud console, go to the **Cloud Build history page** . [Go to the Cloud Build history page](https://console.cloud.google.com/cloud-build/builds) A second Cloud Build pipeline starts in the project. This pipeline creates the staging GKE cluster.\n- To create prod GKE clusters, submit a `pull request` from staging to prod branch:- Go to GitHub and navigate to the repository `gke-infrastructure-repo` .\n- Click **Pull requests** and then **New pull request** .\n- In the **Base** menu, choose **prod** and in the **Compare** menu, choose **staging** .\n- Click **Create pull request** .\n- If you are an administrator on the repository, merge the pull request. Otherwise, get the administrator to merge the pull request.\n- In the Google Cloud console, go to the **Cloud Build history page** . [Go to the Cloud Build history page](https://console.cloud.google.com/cloud-build/builds) A third Cloud Build pipeline starts in the project. This pipeline creates the production GKE cluster.\n### InfrastructureIn this section, you explore the infrastructure that was created by the pipelines.- In the Google Cloud console, go to the **Kubernetes clusters** page. [Go to the Kubernetes clusters page](https://console.cloud.google.com/kubernetes/list) This page lists the clusters that are used for development ( `gke-dev-us-central1` ), staging ( `gke-staging-us-central1` ), and production ( `gke-prod-us-central1` , `gke-prod-us-west1` ): \nThe development cluster ( `gke-dev-us-central1` ) gives your developers access to a namespace that they can use to iterate on their applications. We recommend that teams use tools like [Skaffold](https://skaffold.dev/) that provide an iterative workflow by actively monitoring the code in development and reapplying it to the development environments as changes are made. This iteration loop is similar to [hot reloading](https://reactnative.dev/blog/2016/03/24/introducing-hot-reloading#hot-reloading) . Instead of being programming language-specific, however, the loop works with any application that you can build with a Docker image. You can run the loop inside a Kubernetes cluster.\nAlternatively, your developers can follow the CI/CD loop for a development environment. That loop makes the code changes ready for promotion to higher environments.\nIn the next document in this series, [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) , you use both Skaffold and CI/CD to create the development loop.This cluster runs the staging environment of your applications. In this reference architecture, you create one GKE cluster for staging. Typically, a staging environment is an exact replica of the production environment.In the reference architecture, you have two GKE clusters for your production environments. For geo-redundancy or high-availability (HA) systems, we recommend that you add multiple clusters to each environment. For all clusters where applications are deployed, it's ideal to use [regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) . This approach insulates your applications from zone-level failures and any interruptions caused by cluster or node pool upgrades.\nTo sync the configuration of cluster resources, such as namespaces, quotas, and RBAC, we recommend that you use Config Sync. For more information on how to manage those resources, see [Configuration and policy repositories](#configuration_and_policy_repositories) .## Apply the reference architectureNow that you've explored the reference architecture, you can explore a developer workflow that is based on this implementation. In the next document in this series, [Modern CI/CD with GKE: Apply the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) , you create a new application, add a feature, and then deploy the application to the staging and production environments.## Clean upIf you want to try the next document in this series, [Modern CI/CD with GKE: Applying the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) , don't delete the project or resources associated with this reference architecture. Otherwise, to avoid incurring charges to your Google Cloud account for the resources that you used in the reference architecture, you can delete the project or manually remove the resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Manually remove the resources\n- In Cloud Shell, remove the infrastructure:```\n\u00a0 gcloud container clusters delete gke-dev-us-central1\u00a0 gcloud container clusters delete gke-staging-us-central1\u00a0 gcloud container clusters delete gke-prod-us-central1\u00a0 gcloud container clusters delete gke-prod-us-west1\u00a0 gcloud beta builds triggers delete create-infra\u00a0 gcloud beta builds triggers delete add-team-files\u00a0 gcloud beta builds triggers delete create-app\u00a0 gcloud beta builds triggers delete tf-plan\u00a0 gcloud beta builds triggers delete tf-apply\n```\n## What's next\n- Create a new application by following the steps in [Modern CI/CD with GKE: Applying the developer workflow](/kubernetes-engine/docs/tutorials/modern-cicd-gke-developer-workflow) .\n- Learn about [best practices for setting up identity federation](/solutions/migrating-consumer-accounts-to-cloud-identity-or-g-suite-best-practices-federation) .\n- Read [Kubernetes and the challenges of continuous software deployment](/solutions/addressing-continuous-delivery-challenges-in-a-kubernetes-world) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}