{"title": "Google Kubernetes Engine (GKE) - Serve an LLM on L4 GPUs with Ray", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/serve-llm-l4-ray", "abstract": "# Google Kubernetes Engine (GKE) - Serve an LLM on L4 GPUs with Ray\nThis guide demonstrates how to serve large language models (LLM) with the Ray framework in Google Kubernetes Engine (GKE) mode. This guide is intended for MLOps or DevOps engineers or platform administrator that want to use GKE orchestration capabilities for serving LLMs.\nIn this guide, you can serve any of the following models:\n- [Falcon 7b](https://huggingface.co/tiiuae/falcon-7b-instruct) \n- [Llama2 7b](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) \n- [Falcon 40b](https://huggingface.co/tiiuae/falcon-40b-instruct) \n- [Llama2 70b](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) \nBefore you complete the following steps in GKE, we recommend that you learn [About GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) .\n#", "content": "## Background\nThe [Ray framework](https://www.ray.io/) provides an end-to-end AI/ML platform for training, fine-training, and inferencing of ML workloads. Depending on the data format of the model, the number of GPUs varies. In this guide, each model uses two L4 GPUs. To learn more, see [Calculating the amount of GPUs](#calculate-gpus) .\nThis guide covers the following steps:\n- Create an Autopilot or Standard GKE cluster.\n- Deploy the [KubeRay operator](https://github.com/ray-project/kuberay) .\n- Deploy RayService custom resources to serve LLMs.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- If you want to use the Llama 2 model, ensure that you have the following:- Access to an active license for the [Meta Llama models](https://huggingface.co/meta-llama/Llama-2-7b-hf) .\n- A [HuggingFace token](https://huggingface.co/docs/hub/security-tokens) .\n **Warning:** Getting access and approval for the Llama model might take up to three days.\n- Ensure that you have GPU quota in the `us-central1` region. To learn more, see [GPU quota](/kubernetes-engine/docs/concepts/gpus#gpu-quota) .## Prepare your environment\n- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Clone the sample repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples.gitcd kubernetes-engine-samples/ai-ml/gke-rayexport TUTORIAL_HOME=`pwd`\n```This repository includes the prebuilt `ray-llm` container image that models that provisions different accelerator types. For this guide, you use NVIDIA L4 GPUs, so the `spec.serveConfigV2` in RayService points to a repository that contains models that uses the L4 accelerator type.\n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=us-central1\n```Replace the with your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) . **Note:** If your Cloud Shell instance disconnects throughout the guide execution, repeat the preceding step.## Create a cluster and a GPU node pool\nYou can serve an LLM on L4 GPUs with Ray in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience, or a Standard cluster if your use case requires high scalability or if you want more control over cluster configuration. To choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\nUse Cloud Shell to do the following:\n- Navigate to the `gke-platform` folder:```\ncd ${TUTORIAL_HOME}/gke-platform\n```- For an Autopilot cluster, run the following command:\n```\ncat << EOF > terraform.tfvarsenable_autopilot=trueproject_id=\"${PROJECT_ID}\"EOF\n```- For a Standard cluster, run the following command:\n```\ncat << EOF > terraform.tfvarsproject_id=\"${PROJECT_ID}\"gpu_pool_machine_type=\"g2-standard-24\"gpu_pool_accelerator_type=\"nvidia-l4\"gpu_pool_node_locations=[\"us-central1-a\", \"us-central1-c\"]EOF\n```\n- Deploy the GKE cluster and node pool:```\nterraform initterraform apply --auto-approve\n```As Terraform initializes, it logs progress messages. At the end of the message output, you should see a message that Terraform initialized successfully.Once completed, the Terraform manifests deploy the following components:- GKE cluster\n- CPU node pool\n- GPU node pool\n- KubeRay operator with Ray CustomResourceDefinitions (CRDs)\n- Fetch the provisioned cluster credentials to be used by `kubectl` in the next section of the guide:```\ngcloud container clusters get-credentials ml-cluster --region us-central1\n```\n- Navigate to the `rayserve` folder:```\ncd ${TUTORIAL_HOME}/rayserve\n```## Deploy the LLM model\nIn the cloned repository, the `models` folder includes the configuration that loads the models. For [ray-llm](https://github.com/ray-project/ray-llm/tree/master/models) , the configuration for each model is composed of the following:\n- Deployment: The Ray Serve configuration\n- Engine: The Huggingface model, model parameters, prompt details\n- Scaling: The definition of the Ray resources that the model consumes\n- The specific configurations per model\nIn this guide, you use [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) of 4-bit NormalFloat (NF4), through the HuggingFace transformers, to load LLMs with a reduced GPU memory footprint (two L4 GPUs, which means 48GB GPU memory total). The reduction from 16-bit to 4-bit lowers precision of the weights of the model, but provides flexibility that lets you to test larger models and see if it is sufficient for your use case. For quantization, the sample code uses the HuggingFace and BitsAndBytesConfig libraries to load the quantized versions of larger parameter models, Falcon 40b and Llama2 70b.\nThe following section shows how to set up your workload depending on the model you want to use:\n**Note:** Alternatively, you can build an image as part of the pipeline and include your models.\n- Deploy the RayService and dependencies. Use the command that corresponds to the GKE mode that you created:- Autopilot:\n```\nkubectl apply -f models/falcon-7b-instruct.yamlkubectl apply -f ap_pvc-rayservice.yamlkubectl apply -f ap_falcon-7b.yaml\n```- Standard:\n```\nkubectl apply -f models/falcon-7b-instruct.yamlkubectl apply -f falcon-7b.yaml\n```The creation of the Ray cluster Pod might take several minutes to reach the `Running` state.\n- Wait for the Ray cluster head Pod to be up and running.```\nwatch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl get pod | \\\u00a0 \u00a0 GREP_COLOR='01;92' egrep --color=always -e '^' -e 'Running'\"\n```\n- After the Ray cluster Pod is running, you can verify the status of the model:```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head \\\u00a0 \u00a0 -n default \\\u00a0 \u00a0 -o custom-columns=POD:metadata.name --no-headers)watch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl exec -n default -it $HEAD_POD \\\u00a0 \u00a0 -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'\"\n```The output is similar to the following:```\nproxies:\n 781dc714269818b9b8d944176818b683c00d222d2812a2cc99a33ec6: HEALTHY\n bb9aa9f4bb3e721d7e33e8d21a420eb33c9d44e631ba7d544e23396d: HEALTHY\napplications:\n ray-llm:\n status: RUNNING\n message: ''\n last_deployed_time_s: 1702333577.390653\n deployments:\n  VLLMDeployment:tiiuae--falcon-7b-instruct:\n  status: HEALTHY\n  replica_states:\n   RUNNING: 1\n  message: ''\n  Router:\n  status: HEALTHY\n  replica_states:\n   RUNNING: 2\n  message: ''\n```If the **Status** field is `RUNNING` , then your LLM is ready to chat.\n- Set the default environment variables:```\nexport HF_TOKEN=HUGGING_FACE_TOKEN\n```Replace the `` with your HuggingFace token.\n- Create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) for the HuggingFace token:```\nkubectl create secret generic hf-secret \\\u00a0 \u00a0 --from-literal=hf_api_token=${HF_TOKEN} \\\u00a0 \u00a0 --dry-run=client -o yaml | kubectl apply -f ```\n- Deploy the RayService and dependencies. Use the command that corresponds to the GKE mode that you created:- Autopilot:\n```\nkubectl apply -f models/llama2-7b-chat-hf.yamlkubectl apply -f ap_pvc-rayservice.yamlkubectl apply -f ap_llama2-7b.yaml\n``` **Note:** The GKE Autopilot cluster uses a PersistentVolumeClaim mounted on the Ray worker to persist the model.- Standard:\n```\nkubectl apply -f models/llama2-7b-chat-hf.yamlkubectl apply -f llama2-7b.yaml\n```The creation of the Ray cluster Pod might take several minutes to reach the `Running` state.\n- Wait for the Ray cluster head Pod to be up and running.```\nwatch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl get pod | \\\u00a0 \u00a0 GREP_COLOR='01;92' egrep --color=always -e '^' -e 'Running'\"\n```\n- After the Ray cluster Pod is running, you can verify the status of the model:```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head \\\u00a0 \u00a0 -n default \\\u00a0 \u00a0 -o custom-columns=POD:metadata.name --no-headers)watch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl exec -n default -it $HEAD_POD \\\u00a0 \u00a0 -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'\"\n```The output is similar to the following:```\n proxies:\n 0eb0eb51d667a359b426b825c61f6a9afbbd4e87c99179a6aaf4f833: HEALTHY\n 3a4547b89a8038d5dc6bfd9176d8a13c5ef57e0e67e117f06577e380: HEALTHY\n applications:\n ray-llm:\n  status: RUNNING\n  message: ''\n  last_deployed_time_s: 1702334447.9163773\n  deployments:\n  VLLMDeployment:meta-llama--Llama-2-7b-chat-hf:\n   status: HEALTHYG\n   replica_states:\n   RUNNING: 11\n   message: ''p\n  Router:y\n   status: HEALTHY\n   replica_states:\n   RUNNING: 2T\n   message: ''t\n```If the **Status** field is `RUNNING` , then your LLM is ready to chat.\n- Deploy the RayService and dependencies. Use the command that corresponds to the GKE mode that you created:- Autopilot:\n```\nkubectl apply -f models/quantized-model.yamlkubectl apply -f ap_pvc-rayservice.yamlkubectl apply -f ap_falcon-40b.yaml\n```- Standard:\n```\nkubectl apply -f models/quantized-model.yamlkubectl apply -f falcon-40b.yaml\n```The creation of the Ray cluster Pod might take several minutes to reach the `Running` state.\n- Wait for the Ray cluster head Pod to be up and running.```\nwatch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl get pod | \\\u00a0 \u00a0 GREP_COLOR='01;92' egrep --color=always -e '^' -e 'Running'\"\n```\n- After the Ray cluster Pod is running, you can verify the status of the model:```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head \\\u00a0 \u00a0 -n default \\\u00a0 \u00a0 -o custom-columns=POD:metadata.name --no-headers)watch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl exec -n default -it $HEAD_POD \\\u00a0 \u00a0 -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'\"\n```The output is similar to the following:```\nproxies:\n d9fdd5ac0d81e8eeb1eb6efb22bcd1c4544ad17422d1b69b94b51367: HEALTHY\n 9f75f681caf33e7c496ce69979b8a56f3b2b00c9a22e73c4606385f4: HEALTHY\napplications:\n falcon:s\n status: RUNNING\n message: ''e\n last_deployed_time_s: 1702334848.336201\n deployments:\n  Chat:t\n  status: HEALTHYG\n  replica_states:\n   RUNNING: 11\n  message: ''p\n```If the **Status** field is `RUNNING` , then your LLM is ready to chat.\n- Set the default environment variables:```\nexport HF_TOKEN=HUGGING_FACE_TOKEN\n```Replace the `` with your HuggingFace token.\n- Create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) for the HuggingFace token:```\nkubectl create secret generic hf-secret \\\u00a0 \u00a0 --from-literal=hf_api_token=${HF_TOKEN} \\\u00a0 \u00a0 --dry-run=client -o yaml | kubectl apply -f ```\n- Deploy the RayService and dependencies. Use the command that corresponds to the GKE mode that you created:- Autopilot:\n```\nkubectl apply -f models/quantized-model.yamlkubectl apply -f ap_pvc-rayservice.yamlkubectl apply -f ap_llama2-70b.yaml\n```- Standard:\n```\nkubectl apply -f models/quantized-model.yamlkubectl apply -f llama2-70b.yaml\n```The creation of the Ray cluster Pod might take several minutes to reach the `Running` state.\n- Wait for the Ray cluster head Pod to be up and running.```\nwatch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl get pod | \\\u00a0 \u00a0 GREP_COLOR='01;92' egrep --color=always -e '^' -e 'Running'\"\n```\n- After the Ray cluster Pod is running, you can verify the status of the model:```\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head \\\u00a0 \u00a0 -n default \\\u00a0 \u00a0 -o custom-columns=POD:metadata.name --no-headers)watch --color --interval 5 --no-title \\\u00a0 \u00a0 \"kubectl exec -n default -it $HEAD_POD \\\u00a0 \u00a0 -- serve status | GREP_COLOR='01;92' egrep --color=always -e '^' -e 'RUNNING'\"\n```The output is similar to the following:```\nproxies:\n a71407ddfeb662465db384e0f880a2d3ad9ed285c7b9946b55ae27b5: HEALTHY\n <!-- dd5d4475ac3f5037cd49f1bddc7cfcaa88e4251b25c8784d0ac53c7c: HEALTHY -->\napplications:\n llama-2:\n status: RUNNING\n message: ''\n last_deployed_time_s: 1702335974.8497846\n deployments:\n  Chat:\n  status: HEALTHY\n  replica_states:\n   RUNNING: 1\n  message: ''\n```If the **Status** field is `RUNNING` , then your LLM is ready to chat.## Chat with your model\nFor the Falcon 7b and Llama2 7b models, `ray-llm` implements the [OpenAI API chat spec](https://platform.openai.com/docs/guides/text-generation) . The Falcon 40b and Llama2 70b models use `ray-llm` and only support text generation.\n- Set up port forwarding to the inferencing server:```\nkubectl port-forward service/rayllm-serve-svc 8000:8000\n```The output is similar to the following:```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n- In a new terminal session, use `curl` to chat with your model:```\ncurl http://localhost:8000/v1/chat/completions \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 -d '{\u00a0 \u00a0 \u00a0 \"model\": \"tiiuae/falcon-7b-instruct\",\u00a0 \u00a0 \u00a0 \"messages\": [\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages? Please be brief.\"}\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"temperature\": 0.7\u00a0 \u00a0 }'\n```\n- Set up port forwarding to the inferencing server:```\nkubectl port-forward service/rayllm-serve-svc 8000:8000\n```The output is similar to the following:```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n- In a new terminal session, use `curl` to chat with your model:```\ncurl http://localhost:8000/v1/chat/completions \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d '{\u00a0 \u00a0 \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\u00a0 \u00a0 \"messages\": [\u00a0 \u00a0 \u00a0 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u00a0 \u00a0 \u00a0 {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages? Please be brief.\"}\u00a0 \u00a0 ],\u00a0 \u00a0 \"temperature\": 0.7\u00a0 }'\n```\n- Set up port forwarding to the inferencing server:```\nkubectl port-forward service/rayllm-serve-svc 8000:8000\n```The output is similar to the following:```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n- In a new terminal session, use `curl` to chat with your model:```\ncurl -X POST http://localhost:8000/ \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 -d '{\"text\": \"What are the top 5 most popular programming languages? Please be brief.\"}'\n```\n- Set up port forwarding to the inferencing server:```\nkubectl port-forward service/rayllm-serve-svc 8000:8000\n```The output is similar to the following:```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n- In a new terminal session, use `curl` to chat with your model:```\ncurl -X POST http://localhost:8000/ \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 -d '{\"text\": \"What are the top 5 most popular programming languages? Please be brief.\"}'\n```\n**Success:** At this point, you have deployed an LLM using L4 GPUs in GKE.\n## Create a dialogue with the model\nThe models that you served don't retain any history, so each message and reply must be sent back to the model in order to create the illusion of dialogue. This interaction increases the amount of tokens that you use. To create a single interaction, create a dialogue with your model. You can create a dialogue when using Falcon 7b or Llama2 7b:\n- Create a dialogue with the model using `curl` :```\ncurl http://localhost:8000/v1/chat/completions \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 -d '{\u00a0 \u00a0 \u00a0 \"model\": \"tiiuae/falcon-7b-instruct\",\u00a0 \u00a0 \u00a0 \"messages\": [\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"user\", \"content\": \"What are the top 5 most popular programming languages? Please be brief.\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"assistant\", \"content\": \" \\n1. Java\\n2. Python\\n3. C++\\n4. C#\\n5. JavaScript\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"role\": \"user\", \"content\": \"Can you give me a brief description?\"}\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"temperature\": 0.7}'\n```The output is similar to the following:```\n{\n \"id\": \"tiiuae/falcon-7b-instruct-f7ff36764b4ec5906b5e54858588f17e\",\n \"object\": \"text_completion\",\n \"created\": 1702334177,\n \"model\": \"tiiuae/falcon-7b-instruct\",\n \"choices\": [ {\n  \"message\": {\n  \"role\": \"assistant\", \"content\": \" </s><s>1. Java - a popular\n  programming language used for object-oriented programming and web\n  applications.</s><s>2. Python - an interpreted, high-level\n  programming language used for general-purpose\n  programming.</s><s>3. C++ - a popular programming language used in\n  developing operating systems and applications.</s><s>4. C# - a\n  popular programming language used for developing Windows-based\n  applications.</s><s>5. JavaScript - a popular programming language\n  used for developing dynamic, interactive web applications.</s></s>\n  \\nWhich of the top 5 programming languages are the most commonly\n  used for developing mobile applications?</s><s>1. Java</s><s>2.\n  C++</s><s>3. C#</s><s>4. Objective-C</s><s>5. Swift (for iOS\n  development)</s>\"\n  },\n  \"index\": 0,\n  \"finish_reason\": \"stop\"\n }\n ],\n \"usage\": {\n \"prompt_tokens\": 65,\n \"completion_tokens\": 191,\n \"total_tokens\": 256\n }\n}\n```\n- Create a dialogue with the model using `curl` :```\ncurl http://localhost:8000/v1/chat/completions \\-H \"Content-Type: application/json\" \\-d '{\u00a0 \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\u00a0 \"messages\": [\u00a0 \u00a0 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\u00a0 \u00a0 {\"role\": \"user\", \"content\": \"What are the top 5 most popular\u00a0 \u00a0 programming languages? Please be brief.\"},\u00a0 \u00a0 {\"role\": \"assistant\", \"content\": \" Of course! Here are the top 5 most\u00a0 \u00a0 popular programming languages, based on various sources and\u00a0 \u00a0 metrics:\\n\\n1. JavaScript: Used for web development, game development,\u00a0 \u00a0 and mobile app development.\\n2. Python: General-purpose language used\u00a0 \u00a0 for web development, data analysis, machine learning, and more.\\n3.\u00a0 \u00a0 Java: Object-oriented language used for Android app development, web\u00a0 \u00a0 development, and enterprise software development.\\n4. C++:\u00a0 \u00a0 High-performance language used for systems programming, game\u00a0 \u00a0 development, and high-performance computing.\\n5. C#:\u00a0 \u00a0 Microsoft-developed language used for Windows app development, web\u00a0 \u00a0 development, and enterprise software development.\\n\\nI hope this\u00a0 \u00a0 helps! Let me know if you have any other questions.\"},\u00a0 \u00a0 {\"role\": \"user\", \"content\": \"Can you just list it instead?\"}\u00a0 ],\u00a0 \"temperature\": 0.7}'\n```The output is similar to the following:```\n{\n \"id\": \"meta-llama/Llama-2-7b-chat-hf-940d3bdda1e39920760e286dfdd0b9d7\",\n \"object\": \"text_completion\",\n \"created\": 1696460007,\n \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n \"choices\": [ {\n  \"message\": {\n  \"role\": \"assistant\", \"content\": \" Of course! Here are the top 5\n  most popular programming languages, based on various sources and\n  metrics:\\n1. JavaScript\\n2. Python\\n3. Java\\n4. C++\\n5. C#\\n\\nI\n  hope this helps! Let me know if you have any other questions.\"\n  },\n  \"index\": 0,\n  \"finish_reason\": \"stop\"\n }\n ],\n \"usage\": {\n \"prompt_tokens\": 220,\n \"completion_tokens\": 61,\n \"total_tokens\": 281\n }\n}\n```\n### Deploy a chat interface\nOptionally, you can use [Gradio](https://www.gradio.app/docs/interface) to build a web application that lets you interact with your model. Gradio is a Python library that has a ChatInterface wrapper that creates user interfaces for chatbots.\n- Open the `gradio.yaml` manifest: [  ai-ml/gke-ray/rayserve/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/gke-ray/rayserve/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/gke-ray/rayserve/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"meta-llama/Llama-2-7b-chat-hf\"\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/v1/chat/completions\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://rayllm-serve-svc:8000\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradiospec:\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 80\u00a0 \u00a0 \u00a0 targetPort: 7860\u00a0 type: LoadBalancer\n```\n- Replace the `value` assigned to the `MODEL_ID` with the `tiiuae/falcon-7b-instruct` value:```\n...- name: MODEL_ID\u00a0 value: \"tiiuae/falcon-7b-instruct\"\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Find the external IP address of the Service:```\nEXTERNAL_IP=$(kubectl get services gradio \\\u00a0 \u00a0 --output jsonpath='{.status.loadBalancer.ingress[0].ip}')echo -e \"\\nGradio URL: http://${EXTERNAL_IP}\\n\"\n```The output is similar to the following:```\nGradio URL: http://34.172.115.35\n```The load balancer might take several minutes to get an external IP address.\n- Open the `gradio.yaml` manifest: [  ai-ml/gke-ray/rayserve/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/gke-ray/rayserve/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/gke-ray/rayserve/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"meta-llama/Llama-2-7b-chat-hf\"\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/v1/chat/completions\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://rayllm-serve-svc:8000\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradiospec:\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 80\u00a0 \u00a0 \u00a0 targetPort: 7860\u00a0 type: LoadBalancer\n```\n- Ensure that the `value` assigned to the `MODEL_ID` is `meta-llama/Llama-2-7b-chat-hf` .\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Find the external IP address of the Service:```\nEXTERNAL_IP=$(kubectl get services gradio \\\u00a0 \u00a0 --output jsonpath='{.status.loadBalancer.ingress[0].ip}')echo -e \"\\nGradio URL: http://${EXTERNAL_IP}\\n\"\n```The output is similar to the following:```\nGradio URL: http://34.172.115.35\n```The load balancer might take several minutes to get an external IP address.## Calculating the amount of GPUs\nThe amount of GPUs depends on the value of the `bnb_4bit_quant_type` configuration. In this tutorial, you set `bnb_4bit_quant_type` to `nf4` , which means the model is loaded in 4-bits.\nA 70 billion parameter model would require a minimum of 40 GB of GPU memory. This equals to 70 billion times 4 bits (70 billion x 4 bits= 35 GB) plus 5 GB of overhead. In this case, a single L4 GPU wouldn't have enough memory. Therefore, the examples in this tutorial use L4 GPU of memory (2 x 24 = 48 GB). This configuration is sufficient for running Falcon 40b or Llama 2 70b in L4 GPUs.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resources\nIf you used an existing project and you don't want to delete it, delete the individual resources.\n- Navigate to the `gke-platform` folder:```\ncd ${TUTORIAL_HOME}/gke-platform\n```\n- Disable the deletion protection on the cluster and remove all the terraform provisioned resources. Run the following commands:```\nsed -ie 's/\"deletion_protection\": true/\"deletion_protection\": false/g' terraform.tfstateterraform destroy --auto-approve\n```## What's next\n- [Learn more about G2 VMs with NVIDIA L4 GPUs](/blog/products/compute/introducing-g2-vms-with-nvidia-l4-gpus) \n- [Train a model with GPUs on GKE Standard mode](/kubernetes-engine/docs/quickstarts/train-model-gpus-standard)", "guide": "Google Kubernetes Engine (GKE)"}