{"title": "Google Kubernetes Engine (GKE) - About node auto-provisioning", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning", "abstract": "# Google Kubernetes Engine (GKE) - About node auto-provisioning\nThis page explains how node auto-provisioning works in Standard Google Kubernetes Engine (GKE) clusters. With node auto-provisioning, nodes are [automatically scaled](/kubernetes-engine/docs/concepts/cluster-autoscaler) to meet the requirements of your workloads.\nWith Autopilot clusters, you don't need to manually provision nodes or manage node pools because GKE automatically manages node scaling and provisioning.\n", "content": "## Why use node auto-provisioning\nautomatically manages and scales a set of node pools on the user's behalf. Without node auto-provisioning, the GKE cluster autoscaler creates nodes only from user-created node pools. With node auto-provisioning, GKE automatically creates and deletes node pools.\n### Unsupported features\nNode auto-provisioning doesn't node pools that use any of the following features. However, the cluster autoscaler scales nodes in node pools with these features:\n- [GKE Sandbox](/kubernetes-engine/docs/how-to/sandbox-pods) .\n- [Windows operating systems](/kubernetes-engine/docs/concepts/node-images#windows-server) .\n- Controlling [reservation affinity](/kubernetes-engine/docs/how-to/consuming-reservations) .\n- Autoscaling [local PersistentVolumes](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd#example_local_pvs) .\n- Auto-provisioning nodes with [local SSDs as ephemeral storage](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd#creating_a_node_pool_using_ephemeral_storage_on_local_ssds) .\n- Auto-provisioning through custom scheduling that uses altered [Filters](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#filter) .\n- Configuring [simultaneous multi-threading (SMT)](/kubernetes-engine/docs/how-to/configure-smt) .## How node auto-provisioning works\nNode auto-provisioning is a mechanism of the cluster autoscaler, which only [scales existing node pools](/kubernetes-engine/docs/concepts/cluster-autoscaler#how_cluster_autoscaler_works) . With node auto-provisioning enabled, the cluster autoscaler can create node pools automatically based on the [specifications](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates) of unschedulable Pods.\n**Note:** The cluster autoscaler is automatically enabled when using node auto-provisioning.\nNode auto-provisioning creates node pools based on the following information:\n- CPU, memory, and ephemeral storage [resource requests](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) .\n- [GPU requests](/kubernetes-engine/docs/concepts/gpus) .\n- Pending Pods' [node affinities and label selectors](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/) .\n- Pending Pods' [node taints and tolerations](/kubernetes-engine/docs/how-to/node-taints) .\n**Note:** For node auto-provisioning to work as expected, Pod resource requests need to be large enough for the Pod to function normally. If resource requests are too small, auto-provisioned nodes might not have the resources to launch pods.\n### Resource limits\nNode auto-provisioning and the cluster autoscaler have limits at the following levels:\n- **Node pool level** : Auto-provisioned node pools are limited to 1000 nodes.\n- **Cluster level** :- Any auto-provisioning limits that you define are enforced based on the total CPU and memory resources used acrossnode pools, not just auto-provisioned pools.\n- The cluster autoscaler does not create new nodes if doing so would exceed one of the defined limits. If limits are already exceeded, GKE doesn't delete the nodes.\n### Workload separation\nIf pending Pods have node affinities and tolerations, node auto-provisioning can provision nodes with matching labels and taints.\nNode auto-provisioning might create node pools with labels and taints if all of the following conditions are met:\n- A pending Pod requires a node with a specific label key and value.\n- The Pod has a toleration for a taint with the same key.\n- The toleration is for the`NoSchedule`effect,`NoExecute`effect, or all effects.\nFor instructions, refer to [Configure workload separation in GKE](/kubernetes-engine/docs/how-to/workload-separation) .\n### Deletion of auto-provisioned node pools\nWhen there are no nodes in an auto-provisioned node pool, GKE deletes the node pool. GKE does not delete node pools that are not auto-provisioned.\n### Supported machine types\nNode auto-provisioning considers the Pod requirements in your cluster to determine what type of nodes would best fit those Pods.\nBy default, GKE uses the [E2 machine series](/compute/docs/general-purpose-machines#e2_machine_types) unless any of the following conditions apply:\n- The workload requests a feature that is not available in the E2 machine series. For example, if a GPU is requested by the workload, the [N1 machine series](/compute/docs/general-purpose-machines#n1_machines) is used for the new node pool.\n- The workload requests TPU resources. To learn more about TPUs, see the [Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) .\n- The workload uses the`machine-family`label. For more information, see [Using a custom machine family](/kubernetes-engine/docs/how-to/node-auto-provisioning#custom_machine_family) .\nIf the Pod requests GPUs, node auto-provisioning assigns a machine type sufficiently large to support the number of GPUs that the Pod requests. The number of GPUs restricts the CPU and memory that the node can have. For more information, see [GPU platforms](/compute/docs/gpus) .\n**Note:** On GKE versions earlier than 1.25, node auto-provisioning assigns a machine type with the exact GPU count. A known issue causes Pods with very high CPU:GPU or memory:GPU ratios to get stuck in the `Pending` status. We recommend you to [upgrade](/kubernetes-engine/versioning) to GKE version 1.25 or later.\n### Supported node images\nNode auto-provisioning creates node pools using one of the following [node images](/kubernetes-engine/docs/concepts/node-images) :\n- Container-Optimized OS (`cos_containerd`).\n- Ubuntu (`ubuntu_containerd`).\n### Supported machine learning accelerators\nNode auto-provisioning can create node pools with hardware accelerators such as GPU and Cloud TPU. Node auto-provisioning supports TPUs in GKE version 1.28 and later.\nIf the Pod requests GPUs, node auto-provisioning assigns a machine type sufficiently large to support the number of GPUs that the Pod requests. The number of GPUs restricts the CPU and memory that the node can have. For more information, see [GPU platforms](/compute/docs/gpus) .\n**Note:** On GKE versions earlier than 1.25, node auto-provisioning assigns a machine type with the exact GPU count. A known issue causes Pods with very high CPU:GPU or memory:GPU ratios to get stuck in the Pending status. We recommend that you upgrade to GKE version 1.25 or later.\nGKE supports Tensor Processing Units (TPUs) to accelerate machine learning workloads. Both [single-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) and [multi-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) support autoscaling and auto-provisioning.\nWith the [--enable-autoprovisioning](/sdk/gcloud/reference/container/clusters/update#--enable-autoprovisioning) flag on a GKE cluster, GKE creates or deletes single-host or multi-host TPU slice node pools with a TPU version and topology that meets the requirements of pending workloads.\nWhen you use [--enable-autoscaling](/sdk/gcloud/reference/container/node-pools/create#--enable-autoscaling) , GKE scales the node pool based on its type, as follows:\n- TPU slice node pool: GKE adds or removes TPU nodes in the existing node pool. The node pool may contain any number of TPU nodes between zero and the maximum size of the node pool as determined by the [--max-nodes](/sdk/gcloud/reference/container/node-pools/create#--max-nodes) and the [--total-max-nodes](/sdk/gcloud/reference/container/node-pools/create#--total-max-nodes) flags. When the node pool scales, all the TPU nodes in the node pool have the same machine type and topology. To learn more how to create a single-host TPU slice node pool, see [Create a nodepool](/kubernetes-engine/docs/how-to/tpus#single-host) .\n- TPU slice node pool: GKE atomically scales up the node pool from zero to the number of nodes required to satisfy the TPU topology. For example, with a TPU node pool with a machine type `ct5lp-hightpu-4t` and a topology of `16x16` , the node pool contains 64 nodes. The GKE autoscaler ensures that this node pool has exactly 0 or 64 nodes. When scaling back down, GKE evicts all scheduled pods, and drains the entire node pool to zero. To learn more how to create a multi-host TPU slice node pool, see [Create a node pool](/kubernetes-engine/docs/how-to/tpus#multi-host) .\nIf a specific TPU slice has no Pods that are running or are pending to be scheduled, GKE scales down the node pool. Multi-host TPU slice node pools are scaled down atomically. Single-host TPU slice node pools are scaled down by removing individual single-host TPU slices.\nWhen you enable node auto-provisioning with TPUs, GKE makes scaling decisions based on the values defined in the Pod request. The following manifest is an example of a Deployment specification that results in one node pool that contains TPU v4 slice with a `2x2x2` topology and two `ct4p-hightpu-4t` machines:\n```\n\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 name: tpu-workload\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: tpu-workload\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 replicas: 2\u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: nginx-tpu\u00a0 \u00a0 \u00a0 template:\u00a0 \u00a0 \u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: nginx-tpu\u00a0 \u00a0 \u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x2x2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/reservation-name: my-reservation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: nginx:1.14.2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\n```\nWhere:\n- `cloud.google.com/gke-tpu-accelerator`: The TPU version and type. For example, TPU v4 with`tpu-v4-podslice`or TPU v5e with`tpu-v5-lite-podslice`.\n- `cloud.google.com/gke-tpu-topology`: The number and physical arrangement of TPU chips within a TPU slice. When creating a node pool and enabling node auto-provisioning, you select the TPU topology. For more information about Cloud TPU topologies, see [TPU configurations](/tpu/docs/supported-tpu-configurations) .\n- `limit.google.com/tpu`: The number of TPU chips on the TPU VM. Most configurations have just one correct value. However, the`tpu-v5-lite-podslice`with`2x4`topology configuration:- If you specify`google.com/tpu = 8`, node auto-provisioning scales up single-host TPU slice node pool adding one`ct5lp-hightpu-8t`machine.\n- If you specify`google.com/tpu = 4`, node auto-provisioning creates a multi-host TPU slice node pool with two`ct5lp-hightpu-4t`machines.\n- `cloud.google.com/reservation-name`: The name of the reservation that the workload uses. If omitted, the workload doesn't use any reservation.\nIf you set `tpu-v4-podslice` , node auto-provisioning makes the following decisions:\n| ('Values set in the Pod manifest', 'gke-tpu-topology') | ('Values set in the Pod manifest', 'limit.google.com/tpu') | ('Decided by node auto-provisioning', 'Type of node pool') | ('Decided by node auto-provisioning', 'Node pool size') | ('Decided by node auto-provisioning', 'Machine type') |\n|:---------------------------------------------------------|-------------------------------------------------------------:|:-------------------------------------------------------------|:----------------------------------------------------------|:--------------------------------------------------------|\n| 2x2x1             |               4 | Single-host TPU slice          | Flexible             | ct4p-hightpu-4t           |\n| {A}x{B}x{C}            |               4 | Multi-host TPU slice           | {A}x{B}x{C}/4            | ct4p-hightpu-4t           |\nThe product of {A}x{B}x{C} defines the number of chips in the node pool. For example, you can define a small topology of 64 chips with combinations such as `4x4x4` . If you use topologies larger than 64 chips, the values you assign to {A},{B}, and {C} must meet the following conditions:\n- {A},{B}, and {C} are either all lower than or equal to four, or multiples of four.\n- The largest topology supported is`12x16x16`.\n- The assigned values keep the A \u2264 B \u2264 C pattern. For example,`2x2x4`or`2x4x4`for small topologies.\nIf you set `tpu-v5-lite-podslice` , node auto-provisioning makes the following decisions:\n| ('Values set in the Pod manifest', 'gke-tpu-topology') | ('Values set in the Pod manifest', 'limit.google.com/tpu') | ('Decided by node auto-provisioning', 'Type of node pool') | ('Decided by node auto-provisioning', 'Node pool size') | ('Decided by node auto-provisioning', 'Machine type') |\n|:---------------------------------------------------------|-------------------------------------------------------------:|:-------------------------------------------------------------|:----------------------------------------------------------|:--------------------------------------------------------|\n| 1x1              |               1 | Single-host TPU slice          | Flexible             | ct5lp-hightpu-1t          |\n| 2x2              |               4 | Single-host TPU slice          | Flexible             | ct5lp-hightpu-4t          |\n| 2x4              |               8 | Single-host TPU slice          | Flexible             | ct5lp-hightpu-8t          |\n| 2x41              |               4 | Multi-host TPU slice           | 2 (8/4)             | ct5lp-hightpu-4t          |\n| 4x4              |               4 | Multi-host TPU slice           | 4 (16/4)             | ct5lp-hightpu-4t          |\n| 4x8              |               4 | Multi-host TPU slice           | 8 (32/4)             | ct5lp-hightpu-4t          |\n| 4x8              |               4 | Multi-host TPU slice           | 16 (32/4)             | ct5lp-hightpu-4t          |\n| 8x8              |               4 | Multi-host TPU slice           | 16 (64/4)             | ct5lp-hightpu-4t          |\n| 8x16              |               4 | Multi-host TPU slice           | 32 (128/4)            | ct5lp-hightpu-4t          |\n| 16x16             |               4 | Multi-host TPU slice           | 64 (256/4)            | ct5lp-hightpu-4t          |\n- Special case where the machine type depends on the value you defined in  the `google.com/tpu` limits field. [\u21a9](#fnref1) \nIf you set the accelerator type to `tpu-v5-lite-device` , node auto-provisioning makes the following decisions:\n| ('Values set in the Pod manifest', 'gke-tpu-topology') | ('Values set in the Pod manifest', 'limit.google.com/tpu') | ('Decided by node auto-provisioning', 'Type of node pool') | ('Decided by node auto-provisioning', 'Node pool size') | ('Decided by node auto-provisioning', 'Machine type') |\n|:---------------------------------------------------------|-------------------------------------------------------------:|:-------------------------------------------------------------|:----------------------------------------------------------|:--------------------------------------------------------|\n| 1x1              |               1 | Single-host TPU slice          | Flexible             | ct5l-hightpu-1t           |\n| 2x2              |               4 | Single-host TPU slice          | Flexible             | ct5l-hightpu-4t           |\n| 2x4              |               8 | Single-host TPU slice          | Flexible             | ct5l-hightpu-8t           |\nTo learn how to set up node auto-provisioning, see [Configuring TPUs](/kubernetes-engine/docs/how-to/node-auto-provisioning#configuring_tpus) .\n### Support for Spot VMs\nNode auto-provisioning supports creating node pools based on [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\nCreating node pools based on Spot VMs is only considered if unschedulable pods with a toleration for the `cloud.google.com/gke-spot=\"true\":NoSchedule` taint exist. The taint is automatically applied to nodes in auto-provisioned node pools that are based on Spot VMs.\nYou can combine using the toleration with a `nodeSelector` or node affinity rule for the `cloud.google.com/gke-spot=\"true\"` or `cloud.google.com/gke-provisioning=spot` (for nodes running GKE version 1.25.5-gke.2500 or later) node labels to ensure that your workloads only run on node pools based on Spot VMs.\n**Note:** Node auto-provisioning also supports creating node pools based on [preemptible VMs](/compute/docs/instances/preemptible) by using the `cloud.google.com/gke-preemptible=\"true\":NoSchedule` taint and corresponding toleration. However, Spot VMs are strongly recommended instead.\n**Note:** Creating node pools based on Spot VMs is only supported in GKE version 1.21 and later. Preemptible VMs can be used in earlier GKE versions. If a Pod has a toleration for the `cloud.google.com/gke-spot=\"true\"` label in GKE version 1.20 and earlier, node auto-provisioning creates a node pool with this label, but the node pool won't actually use Spot VMs.\n### Support for Pods requesting ephemeral storage\nNode auto-provisioning supports creating node pools when Pods request ephemeral storage. The size of the boot disk provisioned in the node pools is constant for all new auto-provisioned node pools. This size of the boot disk can be [customized](/kubernetes-engine/docs/how-to/node-auto-provisioning#custom_boot_disk) .\nThe default is 100 GiB. [Ephemeral storage backed by local SSDs](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd#creating_a_node_pool_using_ephemeral_storage_on_local_ssds) is not supported.\nNode auto-provisioning will provision a node pool only if the [allocatable](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage) ephemeral storage of a node with a specified boot disk is greater than or equal to the ephemeral storage request of a pending Pod. If the ephemeral storage request is higher than what is allocatable, node auto-provisioning will not provision a node pool. Disk sizes for nodes are not dynamically configured based on ephemeral storage requests of pending Pods.\n## Scalability limitations\nNode auto-provisioning has the same limitations as the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations) , as well as the following additional limitations:\n## What's next\n- [Learn more how to enable node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) \n- [Learn more about cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler)", "guide": "Google Kubernetes Engine (GKE)"}