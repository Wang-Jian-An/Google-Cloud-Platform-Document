{"title": "Google Kubernetes Engine (GKE) - Scale your storage performance with Hyperdisk", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/hyperdisk", "abstract": "# Google Kubernetes Engine (GKE) - Scale your storage performance with Hyperdisk\nThe Compute Engine Persistent Disk CSI driver is the primary way for you to access [Hyperdisk storage with GKE clusters](/kubernetes-engine/docs/concepts/hyperdisk) .\n**Note:** Hyperdisk support is based on the machine type of your nodes. For the most up-to-date information, see [Machine type support](/compute/docs/disks/hyperdisks#machine-type-support) in the Compute Engine documentation.\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Set your default region and zone to one of the [supported values](/compute/docs/disks/hyperdisks#hyperdisk_regions) .\n### Requirements\nTo use Hyperdisk volumes in GKE, your clusters must meet the following requirements:\n- Use Linux clusters running GKE version 1.26 or later. If you use a [release channel](/kubernetes-engine/docs/concepts/release-channels) , ensure that the channel has the minimum GKE version or later that is required for this driver.\n- Make sure that the [Compute Engine Persistent Disk CSI driver](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver) is enabled. The Compute Engine Persistent Disk driver is enabled by default on new Autopilot and Standard clusters and cannot be disabled or edited when using Autopilot. If you need to manually add or remove the Compute Engine Persistent Disk CSI driver from your cluster, see [Enabling the Compute Engine Persistent Disk CSI Driver on an existing cluster](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver#enabling_the_on_an_existing_cluster) .## Create a Hyperdisk volume for GKE\nThis section provides an overview of creating a Hyperdisk volume backed by the Compute Engine CSI driver in GKE.\n**Note:** When you create the PersistentVolumeClaim associated with the StorageClass, GKE automatically creates the underlying Google Cloud Hyperdisk backing storage and attaches the storage to a node. You don't need to separately create and attach Google Cloud Hyperdisk storage to your nodes.\n### Create a StorageClass\nThe following [Persistent Disk storage Type fields](/compute/docs/disks#disk-types) are provided by the Compute Engine Persistent Disk CSI driver to support Hyperdisk:\n- `hyperdisk-balanced`\n- `hyperdisk-throughput`\n- `hyperdisk-extreme`\nTo create a new StorageClass with the throughput or IOPS level you want, use `pd.csi.storage.gke.io` in the provisioner field, and specify one of the Hyperdisk storage types.\nEach Hyperdisk type has [default values](/compute/docs/disks/hyperdisks#provisioning-performance) for performance determined by the initial disk size provisioned. When creating the StorageClass, you can optionally specify the following parameters depending on your Hyperdisk type. If you omit these parameters, GKE uses the capacity based disk type defaults instead.\n| Parameter      | Hyperdisk Type       | Usage                                         |\n|:---------------------------------|:-----------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| provisioned-throughput-on-create | Hyperdisk Balanced, Hyperdisk Throughput | Express the throughput value in MiBps using the \"Mi\" qualifier; for example, if your required throughput is 250\u00a0MiBps, specify \"250Mi\" when creating the StorageClass. |\n| provisioned-iops-on-create  | Hyperdisk Balanced, Hyperdisk IOPS  | The IOPS value should be expressed without any qualifiers; for example, if you require 7,000 IOPS, specify \"7000\" when creating the StorageClass.      |\nFor guidance on allowable values for throughput or IOPS, see [Plan the performance level for your Hyperdisk volume](/kubernetes-engine/docs/concepts/hyperdisk#plan) .\nThe following examples show how you can create a StorageClass for each Hyperdisk type:\n- Save the following manifest in a file named `hdb-example-class.yaml` :```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: balanced-storageprovisioner: pd.csi.storage.gke.iovolumeBindingMode: WaitForFirstConsumerallowVolumeExpansion: trueparameters:\u00a0 type: hyperdisk-balanced\u00a0 provisioned-throughput-on-create: \"250Mi\"\u00a0 provisioned-iops-on-create: \"7000\"\n```\n- Create the StorageClass:```\nkubectl create -f hdb-example-class.yaml\n```\n- Save the following manifest in a file named `hdt-example-class.yaml` :```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: throughput-storageprovisioner: pd.csi.storage.gke.iovolumeBindingMode: WaitForFirstConsumerallowVolumeExpansion: trueparameters:\u00a0 type: hyperdisk-throughput\u00a0 provisioned-throughput-on-create: \"50Mi\"\n```\n- Create the StorageClass:```\nkubectl create -f hdt-example-class.yaml\n```\n- Save the following manifest in a file named `hdx-example-class.yaml` :```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: extreme-storageprovisioner: pd.csi.storage.gke.iovolumeBindingMode: WaitForFirstConsumerallowVolumeExpansion: trueparameters:\u00a0 type: hyperdisk-extreme\u00a0 provisioned-iops-on-create: \"50000\"\n```\n- Create the StorageClass:```\nkubectl create -f hdx-example-class.yaml\n```\nTo find the name of the StorageClasses available in your cluster, run the following command:\n```\nkubectl get sc\n```\n### Create a PersistentVolumeClaim\nYou can create a PersistentVolumeClaim that references the Compute Engine Persistent Disk CSI driver's StorageClass.\nIn this example, you specify the targeted storage capacity of the Hyperdisk Balanced volume as 20\u00a0GiB.- Save the following PersistentVolumeClaim manifest in a file named `pvc-example.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 storageClassName: balanced-storage\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 20Gi\n```\n- Apply the PersistentVolumeClaim that references the StorageClass you created from the earlier example:```\nkubectl apply -f pvc-example.yaml\n```\nIn this example, you specify the targeted storage capacity of the Hyperdisk Throughput volume as 2\u00a0TiB.- Save the following PersistentVolumeClaim manifest in a file named `pvc-example.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 storageClassName: throughput-storage\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 2Ti\n```\n- Apply the PersistentVolumeClaim that references the StorageClass you created from the earlier example:```\nkubectl apply -f pvc-example.yaml\n```\nIn this example, you specify the minimum storage capacity of the Hyperdisk Extreme volume as 64\u00a0GiB.- Save the following PersistentVolumeClaim manifest in a file named `pvc-example.yaml` :```\nkind: PersistentVolumeClaimapiVersion: v1metadata:\u00a0 name: podpvcspec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 storageClassName: extreme-storage\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 64Gi\n```\n- Apply the PersistentVolumeClaim that references the StorageClass you created from the earlier example:```\nkubectl apply -f pvc-example.yaml\n```\n### Create a Deployment to consume the Hyperdisk volume\nWhen using Pods with PersistentVolumes, we recommend that you use a workload controller (such as a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) or [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) ).\n- The following example creates a manifest that configures a Pod for deploying a Nginx web server using the PersistentVolumeClaim created in the previous section. Save the following example manifest as `hyperdisk-example-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: web-server-deployment\u00a0 labels:\u00a0 \u00a0 app: nginxspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /var/lib/www/html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: mypvc\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: mypvc\u00a0 \u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 claimName: podpvc\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: false\n```\n- To create a Deployment based on the `hyperdisk-example-deployment.yaml` manifest file, run the following command:```\nkubectl apply -f hyperdisk-example-deployment.yaml\n```\n- Confirm the Deployment was successfully created:```\nkubectl get deployment\n```It might take a few minutes for Hyperdisk instances to complete provisioning. When the deployment completes provisioning, it reports a `READY` status.\n- You can check the progress by monitoring your PersistentVolumeClaim status by running the following command:```\nkubectl get pvc\n```## Provision a Hyperdisk volume from a snapshot\nTo create a new Hyperdisk volume from an existing Persistent Disk snapshot, use the Google Cloud console, the Google Cloud CLI, or the [Compute Engine API](/compute/docs/reference/rest/v1/disks) . To learn how to create a Persistent Disk snapshot, see [Creating and using volume snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots#creating_and_using_a_volume_snapshot) .\n- Go to the **Disks** page in the Google Cloud console. [Go to Disks](https://console.cloud.google.com/compute/disks) \n- Click **Create Disk.** \n- Under **Disk Type** , choose one of the following for disk type:- **Hyperdisk Balanced** \n- **Hyperdisk Extreme** \n- **Hyperdisk Throughput** \n- Under **Disk source type** , click **Snapshot** .\n- Select the name of the snapshot to restore.\n- Select the size of the new disk, in GiB. This number must be equal to or larger than the original source disk for the snapshot.\n- Set the **Provisioned throughput** or **Provisioned IOPS** you want for the disk, if different from the default values.\n- Click **Create** to create the Hyperdisk volume.\nRun the [gcloud compute disks create command](/sdk/gcloud/reference/compute/disks/create) to create the Hyperdisk volume from a snapshot.\n```\ngcloud compute disks create DISK_NAME \\\u00a0 \u00a0 --size=SIZE \\\u00a0 \u00a0 --source-snapshot=SNAPSHOT_NAME \\\u00a0 \u00a0 --provisioned-throughput=TRHROUGHPUT_LIMIT \\\u00a0 \u00a0 --provisioned-iops=IOPS_LIMIT \\\u00a0 \u00a0 --type=hyperdisk-balanced\n```\nReplace the following:- ``: the name of the new disk.\n- ``: the size, in gibibytes (GiB) or tebibytes (TiB), of the new disk. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest capacity limitations.\n- ``: the name of the snapshot being restored.\n- ``: Optional. For Hyperdisk Balanced disks, this is an integer that represents the throughput, measured in MiBps, that the disk can handle. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest limitations.\n- ``: Optional. For Hyperdisk Balanced disks, this is the number of IOPS that the disk can handle. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest performance limitations.\n```\ngcloud compute disks create DISK_NAME \\\u00a0 \u00a0 --size=SIZE \\\u00a0 \u00a0 --source-snapshot=SNAPSHOT_NAME \\\u00a0 \u00a0 --provisioned-throughput=TRHROUGHPUT_LIMIT \\\u00a0 \u00a0 --type=hyperdisk-throughput\n```\nReplace the following:- ``: the name of the new disk.\n- ``: the size, in gibibytes (GiB or GB) or tebibytes (TiB or TB), of the new disk. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest capacity limitations.\n- ``: the name of the snapshot being restored.\n- ``: Optional: For Hyperdisk Throughput disks, this is an integer that represents the throughput, measured in MiBps, that the disk can handle. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest performance limitations.\n```\ngcloud compute disks create DISK_NAME \\\u00a0 \u00a0 --size=SIZE \\\u00a0 \u00a0 --source-snapshot=SNAPSHOT_NAME \\\u00a0 \u00a0 --provisioned-iops=IOPS_LIMIT \\\u00a0 \u00a0 --type=hyperdisk-iops\n```\nReplace the following:- ``: the name of the new disk.\n- ``: the size, in gibibytes (GiB or GB) or tebibytes (TiB or TB), of the new disk. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest capacity limitations.\n- ``: the name of the snapshot being restored.\n- ``: Optional: For Hyperdisk Extreme disks, this is the number of I/O operations per second that the disk can handle. Refer to the [Compute Engine documentation](/compute/docs/disks/hyperdisks) for the latest performance limitations.## Create a snapshot for a Hyperdisk volume\nTo create a snapshot from a Hyperdisk volume, follow the same steps as creating a snapshot for a Persistent Disk volume:\n- [Create a snapshot in Google Cloud console](/compute/docs/disks/create-snapshots#console) \n- [Create a snapshot using the gcloud CLI](/compute/docs/disks/create-snapshots#gcloud) ## Update the provisioned throughput or IOPS of an existing Hyperdisk volume\nThis section covers how to modify provisioned performance for Hyperdisk volumes.\nUpdating the provisioned throughput is supported for Hyperdisk Balanced and Hyperdisk Throughput volumes only.\nTo update the provisioned throughput level of your Hyperdisk volume, follow the Google Cloud console, gcloud CLI, or Compute Engine API instructions in [Changing the provisioned performance for a Hyperdisk volume](/compute/docs/disks/modify-hyperdisks#changing_the_provisioned_performance_for_a_volume) .\nYou can change the provisioned throughput level (up to once every 4 hours) for a Hyperdisk volume after volume creation. New throughput levels might take up to 15 minutes to take effect. During the performance change, any performance SLA and SLO are not in effect. You can change the throughput level of an existing volume at any time, regardless of whether the disk is attached to a running instance or not.\nThe new throughput level you specify must adhere to the [supported values for Hyperdisk volumes](/compute/docs/disks/hyperdisks#limits-disk) .\nUpdating the provisioned IOPS is supported for Hyperdisk Balanced and Hyperdisk Extreme volumes only.\nTo update the provisioned IOPS level of your Hyperdisk volume, follow the Google Cloud console, gcloud CLI, or Compute Engine API instructions in [Changing the provisioned performance for a Hyperdisk volume](/compute/docs/disks/modify-hyperdisks#changing_the_provisioned_performance_for_a_volume) .\nYou can change the provisioned IOPS level (up to once every 4 hours) for a Hyperdisk IOPS volume after volume creation. New IOPS levels might take up to 15 minutes to take effect. During the performance change, any performance SLA and SLO are not in effect. You can change the IOPS level of an existing volume at any time, regardless of whether the disk is attached to a running instance or not.\nThe new IOPS level you specify must adhere to the [supported values for Hyperdisk volumes](/compute/docs/disks/hyperdisks#limits-disk) .\nTo update the provisioned IOPS level for a Hyperdisk volume, you must identify the name of the Persistent Disk backing your PersistentVolumeClaim and PersistentVolume resources:- Go to the **Object browser** in the Google Cloud console. [Go to Object Browser](https://console.cloud.google.com/kubernetes/object/browser) \n- Find the entry for your PersistentVolumeClaim object.\n- Click the **Volume** link .\n- Open the YAML tab of the associated PersistentVolume. Locate the CSI `volumeHandle` value in this tab.\n- Note the last element of this handle (it should have a value like \" `pvc-XXXXX` \"). This is the name of your PersistentVolumeClaim. You should also take note of the project and zone.## Monitor throughput or IOPS on a Hyperdisk volume\nTo monitor the provisioned performance of your Hyperdisk volume, see [Analyze provisioned IOPS and throughput](/compute/docs/disks/analyze-iops-hyperdisk) in the Compute Engine documentation.\n## Troubleshooting\nThis section provides troubleshooting guidance to resolve issues with Hyperdisk volumes on GKE.\n### Cannot change performance or capacity: ratio out of range\nThe following error occurs when you attempt to change the provisioned performance level or capacity, but the performance level or capacity that you picked is outside of the range that is acceptable for the volume:\n- `Requested provisioned throughput cannot be higher than <value>.`\n- `Requested provisioned throughput cannot be lower than <value>.`\n- `Requested provisioned throughput is too high for the requested disk size.`\n- `Requested provisioned throughput is too low for the requested disk size.`\n- `Requested disk size is too high for current provisioned throughput.`\nThe throughput provisioned for Hyperdisk Throughput volumes must meet the following requirements:\n- At least 10\u00a0MiBps per TiB of capacity, and no more than 90\u00a0MiBps per TiB of capacity.\n- At most 600 MiBps per volume.\nTo resolve this issue, correct the requested throughput or capacity to be within the allowable range and reissue the command.\n### Cannot change performance: rate limited\nThe following error occurs when you attempt to change the provisioned performance level, but the performance level has already been changed within the last 4 hours:\n```\nCannot update provisioned throughput due to being rate limited.\nCannot update provisioned iops due to being rate limited.\n```\nHyperdisk Throughput and IOPS volumes can have their provisioned performance updated once every 4 hours. To resolve this issue, wait for the cool-down timer for the volume to elapse, and then reissue the command.\n## What's next\n- [Learn how to migrate Persistent Disk volumes to Hyperdisk](/compute/docs/disks/migrate-to-hyperdisk#gcloud) .\n- [Learn how to use volume expansion](/kubernetes-engine/docs/how-to/persistent-volumes/volume-expansion) .\n- [Learn how to use volume snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) .\n- [Read more about the driver on GitHub](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) .", "guide": "Google Kubernetes Engine (GKE)"}