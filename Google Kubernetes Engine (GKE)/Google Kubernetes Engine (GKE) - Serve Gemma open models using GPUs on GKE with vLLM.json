{"title": "Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with vLLM", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm", "abstract": "# Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with vLLM\nThis tutorial shows you how to serve a [Gemma](https://ai.google.dev/gemma/docs/) large language model (LLM) using graphical processing units (GPUs) on Google Kubernetes Engine (GKE) with the [vLLM](https://github.com/vllm-project/vllm) serving framework. In this tutorial, you download the 2B and 7B parameter instruction tuned and pre-trained Gemma models from Hugging Face and deploy them on a GKE [Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-autopilot) or [Standard](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-standard) cluster using a container that runs vLLM.\nThis guide is a good starting point if you need the granular control, scalability, resilience, portability, and cost-effectiveness of managed Kubernetes when deploying and serving your AI/ML workloads. If you need a unified managed AI platform to rapidly build and serve ML models cost effectively, we recommend that you try our [Vertex AI](/vertex-ai) deployment solution.", "content": "## BackgroundBy serving Gemma using GPUs on GKE with vLLM, you can implement a robust, production-ready inference serving solution with all the benefits of managed [Kubernetes](https://kubernetes.io/) , including efficient scalability and higher availability. This section describes the key technologies used in this guide.\n### Gemma [Gemma](https://ai.google.dev/gemma/docs/) is a set of openly available, lightweight, generative artificial intelligence (AI) models released under an open license. These AI models are available to run in your applications, hardware, mobile devices, or hosted services. You can use the Gemma models for text generation, however you can also tune these models for specialized tasks.\nTo learn more, see the [Gemma documentation](https://ai.google.dev/gemma/docs) .\n### GPUsGPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\nBefore you use GPUs in GKE, we recommend that you complete the following learning path:- Learn about [current GPU version availability](/compute/docs/gpus) \n- Learn about [GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) \n### vLLMvLLM is a highly optimized open source LLM serving framework that can increase serving throughput on GPUs, with features such as:- Optimized transformer implementation with [PagedAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention) \n- Continuous batching to improve the overall serving throughput\n- Tensor parallelism and distributed serving on multiple GPUs\nTo learn more, refer to the [vLLMdocumentation](https://docs.vllm.ai/en/latest/) .## ObjectivesThis guide is intended for Generative AI customers who use [PyTorch](https://pytorch.org/) , new or existing users of GKE, ML Engineers, MLOps (DevOps) engineers, or platform administrators who are interested in using Kubernetes container orchestration capabilities for serving LLMs on H100, A100, and L4 GPU hardware.\nBy the end of this guide, you should be able to perform the following steps:- Prepare your environment with a GKE cluster in Autopilot or Standard mode.\n- Deploy a vLLM container to your cluster.\n- Use vLLM to serve the Gemma 2B or 7B model through curl and a web chat interface.\n## Before you begin- Make sure that you have the following role or roles on the project:      roles/container.admin, roles/iam.serviceAccountAdmin\n- Create a [Hugging Face](https://huggingface.co/) account, if you don't already have one.\n- [Ensure your project has sufficientquota](/compute/resource-usage#gpu_quota) for GPUs in GKE.\n## Get access to the modelTo get access to the Gemma models for deployment to GKE, you must first sign the license consent agreement then generate a Hugging Face access token.\n### Sign the license consent agreementYou must sign the consent agreement to use Gemma. Follow these instructions:- Access the [model consent page](https://www.kaggle.com/models/google/gemma) on Kaggle.com.\n- Verify consent using your Hugging Face account.\n- Accept the model terms.\n### Generate an access tokenTo access the model through Hugging Face, you need a [Hugging Facetoken](https://huggingface.co/docs/hub/security-tokens) .\nFollow these steps to generate a new token if you don't have one already:- Click **Your Profile > Settings > Access Tokens** .\n- Select **New Token** .\n- Specify a Name of your choice and a Role of at least `Read.\n- Select **Generate a token** .\n- Copy the generated token to your clipboard.\n## Prepare your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) and [gcloud CLI](/sdk/gcloud) .\nTo set up your environment with Cloud Shell, follow these steps:- In the Google Cloud console, launch a Cloud Shell session by clicking **Activate Cloud Shell** in the [Google Cloud console](http://console.cloud.google.com) . This launches a session in the bottom pane of Google Cloud console.\n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=REGIONexport CLUSTER_NAME=vllmexport HF_TOKEN=HF_TOKEN\n```Replace the following values:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : A region that supports the accelerator type you want to use, for example,`us-central1`for L4 GPU.\n- : The Hugging Face token you generated earlier.## Create and configure Google Cloud resourcesFollow these instructions to create the required resources.\n **Note:** You may need to create a capacity reservation for usage of some accelerators. To learn how to reserve and consume reserved resources, see [Consuming reserved zonal resources](/kubernetes-engine/docs/how-to/consuming-reservations) .\n### Create a GKE cluster and node poolYou can serve Gemma on GPUs in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience. To choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\nIn Cloud Shell, run the following command:\n```\ngcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid \\\u00a0 --cluster-version=1.28\n```\nGKE creates an Autopilot cluster with CPU and GPU nodes as requested by the deployed workloads.- In Cloud Shell, run the following command to create a Standard cluster:```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 --release-channel=rapid \\\u00a0 --num-nodes=1\n```The cluster creation might take several minutes.\n- Run the following command to create a [node pool](/kubernetes-engine/docs/concepts/node-pools) for your cluster:```\ngcloud container node-pools create gpupool \\\u00a0 --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --location=${REGION} \\\u00a0 --node-locations=${REGION}-a \\\u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 --machine-type=g2-standard-24 \\\u00a0 --num-nodes=1\n```GKE creates a single node pool containing two L4 GPUs for each node.### Create a Kubernetes secret for Hugging Face credentialsIn Cloud Shell, do the following:- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}\n```\n- Create a Kubernetes Secret that contains the Hugging Face token:```\nkubectl create secret generic hf-secret \\--from-literal=hf_api_token=$HF_TOKEN \\--dry-run=client -o yaml | kubectl apply -f ```\n## Deploy vLLMIn this section, you deploy the vLLM container to serve the Gemma model you want to use. To learn about the instruction tuned and pre-trained models and which one to select for your use case, see [Tuned models](https://ai.google.dev/gemma/docs#tuned-models) .\nFollow these instructions to deploy the Gemma 2B instruction tuned model.- Create the following `vllm-2b-it.yaml` manifest: [  ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f vllm-2b-it.yaml\n```\nFollow these instructions to deploy the Gemma 7B instruction tuned model.- Create the following `vllm-7b-it.yaml` manifest: [  ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f vllm-7b-it.yaml\n```\nFollow these instructions to deploy the Gemma 2B pre-trained model.- Create the following `vllm-2b.yaml` manifest: [  ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-2b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"10Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f vllm-2b.yaml\n```\nFollow these instructions to deploy the Gemma 7B pre-trained model.- Create the following `vllm-7b.yaml` manifest: [  ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/vllm-7b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: vllm-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: vllm\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 command: [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --tensor-parallel-size=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f vllm-7b.yaml\n```\nA Pod in the cluster downloads the model weights from Hugging Face and starts the serving engine.\nWait for the Deployment to be available:\n```\nkubectl wait --for=condition=Available --timeout=700s deployment/vllm-gemma-deployment\n```\nView the logs from the running Deployment:\n```\nkubectl logs -f -l app=gemma-server\n```\nThe Deployment resource downloads the model data. This process can take a few minutes. The output is similar to the following:\n```\nINFO 01-26 19:02:54 model_runner.py:689] Graph capturing finished in 4 secs.\nINFO:  Started server process [1]\nINFO:  Waiting for application startup.\nINFO:  Application startup complete.\nINFO:  Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n```\nMake sure the model is fully downloaded before proceeding to the next section.## Serve the modelIn this section, you interact with the model.\n### Set up port forwardingRun the following command to set up port forwarding to the model:\n```\nkubectl port-forward service/llm-service 8000:8000\n```\nThe output is similar to the following:\n```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n### Interact with the model using curlThis section shows how you can perform a basic smoke test to verify your deployed pre-trained or instruction tuned models. For simplicity, this section describes the testing approach only using the 2B pre-trained and instruction tuned models.\nIn a new terminal session, use `curl` to chat with your model:\n```\nUSER_PROMPT=\"Java is a\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"prompt\": \"${USER_PROMPT}\",\u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \"top_p\": 1.0,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\nThe following output shows an example of the model response:\n```\n{\"predictions\":[\"Prompt:\\nJava is a\\nOutput:\\n<strong>programming language</strong> that is primarily aimed at developers. It was originally created by creators of the Java Virtual Machine (JVM). Java is multi-paradigm, which means it supports object-oriented, procedural, and functional programming paradigms. Java is object-oriented, which means that it is designed to support classes and objects. Java is a dynamically typed language, which means that the type of variables are not determined at compile time. Java is also a multi-paradigm language, which means it supports more than one programming paradigm. Java is also a very lightweight language, which means that it is a very low level language compared to other popular\"]}\n```\nIn a new terminal session, use `curl` to chat with your model:\n```\nUSER_PROMPT=\"I'm new to coding. If you could only recommend one programming language to start with, what would it be and why?\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"prompt\": \"<start_of_turn>user\\n${USER_PROMPT}<end_of_turn>\\n\",\u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \"top_p\": 1.0,\u00a0 \u00a0 \"max_tokens\": 128}EOF\n```\nThe following output shows an example of the model response:\n```\n{\"predictions\":[\"Prompt:\\n<start_of_turn>user\\nI'm new to coding. If you could only recommend one programming language to start with, what would it be and why?<end_of_turn>\\nOutput:\\n**Python** is an excellent choice for beginners due to the following reasons:\\n\\n* **Clear and simple syntax:** Python boasts a simple and straightforward syntax that makes it easy to learn the fundamentals of programming.\\n* **Extensive libraries and modules:** Python comes with a vast collection of libraries and modules that address various programming tasks, including data manipulation, machine learning, and web development.\\n* **Large and supportive community:** Python has a vibrant and active community that offers resources, tutorials, and support to help you along your journey.\\n* **Cross-platform compatibility:** Python can be run on various platforms, including Windows, macOS, and\"]}\n```\n **Success:** You've successfully served Gemma using GPUs on GKE with vLLM. You can now interact with the model.\n### (Optional) Interact with the model through a Gradio chat interfaceIn this section, you build a web chat application that lets you interact with your instruction tuned model. For simplicity, this section describes only the testing approach using the 2B-it model.\n [Gradio](https://github.com/gradio-app/gradio) is a Python library that has a `ChatInterface` wrapper that creates user interfaces for chatbots.\n- In Cloud Shell, save the following manifest as `gradio.yaml` : [  ai-ml/llm-serving-gemma/vllm/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/vllm/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service:8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"vllm\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gemma\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>user\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>model\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradiospec:\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 targetPort: 7860\u00a0 type: ClusterIP\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Wait for the deployment to be available:```\nkubectl wait --for=condition=Available --timeout=300s deployment/gradio\n```\n- In Cloud Shell, run the following command:```\nkubectl port-forward service/gradio 8080:8080\n```This creates a port forward from Cloud Shell to the Gradio service.\n- Click the **Web Preview** button which can be found on the top right of the Cloud Shell taskbar. Click **Preview on Port 8080** . A new tab opens in your browser.\n- Interact with Gemma using the Gradio chat interface. Add a prompt and click **Submit** .\n## Troubleshoot issues\n- If you get the message`Empty reply from server`, it's possible the container has not finished downloading the model data. [Check the Pod's logs](#deploy-vllm) again for the`Connected`message which indicates that the model is ready to serve.\n- If you see`Connection refused`, verify that your [port forwarding is active](#setup-port-forwarding) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the deployed resourcesTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:\n```\ngcloud container clusters delete ${CLUSTER_NAME} \\\u00a0 --region=${REGION}\n```## What's next\n- Learn more about [GPUs inGKE](/kubernetes-engine/docs/concepts/gpus) .\n- Learn how to use Gemma with vLLM on other accelerators, including A100 and H100 GPUs, by [viewing the sample code in GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/ai-ml/llm-serving-gemma/vllm) .\n- Learn how to [deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .\n- Learn how to [deploy GPU workloads in Standard](/kubernetes-engine/docs/how-to/gpus) .\n- Explore the vLLM [GitHub repository](https://github.com/vllm-project/vllm) and [documentation](https://docs.vllm.ai/en/latest/) .\n- Explore the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) .\n- Discover how to run optimized AI/ML workloads with [GKEplatform orchestrationcapabilities](/kubernetes-engine/docs/integrations/ai-infra) .", "guide": "Google Kubernetes Engine (GKE)"}