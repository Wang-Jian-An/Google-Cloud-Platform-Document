{"title": "Google Kubernetes Engine (GKE) - Extend the run time of Autopilot Pods", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/extended-duration-pods", "abstract": "# Google Kubernetes Engine (GKE) - Extend the run time of Autopilot Pods\nThis page shows you how to request extended run times for Pods before they're evicted by Google Kubernetes Engine (GKE).\n", "content": "## About GKE-initiated Pod eviction\nPod evictions are a normal part of running workloads on Kubernetes. GKE evicts workloads during scheduled events, such as automatic node upgrades and autoscaling scale-downs, to ensure that your nodes are up-to-date and optimized for efficient resource usage. By default, GKE sends a termination signal to the container as soon as the event occurs, after which the container has a grace period to terminate before Kubernetes evicts the Pod. For automatic node upgrades, the grace period can be up to one hour. For scale-down events, the grace period can be up to 10 minutes.\nKubernetes has built-in features that containers can use to gracefully handle evictions, such as [PodDisruptionBudgets](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [graceful termination periods](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) . However, some workloads, such as batch queues or multiplayer game servers, need to run for a longer period of time before being evicted. The default grace period that GKE grants during GKE-initiated evictions might not be enough for these workloads. In these situations, you can tell Autopilot to avoid evicting specific workloads for up to 7 days.\n### Use cases\nSome situations in which you might want to tell GKE to avoid evicting workloads include the following:\n- You run multiplayer game servers that would kick players out of their sessions if the servers terminated early.\n- You run audio or video conferencing software that would disrupt in-progress meetings if the servers terminated.\n- You run tasks that need time to complete, and early termination would cause a loss of in-progress work.\n- You run a stateful service that is less tolerant to disruption and you want to minimize how often disruptions occur.## Pricing\nYou can request extended run times for your Pods at no additional charge. However, consider the following behavioral changes that might impact your pricing:\n- Autopilot clusters enforce [higher minimum values](/kubernetes-engine/docs/concepts/autopilot-resource-requests#workload-separation) for the resource requests of extended duration Pods. Autopilot clusters charge you for the resource requests of your running Pods. You're not charged for system overhead or for unused node capacity.\n- Using extended duration Pods might increase the number of nodes in your cluster, which might affect IP address usage and scalability. If you have DaemonSets that run on every node, this results in more DaemonSets in the cluster,\nFor pricing details, see [Autopilot pricing](/kubernetes-engine/pricing#autopilot_mode) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have an [Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) running version 1.27 or later.\n### Limitations\n- You can't request extended run times for your Spot Pods.\n- Image pull times are counted when calculating the extended run time.\n- You can have a maximum of 50 extended duration workloads (with different CPU requests) in each cluster. This means that up to 50 different sets of CPU request values, after passing Autopilot resource minimums, ratios, and increment size checks, can have extended duration in each cluster.\n- You can't use Kubernetes inter-Pod affinity in extended duration Pods.\n- Whenever possible, GKE places each extended run time Pod on its own node. This behavior ensures that nodes can scale down if they're under-utilized.## Request extended run time\nTo request extended run time for a Pod, set the Kubernetes `cluster-autoscaler.kubernetes.io/safe-to-evict` annotation to `false` in the Pod specification.\n- Save the following manifest as `extended-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: extended-pods\u00a0 labels:\u00a0 \u00a0 duration: extendedspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 duration: extended\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 annotations:\u00a0 \u00a0 \u00a0 \u00a0 cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 duration: extended\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: example-container\u00a0 \u00a0 \u00a0 \u00a0 image: registry.k8s.io/pause\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 200m\n```\n- Create the Deployment:```\nkubectl create -f extended-deployment.yaml\n```\nThe Pods continue to run for at least 7 days before a scale-down or a node auto-upgrade can occur.\n## Considerations and recommendations\nWhen you use this functionality, consider the following:\n- Extended duration Pods aren't protected from priority-based eviction. If you use [Kubernetes PriorityClasses](/kubernetes-engine/docs/how-to/capacity-provisioning) , consider the following methods to minimize the probability of priority-based eviction:- Ensure that your extended duration Pods use the highest priority PriorityClass, so that other user Pods don't evict your extended duration Pods.\n- Use [workload separation](/kubernetes-engine/docs/how-to/workload-separation) to run extended duration Pods separately from other Pods.\n- System Pods run with the highest priority and will always be able to evict extended duration Pods. To minimize the probability of this, GKE schedules system Pods on the node before scheduling the extended duration Pod.\n- Extended duration Pods can still be evicted early in the following situations:- Eviction to make space for higher-priority user Pods (using a higher PriorityClass)\n- Eviction to make space for Kubernetes system components\n- [kubelet out-of-memory eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/) if the Pod uses more memory than it requested (OOMKill)\n- Compute Engine VM maintenance events. [Accelerator-optimized machine types](/compute/docs/accelerator-optimized-machines) are more likely to be affected by these events because those machines don't support [live migration](/compute/docs/instances/live-migration-process) .\n- Node auto-repairs\n- User-initiated events such as draining a node\n- You can use the`cluster-autoscaler.kubernetes.io/safe-to-evict`annotation in Standard clusters, but the result is not the same. Pods run indefinitely even if a scale-down event occurs, preventing deletion of underutilized nodes and resulting in you continuing to pay for those nodes. Pods also aren't protected from evictions caused by node auto-upgrades.## What's next\n- [Use PriorityClasses to provision spare capacity in Autopilot forrapid Pod scaling](/kubernetes-engine/docs/how-to/capacity-provisioning)", "guide": "Google Kubernetes Engine (GKE)"}