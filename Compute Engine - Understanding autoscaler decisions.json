{"title": "Compute Engine - Understanding autoscaler decisions", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Understanding autoscaler decisions\nAutoscaling automatically adds VMs (scales out) or removes VMs (scales in) your [managed instance group (MIG)](/compute/docs/instance-groups) . This document explains how an autoscaler determines when to scale your MIGs.\n", "content": "## How autoscalers calculate recommended size and affect target size\nWhen you configure an autoscaler for a MIG, the autoscaler constantly monitors the group and sets the group's recommended size to the number of virtual machine (VM) instances that are required in order to serve peak load over the [stabilization period](/compute/docs/autoscaler#stabilization_period) .\nThe recommended size is constrained by the minimum and maximum number of instances that you set in the autoscaling policy.\nIf your autoscaling policy includes [scale-in controls](/compute/docs/autoscaler#scale-in_controls) , then the recommended size is further constrained by your scale-in controls.\nIf you enable [predictive autoscaling](/compute/docs/autoscaler/predictive-autoscaling) , then the autoscaler uses historical CPU utilization patterns to forecast future load, and it sets the group's recommended size based on its prediction.\nThe MIG's response to the autoscaler's recommended size depends on how you configure the autoscaler's [mode](/compute/docs/autoscaler#autoscaling_mode) :\n- `ON`. The MIG sets its target size to the recommended size, and then Compute Engine automatically scales out the MIG to meet its target size.\n- `ONLY_SCALE_OUT`. The MIG's target size can only be increased in response to an increased recommended size.\n- `OFF`. The target size is unaffected by the recommended size. However, the recommended size is still calculated.\nIf the autoscaler configuration is deleted, then no recommended size is calculated.\n## Gaps between target and actual utilization metrics\nWhen using an autoscaling policy with [metrics-based signals](/compute/docs/autoscaler#target_utilization) , you might notice that, for smaller instance groups, the actual utilization of the instance group and the target utilization might seem far apart. This is because an autoscaler always acts conservatively by rounding up or down when it interprets utilization data and determines how many instances to add or remove. This prevents the autoscaler from adding an insufficient number of resources or removing too many resources.\nFor example, if you set a utilization target of 0.7 and your application exceeds the utilization target, the autoscaler might determine that adding 1.5 virtual machine (VM) instances would decrease the utilization to close to 0.7. Because you cannot add 1.5 VM instances, the autoscaler rounds up and adds two instances. This might decrease the average CPU utilization to a value below 0.7 but ensures that your app has enough resources to support it.\nSimilarly, if the autoscaler determines that removing 1.5 VM instances would increase your utilization too close to 0.7, it will remove just one instance.\nFor larger groups with more VM instances, the utilization is divided up over a larger number of instances, and adding or removing VM instances causes less of a gap between actual utilization and target utilization.\nIf you use schedule-based autoscaling with another autoscaling signal, an active schedule might require more VMs than your utilization needs. In these situations, your actual utilization is lower than your target utilization because the autoscaling schedule determines the recommended size of the instance group.\n### Regional MIGs and uneven VM distributions\nIf a region has an unbalanced number of instances between zones, whether due to recovering from a zonal failure or due to an unevenly distributed workload, autoscaling keeps more instances running in zones that have a higher than average actual utilization. Compute Engine takes this precaution to guarantee high availability across the region as a whole as well as all zones individually, even if some of the zones experience heavier load than others.\n## Delays in scaling out\nWhen you configure autoscaling, you specify an [initialization period](/compute/docs/autoscaler#cool_down_period) that reflects the length of time it takes for your VMs to initialize. The autoscaler only recommends scaling out if the average utilization from instances that are not initializing is greater than the target utilization.\nIf you set an initialization period value that is significantly longer than the time it takes for an instance to initialize, then your autoscaler might ignore legitimate utilization data, and it might underestimate the required size of your group.\n## Delays in scaling in\nFor the purposes of scaling in, the autoscaler calculates the group's recommended target size based on peak load over either the last 10 minutes or the [initialization period](/compute/docs/autoscaler#cool_down_period) that you set, whichever is longer. This duration is referred to as the [stabilization period](/compute/docs/autoscaler#stabilization_period) .\nObserving the usage during the stabilization period helps the autoscaler:\n- Ensure that usage information collected from the instance group is stable.\n- Prevent behavior where an autoscaler continuously adds or removes instances at an excessive rate.\n- Safely remove instances by determining that the smaller group size is enough to support peak load from the stabilization period.\n- If your application takes longer than 10 minutes to initialize on a new VM, then the group uses the initialization period as the stabilization period. This ensures that the autoscaler decision to delete VM takes into account how long it takes to get back the serving capacity.\nThe stabilization period might appear as a delay in scaling in, but it is actually a built-in feature of autoscaling. The stabilization period also ensures that if a new instance is added to the managed instance group, the instance complete its initialization period or runs for at least 10 minutes before it is eligible for being deleted.\n[Initialization periods](/compute/docs/autoscaler#cool_down_period) for new instances are ignored when deciding whether to scale in a group.\n### Connection draining causing delays\nIf the group is part of a [backend service](/compute/docs/load-balancing/http/backend-service) that has [enabled connection draining](/compute/docs/load-balancing/enabling-connection-draining) , it can take up to an additional 60 seconds after the connection draining duration has elapsed before the VM instance is removed or deleted.\n## Scale-in controls\nWhen you [configure autoscaler scale-in controls](/compute/docs/autoscaler/managing-autoscalers#configure_scale_in_controls) , you control the speed of scaling in. The autoscaler never scales in faster than your configured rate:- When load declines, the autoscaler maintains the size for the group at a level required to serve the peak load observed in the ( [stabilizationperiod](/compute/docs/autoscaler#stabilization_period) ). This works the same with and without scale-in controls.\n- An autoscaler without scale-in controls keeps only enough instances required to handle recently observed load. After the stabilization period, the autoscaler removes all unneeded instances in one step. With a sudden drop in load this can lead to a dramatic reduction of instance group size.\n- An autoscaler with scale-in controls limits how many VM instances can be removed in a configured period of time (here 10 VMs in 20 minutes). This slows down the instance reduction rate.\n- With a new load spike, the autoscaler adds new instances to handle the load. However, due to a long initialization time, the new VMs are not ready to serve the load. With scale-in controls, the previous capacity was kept, allowing existing VMs to serve the spike.\nYou control the scale-in rate by configuring the Autoscaler's within a , specifically:\n- **Maximum allowed reduction** (`maxScaledInReplicas`: number or % of VM instances). The number of instances that your workload can afford to lose (from the group's peak size) within the specified trailing time window. Use this parameter to constrain how much your group can be scaled in so that you can still serve a likely load spike until more instances start serving. The lower the maximum allowed reduction, the slower the rate of scale in.\n- **Trailing time window** (`timeWindowSec`: seconds). Time during which a load spike is likely to follow a temporary decline and during which you don't want your group size to scale in beyond the maximum allowed reduction. Use this parameter to define the time window in which autoscaler will look for the peak size sufficient to serve historical load. Autoscaler will not resize below the maximum allowed reduction subtracted from the peak size observed in the trailing time window. With a longer trailing time window, the autoscaler considers more historical peak load, which makes scaling in more conservative and stable.\nWhen you set scale-in controls, the autoscaler constrains scale-in operations to the maximum allowed reduction from the peak size observed in the trailing time window. The autoscaler uses the following steps:\n- Continuously monitors the historical peak size observed in the trailing time window.\n- Uses the maximum allowed reduction to calculate the constrained scale-in size (peak size:`maxScaledInReplicas`)\n- Sets the group's [recommended size](/compute/docs/autoscaler/understanding-autoscaler-decisions#how_autoscalers_calculate_recommended_size_and_affect_target_size) to the constrained scale-in size. For example, if an autoscaler would resize an instance group to 20 VMs but scale-in constraints only allow a scale in to 40 VMs, then the recommended size is set to 40 VMs.\nWith scale-in controls, the autoscaler continuously monitors the peak size of an instance group within the configured trailing time window to identify the sufficient size to serve historical load. The autoscaler does not scale in beyond the maximum allowed reduction measured from the observed peak size:\nFor example, in the diagram above, scale-in controls are configured for a 20 VM maximum allowed reduction in a 30-minute trailing time window:\n- When load goes down autoscaler removes 20 VMs, which is the maximum allowed reduction configured in scale-in controls.\n- As the load goes up and down, the autoscaler constantly monitors the last 30 minutes trailing time window for the peak size sufficient to serve historical load. This peak size is used as a base for scale-in controls to limit scale-in rate. If, in the last 30 minutes, peak size was 70 VMs and the maximum allowed reduction is set to 20 VMs, the autoscaler can scale in to 50 VMs. If the current size is 65 VMs, the autoscaler can remove only 15 VMs.\n- As load decreases, the autoscaler continues to remove VM instances but limits the rate to at most 20 VMs from the peak instance group size measured in the last 30 minutes.\nThe maximum allowed reduction in group size might happen all at once, so you should configure the maximum allowed reduction so that your application can afford losing that many instances at once. Use the maximum allowed reduction parameter to indicate how much reduction in serving capacity your application can tolerate.\nBy limiting the number of VM instances that autoscaling can remove and by increasing the observed trailing time window, applications with load spikes and long initialization times should experience improved availability. In particular, the instance group size doesn't drop abruptly in response to a significant drop in load and instead decreases gradually over time. If load spikes soon after a scale in has occurred, the remaining number of VMs should still be able to absorb the spike within your tolerance. In addition, fewer VMs must be started in order to sufficiently serve the spike.\nYou can configure scale-in controls for autoscaling of both zonal and regional MIGs. The configuration is the same for both cases. Scale-in controls work for any group size.\n### Scale-in controls versus autoscaler stabilization\nConfiguring scale-in controls does not mean switching off the autoscaler's built-in [stabilization mechanism](/compute/docs/autoscaler/understanding-autoscaler-decisions#delays_in_scaling_in) . The autoscaler always maintains the instance group size at a level required to serve peak load, observed during the stabilization period. Scale-in controls give you an additional mechanism to control the rate at which an instance group is resized.\n| Unnamed: 0   | Autoscaler built-in: Stabilization period                                  | Scale-in controls: Trailing time window                               |\n|:--------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Configurable?  | No, non-configurable                                        | Yes, configurable                                     |\n| What is monitored? | Monitors peak load over the previous 10 minutes or the initialization period, whichever is longer.                    | Monitors peak size of the instance group in the previous period set by trailing time window                  |\n| How does this help? | Ensures that the instance group size remains sufficient to serve the peak load that was observed during the last 10 minutes or the initialization period, whichever is longer. | Ensures that the instance group size is not reduced by more VM instances than your workload can tolerate when handling load spikes over a specified time window. |\n### Scale-in controls with autoscaler mode\nThere are two similar yet slightly different scenarios when your MIG is not autoscaled and you want to turn on autoscaling. These depend on whether you are configuring autoscaling for the first time or whether autoscaling is configured but [temporarily restricted or off](/compute/docs/autoscaler/managing-autoscalers#turn_off_or_restrict_an_autoscaler) .\nWhen you have a non-autoscaled MIG and you configure autoscaling from scratch, the autoscaler uses the current MIG size as a starting point. Before scaling in, the autoscaler uses the stabilization period and then it uses scale-in controls to constrain the scale-in rate:\nWith [autoscaling mode](/compute/docs/autoscaler/managing-autoscalers#turn_off_or_restrict_an_autoscaler) , you can temporarily turn off or restrict autoscaling activities. The autoscaler's configuration persists and the autoscaler continues to perform background calculations while the autoscaler is off or restricted. The autoscaler takes into account scale-in controls in its background calculations while in off or restricted mode. All autoscaling activities resume using the most recent calculations when you turn autoscaling on again or when you lift the restriction:- Autoscaler turned ON behaves as usual (with scale-in controls in this case).\n- When you turn autoscaler OFF, it still calculates the recommended instance group size based on load. Autoscaler calculations still consider scale-in controls. However, the autoscaler does not apply size calculations when autoscaler is OFF. The instance group size stays constant until autoscaler is ON again.\n- When you turn the autoscaler ON again it immediately applies the previously calculated size. This allows faster scaling to the correct size. Re-enabling the autoscaler can cause an abrupt scale in (here from 80 to 40 VM instances). This is safe because background calculations already consider scale-in controls.## Predictive autoscaling\nTo learn about predictive autoscaling, including [how it works](/compute/docs/autoscaler/predictive-autoscaling#how_predictive_autoscaling_works) , see [Scaling based on predictions](/compute/docs/autoscaler/predictive-autoscaling) .\n## Preparing to stop instances\nWhen the autoscaler scales in, it determines the number of VM instances to delete. The autoscaler prioritizes VM instances to delete based on several factors, including the following:\n- VMs that are not running for any reason.\n- VMs that are undergoing or scheduled for disruptive changes\u2014for example, refresh, restart, or replace.\n- VMs that are not yet updated to the intended version of the instance template.\n- VMs that have the lowest autoscaling signal. For example, if you configure your MIG to scale based on CPU utilization, and the group needs to scale in, then the autoscaler attempts to remove the VMs that have the lowest CPU utilization.\nBefore an instance is stopped, you might want to make sure these instances perform certain tasks, such as closing any existing connections, gracefully shut down any apps or app servers, uploading logs, and so on. You can instruct your instance to perform these tasks using a [shutdown script](/compute/docs/shutdownscript) . A shutdown script runs, on a [best-effort basis](/compute/docs/shutdownscript#time_period) , in the brief period between when the stopping request is made and when the instance is actually stopped. During this period, Compute Engine attempts to run your shutdown script to perform any tasks you provide in the script.\nThis is particularly useful if you are using load balancing with your managed instance group. If your instance becomes unhealthy, it might take some time for the load balancer to recognize that the instance is unhealthy, causing the load balancer to continue sending new requests to the instance. With a shutdown script, the instance can report that it is unhealthy while it is shutting down so that the load balancer can stop sending traffic to the instance. For more information about load balancing health checks, see the [health checks overview](/load-balancing/docs/health-check-concepts) .\nFor more information about shutdown scripts, see [Shutdown scripts](/compute/docs/shutdownscript) .\nFor more information about instance shutdown, read documentation on [stopping](/compute/docs/instances/stop-start-instance#stopping_an_instance) or [deleting](/compute/docs/instances/deleting-instance) an instance.\n## Monitoring autoscaling charts and logs\nCompute Engine provides several charts and logs that allow you to monitor your managed instance group's behavior at any point in time.\nYou can access the charts and logs in the Google Cloud console.\n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups) \n- Click the name of the managed instance group you want to view.\n- On the managed instance group page, select the **Monitoring** tab.\nThe monitoring charts display the evolution of the following metrics:\n- Group size\n- Autoscaler utilization\n- CPU utilization\n- Disk I/O (bytes)\n- Disk I/O (operations)\n- Network bytes\n- Network packets\nA tooltip next to the title of each chart provides additional contextual details about the metric displayed.\nA **Logs** panel is available at the bottom of the page, where you can find a list of event logs for your managed instance group. To view the logs, click the expander arrow.\nAll charts and logs are bound to a single time frame that you can customize with the time range selector. By clicking and dragging on any chart, you can zoom in on a particular event and analyse the graphs and logs within the selected time range.\n### Monitoring predictive autoscaling\nCompute Engine provides a chart to monitor the autoscaler predictions. To view this chart, click the **Group size** title in the first chart and select **Predictive autoscaling** .\nIf autoscaling is enabled, you can view how the autoscaler predictions determine the size of your instance group. If autoscaling is not enabled, you can still see the autoscaler predictions and use them to inform your decisions regarding your group size.\nUse the following information to understand this chart.\n- The blue line indicates the number of instances in the managed instance group.\n- The green line shows the number of instances predicted by autoscaler.- If the green line is below the blue line, there is a large amount of capacity available and your VM instances are likely under utilized.\n- If the green line is above the blue line, then there is little, if any, remaining capacity and you should add more instances to the instance group.\n- The dashed horizontal red lines indicate the minimum and maximum number of instances allowed in your instance group.## Viewing status messages\nWhen the autoscaler experiences an issue scaling, it returns a warning or error message. You can review these status messages in one of two ways.\n**View status messages on the Instance groups page**\nYou can view status messages directly on the **Instance groups** page in the [Google Cloud console](https://console.cloud.google.com/) .\n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups) \n- Look for any instance groups that have the caution icon preceding their names.For example: \n- Hold the pointer over a status icon to get details of the status message.\n**View status messages on the Instance group overview page**\nYou can go directly to the overview page of a specific instance group to view relevant status messages.\n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups) \n- Click the instance group for which you want to view status messages.\n- On the instance group page, view the status message below the instance group name.## Commonly returned status messages\nWhen the autoscaler experiences an issue scaling, it returns a warning or error message. Here are some commonly returned messages and what they mean.", "guide": "Compute Engine"}