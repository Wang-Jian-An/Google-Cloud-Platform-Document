{"title": "Cloud Architecture Center - Disaster recovery scenarios for data", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Disaster recovery scenarios for data\nLast reviewed 2022-06-10 UTC\nThis article is the third part of a series that discusses [disaster recovery (DR)](/solutions/backup-dr) in Google Cloud. This part discusses scenarios for backing up and recovering data.\nThe series consists of these parts:\n- [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) \n- [Disaster recovery building blocks](/architecture/dr-scenarios-building-blocks) \n- Disaster recovery scenarios for data (this article)\n- [Disaster recovery scenarios for applications](/architecture/dr-scenarios-for-applications) \n- [Architecting disaster recovery for locality-restricted workloads](/architecture/architecting-disaster-recovery-for-locality-restricted-workloads) \n- [Disaster recovery use cases: locality-restricted data analytic applications](/architecture/dr-scenarios-locality-restricted-data-analytics) \n- [Architecting disaster recovery for cloud infrastructure outages](/solutions/disaster-recovery/architecture) ", "content": "## Introduction\nYour disaster recovery plans must specify how you can avoid losing data during a disaster. The term here covers two scenarios. Backing up and then recovering database, log data, and other data types fits into one of the following scenarios:\n- **Data backups** . Backing up data alone involves copying a discrete amount of data from one place to another. Backups are made as part of a recovery plan either to recover from a corruption of data so that you can restore to a known good state directly in the production environment, or so that you can restore data in your DR environment if your production environment is down. Typically, data backups have a small to medium RTO and a small RPO.\n- **Database backups** . Database backups are slightly more complex, because they typically involve recovering to the point in time. Therefore, in addition to considering how to back up and restore the database backups and ensuring that the recovery database system mirrors the production configuration (same version, mirrored disk configuration), you also need to consider how to back up transaction logs. During recovery, after you restore database functionality, you have to apply the latest database backup and then the recovered transaction logs that were backed up after the last backup. Due to the complicating factors inherent to database systems (for example, having to match versions between production and recovery systems), adopting a high-availability-first approach to minimize the time to recover from a situation that could cause unavailability of the database server allows you to achieve smaller RTO and RPO values.\nThe rest of this article discusses examples of how to design some scenarios for data and databases that can help you meet your RTO and RPO goals.\n## Production environment is on-premises\nIn this scenario, your production environment is on-premises, and your disaster recovery plan involves using Google Cloud as the recovery site.\n### Data backup and recovery\nYou can use a number of strategies to implement a process to regularly back up data from on-premises to Google Cloud. This section looks at two of the most common solutions.\n**DR building blocks:**\n- Cloud Storage\nOne option for backing up data is to create a scheduled task that runs a script or application to transfer the data to Cloud Storage. You can automate a backup process to Cloud Storage using the [gsutil](/storage/docs/gsutil) command-line tool or by using one of the Cloud Storage [client libraries](/storage/docs/reference/libraries) . For example, the following `gsutil` command copies all files from a source directory to a specified bucket.\n```\ngsutil -m cp -r [SOURCE_DIRECTORY] gs://[BUCKET_NAME]\n```\nThe following steps outline how to implement a backup and recovery process using the `gsutil` tool.\n- [Install gsutil](/storage/docs/gsutil_install#install) on the on-premises machine that you use to upload your data files from.\n- [Create a bucket](/storage/docs/quickstart-gsutil#create) as the target for your data backup.\n- [Generate a service account key](/iam/docs/creating-managing-service-account-keys#creating_service_account_keys) for a dedicated service account in JSON format. This file is used to [pass credentials to gsutil](/storage/docs/authentication#cliauth) as part of an automated script\n- Copy the service account key to the on-premises machine where you run the script that you use to upload your backups. **Note:** Make sure that you keep your [service account keys safely](https://cloudplatform.googleblog.com/2017/07/help-keep-your-Google-Cloud-service-account-keys-safe.html?m=1) .\n- Create an [IAM policy](/storage/docs/access-control/using-iam-permissions) to restrict who can access the bucket and its objects. (Include the service account created specifically for this purpose and an on-premises operator account). For details about permissions for access to Cloud Storage, see [IAM permissions for gsutil commands](/storage/docs/access-control/iam-gsutil) .\n- Test that you can [upload and download](/storage/docs/gsutil/commands/cp) files in the target bucket.\n- Follow the guidance in [scripting data transfer tasks](/storage/docs/gsutil/addlhelp/ScriptingProductionTransfers#scripting-data-transfer-tasks) to set up a scheduled script.\n- Configure a recovery process that uses `gsutil` to recover your data to your recovery DR environment on Google Cloud.\nYou can also use the [gsutil rsync](/storage/docs/gsutil/commands/rsync) command to perform real-time incremental syncs between your data and a Cloud Storage bucket.\nFor example, the following `gsutil rsync` command makes the contents in a Cloud Storage bucket the same as the contents in the source directory by copying any missing files or objects or those whose data has changed. If the volume of data that has changed between successive backup sessions is small relative to the entire volume of the source data, using `gsutil rsync` can be more efficient than using `gsutil cp` . This in turns allows for a more frequent backup schedule and lets you achieve lower RPO value.\n```\ngsutil -m rsync -r [SOURCE_DIRECTORY] gs://[BUCKET_NAME]\n```\nFor more information, see [Transfer from colocation or on-premises storage](/solutions/transferring-big-data-sets-to-gcp#transfer_from_colocation_or_on-premises_storage) , which includes ways to optimize the transfer process.\n**DR building blocks:**\n- Cloud Storage\n- Transfer service for on-premises data\nTransferring large amounts of data across a network often requires careful planning and robust execution strategies. It is a non-trivial task to develop custom scripts that are scalable, reliable, and maintainable. Custom scripts can often lead to lowered RPO values and even increased risks of data loss.\nOne way to implement large-scale data transfer is to use [Transfer service for on-premises data](/storage-transfer/docs/on-prem-overview) . This service is a scalable, reliable, and managed service that enables you to transfer large amounts of data from your data center to a Cloud Storage bucket without investing in engineering teams or buying transfer solutions.\n**DR building blocks:**\n- Cloud Interconnect\n- Cloud Storage tiered storage\nOn-premises applications are often integrated with third-party solutions that can be used as part of your data backup and recovery strategy. The solutions often use a tiered storage pattern where you have the most recent backups on faster storage, and slowly migrate your older backups to cheaper (slower) storage. When you use Google Cloud as the target, you have several [storage class options](/storage/docs/storage-classes) available to use as the equivalent of the slower tier.\nOne way to implement this pattern is to use a partner gateway between your on-premises storage and Google Cloud to facilitate this transfer of data to Cloud Storage. The following diagram illustrates this arrangement, with a partner solution that manages the transfer from the on-premises NAS appliance or SAN.\nIn the event of a failure, the data being backed up must be recovered to your DR environment. The DR environment is used to serve production traffic until you are able to revert to your production environment. How you achieve this depends on your application, and on the partner solution and its architecture. (Some end-to-end scenarios are discussed in the [DR application document](/architecture/dr-scenarios-for-applications) .)\nFor further guidance on ways to transfer data from on-premises to Google Cloud, see [Transferring big data sets to Google Cloud](/solutions/transferring-big-data-sets-to-gcp) .\nFor more information about partner solutions, see the [Partners page](/storage/partners) on the Google Cloud website.\n### Database backup and recovery\nYou can use a number of strategies to implement a process to recover a database system from on-premises to Google Cloud. This section looks at two of the most common solutions.\nIt is out of scope in this article to discuss in detail the various built-in backup and recovery mechanisms included with third-party databases. This section provides general guidance, which is implemented in the solutions discussed here.- Create a database backup using the built-in backup mechanisms of your database management system.\n- Connect your on-premises network and your Google Cloud network.\n- [Create a Cloud Storage bucket](/storage/docs/quickstart-gsutil#create) as the target for your data backup.\n- Copy the backup files to Cloud Storage using`gsutil`or a partner gateway solution (see the [steps](#solution_1_back_up_to_storagenameshort_using_a_scheduled_task_id%E2%80%9Dback-up-to-cloud-storage-using-a-scheduled-task%E2%80%9D) discussed earlier in the data backup and recovery section). For details, see [Transferring big data sets to Google Cloud](/solutions/transferring-big-data-sets-to-gcp) .\n- Copy the transaction logs to your recovery site on Google Cloud. Having a backup of the transaction logs helps keep your RPO values small.\nAfter configuring this backup topology, you must ensure that you can recover to the system that's on Google Cloud. This step typically involves not only restoring the backup file to the target database but also replaying the transaction logs to get to the smallest RTO value. A typical recovery sequence looks like this:\n- Create a [custom image](/compute/docs/images/create-delete-deprecate-private-images#creating_a_custom_image) of your database server on Google Cloud. The database server should have the same configuration on the image as your on-premises database server.\n- Implement a process to copy your on-premises backup files and transaction log files to Cloud Storage. See [solution 1](#back-up-to-cloud-storage-using-a-scheduled-task) for an example implementation.\n- [Start a minimally sized instance from the custom image](/compute/docs/instances/create-start-instance#creating_an_instance_from_a_custom_image) and attach any persistent disks that are needed.\n- Set the [auto delete flag](/compute/docs/disks/add-persistent-disk#updateautodelete) to false for the persistent disks.\n- Apply the latest backup file that was previously copied to Cloud Storage, following the instructions from your database system for recovering backup files.\n- Apply the latest set of transaction log files that have been copied to Cloud Storage.\n- [Replace the minimal instance with a larger instance](/compute/docs/instances/changing-machine-type-of-stopped-instance) that is capable of accepting production traffic.\n- Switch clients to point at the recovered database in Google Cloud.\nWhen you have your production environment running and able to support production workloads, you have to reverse the steps that you followed to fail over to the Google Cloud recovery environment. A typical sequence to return to the production environment looks like this:\n- Take a backup of the database running on Google Cloud.\n- Copy the backup file to your production environment.\n- Apply the backup file to your production database system.\n- Prevent clients from connecting to the database system in Google Cloud; for example, by stopping the database system service. From this point, your application will be unavailable until you finish restoring the production environment.\n- Copy any transaction log files over to the production environment and apply them.\n- Redirect client connections to the production environment.One way to achieve very small RTO and RPO values is to replicate (not just back up) data and in some cases database state in real time to a hot standby of your database server.\n- Connect your on-premises network and your Google Cloud network.\n- Create a [custom image](/compute/docs/images/create-delete-deprecate-private-images#creating_a_custom_image) of your database server on Google Cloud. The database server should have the same configuration on the image as the configuration of your on-premises database server.\n- [Start an instance from the custom image](/compute/docs/instances/create-start-instance#creating_an_instance_from_a_custom_image) and attach any persistent disks that are needed.\n- Set the [auto delete flag](/compute/docs/disks/add-persistent-disk#updateautodelete) to false for the persistent disks.\n- Configure replication between your on-premises database server and the target database server in Google Cloud following the instructions specific to the database software.\n- Clients are configured in normal operation to point to the database server on premises.\nAfter configuring this replication topology, switch clients to point to the standby server running in your Google Cloud network.\nWhen you have your production environment back up and able to support production workloads, you have to resynchronize the production database server with the Google Cloud database server and then switch clients to point back to the production environment\n## Production environment is Google Cloud\nIn this scenario, both your production environment and your disaster recovery environment run on Google Cloud.\n### Data backup and recovery\nA common pattern for data backups is to use a tiered storage pattern. When your production workload is on Google Cloud, the tiered storage system looks like the following diagram. You migrate data to a tier that has lower storage costs, because the requirement to access the backed-up data is less likely.\n**DR building blocks:**\n- Cloud Storage tiered storage classes ( [Nearline](/storage/docs/storage-classes#nearline) , [Coldline](/storage/docs/storage-classes#coldline) , and [Archive](/storage/docs/storage-classes#archive) )Because the Nearline, Coldline, and Archive storage classes are intended for storing infrequently accessed data, there are [additional costs](/storage/pricing) associated with retrieving data or metadata stored in these classes, as well as minimum storage durations that you are charged for.\n### Database backup and recovery\nWhen you use a self-managed database (for example, you've installed MySQL, PostgreSQL, or SQL Server on an instance of Compute Engine), the same operational concerns apply as managing production databases on premises, but you no longer need to manage the underlying infrastructure.\n**Note:** The scenario for using a managed Google Cloud database is discussed in the next section.\nYou can set up HA configurations by using the appropriate [DR building block features](/architecture/dr-scenarios-building-blocks) to keep RTO small. You can design your database configuration to make it easy to recover to a state as close as possible to the pre-disaster state; this helps keep your RPO values small. Google Cloud provides a wide variety of options for this scenario.\nTwo common approaches to designing your database recovery architecture for self-managed databases on Google Cloud are discussed in this section.\nA common pattern is to enable recovery of a database server that does not require system state to be synchronized with a hot standby.\n**DR building blocks:**\n- Compute Engine\n- Managed instance groups\n- Cloud Load Balancing (internal load balancing)\nThe following diagram illustrates an example architecture that addresses the scenario. By implementing this architecture, you have a DR plan that reacts automatically to a failure without requiring manual recovery.\nThe following steps outline how to configure this scenario:\n- [Create a VPC network](/vpc/docs/create-modify-vpc-networks) .\n- Create a [custom image](/compute/docs/images/create-delete-deprecate-private-images#creating_a_custom_image) that is configured with the database server by doing the following:- Configure the server so the database files and log files are written to an attached standard persistent disk.\n- [Create a snapshot](/compute/docs/disks/create-snapshots#creating_snapshots) from the attached persistent disk.\n- Configure a startup script to create a persistent disk from the snapshot and to mount the disk.\n- Create a custom image of the boot disk.\n- Create an [instance template](/compute/docs/instance-templates/create-instance-templates) that uses the image.\n- Using the instance template, configure a [managed instance group](/compute/docs/instance-groups/creating-groups-of-managed-instances) with a target size of 1.\n- Configure health checking using Cloud Monitoring metrics.\n- Configure [internal load balancing](/compute/docs/load-balancing/internal#setting-up-internal-load-balancing) using the managed instance group.\n- Configure a scheduled task to create regular snapshots of the persistent disk.\nIn the event a replacement database instance is needed, this configuration automatically does the following:\n- Brings up another database server of the correct version in the same zone.\n- Attaches a persistent disk that has the latest backup and transaction log files to the newly created database server instance.\n- Minimizes the need to reconfigure clients that communicate with your database server in response to an event.\n- Ensures that the Google Cloud security controls (IAM policies, firewall settings) that apply to the production database server apply to the recovered database server.\nBecause the replacement instance is created from an instance template, the controls that applied to the original apply to the replacement instance.\nThis scenario takes advantage of some of the HA features available in Google Cloud; you don't have to initiate any failover steps, because they occur automatically in the event of a disaster. The internal load balancer ensures that even when a replacement instance is needed, the same IP address is used for the database server. The instance template and custom image ensure that the replacement instance is configured identically to the instance it is replacing. By taking regular snapshots of the persistent disks, you ensure that when disks are re-created from the snapshots and attached to the replacement instance, the replacement instance is using data recovered according to an RPO value dictated by the frequency of the snapshots. In this architecture, the latest transaction log files that were written to the persistent disk are also automatically restored.\nThe managed instance group provides HA in depth. It provides mechanisms to react to failures at the application or instance level, and you don't have to manually intervene if any of those scenarios occur. Setting a target size of one ensures you only ever have one active instance that runs in the managed instance group and serves traffic.\nStandard persistent disks are zonal, so if there's a zonal failure, snapshots are required to re-create disks. Snapshots are also available across regions, which lets you restore a disk to a different region as easily as you can restore it to the same region.\nA variation on this configuration is to use regional persistent disks in place of standard persistent disks. In this case, you don't need to restore the snapshot as part of the recovery step.\nThe variation you choose is dictated by your budget and RTO and RPO values.\nFor more information about database configurations designed for HA and DR scenarios on Google Cloud, see the following:\n- [Using Cloud Storage for Cassandra Disaster Recovery](/solutions/google-cloud-storage-for-cassandra-disaster-recovery) describes how to implement a backup and recovery process for a Cassandra environment running on Compute Engine. The article describes a process to implement a backup to local disk of database snapshots and incremental backups, and then copy those to Cloud Storage.\n- [Deploying MongoDB on Compute Engine](/solutions/deploy-mongodb) describes an HA architecture where [MongoDb has replica sets deployed to a second region](/solutions/deploy-mongodb#example_architectures) .If you're using a database that's capable of storing petabytes of data, you might experience an outage that affects some of the data, but not all of it. In that case, you want to minimize the amount of data that you need to restore; you don't need to (or want to) recover the entire database just to restore some of the data.\nThere are a number of mitigating strategies you can adopt:\n- Store your data in different tables for specific time periods. This method ensures that you need to restore only a subset of data to a new table, rather than a whole dataset.\n- Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table. **Note:** This method provides good availability, with only a small interruption as you point your applications to the new store. However, unless you have implemented application-level controls to prevent access to the corrupted data, this method can result in inaccurate results during later analysis.\nAdditionally, if your RTO permits, you can prevent access to the table that has the corrupted data by leaving your applications offline until the uncorrupted data has been restored to a new table.\n### Managed database services on Google Cloud\nThis section discusses some methods you can use to implement appropriate backup and recovery mechanisms for the managed database services on Google Cloud.\nManaged databases are designed for scale, so the traditional backup and restore mechanisms you see with traditional RDMBSs are usually not available. As in the case of self-managed databases, if you are using a database that is capable of storing petabytes of data, you want to minimize the amount of data that you need to restore in a DR scenario. There are a number of strategies for each managed database to help you achieve this goal.\n**Bigtable** provides [Bigtable replication](/bigtable/docs/replication-overview) . A replicated Bigtable database can provide higher availability than a single cluster, additional read throughput, and higher durability and resilience in the face of zonal or regional failures.\nBigtable [backups](/bigtable/docs/backups) is a fully managed service that lets you save a copy of a table's schema and data, then restore from the backup to a new table at a later time.\nYou can also export tables from Bigtable as a series of Hadoop [sequence files](/bigtable/docs/exporting-sequence-files) . You can then store these files in Cloud Storage or use them to import the data back into another instance of Bigtable. You can replicate your Bigtable dataset asynchronously across zones within a Google Cloud region.\n**BigQuery** . If you want to archive data, you can take advantage of BigQuery's [long term storage](/bigquery/pricing#long-term-storage) . If a table is not edited for 90 consecutive days, the price of storage for that table automatically drops by 50 percent. There is no degradation of performance, durability, availability, or any other functionality when a table is considered long term storage. If the table is edited, though, it reverts back to the regular storage pricing and the 90 day countdown starts again.\nBigQuery is [replicated to 2 zones in a single region](https://cloud.google.com/bigquery/docs/reliability-disaster#availability_and_durability_) , but this won't help with corruption in your tables. Therefore, you need to have a plan to be able to recover from that scenario. For example, you can do the following:\n- If the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using [snapshot decorators](/bigquery/table-decorators#snapshot_decorators) .\n- [Export the data from BigQuery](/bigquery/exporting-data-from-bigquery#exportingmultiple) , and create a new table that contains the exported data but excludes the corrupted data.\n- Store your data in different tables for specific time periods. This method ensures that you will need to restore only a subset of data to a new table, rather than a whole dataset.\n- Make [copies of your dataset](/bigquery/docs/copying-datasets) at specific time periods. You can use these copies if a data-corruption event occurred beyond what a point-in-time query can capture (for example, more than 7 days ago). You can also copy a dataset from one region to another to ensure data availability in the event of region failures.\n- Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\n**Firestore** . The [managed export and import service](/firestore/docs/manage-data/export-import) allows you to import and export Firestore entities using a Cloud Storage bucket. This in turn allows you to implement a process that you can use to recover from accidental deletion of data.\n**Cloud SQL** . If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup and recovers it to a fresh Cloud SQL instance. For more details, see [Cloud SQL Backups and Recovery](/sql/docs/backup-recovery) .\nYou can also configure Cloud SQL in an [HA configuration](/sql/docs/mysql/high-availability) and [cross-region replicas](/sql/docs/mysql/replication/cross-region-replicas) to maximize up time in the event of zonal or regional failure.\n**Spanner** . You can use Dataflow templates for making a full [export](/spanner/docs/export) of your database to a set of [Avro files](https://wikipedia.org/wiki/Apache_Avro) in a Cloud Storage bucket, and use another template for [re-importing](/spanner/docs/import) the exported files into a new Spanner database.\nFor more controlled backups, the [Dataflow connector](/spanner/docs/dataflow-connector) allows you to write code to read and write data to Spanner in a Dataflow pipeline. For example, you can use the connector to copy data out of Spanner and into Cloud Storage as the backup target. The speed at which data can be read from Spanner (or written back to it) depends on the number of configured [nodes.](/spanner/docs/instances#node_count) This has a direct impact on your RTO values.\nThe Spanner [commit timestamp](/spanner/docs/commit-timestamp) feature can be useful for incremental backups, by allowing you to select only the rows that have been added or modified since the last full backup.\nFor managed backups, [Spanner Backup and Restore](/spanner/docs/backup) allows you to create consistent backups that can be retained for up to 1 year. The RTO value is lower compared to [export](/spanner/docs/export) because the restore operation directly mounts the backup without copying the data.\nFor small RTO values, you could set up a warm standby Spanner instance configured with the minimum number of nodes required to meet your storage and read and write throughput requirements.\nSpanner [point-in-time-recovery (PITR)](/spanner/docs/pitr) lets you recover data from a specific point in time in the past. For example, if an operator inadvertently writes data or an application rollout corrupts the database, with PITR you can recover the data from a point in time in the past, up to a maximum of 7 days.\n**Cloud Composer** . You can use Cloud Composer (a managed version of Apache Airflow) to schedule regular backups of multiple Google Cloud databases. You can create a directed acyclic graph (DAG) to run on a schedule (for example, daily) to either copy the data to another project, dataset, or table (depending on the solution used), or to export the data to Cloud Storage.\nExporting or copying data can be done using the various [Cloud Platform operators](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/index.html) .\nFor example, you can create a DAG to do any of the following:\n- Export a BigQuery table to Cloud Storage using the [BigQueryToCloudStorageOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/bigquery_to_gcs/index.html) .\n- Export Firestore in Datastore mode to Cloud Storage using the [DatastoreExportOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/datastore/index.html) .\n- Export MySQL tables to Cloud Storage using the [MySqlToGoogleCloudStorageOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/mysql_to_gcs/index.html) .\n- Export Postgres tables to Cloud Storage using the [PostgresToGoogleCloudStorageOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/postgres_to_gcs/index.html) .## Production environment is another cloud\nIn this scenario, your production environment uses another cloud provider, and your disaster recovery plan involves using Google Cloud as the recovery site.\n### Data backup and recovery\nTransferring data between object stores is a common use case for DR scenarios. [Storage Transfer Service](/storage/transfer) is compatible with Amazon S3 and is the recommended way to [transfer objectsfrom Amazon S3 to Cloud Storage](/solutions/transferring-data-from-amazon-s3-to-cloud-storage-using-vpc-service-controls-and-storage-transfer-service) .\nYou can configure a transfer job to schedule periodic synchronization from data source to data sink, with advanced filters based on file creation dates, filename filters, and the times of day you prefer to transfer data. To achieve the RPO that you want, you must consider the following factors:\n- **Rate of change** . The amount of data that's being generated or updated for a given amount of time. The higher the rate of change, the more resources are needed to transfer the changes to the destination at each incremental transfer period.\n- **Transfer performance** . The time it takes to transfer files. For large file transfers, this is typically determined by the available bandwidth between source and destination. However, if a transfer job consists of a large number of small files, QPS can become a limiting factor. If that's the case, you can schedule multiple concurrent jobs to scale the performance as long as sufficient bandwidth is available. We recommend you that you measure the transfer performance using a representative subset of your real data.\n- **Frequency** . The interval between backup jobs. The freshness of data at the destination is as recent as the last time a transfer job was scheduled. Therefore, it's important that the intervals between successive transfer jobs are not longer than your RPO objective. For example, if the RPO objective is 1 day, the transfer job must be scheduled at least once a day.\n- **Monitoring and alerts** . Storage Transfer Service provides [Pub/Sub notifications](/storage-transfer/docs/pub-sub-transfer) on a variety of events. We recommend that you subscribe to these notifications to handle unexpected failures or changes in job completion times.\nAnother option for moving data from AWS to Google Cloud is to use [boto](/storage/docs/boto-gsutil) , which is a Python tool that is compatible with Amazon S3 and Cloud Storage. It can be installed as a plugin to the `gsutil` command line tool.\n### Database backup and recovery\nIt is out of scope in this article to discuss in detail the various built-in backup and recovery mechanisms included with third-party databases or the backup and recovery techniques used on other cloud providers. If you are operating non-managed databases on the compute services, you can take advantage of the HA facilities that your production cloud provider has available. You can extend those to incorporate a HA deployment to Google Cloud, or use Cloud Storage as the ultimate destination for the cold storage of your database backup files.\n## What's next?\n- Read about [Google Cloud geography and regions](/docs/geography-and-regions) .\n- Read other articles in this DR series:- [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) \n- [Disaster recovery building blocks](/architecture/dr-scenarios-building-blocks) \n- [Disaster recovery scenarios for applications](/architecture/dr-scenarios-for-applications) \n- [Architecting disaster recovery for locality-restricted workloads](/architecture/architecting-disaster-recovery-for-locality-restricted-workloads) \n- [Disaster recovery use cases: locality-restricted data analytic applications](/architecture/dr-scenarios-locality-restricted-data-analytics) \n- [Architecting disaster recovery for cloud infrastructure outages](/solutions/disaster-recovery/architecture) \n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}