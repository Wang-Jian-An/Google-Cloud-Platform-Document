{"title": "Cloud Architecture Center - Networking for secure intra-cloud access: Reference architectures", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Networking for secure intra-cloud access: Reference architectures\nLast reviewed 2023-11-13 UTC\nThis document is part of a series that describes networking and security architectures for enterprises that are migrating data center workloads to Google Cloud.\nThe series consists of the following documents:\n- [Designing networks for migrating enterprise workloads: Architectural approaches](/architecture/network-architecture) \n- Networking for secure intra-cloud access: Reference architectures (this document)\n- [Networking for internet-facing application delivery: Reference architectures](/architecture/network-application-delivery) \n- [Networking for hybrid and multi-cloud workloads: Reference architectures](/architecture/network-hybrid-multicloud) \nWorkloads for intra-cloud use cases reside in VPC networks and need to connect to other resources in Google Cloud. They might consume services that are provided natively in the cloud, like BigQuery. The security perimeter is provided by a variety of first-party (1P) and third-party (3P) capabilities like firewalls, VPC Service Controls, and network virtual appliances.\nIn many cases, these workloads span multiple Google Cloud VPC networks, and the boundaries between the VPC networks need to be secured. This document covers these security and connectivity architectures in depth.\n", "content": "## Lift-and-shift architecture\nThe first scenario for an intra-cloud use case is a lift-and-shift architecture where you're moving established workloads to the cloud as is.\n### Firewall\nYou can help establish a secure perimeter by configuring [firewall rules](/vpc/docs/firewalls) . You can use [network tags to apply fine-grained firewall rules](/vpc/docs/add-remove-network-tags#using_tags_with_firewall_rules_and_routes) to a collection of VMs. A tag is an arbitrary attribute that's made up of a character string added to the `tags` field of the VM at the time of VM creation. A tag can also be assigned later by editing the VM. For implementation guidelines on how to [manage traffic with Google Cloud firewall rules](/architecture/best-practices-vpc-design#manage_traffic_with_gcp_native_firewall_rules) , see [Network firewall policies](/architecture/security-foundations/networking#network_firewall_policies) in the enterprise foundations blueprint.\nYou can also use firewall logging to audit and verify the effects of the firewall rule setting.\nYou can use VPC Flow Logs for network forensics and [stream the logs](/pubsub/docs) to integrate with [SIEM](https://wikipedia.org/wiki/Security_information_and_event_management) . This overall system can provide real-time monitoring, correlation of events, analysis, and security alerts.\nFigure 1 shows how firewall rules can use VM tags to help restrict traffic among VMs in a VPC network.\n**Figure 1** . Network firewall configuration that uses network tags to apply fine-grained egress control.\n**Note:** You cannot share VPC firewall rules among VPC networks, including networks that are connected by VPC Network Peering or by using Cloud VPN tunnels. For more information about firewall specifications, see the [Specifications](/vpc/docs/firewalls#specifications) section of the VPC firewall rules documentation. You share [hierarchical firewall policy rules](/vpc/docs/firewall-policies) among VPC networks in your organization.\n### Network virtual appliance\nA [network virtual appliance (NVA)](/architecture/architecture-centralized-network-appliances-on-google-cloud) is a VM that has multiple network interfaces. The NVA lets you connect directly to several VPC networks. Security functions such as web application firewalls (WAF) and security application-level firewalls can be implemented on the VMs. You can use NVAs to implement security functions for east-west traffic, especially when you're using a hub-spoke configuration, as shown in figure 2.\nFor implementation guidelines about how to use NVAs on Google Cloud, see [Centralized network appliances on Google Cloud](/architecture/architecture-centralized-network-appliances-on-google-cloud) .\n**Figure 2** . Centralized network appliance configuration in a Shared VPC network.\n**Note:** In order to enable packets to be forwarded by the network virtual appliance, you need to [enable IP forwarding](/vpc/docs/using-routes#canipforward) when you create the VM. Doing so disables the packet source and destination checking (anti-spoofing). Additionally, you must configure the required static routes with the next hop as the instance name or the internal IP address. For more information, see [Considerations for next hop instances](/vpc/docs/routes#next-hop-vm-only) in the routes documentation.\n### Cloud IDS\nCloud Intrusion Detection System (Cloud IDS) lets you implement native security inspection and logging by mirroring traffic from a subnet in your VPC network. By using Cloud IDS, you can inspect and monitor a wide variety of threats at the network layer and at the application layer for analysis. You create [Cloud IDS endpoints](/intrusion-detection-system/docs/configuring-ids#create_a_endpoint) in your VPC network that's associated with your Google Cloud project. These endpoints monitor ingress and egress traffic to and from that network, as well as intra-VPC network traffic, by using the packet mirroring functionality that's built into the Google Cloud networking stack. You must enable [private services access](/intrusion-detection-system/docs/configuring-ids#set_up_private_services_access) in order to connect to the [service producer](/service-infrastructure/docs/glossary#producer) project (the Google-managed project) that hosts the Cloud IDS processes.\nIf you have a hub-and-spoke architecture, traffic from each of the spokes can be mirrored to the Cloud IDS instances, as shown in figure 3.\n**Figure 3** . Cloud IDS configuration to mirror VPC traffic that uses private services access.\nCloud IDS can be secured in your VPC Service Controls service perimeter using an [additional step](/intrusion-detection-system/docs/configuring-ids#optional_enable) . You can read more about VPC Service Controls support in [supported products](/vpc-service-controls/docs/supported-products#table_cloud_ids) .\n### VPC Network Peering\nFor applications that span multiple VPC networks, whether they belong to the same Google Cloud project or to the same organization resource, VPC Network Peering enables connectivity between VPC networks. This connectivity lets traffic stay within Google's network so that it does not traverse the public internet.\nThere are two models for using VPC Network Peering in a lift-and-shift architecture. One is with a \"pure\" hub-and-spoke architecture, and the other is in a hub-and-spoke architecture with transitivity\u2014where traffic from one spoke can reach another spoke. The following sections provide details about how to use VPC Network Peering with these different architectures.\nA [hub-and-spoke architecture](/architecture/deploy-hub-spoke-vpc-network-topology#peering) is a popular model for VPC connectivity that uses VPC Network Peering. This model is useful when an enterprise has various applications that need to access a common set of services, such as logging or authentication. The model is also useful if the enterprise needs to implement a common set of security policies for traffic that's exiting the network through the hub. In a pure hub-and-spoke model, the traffic exchange between the spokes (known as ) is disallowed. Figure 4 shows a pure hub-and-spoke architecture that uses VPC Network Peering to connect the spokes to the hub. For implementation guidelines for building hub-and-spoke networks, see [Hub-and-spoke network topology](/architecture/security-foundations/networking#hub-spoke-network-topology) in the enterprise foundations blueprint.\nHowever, If you don't need VPC-level separation, you can use a [Shared VPC](/vpc/docs/shared-vpc) architecture, which might provide a simpler model for some enterprises that are just starting on Google Cloud.\n**Figure 4** . Hub-and-spoke network architecture that uses VPC Network Peering for network isolation and non-transitive connectivity.\n**Note:** You might need to consider the peering group limits when configuring VPC Network Peering. For more information, see [VPC Network Peering limits](/vpc/docs/quota#vpc-peering) .\nTo enable hub-and-spoke with [transitivity](/architecture/security-foundations/networking#hub-spoke-network-topology) (traffic from a spoke can reach other spokes through the hub), there are several approaches that use VPC Network Peering. You can use VPC Network Peering in a full mesh topology, where every VPC network directly peers with every other VPC network that it needs to reach.\nAlternatively, you can use an NVA to connect the hub and the spokes together. The NVA then resides behind internal load balancers that are used as the next-hop for traffic from the VPC spokes. Figure 5 shows both of these options.\nAdditionally, you can use VPNs to connect between the hub and the spoke VPC networks. This arrangement enables reachability across spoke-spoke connections, which provides transitivity across the hub VPC network.\n**Figure 5** . Hub-and-spoke network configuration that uses Cloud VPN for network-isolation and transitive connectivity.\n**Note:** You need to consider the total bandwidth that's supported by each VPN tunnel. For more information, see [Network bandwidth](/network-connectivity/docs/vpn/concepts/overview#network-bandwidth) in the Cloud VPN overview.\n### Shared VPC\nYou can use [Shared VPC](/vpc/docs/shared-vpc) , to maintain centralized control over network resources like subnets, routes, and firewalls in host projects. This level of control lets you implement the security best practice of least privilege for network administration, auditing, and access control because you can delegate network administration tasks to network and security administrators. You can assign the ability to create and manage VMs to instance administrators by using service projects. Using a service project ensures that the VM administrators are only given the ability to create and manage instances, and that they are not allowed to make any network-impacting changes in the Shared VPC network.\nFor example, you can provide more isolation by defining two VPC networks that are in two host projects and by attaching multiple service projects to each network, one for production and one for testing. Figure 6 shows an architecture that isolates a production environment from a testing environment by using separate projects.\nFor more information about best practices for building VPC networks, see [Best practices and reference architectures for VPC design](/architecture/best-practices-vpc-design) .\n**Figure 6** . Shared VPC network configuration that uses multiple isolated hosts and service projects (test and production environments).\n**Note:** You need to consider the [per-project VPC quotas](/vpc/docs/quota#per_project) for Shared VPC host project and [per-network](/vpc/docs/quota#per_network) and [per-instance](/vpc/docs/quota#per_instance) limits for Shared VPC networks. Additionally, the relationships between host and service projects are governed by [limits specific to Shared VPC](/vpc/docs/quota#shared-vpc) .\n## Hybrid services architecture\nThe hybrid services architecture provides additional cloud-native services that are designed to let you connect and secure services in a multi-VPC environment. These cloud-native services supplement what is available in the lift-and-shift architecture and can make it easier to manage a VPC-segmented environment at scale.\n### Private Service Connect\n[Private Service Connect](/vpc/docs/private-service-connect) lets a service that's hosted in one VPC network be surfaced in another VPC network. There is no requirement that the services be hosted by the same organization resource, so Private Service Connect can be used to privately consume services from another VPC network, even if it's attached to another organization resource.\nYou can use Private Service Connect in two ways: to access Google APIs or to access services hosted in other VPC networks.\nWhen you use Private Service Connect, you can expose [Google APIs](/vpc/docs/about-accessing-google-apis-endpoints) by using a Private Service Connect endpoint that's a part of your VPC network, as shown in figure 7.\n**Figure 7** . Private Service Connect configuration to send traffic to Google APIs by using a Private Service Connect endpoint that's private to your VPC network.\nWorkloads can send traffic to a [bundle of global Google APIs](/vpc/docs/about-accessing-google-apis-endpoints#supported-apis) by using a Private Service Connect endpoint. In addition, you can use a [Private Service Connect backend](/vpc/docs/private-service-connect-backends) to access a single Google API, extending the security features of load balancers to API services. Figure 8 shows this configuration.\n**Figure 8** . Private Service Connect configuration to send traffic to Google APIs by using a Private Service Connect backend.\nPrivate Service Connect also lets a service producer offer services to a service consumer in another VPC network either in the same organization resource or in a different one. A service producer VPC network can [support multiple service consumers](/vpc/docs/about-vpc-hosted-services) . The consumer can connect to the producer service by sending traffic to a Private Service Connect endpoint located in the consumer's VPC network. The endpoint forwards the traffic to the VPC network containing the published service.\n**Note:** If you're configuring Private Service Connect by using forwarding rules, the consumer's source IP address is translated by using source NAT (SNAT) to an IP address that's selected from one of the Private Service Connect subnets. If you want to retain the consumer connection IP address information, you need to configure that option. For more information, see [View consumer connection information](/vpc/docs/configure-private-service-connect-producer#proxy-protocol) .\n**Figure 9** . Private Service Connect configuration to publish a managed service through a service attachment and consume the service through an endpoint.\n### VPC serverless access connector\nA VPC [serverless access connector](/vpc/docs/serverless-vpc-access) handles traffic between your serverless environment and your VPC network. When you create a connector in your Google Cloud project, you attach it to a specific VPC network and region. You can then configure your serverless services to use the connector for outbound network traffic. You can specify a connector by using a subnet or a CIDR range. Traffic sent through the connector into the VPC network originates from the subnet or the CIDR range that you specified, as shown in figure 10.\n**Figure 10** . Serverless VPC access connector configuration to access Google Cloud serverless environments by using internal IP addresses inside your VPC network.\nServerless VPC Access connectors are supported in every region that supports Cloud Run, Cloud Functions, or the App Engine standard environment. For more information, see the list of [supported services](/vpc/docs/serverless-vpc-access#supported_services) and [supported networking protocols](/vpc/docs/serverless-vpc-access#supported_networking_protocols) for using VPC Serverless access connector.\n### VPC Service Controls\n[VPC Service Controls](/vpc-service-controls/docs/overview) helps you prevent data exfiltration from services such as Cloud Storage or BigQuery by preventing authorized accesses from the internet or from projects that are not a part of a security perimeter. For example, consider a scenario where human error or incorrect automation causes IAM policies to be set incorrectly on a service such as Cloud Storage or BigQuery. As a result, resources in these services become publicly accessible. In that case, there is a risk of data exposure. If you have these services configured as part of the VPC Service Controls perimeter, ingress access to the resources is blocked, even if IAM policies allow access.\nVPC Service Controls can create perimeters based on client attributes such as identity type (service account or user) and network origin (IP address or VPC network).\nVPC Service Controls helps mitigate the following security risks:\n- Access from unauthorized networks that use stolen credentials.\n- Data exfiltration by malicious insiders or compromised code.\n- Public exposure of private data caused by misconfigured IAM policies.\nFigure 11 shows how VPC Service Controls lets you establish a service perimeter to help mitigate these risks.\n**Figure 11** . VPC service perimeter extended to hybrid environments by using private access services.\nBy using ingress and egress rules, you can enable communication between two service perimeters, as shown in figure 12.\n**Figure 12** . Configuring ingress and egress rules to communicate between service perimeters.\nFor detailed recommendations for VPC Service Controls deployment architectures, see [Design and architect service perimeters](/vpc-service-controls/docs/architect-perimeters) . For more information about the list of services that are supported by VPC Service Controls, see [Supported products and limitations](/vpc-service-controls/docs/supported-products) .\n## Zero Trust Distributed Architecture\nNetwork perimeter security controls are necessary but not sufficient to support the security principles of least privilege and defense in depth. Zero Trust Distributed Architectures build on, but do not solely rely on, the network perimeter edge for security enforcement. As distributed architectures, they are composed of microservices with per-service enforcement of security policy, strong authentication, and workload identity.\nYou can implement Zero Trust Distributed Architectures as services managed by Traffic Director and Anthos Service Mesh.\n### Traffic Director\nTraffic Director can be configured to provide a Zero Trust Distributed Architecture microservice mesh inside a GKE cluster by using [service security](/traffic-director/docs/security-overview) . In this model, in GKE services that have either Envoy sidecars or proxyless gRPC, identity, certificates, and authorization policy are all managed by all of the following: Traffic Director, workload identity, [Certificate Authority Service](/certificate-authority-service/docs) , and IAM. Certificate management and secure naming is provided by the platform, and all service communication is subject to mTLS transport security. Figure 13 shows a cluster with this configuration.\n**Figure 13** . Single-cluster Zero Trust Distributed Architecture mesh that uses Traffic Director.\nAn authorization policy specifies how a server authorizes incoming requests or RPCs. The authorization policy can be configured to allow or deny an incoming request or RPC based on various parameters, such as the identity of the client that sent the request, the host, the headers, and other HTTP attributes. Implementation guidelines are available for configuring authorization policies on meshes based on [gRPC](/traffic-director/docs/security-proxyless-setup) and [Envoy](/traffic-director/docs/security-envoy-setup) .\nIn figure 13, the architecture has a single cluster and [flat networking](/service-mesh/docs/unified-install/gke-install-multi-cluster) (shared IP address space). Multiple clusters are typically used in Zero Trust Distributed Architecture for [isolation, location, and scale](/anthos/fleet-management/docs/multi-cluster-use-cases) .\nIn more complex environments, multiple clusters can share managed identity when the clusters are grouped by [fleets](/anthos/multicluster-management/fleets) . In that case, you can configure networking connectivity across independent VPC networks by using [Private Service Connect](#private-service-connect) . This approach is similar to the [hybrid workload access multi-clusternetwork connectivity](/architecture/network-hybrid-multicloud) approach, as described later in this document.\nFor information about fine-grained control of how traffic is handled with Traffic Director, see [Advanced traffic management overview](/traffic-director/docs/advanced-traffic-management) .\n### Anthos Service Mesh\n[Anthos Service Mesh](/service-mesh/docs/overview) provides an out-of-the-box [mTLS](/service-mesh/docs/security/security-overview) Zero Trust Distributed Architecture microservice mesh that's built on Istio foundations. You set up the mesh by using an integrated [flow](/service-mesh/docs/managed/provision-managed-anthos-service-mesh) . [Managed Anthos Service Mesh](/service-mesh/docs/overview#managed_anthos_service_mesh) , with Google-managed data and control planes, is [supported](/service-mesh/docs/managed/supported-features-mcp#regions) on GKE. An [in-cluster control plane](/service-mesh/docs/overview#in-cluster_control_plane) is also available, which is suitable for [other environments](/service-mesh/docs/supported-features#platform_differences) such as Google Distributed Cloud Virtualises or GKE Multi-Cloud. Anthos Service Mesh manages identity and certificates for you, providing an Istio-based [authorization policy model](/service-mesh/docs/security/authorization-policy-overview) .\nAnthos Service Mesh relies on [fleets](/anthos/multicluster-management) for managing [multi-cluster service](/kubernetes-engine/docs/how-to/multi-cluster-services) deployment configuration and identity. As with Traffic Director, when your workloads operate in a flat (or shared) VPC network connectivity environment, there are no special network connectivity requirements beyond firewall configuration. When your architecture includes multiple Anthos Service Mesh clusters across separate VPC networks or networking environments, such as across a Cloud Interconnect connection, you also need an [east-west gateway](/service-mesh/docs/unified-install/off-gcp-multi-cluster-setup#install_the_east-west_gateway) . Best practices for networking for Anthos Service Mesh are the same as those that are described in [Best practices for GKE networking](/kubernetes-engine/docs/best-practices/networking) .\nAnthos Service Mesh also integrates with [Identity-Aware Proxy (IAP)](/service-mesh/v1.4/docs/iap-integration) . IAP lets you set fine-grained [access policies](/access-context-manager/docs/overview#access-levels) so that you can control user access to a workload based on attributes of the originating request, such as user identity, IP address, and device type. This level of control enables an end-to-end zero-trust environment.\nYou need to consider GKE cluster requirements when you use Anthos Service Mesh. For more information, see the [Requirements](/service-mesh/v1.9/docs/scripted-install/asm-onboarding#requirements) section in the \"Single project installation on GKE\" documentation.\n## What's next\n- [Networking for internet-facing application delivery: Reference architectures](/architecture/network-application-delivery) .\n- [Networking for hybrid and multi-cloud workloads: Reference architectures](/architecture/network-hybrid-multicloud) .\n- [Migration to Google Cloud](/solutions/migration-to-gcp-getting-started) can help you to plan, design, and implement the process of migrating your workloads to Google Cloud.\n- [Landing zone design in Google Cloud](/architecture/landing-zones) has guidance for creating a landing zone network.\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}