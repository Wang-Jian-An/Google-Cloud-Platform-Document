{"title": "Google Kubernetes Engine (GKE) - Troubleshooting", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshooting\nLearn about troubleshooting steps that you might find helpful if you run into problems using Google Kubernetes Engine (GKE).\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## Debugging Kubernetes resources\nIf you are experiencing an issue related to your cluster, refer to [Troubleshooting Clusters](https://kubernetes.io/docs/tasks/debug/debug-cluster/) in the Kubernetes documentation.\nIf you are having an issue with your application, its Pods, or its controller object, refer to [Troubleshooting Applications](https://kubernetes.io/docs/tasks/debug/debug-application/) .\nIf you are having an issue related to connectivity between Compute Engine VMs that are in the same Virtual Private Cloud (VPC) network or two VPC networks connected with VPC Network Peering, refer to [Troubleshooting connectivity between virtual machine (VM) instances withinternal IP addresses](/vpc/docs/ts-vm-vm-internal) .\nIf you are experiencing packet loss when sending traffic from a cluster to an external IP address using [Cloud NAT](/nat/docs/overview) , [VPC-native clusters](/vpc/docs/alias-ip) , or [IP masquerade agent](/kubernetes-engine/docs/concepts/ip-masquerade-agent) , see [Troubleshooting Cloud NAT packet loss from a GKE cluster](#troubleshooting_packet_loss_from_a_cluster) .\n## Troubleshooting issues with kubectl command\n### The kubectl command isn't found\n- Install the `kubectl` binary by running the following command:```\ngcloud components update kubectl\n```\n- Answer \"yes\" when the installer prompts you to modify your `$PATH` environment variable. Modifying this variable enables you to use `kubectl` commands without typing their full file path.Alternatively, add the following line to `~/.bashrc` (or `~/.bash_profile` in macOS, or wherever your shell stores environment variables):```\nexport PATH=$PATH:/usr/local/share/google/google-cloud-sdk/bin/\n```\n- Run the following command to load your updated `.bashrc` (or `.bash_profile` ) file:```\nsource ~/.bashrc\n```\n### kubectl commands return \"connection refused\" error\nSet the cluster context with the following command:\n```\ngcloud container clusters get-credentials CLUSTER_NAME\n```\nIf you are unsure of what to enter for `` , use the following command to list your clusters:\n```\ngcloud container clusters list\n```\n### kubectl command times out\nAfter creating a cluster, attempting to run the `kubectl` command against the cluster returns an error, such as `Unable to connect to the server: dial tcp IP_ADDRESS: connect: connection timed out` or `Unable to connect to the server: dial tcp IP_ADDRESS: i/o timeout` .\nThis can occur when `kubectl` is unable to communicate with the cluster control plane.\nTo resolve this issue, verify the context were the cluster is set:\n- Go to `$HOME/.kube/config` or run the command `kubectl config view` to verify the config file contains the cluster context and the external IP address of the control plane.\n- Set the cluster credentials:```\ngcloud container clusters get-credentials CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --project=PROJECT_ID\n```Replace the following:- ``: the name of your cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) .\n- ``: ID of the project in which the GKE cluster was created.\n- If the cluster is a [private GKE cluster](/kubernetes-engine/docs/concepts/private-cluster-concept) , then ensure that the outgoing IP of the machine you are attempting to connect from is included in the list of existing authorized networks. You can find your existing authorized networks in the console or by running the following command:```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --format \"flattened(masterAuthorizedNetworksConfig.cidrBlocks[])\"\n```\nIf the outgoing IP of the machine is not included in the list of authorized networks from the output of the command above, then follow steps in [Can't reach control plane of a private cluster](/kubernetes-engine/docs/how-to/private-clusters#cant_reach_control_plane_of_a_private_cluster) , or [Using Cloud Shell to access a private cluster](/kubernetes-engine/docs/how-to/private-clusters#cloud_shell) if connecting from Cloud Shell.\n### kubectl commands return \"failed to negotiate an api version\" error\nEnsure kubectl has authentication credentials:\n```\ngcloud auth application-default login\n```\n### The kubectl logs, attach, exec, and port-forward commands stops responding\nThese commands rely on the cluster's control plane being able to talk to the nodes in the cluster. However, because the control plane isn't in the same Compute Engine network as your cluster's nodes, we rely on either SSH or [Konnectivityproxy](https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/#konnectivity-service) tunnels to enable secure communication.\nGKE saves an SSH public key file in your Compute Engine project [metadata](/compute/docs/metadata) . All Compute Engine VMs using Google-provided images regularly check their project's common metadata and their instance's metadata for SSH keys to add to the VM's list of authorized users. GKE also adds a firewall rule to your Compute Engine network allowing SSH access from the control plane's IP address to each node in the cluster.\nIf any of the above `kubectl` commands don't run, it's likely that the API server is unable to communicate with the nodes. Check for these potential causes:\n- The cluster doesn't have any nodes.If you've scaled down the number of nodes in your cluster to zero, the commands won't work.To fix it, [resize your cluster](/kubernetes-engine/docs/how-to/resizing-a-container-cluster) to have at least one node.- Your network's firewall rules don't allow for SSH access from the control plane.All Compute Engine networks are created with a firewall rule called `default-allow-ssh` that allows SSH access from all IP addresses (requiring a valid private key, of course). GKE also inserts an SSH rule for each public cluster of the form `gke-` `` `-RANDOM_CHARACTERS-ssh` that allows SSH access specifically from the cluster's control plane to the cluster's nodes. If neither of these rules exists, then the control plane can't open SSH tunnels.To fix it, [re-add a firewall rule](/compute/docs/vpc/using-firewalls) allowing access to VMs with the tag that's on all the cluster's nodes from the control plane's IP address.\n- Your project's common metadata entry for \"ssh-keys\" is full.If the project's metadata entry named \"ssh-keys\" is close to [maximum size limit](/compute/docs/storing-retrieving-metadata#custom_metadata_size_limitations) , then GKE isn't able to add its own SSH key to enable it to open SSH tunnels. You can see your project's metadata by running the following command:```\ngcloud compute project-info describe [--project=PROJECT_ID]\n```And then check the length of the list of ssh-keys.To fix it, [delete some of the SSH keys](/compute/docs/connect/restrict-ssh-keys#remove-project-key) that are no longer needed.\n- You have set a metadata field with the key \"ssh-keys\" on the VMs in the cluster.The node agent on VMs prefers per-instance ssh-keys to project-wide SSH keys, so if you've set any SSH keys specifically on the cluster's nodes, then the control plane's SSH key in the project metadata won't be respected by the nodes. To check, run `gcloud compute instances describe VM_NAME` and look for an `ssh-keys` field in the metadata.To fix it, [delete the per-instance SSH keys](/compute/docs/connect/restrict-ssh-keys#remove-instance-key) from the instance metadata.- Determine if your cluster uses the Konnectivity proxy by checking for the following system Deployment:```\nkubectl get deployments konnectivity-agent --namespace kube-system\n```\n- Your network's firewall rules don't allow for Konnectivity agent access to the control plane.On cluster creation, Konnectivity agent pods establish and maintain a connection to the control plane on port `8132` . When one of the `kubectl` commands is run, the API server uses this connection to communicate with the cluster.If your network's firewall rules contain Egress Deny rule(s), it can prevent the agent from connecting. You must allow Egress traffic to the cluster control plane on port 8132. (For comparison, the API server uses 443).\nIt's worth noting that these features are not required for the correct functioning of the cluster. If you prefer to keep your cluster's network locked down from all outside access, be aware that features like these won't work.\n## Troubleshooting error 4xx issues\n### Authentication and authorization errors when connecting to GKE clusters\nThis issue might occur when you try to run a `kubectl` command in your GKE cluster from a local environment. The command fails and displays an error message, usually with HTTP status code 401 (Unauthorized).\nThe cause of this issue might be one of the following:\n- The`gke-gcloud-auth-plugin`authentication plugin is not correctly installed or configured.\n- You lack the permissions to connect to the cluster API server and run`kubectl`commands.\nTo diagnose the cause, do the following:\nUsing `curl` bypasses the `kubectl` CLI and the `gke-gcloud-auth-plugin` plugin.\n- Set environment variables:```\nAPISERVER=https://$(gcloud container clusters describe CLUSTER_NAME --location=COMPUTE_LOCATION --format \"value(endpoint)\")TOKEN=$(gcloud auth print-access-token)\n```\n- Verify that your access token is valid:```\ncurl https://oauth2.googleapis.com/tokeninfo?access_token=$TOKEN\n```\n- Check that you can connect to the core API endpoint in the API server:```\ngcloud container clusters describe CLUSTER_NAME --location=COMPUTE_LOCATION --format \"value(masterAuth.clusterCaCertificate)\" | base64 -d > /tmp/ca.crtcurl -s -X GET \"${APISERVER}/api/v1/namespaces\" --header \"Authorization: Bearer $TOKEN\" --cacert /tmp/ca.crt\n```\nIf the `curl` command fails with an output that is similar to the following, check that you have the correct permissions to access the cluster:\n```\n{\n\"kind\": \"Status\",\n\"apiVersion\": \"v1\",\n\"metadata\": {},\n\"status\": \"Failure\",\n\"message\": \"Unauthorized\",\n\"reason\": \"Unauthorized\",\n\"code\": 401\n}\n```\nIf the `curl` command succeeds, check whether the plugin is the cause.\nThe following steps configure your local environment to ignore the `gke-gcloud-auth-plugin` binary when authenticating to the cluster. In Kubernetes clients running version 1.25 and later, the `gke-gcloud-auth-plugin` binary is **required** , so use these steps if you want to access your cluster without needing the plugin.\n- Install `kubectl` CLI version 1.24 using `curl` :```\ncurl -LO https://dl.k8s.io/release/v1.24.0/bin/linux/amd64/kubectl\n```You can use any `kubectl` CLI version 1.24 or earlier.\n- Open your shell startup script file, such as `.bashrc` for the Bash shell, in a text editor:```\nvi ~/.bashrc\n```\n- Add the following line to the file and save it:```\nexport USE_GKE_GCLOUD_AUTH_PLUGIN=False\n```\n- Run the startup script:```\nsource ~/.bashrc\n```\n- Get credentials for your cluster, which sets up your `.kube/config` file:```\ngcloud container clusters get-credentials CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace the following:- ``: the name of the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) .\n- Run a `kubectl` command:```\nkubectl cluster-info\n```\nIf you get a 401 error or a similar authorization error, ensure that you have the correct permissions to perform the operation.\n### Error 400: Node pool requires recreation\nThe following issue occurs when you try to perform an action that recreates your control plane and nodes, such as when you [complete an ongoing credential rotation](/kubernetes-engine/docs/how-to/credential-rotation#completing_the_rotation) .\nThe operation fails because GKE has not recreated one or more node pools in your cluster. On the backend, node pools are marked for recreation, but the actual recreation operation might take some time to begin.\nThe error message is similar to the following:\n```\nERROR: (gcloud.container.clusters.update) ResponseError: code=400, message=Node pool \"test-pool-1\" requires recreation.\n```\nTo resolve this issue, do one of the following:\n- Wait for the recreation to happen. This might take hours, days, or weeks depending on factors such as existing maintenance windows and exclusions.\n- Manually start a recreation of the affected node pools by starting a version upgrade to the same version as the control plane. To start a recreation, run the following command:```\ngcloud container clusters upgrade CLUSTER_NAME \\\u00a0 \u00a0 --node-pool=POOL_NAME\n```After the upgrade completes, try the operation again.\n### Error 403: Insufficient permissions\nThe following error occurs when you try to connect to a GKE cluster using `gcloud container clusters get-credentials` , but the account doesn't have permission to access the Kubernetes API server.\n```\nERROR: (gcloud.container.clusters.get-credentials) ResponseError: code=403, message=Required \"container.clusters.get\" permission(s) for \"projects/<your-project>/locations/<region>/clusters/<your-cluster>\".\n```\nTo resolve this issue, do the following:\n- Identify the account that has the access issue:```\ngcloud auth list\n```\n- Grant the required access to the account using the instructions in [Authenticating to the Kubernetes API server](/kubernetes-engine/docs/how-to/api-server-authentication#applicat%0Aions_within) .\n### Error 404: Resource \"not found\" when calling gcloud container commands\nRe-authenticate to the Google Cloud CLI:\n```\ngcloud auth login\n```\n### Error 400/403: Missing edit permissions on account\nYour [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) , the [Google APIs Service Agent](/compute/docs/access/service-accounts#google_apis_service_agent) , or the [service account associated with GKE](/kubernetes-engine/docs/concepts/security-overview#authentication_and_authorization) has been deleted or edited manually.\nWhen you enable the Compute Engine or Kubernetes Engine API, Google Cloud creates the following service accounts and agents:\n- with edit permissions on your project.\n- with edit permissions on your project.\n- with therole on your project.\nIf at any point you edit those permissions, remove the role bindings on the project, remove the service account entirely, or disable the API, cluster creation and all management functionality will fail.\n**Note:** If the Compute Engine default service account is not used in your node pools when creating a GKE cluster, or adding node pools to an existing cluster, then the permissions for this service account are not required on the project for cluster creation and management functionality.\nThe name of your Google Kubernetes Engine service account is as follows, where `` is your [project number](https://console.cloud.google.com/) :\n```\nservice-PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com\n```\nThe following command can be used to verify that the Google Kubernetes Engine service account has the role assigned on the project:\n```\ngcloud projects get-iam-policy PROJECT_ID\n```\nReplace `` with your project ID.\nTo resolve the issue, if you have removed the role from your Google Kubernetes Engine service account, add it back. Otherwise, you can re-enable the Kubernetes Engine API, which will correctly restore your service accounts and permissions.\n- Go to the **APIs & Services** page in the Google Cloud console. [Go to APIs & Services](https://console.cloud.google.com/projectselector/apis) \n- Select your project.\n- Click **Enable APIs and Services** .\n- Search for Kubernetes, then select the API from the search results.\n- Click **Enable** . If you have previously enabled the API, you must first disable it and then enable it again. It can take several minutes for the API and related services to be enabled.\nRun the following command in the gcloud CLI to add back the service account:\n```\nPROJECT_NUMBER=$(gcloud projects describe \"PROJECT_ID\" --format 'get(projectNumber)')gcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0--member \"serviceAccount:service-${PROJECT_NUMBER?}@container-engine-robot.iam.gserviceaccount.com\" \\\u00a0--role roles/container.serviceAgent\n```\n## Troubleshooting issues with GKE cluster creation\n### Error CONDITION_NOT_MET: Constraint constraints/compute.vmExternalIpAccess violated\nYou have the organization policy constraint [constraints/compute.vmExternalIpAccess](/resource-manager/docs/organization-policy/org-policy-constraints) configured to `Deny All` or to restrict external IPs to specific VM instances at the organization, folder, or project level in which you are trying to create a public GKE cluster.\n**Note:** This only affects public GKE clusters, including GKE Autopilot clusters.\nWhen you create public GKE clusters, the underlying Compute Engine VMs, which make up the worker nodes of this cluster, have [external IP addresses](/compute/docs/ip-addresses#externaladdresses) assigned. If you configure the organization policy constraint [constraints/compute.vmExternalIpAccess](/resource-manager/docs/organization-policy/org-policy-constraints) to `Deny All` or to restrict external IPs to specific VM instances, then the policy prevents the GKE worker nodes from obtaining external IP addresses, which results in cluster creation failure.\nTo find the logs of the cluster creation operation, you can review the [GKE Cluster Operations Audit Logs](/kubernetes-engine/docs/how-to/audit-logging) using [Logs Explorer](/logging/docs/view/logs-explorer-interface) with a search query similar to the following:\n```\nresource.type=\"gke_cluster\"\nlogName=\"projects/test-last-gke-sa/logs/cloudaudit.googleapis.com%2Factivity\"\nprotoPayload.methodName=\"google.container.v1beta1.ClusterManager.CreateCluster\"\nresource.labels.cluster_name=\"CLUSTER_NAME\"\nresource.labels.project_id=\"PROJECT_ID\"\n```\nTo resolve this issue, ensure that the effective policy for the constraint `constraints/compute.vmExternalIpAccess` is `Allow All` on the project where you are trying to create a GKE public cluster. See [Restricting external IP addresses to specific VM instances](/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip) for information on working with this constraint. After setting the constraint to `Allow All` , delete the failed cluster and create a new cluster. This is required because repairing the failed cluster is not possible.\n## Troubleshooting issues with deployed workloads\nGKE returns an error if there are issues with a workload's Pods. You can check the status of a Pod using the `kubectl` command-line tool or the Google Cloud console.\nTo see all Pods running in your cluster, run the following command:\n```\nkubectl get pods\n```\nOutput:\n```\nNAME  READY STATUS    RESTARTS AGE\nPOD_NAME 0/1 CrashLoopBackOff 23  8d\n```\nTo get more details information about a specific Pod, run the following command:\n```\nkubectl describe pod POD_NAME\n```\nReplace `` with the name of the desired Pod.\nPerform the following steps:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Select the desired workload. The **Overview** tab displays the status of the workload.\n- From the **Managed Pods** section, click the error status message.\nThe following sections explain some common errors returned by workloads and how to resolve them.\n### CrashLoopBackOff\n`CrashLoopBackOff` indicates that a container is repeatedly crashing after restarting. A container might crash for many reasons, and checking a Pod's logs might aid in troubleshooting the root cause.\nBy default, crashed containers restart with an exponential delay limited to five minutes. You can change this behavior by setting the `restartPolicy` field Deployment's Pod specification under `spec: restartPolicy` . The field's default value is `Always` .\nYou can troubleshoot `CrashLoopBackOff` errors using the Google Cloud console:\n- Go to the Crashlooping Pods Interactive Playbook: [Go to Playbook](https://console.cloud.google.com/monitoring/dashboards/gke-troubleshooting/crashloop) \n- For filter_list **Cluster** , enter the name of the cluster you want to troubleshoot.\n- For filter_list **Namespace** , enter the namespace you want to troubleshoot.\n- (Optional) Create an alert to notify you of future `CrashLoopBackOff` errors:- In the **Future Mitigation Tips** section, select **Create an Alert** .\nYou can find out why your Pod's container is crashing using the `kubectl` command-line tool or the Google Cloud console.\nTo see all Pods running in your cluster, run the following command:\n```\nkubectl get pods\n```\nLook for the Pod with the `CrashLoopBackOff` error.\nTo get the Pod's logs, run the following command:\n```\nkubectl logs POD_NAME\n```\nReplace `` with the name of the problematic Pod.\nYou can also pass in the `-p` flag to get the logs for the previous instance of a Pod's container, if it exists.\nPerform the following steps:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Select the desired workload. The **Overview** tab displays the status of the workload.\n- From the **Managed Pods** section, click the problematic Pod.\n- From the Pod's menu, click the **Logs** tab.You can find the exit code by performing the following tasks:\n- Run the following command:```\nkubectl describe pod POD_NAME\n```Replace `` with the name of the Pod.\n- Review the value in the `containers: CONTAINER_NAME: last state: exit code` field:- If the exit code is 1, the container crashed because the application crashed.\n- If the exit code is 0, verify for how long your app was running.\nContainers exit when your application's main process exits. If your app finishes execution very quickly, container might continue to restart.Open a shell to the Pod:\n```\nkubectl exec -it POD_NAME -- /bin/bash\n```\nIf there is more than one container in your Pod, add `-c CONTAINER_NAME` .\nNow, you can run bash commands from the container: you can test the network or check if you have access to files or databases used by your application.\n### ImagePullBackOff and ErrImagePull\n`ImagePullBackOff` and `ErrImagePull` indicate that the image used by a container cannot be loaded from the image registry.\nYou can verify this issue using the Google Cloud console or the `kubectl` command-line tool.\nTo get more information about a Pod's container image, run the following command:\n```\nkubectl describe pod POD_NAME\n```\nPerform the following steps:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Select the desired workload. The **Overview** tab displays the status of the workload.\n- From the **Managed Pods** section, click the problematic Pod.\n- From the Pod's menu, click the **Events** tab.If your image is not found:\n- Verify that the image's name is correct.\n- Verify that the image's tag is correct. (Try`:latest`or no tag to pull the latest image).\n- If the image has full registry path, verify that it exists in the Docker registry you are using. If you provide only the image name, check the Docker Hub registry.\n- Try to pull the docker image manually:- [SSH](https://cloud.google.com/sdk/gcloud/reference/compute/ssh) into the node:For example, to SSH into a VM:```\ngcloud compute ssh VM_NAME --zone=ZONE_NAME\n```Replace the following:- ``: the name of the VM.\n- ``: a [Compute Engine zone](/compute/docs/regions-zones/viewing-regions-zones) .\n- Run `docker-credential-gcr configure-docker` . This command generates a config file at `/home/[USER]/.docker/config.json` . Ensure that this file includes the registry of the image in the `credHelpers` field. For example, the following file includes authentication information for images hosted at asia.gcr.io, eu.gcr.io, gcr.io, marketplace.gcr.io, and us.gcr.io:```\n{\u00a0 \"auths\": {},\u00a0 \"credHelpers\": {\u00a0 \u00a0 \"asia.gcr.io\": \"gcr\",\u00a0 \u00a0 \"eu.gcr.io\": \"gcr\",\u00a0 \u00a0 \"gcr.io\": \"gcr\",\u00a0 \u00a0 \"marketplace.gcr.io\": \"gcr\",\u00a0 \u00a0 \"us.gcr.io\": \"gcr\"\u00a0 }}\n```\n- Run `docker pull` `` .\nIf this option works, you probably need to specify [ImagePullSecrets](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod) on a Pod. Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.If you encounter a \"permission denied\" or \"no pull access\" error, verify that you are logged in and have access to the image. Try one of the following methods depending on the registry in which you host your images.\n**Note:** Container Registry is deprecated. Organizations that haven't used Container Registry prior to January 8, 2024 will have new gcr.io repositories hosted on Artifact Registry by default. After May 15, 2024, Google Cloud projects without previous usage of Container Registry will only support hosting and managing images for the `gcr.io` domain in [Artifact Registry](/artifact-registry/docs) . For details, see [Container Registry deprecation](/container-registry/docs/deprecations/container-registry-deprecation) .\nIf your image is in Artifact Registry, your [node pool's service account](/artifact-registry/docs/access-control#gke) needs read access to the repository that contains the image.\nGrant the [artifactregistry.reader role](/artifact-registry/docs/access-control#roles) to the service account:\n```\ngcloud artifacts repositories add-iam-policy-binding REPOSITORY_NAME \\\u00a0 \u00a0 --location=REPOSITORY_LOCATION \\\u00a0 \u00a0 --member=serviceAccount:SERVICE_ACCOUNT_EMAIL \\\u00a0 \u00a0 --role=\"roles/artifactregistry.reader\"\n```\nReplace the following:- ``: the name of your Artifact Registry repository.\n- ``: the [region](/artifact-registry/docs/repo-locations) of your Artifact Registry repository.\n- ``: the email address of the IAM service account associated with your node pool.\nIf your image is in Container Registry, your [node pool's service account](/container-registry/docs/access-control#integration) needs read access to the Cloud Storage bucket that contains the image.\nGrant the [roles/storage.objectViewer role](/storage/docs/access-control/iam-roles#standard-roles) to the service account so that it can read from the bucket:\n```\ngsutil iam ch \\serviceAccount:SERVICE_ACCOUNT_EMAIL:roles/storage.objectViewer \\\u00a0 gs://BUCKET_NAME\n```\nReplace the following:- ``: the email of the service account associated with your node pool. You can list all the service accounts in your project using`gcloud iam service-accounts list`.\n- ``: the name of the Cloud Storage bucket that contains your images. You can list all the buckets in your project using`gsutil ls`.\nIf your registry administrator set up [gcr.io repositories in Artifact Registry](/artifact-registry/docs/transition/setup-gcr-repo) to store images for the `gcr.io` domain instead of Container Registry, you must grant read access to Artifact Registry instead of Container Registry.If your image is in a private registry, you might require keys to access the images. See [Using private registries](https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry) for more information.\nAn error similar to the following might occur when you pull an image from a private Container Registry repository:\n```\ngcr.io/PROJECT_ID/IMAGE:TAG: rpc error: code = Unknown desc = failed to pull and\nunpack image gcr.io/PROJECT_ID/IMAGE:TAG: failed to resolve reference\ngcr.io/PROJECT_ID/IMAGE]:TAG: unexpected status code [manifests 1.0]: 401 Unauthorized\nWarning Failed  3m39s (x4 over 5m12s) kubelet   Error: ErrImagePull\nWarning Failed  3m9s (x6 over 5m12s) kubelet   Error: ImagePullBackOff\nNormal BackOff 2s (x18 over 5m12s) kubelet   Back-off pulling image\n```\n- Identify the node running the pod:```\nkubectl describe pod POD_NAME | grep \"Node:\"\n```\n- Verify the node has the storage scope:```\ngcloud compute instances describe NODE_NAME \\\u00a0 \u00a0 --zone=COMPUTE_ZONE --format=\"flattened(serviceAccounts[].scopes)\"\n```The node's access scope should contain at least one of the following:```\nserviceAccounts[0].scopes[0]: https://www.googleapis.com/auth/devstorage.read_only\nserviceAccounts[0].scopes[0]: https://www.googleapis.com/auth/cloud-platform\n```\n- Recreate node pool the node belongs to with sufficient scope. You cannot modify existing nodes, you must recreate the node with the correct scope.- Recommended: create a new node pool with the `gke-default` scope:```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --scopes=\"gke-default\"\n```\n- Create a new node pool with only storage scope:```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --scopes=\"https://www.googleapis.com/auth/devstorage.read_only\"\n```\n### Pod unschedulable\n`PodUnschedulable` indicates that your Pod cannot be scheduled because of insufficient resources or some configuration error.\nIf you have [configured](/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics) your GKE cluster to send Kubernetes API server and Kubernetes scheduler metrics to Cloud Monitoring, you can find more information about these errors in [scheduler metrics](/stackdriver/docs/solutions/gke/control-plane-metrics#scheduler) and [API server metrics](/stackdriver/docs/solutions/gke/control-plane-metrics#api-server) .\nYou can troubleshoot `PodUnschedulable` errors using the Google Cloud console:\n- Go to the Unschedulable Pods Interactive Playbook: [Go to Playbook](https://console.cloud.google.com/monitoring/dashboards/gke-troubleshooting/unschedulable) \n- For filter_list **Cluster** , enter the name of the cluster you want to troubleshoot.\n- For filter_list **Namespace** , enter the namespace you want to troubleshoot.\n- (Optional) Create an alert to notify you of future `PodUnschedulable` errors:- In the **Future Mitigation Tips** section, select **Create an Alert** .\nYou might encounter an error indicating a lack of CPU, memory, or another resource. For example: \"No nodes are available that match all of the predicates: Insufficient cpu (2)\" which indicates that on two nodes there isn't enough CPU available to fulfill a Pod's requests.\nIf your Pod resource requests exceed that of a single node from any eligible node pools, GKE does not schedule the Pod and also does not trigger scale up to add a new node. For GKE to schedule the Pod, you must either request fewer resources for the Pod, or create a new node pool with sufficient resources.\nYou can also enable [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) so that GKE can automatically create node pools with nodes where the unscheduled Pods can run.\nThe default CPU request is 100m or 10% of a CPU (or [one core](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu) ). If you want to request more or fewer resources, specify the value in the Pod specification under `spec: containers: resources: requests` .\n**Note:** Your cluster runs system containers in the `kube-system` namespace. Those containers also use cluster resources.\n`MatchNodeSelector` indicates that there are no nodes that match the Pod's label selector.\nTo verify this, check the labels specified in the Pod specification's `nodeSelector` field, under `spec: nodeSelector` .\nTo see how nodes in your cluster are labelled, run the following command:\n```\nkubectl get nodes --show-labels\n```\nTo attach a label to a node, run the following command:\n```\nkubectl label nodes NODE_NAME LABEL_KEY=LABEL_VALUE\n```\nReplace the following:\n- ``: the desired node.\n- ``: the label's key.\n- ``: the label's value.\nFor more information, refer to [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/) .\n`PodToleratesNodeTaints` indicates that the Pod can't be scheduled to any node because no node currently tolerates its [node taint](/kubernetes-engine/docs/how-to/node-taints) .\nTo verify that this is the case, run the following command:\n```\nkubectl describe nodes NODE_NAME\n```\nIn the output, check the `Taints` field, which lists key-value pairs and scheduling effects.\nIf the effect listed is `NoSchedule` , then no Pod can be scheduled on that node unless it has a matching [toleration](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/) .\nOne way to resolve this issue is to remove the taint. For example, to remove a NoSchedule taint, run the following command:\n```\nkubectl taint nodes NODE_NAME key:NoSchedule```\n`PodFitsHostPorts` indicates that a port that a node is attempting to use is already in use.\nTo resolve this issue, check the Pod specification's `hostPort` value under `spec: containers: ports: hostPort` . You might need to change this value to another port.\nIf a node has adequate resources but you still see the `Does not have minimum availability` message, check the Pod's status. If the status is `SchedulingDisabled` or `Cordoned` status, the node cannot schedule new Pods. You can check the status of a node using the Google Cloud console or the `kubectl` command-line tool.\nTo get statuses of your nodes, run the following command:\n```\nkubectl get nodes\n```\nTo enable scheduling on the node, run:\n```\nkubectl uncordon NODE_NAME\n```\nPerform the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Select the desired cluster. The **Nodes** tab displays the Nodes and their status.\nTo enable scheduling on the Node, perform the following steps:- From the list, click the desired Node.\n- From the Node Details, click **Uncordon** button.If the [Maximum pods per node](/kubernetes-engine/docs/how-to/flexible-pod-cidr#configuring_maximum_pods_per_node) limit is reached by all nodes in the cluster, the Pods will be stuck in Unschedulable state. Under the Pod **Events** tab, you will see a message including the phrase `Too many pods` .\n- Check the `Maximum pods per node` configuration from the Nodes tab in GKE cluster details in the Google Cloud console.\n- Get a list of nodes:```\nkubectl get nodes\n```\n- For each node, verify the number of Pods running on the node:```\nkubectl get pods -o wide | grep NODE_NAME | wc -l\n```\n- If limit is reached, add a new node pool or add additional nodes to the existing node pool.If the node pool has reached its [maximumsize](/kubernetes-engine/docs/concepts/cluster-autoscaler#minimum_and_maximum_node_pool_size) according to its cluster autoscaler configuration, GKE does not trigger scale up for the Pod that would otherwise be scheduled with this node pool. If you want the Pod to be scheduled with this node pool, [change thecluster autoscalerconfiguration](/kubernetes-engine/docs/how-to/cluster-autoscaler) .\nIf the node pool has reached its maximum number of nodes, and cluster autoscaler is disabled, GKE cannot schedule the Pod with the node pool. [Increase the size of your nodepool](/kubernetes-engine/docs/how-to/resizing-a-cluster#increase) or [enableclusterautoscaler](/kubernetes-engine/docs/how-to/cluster-autoscaler#enable_autoscaling) for GKE to resize your cluster automatically.\n`Unbound PersistentVolumeClaims` indicates that the Pod references a PersistentVolumeClaim that is not bound. This error might happen if your PersistentVolume failed to provision. You can verify that provisioning failed by getting the events for your PersistentVolumeClaim and examining them for failures.\nTo get events, run the following command:\n```\nkubectl describe pvc STATEFULSET_NAME-PVC_NAME-0\n```\nReplace the following:\n- ``: the name of the StatefulSet object.\n- ``: the name of the PersistentVolumeClaim object.\nThis may also happen if there was a configuration error during your manual pre-provisioning of a PersistentVolume and its binding to a PersistentVolumeClaim. You can try to pre-provision the volume again.\nVerify that your project has sufficient Compute Engine quota for GKE to scale up your cluster. If GKE attempts to add a node to your cluster to schedule the Pod, and scaling up would exceed your project's available quota, you receive the `scale.up.error.quota.exceeded` error message.\nTo learn more, see [ScaleUp errors](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility#scaleup-errors) .\nEnsure that you are not using deprecated APIs that are removed with your cluster's minor version. To learn more, see [GKE deprecations](/kubernetes-engine/docs/deprecations) .\n### Connectivity issues\nAs mentioned in the [Network Overview](/kubernetes-engine/docs/concepts/network-overview#pods) discussion, it is important to understand how Pods are wired from their network namespaces to the root namespace on the node in order to troubleshoot effectively. For the following discussion, unless otherwise stated, assume that the cluster uses GKE's native CNI rather than Calico's. That is, no [network policy](/kubernetes-engine/docs/how-to/network-policy) has been applied.\nIf Pods on select nodes have no network connectivity, ensure that the Linux bridge is up:\n```\nip address show cbr0\n```\nIf the Linux bridge is down, raise it:\n```\nsudo ip link set cbr0 up\n```\nEnsure that the node is learning Pod MAC addresses attached to cbr0:\n```\narp -an\n```\nIf Pods on select nodes have minimal connectivity, you should first confirm whether there are any lost packets by running `tcpdump` in the toolbox container:\n```\nsudo toolbox bash\n```\nInstall `tcpdump` in the toolbox if you have not done so already:\n```\napt install -y tcpdump\n```\nRun `tcpdump` against cbr0:\n```\ntcpdump -ni cbr0 host HOSTNAME and port PORT_NUMBER and [TCP|UDP|ICMP]\n```\nShould it appear that large packets are being dropped downstream from the bridge (for example, the TCP handshake completes, but no SSL hellos are received), ensure that the MTU for each Linux Pod interface is correctly set to the MTU of the [cluster's VPC network](#gke-mtu) .\n```\nip address show cbr0\n```\nWhen overlays are used (for example, Weave or Flannel), this MTU must be further reduced to accommodate encapsulation overhead on the overlay.\nThe MTU selected for a Pod interface is dependent on the Container Network Interface (CNI) used by the cluster Nodes and the underlying VPC MTU setting. For more information, see [Pods](/kubernetes-engine/docs/concepts/network-overview#pods) .\nAutopilot is configured to always inherit the VPC MTU.\nThe Pod interface MTU value is either `1460` or inherited from the primary interface of the Node.\n| CNI          | MTU  | GKE Standard                              |\n|:----------------------------------------|:----------|:------------------------------------------------------------------------------------------------------------------------------------|\n| kubenet         | 1460  | Default                                |\n| kubenet (GKE version 1.26.1 and later) | Inherited | Default                                |\n| Calico         | 1460  | Enabled by using --enable-network-policy. For details, see Control communication between Pods and Services using network policies. |\n| netd         | Inherited | Enabled by using any of the following: Intranode visibility workload identity federation for GKE IPv4/IPv6 dual-stack networking |\n| GKE Dataplane V2      | Inherited | Enabled by using --enable-dataplane-v2. For details, see Using GKE Dataplane V2.             |\nConnections to and from the Pods are forwarded by iptables. Flows are tracked as entries in the conntrack table and, where there are many workloads per node, conntrack table exhaustion may manifest as a failure. These can be logged in the serial console of the node, for example:\n```\nnf_conntrack: table full, dropping packet\n```\nIf you are able to determine that intermittent issues are driven by conntrack exhaustion, you may increase the size of the cluster (thus reducing the number of workloads and flows per node), or increase `nf_conntrack_max` :\n```\nnew_ct_max=$(awk '$1 == \"MemTotal:\" { printf \"%d\\n\", $2/32; exit; }' /proc/meminfo)sysctl -w net.netfilter.nf_conntrack_max=\"${new_ct_max:?}\" \\\u00a0 && echo \"net.netfilter.nf_conntrack_max=${new_ct_max:?}\" >> /etc/sysctl.conf\n```\nYou can also use [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) to reduce connection tracking entries.\nA container in a Pod is unable to start because according to the container logs, the port where the application is trying to bind to is already reserved. The container is crash looping. For example, in Cloud Logging:\n```\nresource.type=\"container\"\ntextPayload:\"bind: Address already in use\"\nresource.labels.container_name=\"redis\"\n2018-10-16 07:06:47.000 CEST 16 Oct 05:06:47.533 # Creating Server TCP listening socket *:60250: bind: Address already in use\n2018-10-16 07:07:35.000 CEST 16 Oct 05:07:35.753 # Creating Server TCP listening socket *:60250: bind: Address already in use\n```\nWhen Docker crashes, sometimes a running container gets left behind and is stale. The process is still running in the network namespace allocated for the Pod, and listening on its port. Because Docker and the kubelet don't know about the stale container they try to start a new container with a new process, which is unable to bind on the port as it gets added to the network namespace already associated with the Pod.\nTo diagnose this problem:\n- You need the UUID of the Pod in the `.metadata.uuid` field:```\nkubectl get pod -o custom-columns=\"name:.metadata.name,UUID:.metadata.uid\" ubuntu-6948dd5657-4gsgg\nname      UUID\nubuntu-6948dd5657-4gsgg db9ed086-edba-11e8-bdd6-42010a800164\n```\n- Get the output of the following commands from the node:```\ndocker ps -aps -eo pid,ppid,stat,wchan:20,netns,comm,args:50,cgroup --cumulative -H | grep [Pod UUID]\n```\n- Check running processes from this Pod. Because the UUID of the cgroup namespaces contain the UUID of the Pod, you can grep for the Pod UUID in `ps` output. Grep also the line before, so you will have the `docker-containerd-shim` processes having the container id in the argument as well. Cut the rest of the cgroup column to get a simpler output:```\n# ps -eo pid,ppid,stat,wchan:20,netns,comm,args:50,cgroup --cumulative -H | grep -B 1 db9ed086-edba-11e8-bdd6-42010a800164 | sed s/'blkio:.*'/''/\n1283089  959 Sl futex_wait_queue_me 4026531993  docker-co  docker-containerd-shim 276e173b0846e24b704d4 12:\n1283107 1283089 Ss sys_pause   4026532393   pause   /pause          12:\n1283150  959 Sl futex_wait_queue_me 4026531993  docker-co  docker-containerd-shim ab4c7762f5abf40951770 12:\n1283169 1283150 Ss do_wait    4026532393   sh    /bin/sh -c echo hello && sleep 6000000  12:\n1283185 1283169 S hrtimer_nanosleep 4026532393   sleep   sleep 6000000       12:\n1283244  959 Sl futex_wait_queue_me 4026531993  docker-co  docker-containerd-shim 44e76e50e5ef4156fd5d3 12:\n1283263 1283244 Ss sigsuspend   4026532393   nginx   nginx: master process nginx -g daemon off; 12:\n1283282 1283263 S ep_poll    4026532393   nginx   nginx: worker process\n```\n- From this list, you can see the container ids, which should be visible in `docker ps` as well.In this case:- `docker-containerd-shim 276e173b0846e24b704d4`for pause\n- `docker-containerd-shim ab4c7762f5abf40951770`for sh with sleep (sleep-ctr)\n- `docker-containerd-shim 44e76e50e5ef4156fd5d3`for nginx (echoserver-ctr)\n- Check those in the `docker ps` output:```\n# docker ps --no-trunc | egrep '276e173b0846e24b704d4|ab4c7762f5abf40951770|44e76e50e5ef4156fd5d3'\n44e76e50e5ef4156fd5d383744fa6a5f14460582d0b16855177cbed89a3cbd1f gcr.io/google_containers/echoserver@sha256:3e7b182372b398d97b747bbe6cb7595e5ffaaae9a62506c725656966d36643cc     \"nginx -g 'daemon off;'\"                                                                                          14 hours ago  Up 14 hours        k8s_echoserver-cnt_ubuntu-6948dd5657-4gsgg_default_db9ed086-edba-11e8-bdd6-42010a800164_0\nab4c7762f5abf40951770d3e247fa2559a2d1f8c8834e5412bdcec7df37f8475 ubuntu@sha256:acd85db6e4b18aafa7fcde5480872909bd8e6d5fbd4e5e790ecc09acc06a8b78            \"/bin/sh -c 'echo hello && sleep 6000000'\"                                                                                     14 hours ago  Up 14 hours        k8s_sleep-cnt_ubuntu-6948dd5657-4gsgg_default_db9ed086-edba-11e8-bdd6-42010a800164_0\n276e173b0846e24b704d41cf4fbb950bfa5d0f59c304827349f4cf5091be3327 registry.k8s.io/pause-amd64:3.1\n```In normal cases, you see all container ids from `ps` showing up in `docker ps` . If there is one you don't see, it's a stale container, and probably you will see a child process of the `docker-containerd-shim process` listening on the TCP port that is reporting as already in use.To verify this, execute `netstat` in the container's network namespace. Get the pid of any container process (so NOT `docker-containerd-shim` ) for the Pod.From the above example:- 1283107 - pause\n- 1283169 - sh\n- 1283185 - sleep\n- 1283263 - nginx master\n- 1283282 - nginx worker\n```\n# nsenter -t 1283107 --net netstat -anp\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address   Foreign Address   State  PID/Program name\ntcp  0  0 0.0.0.0:8080   0.0.0.0:*    LISTEN  1283263/nginx: mast\nActive UNIX domain sockets (servers and established)\nProto RefCnt Flags  Type  State   I-Node PID/Program name  Path\nunix 3  [ ]   STREAM  CONNECTED  3097406 1283263/nginx: mast\nunix 3  [ ]   STREAM  CONNECTED  3097405 1283263/nginx: mast\ngke-zonal-110-default-pool-fe00befa-n2hx ~ # nsenter -t 1283169 --net netstat -anp\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address   Foreign Address   State  PID/Program name\ntcp  0  0 0.0.0.0:8080   0.0.0.0:*    LISTEN  1283263/nginx: mast\nActive UNIX domain sockets (servers and established)\nProto RefCnt Flags  Type  State   I-Node PID/Program name  Path\nunix 3  [ ]   STREAM  CONNECTED  3097406 1283263/nginx: mast\nunix 3  [ ]   STREAM  CONNECTED  3097405 1283263/nginx: mast\n```You can also execute `netstat` using `ip netns` , but you need to link the network namespace of the process manually, as Docker is not doing the link:```\n# ln -s /proc/1283169/ns/net /var/run/netns/1283169\ngke-zonal-110-default-pool-fe00befa-n2hx ~ # ip netns list\n1283169 (id: 2)\ngke-zonal-110-default-pool-fe00befa-n2hx ~ # ip netns exec 1283169 netstat -anp\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address   Foreign Address   State  PID/Program name\ntcp  0  0 0.0.0.0:8080   0.0.0.0:*    LISTEN  1283263/nginx: mast\nActive UNIX domain sockets (servers and established)\nProto RefCnt Flags  Type  State   I-Node PID/Program name  Path\nunix 3  [ ]   STREAM  CONNECTED  3097406 1283263/nginx: mast\nunix 3  [ ]   STREAM  CONNECTED  3097405 1283263/nginx: mast\ngke-zonal-110-default-pool-fe00befa-n2hx ~ # rm /var/run/netns/1283169\n```\nMitigation:\nThe short term mitigation is to identify stale processes by the method outlined above, and end the processes using the `kill [PID]` command.\nLong term mitigation involves identifying why Docker is crashing and fixing that. Possible reasons include:\n- Zombie processes piling up, so running out of PID namespaces\n- Bug in docker\n- Resource pressure / OOM\n### Error: \"failed to allocate for range 0: no IP addresses in range set\"\nGKE version 1.18.17 and later fixed an issue where out-of-memory (OOM) events would result in incorrect Pod eviction if the Pod was deleted before its containers were started. This incorrect eviction could result in orphaned pods that continued to have reserved IP addresses from the allocated node range. Over time, GKE ran out of IP addresses to allocate to new pods because of the build-up of orphaned pods. This led to the error message `failed to allocate for range 0: no IP addresses in range set` , because the allocated node range didn't have available IPs to assign to new pods.\nTo resolve this issue, [upgrade your cluster and node pools](/kubernetes-engine/docs/how-to/upgrading-a-cluster) to GKE version 1.18.17 or later.\nTo prevent this issue and resolve it on clusters with GKE versions prior to 1.18.17, [increase your resource limits](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/) to avoid OOM events in the future, and then reclaim the IP addresses by removing the orphaned pods.\nYou can also view [GKE IP address utilization insights](/network-intelligence-center/docs/network-analyzer/insights/kubernetes-engine/gke-ip-utilization) .\nYou can remove the orphaned pods by draining the node, [upgrading the node pool](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes) , or moving the affected directories.\n**Draining the node (recommended)**\n- Cordon the node to prevent new pods from scheduling on it:```\n\u00a0kubectl cordon NODE\n```Replace `` with the name of the node you want to drain.\n- Drain the node. GKE automatically reschedules pods managed by deployments onto other nodes. Use the `--force` flag to drain orphaned pods that don't have a managing resource.```\n\u00a0kubectl drain NODE --force\n```\n- Uncordon the node to allow GKE to schedule new pods on it:```\n\u00a0kubectl uncordon NODE\n```\n**Moving affected directories**\nYou can identify orphaned Pod directories in `/var/lib/kubelet/pods` and move them out of the main directory to allow GKE to terminate the pods.\n**Note:** Avoid **removing** the orphaned directories, as they might still contain mounted data other workloads use.\n## Troubleshooting issues with terminating resources\n### Namespace stuck in Terminating state\nNamespaces use Kubernetes [finalizers](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#finalizers) to prevent deletion when one or more resources within a namespace still exist. When you delete a namespace using the `kubectl delete` command, the namespace enters the `Terminating` state until Kubernetes deletes its dependent resources and clears all finalizers. The namespace lifecycle controller first lists all resources in the namespace that GKE needs to delete. If GKE can't delete a dependent resource, or if the namespace lifecycle controller can't verify that the namespace is empty, the namespace remains in the `Terminating` state until you resolve the issue.\nTo resolve a namespace stuck in the `Terminating` state, you need to identify and remove the unhealthy component(s) blocking the deletion. Try one of the following solutions.- List unavailable API services:```\nkubectl get apiservice | grep False\n```\n- Troubleshoot any unresponsive services:```\nkubectl describe apiservice API_SERVICE\n```Replace `` with the name of the unresponsive service.\n- Check if the namespace is still terminating:```\nkubectl get ns | grep Terminating\n```- List all the resources remaining in the terminating namespace:```\nkubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get -n NAMESPACE\n```Replace `` with the name of the namespace you want to delete.\n- Remove any resources displayed in the output.\n- Check if the namespace is still terminating:```\nkubectl get ns | grep Terminating\n```**Caution:** Use this as a last resort if the previous solutions don't work. Force deleting a namespace could result in resources remaining in your cluster that you can't access.\nYou can remove the finalizers blocking namespace deletion to force the namespace to terminate.\n- Save the namespace manifest as a YAML file:```\nkubectl get ns NAMESPACE -o yaml > ns-terminating.yml\n```\n- Open the manifest in a text editor and remove all values in the `spec.finalizers` field:```\nvi ns-terminating.yml\n```\n- Verify that the finalizers field is empty:```\ncat ns-terminating.yml\n```The output should look like the following:```\napiVersion: v1kind: Namespacemetadata:\u00a0 annotations:\u00a0 name: NAMESPACEspec:\u00a0 finalizers:status:\u00a0 phase: Terminating\n```\n- Start an HTTP proxy to access the Kubernetes API:```\nkubectl proxy\n```\n- Replace the namespace manifest using `curl` :```\ncurl -H \"Content-Type: application/yaml\" -X PUT --data-binary @ns-terminating.yml http://127.0.0.1:8001/api/v1/namespaces/NAMESPACE/finalize\n```\n- Check if the namespace is still terminating:```\nkubectl get ns | grep Terminating\n```## Troubleshooting Cloud NAT packet loss from a GKE cluster\nNode VMs in VPC-native [GKE private clusters](/kubernetes-engine/docs/concepts/private-cluster-concept) don't have external IP addresses and can't connect to the internet by themselves. You can use Cloud NAT to allocate the external IP addresses and ports that allow private clusters to make public connections.\nIf a node VM runs out of its allocation of external ports and IP addresses from Cloud NAT, packets will drop. To avoid this, you can reduce the outbound packet rate or increase the allocation of available Cloud NAT source IP addresses and ports. The following sections describe how to diagnose and troubleshoot packet loss from Cloud NAT in the context of GKE private clusters.\n### Diagnosing packet loss\nThis section explains how to log dropped packets using Cloud Logging, and diagnose the cause of dropped packets using Cloud Monitoring.\nYou can log dropped packets with the following query in Cloud Logging:\n```\nresource.type=\"nat_gateway\"\nresource.labels.region=REGION\nresource.labels.gateway_name=GATEWAY_NAME\njsonPayload.allocation_status=\"DROPPED\"\n```\n- ``: the name of the region that the cluster is in.\n- ``: the name of the Cloud NAT gateway.\nThis command returns a list of all packets dropped by a Cloud NAT gateway, but does not identify the cause.\nTo identify causes for dropped packets, query the [Metrics observer](https://console.cloud.google.com/monitoring/metrics-explorer) in Cloud Monitoring. Packets drop for one of three reasons:\n- [OUT_OF_RESOURCES](/nat/docs/troubleshooting#insufficient-ports) \n- [ENDPOINT_INDEPENDENT_CONFLICT](/nat/docs/troubleshooting#insufficient-ports) \n- [NAT_ALLOCATION_FAILED](/nat/docs/troubleshooting#allocate-more-IPs) \nTo identify packets dropped due to `OUT_OF_RESOURCES` or `ENDPOINT_ALLOCATION_FAILED` error codes, use the following query:\n```\nfetch nat_gateway\n metric 'router.googleapis.com/nat/dropped_sent_packets_count'\n filter (resource.gateway_name == NAT_NAME)\n align rate(1m)\n every 1m\n group_by [metric.reason],\n [value_dropped_sent_packets_count_aggregate:\n  aggregate(value.dropped_sent_packets_count)]\n```\nTo identify packets dropped due to the `NAT_ALLOCATION_FAILED` error code, use the following query:\n```\nfetch nat_gateway\n metric 'router.googleapis.com/nat/nat_allocation_failed'\n group_by 1m,\n [value_nat_allocation_failed_count_true:\n  count_true(value.nat_allocation_failed)]\n every 1m\n```\nIf the previous queries return empty results, and GKE Pods are unable to communicate to external IP addresses, troubleshoot your configuration:\n| 0                        | 1                                                                                                                                                                                                                                                                                                                                                                                |\n|:-------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Configuration                     | Troubleshooting                                                                                                                                                                                                                                                                                                                                                                            |\n| Cloud NAT configured to apply only to the subnet's primary IP address range.     | When Cloud NAT is configured only for the subnet's primary IP address range, packets sent from the cluster to external IP addresses must have a source node IP address. In this Cloud NAT configuration: Pods can send packets to external IP addresses if those external IP address destinations are subject to IP masquerading. When deploying the ip-masq-agent, verify that the nonMasqueradeCIDRs list doesn't contain the destination IP address and port. Packets sent to those destinations are first converted to source node IP addresses before being processed by Cloud NAT. To allow the Pods to connect to all external IP addresses with this Cloud NAT configuration, ensure the ip-masq-agent is deployed and that the nonMasqueradeCIDRs list contains only the node and Pod IP address ranges of the cluster. Packets sent to destinations outside of the cluster are first converted to source node IP addresses before being processed by Cloud NAT. To prevent Pods from sending packets to some external IP addresses, you need to explicitly block those addresses so they are not masqueraded. With the ip-masq-agent is deployed, add the external IP addresses you wish to block to the nonMasqueradeCIDRs list. Packets sent to those destinations leave the node with their original Pod IP address sources. The Pod IP addresses come from a secondary IP address range of the cluster's subnet. In this configuration, Cloud NAT won't operate on that secondary range. |\n| Cloud NAT configured to apply only to the subnet's secondary IP address range used for Pod IPs. | When Cloud NAT is configured only for the subnet's secondary IP address range used by the cluster's Pod IPs, packets sent from the cluster to external IP addresses must have a source Pod IP address. In this Cloud NAT configuration: Using an IP masqeurade agent causes packets to lose their source Pod IP address when processed by Cloud NAT. To keep the source Pod IP address, specify destination IP address ranges in a nonMasqueradeCIDRs list. With the ip-masq-agent deployed, any packets sent to destinations on the nonMasqueradeCIDRslist retain their source Pod IP addresses before being processed by Cloud NAT. To allow the Pods to connect to all external IP addresses with this Cloud NAT configuration, ensure the ip-masq-agent is deployed and that the nonMasqueradeCIDRs list is as large as possible (0.0.0.0/0 specifies all IP address destinations). Packets sent to all destinations retain source Pod IP addresses before being processed by Cloud NAT.                                                                                                                             |\n### Optimizations to avoid packet loss\nYou can stop packet loss by:\n- Configuring the Cloud NAT gateway to use [dynamic port allocation](/nat/docs/ports-and-addresses#dynamic-port) and [increase the maximum number of ports per VM](/nat/docs/set-up-network-address-translation#set-up-dynamic-port) .\n- [Increase the number of minimum ports per VM](/nat/docs/set-up-network-address-translation#specify_a_different_minimum_number_of_default_ports_per_vm_for_nat) if using [static port allocation](/nat/docs/ports-and-addresses#static-port) .When an application makes multiple outbound connections to the same destination IP address and port, it can quickly consume all connections Cloud NAT can make to that destination using the number of allocated NAT source addresses and source port tuples. In this scenario, reducing the application's outbound packet rate helps to reduce packet loss.\nFor details about the how Cloud NAT uses NAT source addresses and source ports to make connections, including limits on the number of simultaneous connections to a destination, refer to [Ports and connections](/nat/docs/ports-and-addresses#ports-and-connections) .\nReducing the rate of outbound connections from the application can help to mitigate packet loss. You can accomplish this by reusing open connections. Common methods of reusing connections include connection pooling, multiplexing connections using protocols such as [HTTP/2](https://datatracker.ietf.org/doc/html/rfc7540) , or establishing persistent connections reused for multiple requests. For more information, see [Ports and Connections](/nat/docs/ports-and-addresses#ports-and-connections) .\n## Node version not compatible with control plane version\nCheck what version of Kubernetes your cluster's control plane is running, and then check what version of Kubernetes your cluster's node pools are running. If any of the cluster's node pools are more than two minor versions older than the control plane, this might be causing issues with your cluster.\nPeriodically, the GKE team performs upgrades of the cluster control plane on your behalf. Control planes are upgraded to newer stable versions of Kubernetes. By default, a cluster's nodes have [auto-upgrade](/kubernetes-engine/docs/concepts/cluster-upgrades#node_pool_upgrades) enabled, and it is recommended that you do not [disable it](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) .\nIf auto-upgrade is disabled for a cluster's nodes, and you do not [manuallyupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_nodes) your node pool version to a version that is compatible with the control plane, your control plane will eventually become incompatible with your nodes as the control plane is automatically upgraded over time. Incompatibility between your cluster's control plane and the nodes can cause unexpected issues.\nThe [Kubernetes version and version skew support policy](https://kubernetes.io/docs/setup/release/version-skew-policy/) guarantees that control planes are compatible with nodes up to two minor versions older than the control plane. For example, Kubernetes 1.19 control planes are compatible with Kubernetes 1.19, 1.18, and 1.17 nodes. To resolve this issue, manually upgrade the node pool version to a version that is compatible with the control plane.\nIf you are concerned about the upgrade process causing disruption to workloads running on the affected nodes, do the following steps to migrate your workloads to a new node pool:\n- [Create a new node pool](/kubernetes-engine/docs/how-to/node-pools#add) with a compatible version.\n- [Cordon](https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration) the nodes of the existing node pool.\n- Optionally, update your workloads running on the existing node pool to add a nodeSelector for the label`cloud.google.com/gke-nodepool:NEW_NODE_POOL_NAME`, where``is the name of the new node pool. This ensures that GKE places those workloads on nodes in the new node pool.\n- [Drain](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/) the existing node pool.\n- Check that the workloads are running successfully in the new node pool. If they are, you can delete the old node pool. If you notice workload disruptions, reschedule the workloads on the existing nodes by uncordoning the nodes in the existing node pool and draining the new nodes. Troubleshoot the issue and try again.## Metrics from your cluster aren't showing up in Cloud Monitoring\nEnsure that you have activated the [Cloud Monitoring API](https://console.cloud.google.com/apis/api/monitoring/overview) and the [Cloud Logging API](https://console.cloud.google.com/apis/api/logging/overview) on your project, and that you are able to view your project in [Cloud Monitoring](https://console.cloud.google.com/monitoring) .\nIf the issue persists, check the following potential causes:\n- Ensure that you have enabled monitoring on your cluster.Monitoring is enabled by default for clusters created from the Google Cloud console and from the Google Cloud CLI, but you can verify by running the following command or clicking into the cluster's details in [the Google Cloud console](https://console.cloud.google.com/) :```\ngcloud container clusters describe CLUSTER_NAME\n```The output from this command should include `SYSTEM_COMPONENTS` in the list of `enableComponents` in the `monitoringConfig` section similar to this:```\nmonitoringConfig:\u00a0 componentConfig:\u00a0 \u00a0 enableComponents:\u00a0 \u00a0 - SYSTEM_COMPONENTS\n```If monitoring is not enabled, run the following command to enable it:```\ngcloud container clusters update CLUSTER_NAME --monitoring=SYSTEM\n```\n- How long has it been since your cluster was created or had monitoring enabled?It can take up to an hour for a new cluster's metrics to start appearing in Cloud Monitoring.\n- Is a `heapster` or `gke-metrics-agent` (the OpenTelemetry Collector) running in your cluster in the \"kube-system\" namespace?This pod might be failing to schedule workloads because your cluster is running low on resources. Check whether Heapster or OpenTelemetry is running by calling `kubectl get pods --namespace=kube-system` and checking for pods with `heapster` or `gke-metrics-agent` in the name.\n- Is your cluster's control plane able to communicate with the nodes?Cloud Monitoring relies on that. You can check whether this is the case by running the following command:```\nkubectl logs POD_NAME\n```If this command returns an error, then the SSH tunnels may be causing the issue. See [this section](/kubernetes-engine/docs/troubleshooting#kubect_commands_stops) for further information.\nIf you are having an issue related to the Cloud Logging agent, see its [troubleshooting documentation](/logging/docs/agent/troubleshooting) .\nFor more information, refer to the [Logging documentation](/logging/docs) .\n## Missing permissions on account for Shared VPC clusters\nFor Shared VPC clusters, ensure that the service project's GKE service account has a binding for the [Host Service Agent User](/kubernetes-engine/docs/how-to/iam#host_service_agent_user) role on the host project. You can do this using the gcloud CLI.\nTo check if the role binding exists, run the following command in your host project:\n```\ngcloud projects get-iam-policy PROJECT_ID \\\u00a0 --flatten=\"bindings[].members\" \\\u00a0 --format='table(bindings.role)' \\\u00a0 --filter=\"bindings.members:SERVICE_ACCOUNT_NAME\n```\nReplace the following:\n- ``: your host project ID.\n- ``: the GKE service account name.\nIn the output, look for the `roles/container.hostServiceAgentUser` role:\n```\nROLE\n...\nroles/container.hostServiceAgentUser\n...\n```\nIf the `hostServiceAgentUser` role isn't in the list, follow the instructions in [Granting the Host Service Agent User role](/kubernetes-engine/docs/how-to/cluster-shared-vpc#grant_host_service_agent_role) to add the binding to the service account.\n## Restore default service account to your Google Cloud project\nGKE's default service account, `container-engine-robot` , can accidentally become unbound from a project. is an [Identity and Access Management (IAM) role](/iam/docs/understanding-roles) that grants the service account the permissions to manage cluster resources. If you remove this role binding from the service account, the default service account becomes unbound from the project, which can prevent you from deploying applications and performing other cluster operations.\nYou can check to see if the service account has been removed from your project using gcloud CLI or the Google Cloud console.\nRun the following command:\n```\ngcloud projects get-iam-policy PROJECT_ID\n```\nReplace `` with your project ID.\nVisit the [IAM & Admin](https://console.cloud.google.com/iam-admin/iam) page in the Google Cloud console.\nIf the command or the dashboard do not display `container-engine-robot` among your service accounts, the service account has become unbound.\nIf you removed the GKE Service Agent role binding, run the following commands to restore the role binding:\n```\nPROJECT_NUMBER=$(gcloud projects describe \"PROJECT_ID\" --format 'get(projectNumber)')gcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 --member \"serviceAccount:service-${PROJECT_NUMBER?}@container-engine-robot.iam.gserviceaccount.com\" \\\u00a0 --role roles/container.serviceAgent\n```\nTo confirm that the role binding was granted:\n```\ngcloud projects get-iam-policy $PROJECT_ID\n```\nIf you see the service account name along with the `container.serviceAgent` role, the role binding has been granted. For example:\n```\n- members:\n - serviceAccount:service-1234567890@container-engine-robot.iam.gserviceaccount.com\n role: roles/container.serviceAgent\n```\n## Enable Compute Engine default service account\nYour nodes might fail to register with the cluster if the service account used for the node pool is disabled, which usually is the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) .\nYou can verify if the service account has been disabled in your project using gcloud CLI or the Google Cloud console.\nRun the following command:\n```\ngcloud iam service-accounts list \u00a0--filter=\"NAME~'compute' AND disabled=true\"\n```\nGo to the [IAM & Admin](https://console.cloud.google.com/iam-admin/serviceaccounts) page in the Google Cloud console.\nIf the command or the dashboard shows the service account is disabled, run the following command to enable the service account:\n```\ngcloud iam service-accounts enable PROJECT_ID-compute@developer.gserviceaccount.com\n```\nReplace `` with your project ID.\nIf this doesn't solve your node registration issues, refer to [Troubleshoot node registration](/kubernetes-engine/docs/troubleshooting/troubleshoot-node-registration) for further troubleshooting instructions.\n## Pods stuck in pending state after enabling Node Allocatable\nIf you are experiencing an issue with Pods stuck in state after enabling [Node Allocatable](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources) , please note the following:\nStarting with version 1.7.6, GKE reserves CPU and memory for Kubernetes overhead, including Docker and the operating system. See [Cluster architecture](/kubernetes-engine/docs/concepts/cluster-architecture) for information on how much of each machine type can be scheduled by Pods.\nIf Pods are pending after an upgrade, we suggest the following:\n- Ensure CPU and Memory requests for your Pods do not exceed their peak usage. With GKE reserving CPU and memory for overhead, Pods cannot request these resources. Pods that request more CPU or memory than they use prevent other Pods from requesting these resources, and might leave the cluster underutilized. For more information, see [How Pods with resource requests are scheduled](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-requests-are-scheduled) .\n- Consider resizing your cluster. For instructions, see [Resizing a cluster](/kubernetes-engine/docs/resize-cluster) .\n- Revert this change by downgrading your cluster. For instructions, see [Manually upgrading a cluster or node pool](/kubernetes-engine/docs/clusters/upgrade) .\n- Configure your cluster to [send Kubernetes scheduler metrics to Cloud Monitoring](/stackdriver/docs/solutions/gke/managing-metrics#enable-control-plane-metrics) and view [scheduler metrics](/stackdriver/docs/solutions/gke/control-plane-metrics#scheduler) .## Cluster's root Certificate Authority is expiring soon\nYour cluster's root Certificate Authority is expiring soon. To prevent normal cluster operations from being interrupted, you must perform a [credentialrotation](/kubernetes-engine/docs/how-to/credential-rotation) .\n## Seeing error \"Instance 'Foo' does not contain 'instance-template' metadata\"\nYou may see an error \"Instance 'Foo' does not contain 'instance-template' metadata\" as a status of a node pool that fails to upgrade, scale, or perform automatic node repair.\nThis message indicates that the metadata of VM instances, allocated by GKE, was corrupted. This typically happens when custom-authored automation or scripts attempt to add new instance metadata (like [block-project-ssh-keys](/compute/docs/connect/restrict-ssh-keys#block-keys) ), and instead of just adding or updating values, it also deletes existing metadata. You can read about VM instance metadata in [Setting custom metadata](/compute/docs/storing-retrieving-metadata#custom) .\nIn case any of the critical metadata values (among others: `instance-template` , `kube-labels` , `kubelet-config` , `kubeconfig` , `cluster-name` , `configure-sh` , `cluster-uid` ) were deleted, the node or entire node pool might render itself into an unstable state as these values are crucial for GKE operations.\nIf the instance metadata was corrupted, the best way to recover the metadata is to re-create the node pool that contains the corrupted VM instances. You will need to [add a node pool](/kubernetes-engine/docs/how-to/node-pools#add) to your cluster and increase the node count on the new node pool, while cordoning and removing nodes on another. See the instructions to [migrate workloads betweennodepools](/kubernetes-engine/docs/troubleshooting/troubleshoot-node-pools#migrate-node-pools) .\nTo find who and when instance metadata was edited, you can review [Compute Engine audit logging information](/compute/docs/logging/audit-logging) or find logs using [Logs Explorer](/logging/docs/view/logs-explorer-interface) with the search query similar to this:\n```\nresource.type=\"gce_instance_group_manager\"\nprotoPayload.methodName=\"v1.compute.instanceGroupManagers.setInstanceTemplate\"\n```\nIn the logs you may find the request originator IP address and user agent:\n```\nrequestMetadata: {\u00a0 callerIp: \"REDACTED\"\u00a0 callerSuppliedUserAgent: \"google-api-go-client/0.5 GoogleContainerEngine/v1\"}\n```\n## Cloud KMS key is disabled.\nThe following error message occurs if GKE's default service account cannot access the Cloud KMS key.\n```\nCluster problem detected (Kubernetes Engine Service Agent account unable to use CloudKMS key configured for Application Level encryption).\n```\nTo resolve this issue, [re-enable the disabled key](/kms/docs/enable-disable#enable) .\nFor more information about secrets in GKE, see [Encrypt secrets at the application layer](/kubernetes-engine/docs/how-to/encrypting-secrets) .", "guide": "Google Kubernetes Engine (GKE)"}